{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, Conv2DTranspose, PReLU\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Concatenate\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, HTML\n",
    "import glob\n",
    "from keras.layers import AveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Add\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import Input\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install scikit-image\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Reshape, UpSampling2D, \\\n",
    "    BatchNormalization, Activation, Input, LeakyReLU, ZeroPadding2D, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "#import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('emnist-letters-train.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {1: 0, \n",
    "           2: 1, \n",
    "           3: 2, \n",
    "           4: 3, \n",
    "           5: 4, \n",
    "           6: 5, \n",
    "           7: 6, \n",
    "           8: 7, \n",
    "           9: 8, \n",
    "           10: 9, \n",
    "           11: 10, \n",
    "           12: 11, \n",
    "           13: 12, \n",
    "           14: 13, \n",
    "           15: 14, \n",
    "           16: 15, \n",
    "           17: 16, \n",
    "           18: 17, \n",
    "           19: 18, \n",
    "           20: 19, \n",
    "           21: 20, \n",
    "           22: 21, \n",
    "           23: 22, \n",
    "           24: 23, \n",
    "           25: 24, \n",
    "           26: 25, \n",
    "           27: 26}\n",
    "\n",
    "        # Map the labels column to its corresponding value\n",
    "df[0] = df[0].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = np.array(df.iloc[:,0].values)\n",
    "y_pre = pd.Categorical(y_pre)\n",
    "X = np.array(df.iloc[:,1:].values)\n",
    "X = X.reshape(-1,28,28,1)\n",
    "preprocessed = []\n",
    "for image in X:\n",
    "    rotated_image = rotate(image, 90, reshape=False)\n",
    "    flipped_image = np.flipud(rotated_image)\n",
    "    preprocessed.append(flipped_image)\n",
    "X_pre = np.array(preprocessed)\n",
    "X = X_pre\n",
    "X = X.astype('float32')\n",
    "X_pre = (X - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pre\n",
      "[22, 6, 15, 14, 16, ..., 19, 8, 5, 11, 0]\n",
      "Length: 26\n",
      "Categories (26, int64): [0, 1, 2, 3, ..., 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f'y_pre\\n{y_pre.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z = 100):\n",
    "        # Input shape\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        optimizer = Adam(0.00002, 0.5)\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'])\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        # The discriminator takes generated images as input and\n",
    "        # determines validity\n",
    "        valid = self.discriminator(img)\n",
    "        # The combined model (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim, kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(32, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        \n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        # gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        os.makedirs('generated_mnist', exist_ok=True)\n",
    "        fig.savefig(\"generated_mnist/dcgan_mnist_improved_{:d}.png\".format(epoch))\n",
    "        plt.close()\n",
    "        \n",
    "    def train(self, epochs, batch_size=1024, save_interval=1, gen_steps=2):\n",
    "        # Load the dataset\n",
    "        X_train = X_pre\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "\n",
    "                # ---------------------\n",
    "                # Train Discriminator\n",
    "                # ---------------------\n",
    "                # Select a random half of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "                # Train the discriminator (real classified as ones\n",
    "                # and generated as zeros)\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                # ---------------------\n",
    "                # Train Generator\n",
    "                # ---------------------\n",
    "                # Train the generator (wants discriminator to mistake\n",
    "                # images as real)\n",
    "                # Sample noise and generate a batch of new images\n",
    "\n",
    "                for _ in range(1):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    g_loss = self.combined.train_on_batch(noise, valid)\n",
    "                    \n",
    "                # Plot the progress\n",
    "                print (\"Epoch: %d/%d  Batch Size: %d/%d [loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,epochs,batch,batches_per_epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_172 (Conv2D)         (None, 14, 14, 16)        160       \n",
      "                                                                 \n",
      " leaky_re_lu_155 (LeakyReLU)  (None, 14, 14, 16)       0         \n",
      "                                                                 \n",
      " dropout_85 (Dropout)        (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " conv2d_173 (Conv2D)         (None, 7, 7, 32)          4640      \n",
      "                                                                 \n",
      " leaky_re_lu_156 (LeakyReLU)  (None, 7, 7, 32)         0         \n",
      "                                                                 \n",
      " dropout_86 (Dropout)        (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " conv2d_174 (Conv2D)         (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " zero_padding2d_20 (ZeroPadd  (None, 5, 5, 64)         0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " batch_normalization_132 (Ba  (None, 5, 5, 64)         256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_157 (LeakyReLU)  (None, 5, 5, 64)         0         \n",
      "                                                                 \n",
      " dropout_87 (Dropout)        (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " conv2d_175 (Conv2D)         (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_133 (Ba  (None, 3, 3, 128)        512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_158 (LeakyReLU)  (None, 3, 3, 128)        0         \n",
      "                                                                 \n",
      " dropout_88 (Dropout)        (None, 3, 3, 128)         0         \n",
      "                                                                 \n",
      " conv2d_176 (Conv2D)         (None, 2, 2, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_134 (Ba  (None, 2, 2, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_159 (LeakyReLU)  (None, 2, 2, 256)        0         \n",
      "                                                                 \n",
      " dropout_89 (Dropout)        (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " flatten_20 (Flatten)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 395,137\n",
      "Trainable params: 394,241\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_41 (Dense)            (None, 6272)              633472    \n",
      "                                                                 \n",
      " reshape_20 (Reshape)        (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_40 (UpSamplin  (None, 14, 14, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_177 (Conv2D)         (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_135 (Ba  (None, 14, 14, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_160 (LeakyReLU)  (None, 14, 14, 128)      0         \n",
      "                                                                 \n",
      " up_sampling2d_41 (UpSamplin  (None, 28, 28, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_178 (Conv2D)         (None, 28, 28, 64)        73792     \n",
      "                                                                 \n",
      " batch_normalization_136 (Ba  (None, 28, 28, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_161 (LeakyReLU)  (None, 28, 28, 64)       0         \n",
      "                                                                 \n",
      " conv2d_179 (Conv2D)         (None, 28, 28, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_137 (Ba  (None, 28, 28, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_162 (LeakyReLU)  (None, 28, 28, 32)       0         \n",
      "                                                                 \n",
      " conv2d_180 (Conv2D)         (None, 28, 28, 16)        4624      \n",
      "                                                                 \n",
      " batch_normalization_138 (Ba  (None, 28, 28, 16)       64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_163 (LeakyReLU)  (None, 28, 28, 16)       0         \n",
      "                                                                 \n",
      " conv2d_181 (Conv2D)         (None, 28, 28, 1)         145       \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 879,041\n",
      "Trainable params: 878,561\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 0/86 [loss: 0.803431, acc.: 34.08%] [G loss: 0.690364]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 1/86 [loss: 0.663205, acc.: 60.94%] [G loss: 0.688100]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 2/86 [loss: 0.577486, acc.: 72.17%] [G loss: 0.688202]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 3/86 [loss: 0.546716, acc.: 73.68%] [G loss: 0.682374]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 4/86 [loss: 0.536550, acc.: 73.97%] [G loss: 0.680908]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 5/86 [loss: 0.525882, acc.: 74.02%] [G loss: 0.674790]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 6/86 [loss: 0.549509, acc.: 72.90%] [G loss: 0.671473]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 7/86 [loss: 0.571783, acc.: 69.97%] [G loss: 0.659882]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 8/86 [loss: 0.604166, acc.: 68.12%] [G loss: 0.652819]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 9/86 [loss: 0.634024, acc.: 66.36%] [G loss: 0.637206]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 10/86 [loss: 0.695674, acc.: 60.40%] [G loss: 0.619212]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 11/86 [loss: 0.721561, acc.: 58.35%] [G loss: 0.610563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 12/86 [loss: 0.729289, acc.: 58.54%] [G loss: 0.604084]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 13/86 [loss: 0.754496, acc.: 55.76%] [G loss: 0.615665]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 14/86 [loss: 0.778124, acc.: 53.42%] [G loss: 0.604272]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 15/86 [loss: 0.743769, acc.: 54.93%] [G loss: 0.632415]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 16/86 [loss: 0.796703, acc.: 52.98%] [G loss: 0.642546]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 17/86 [loss: 0.770364, acc.: 53.86%] [G loss: 0.647380]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 18/86 [loss: 0.778289, acc.: 52.98%] [G loss: 0.675541]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 19/86 [loss: 0.765027, acc.: 53.08%] [G loss: 0.674687]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 20/86 [loss: 0.772687, acc.: 52.73%] [G loss: 0.677155]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 21/86 [loss: 0.756195, acc.: 54.39%] [G loss: 0.672441]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 22/86 [loss: 0.759540, acc.: 54.15%] [G loss: 0.685146]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 23/86 [loss: 0.764873, acc.: 52.69%] [G loss: 0.701905]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 24/86 [loss: 0.743190, acc.: 54.20%] [G loss: 0.709517]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 25/86 [loss: 0.762946, acc.: 53.42%] [G loss: 0.709232]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 26/86 [loss: 0.741424, acc.: 54.79%] [G loss: 0.715486]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 27/86 [loss: 0.775566, acc.: 52.34%] [G loss: 0.708809]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 28/86 [loss: 0.744614, acc.: 55.27%] [G loss: 0.714938]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 29/86 [loss: 0.761749, acc.: 54.25%] [G loss: 0.717537]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 30/86 [loss: 0.748256, acc.: 53.17%] [G loss: 0.705636]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 31/86 [loss: 0.751054, acc.: 52.54%] [G loss: 0.703651]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 32/86 [loss: 0.736895, acc.: 55.37%] [G loss: 0.699193]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 33/86 [loss: 0.744881, acc.: 53.61%] [G loss: 0.704987]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 34/86 [loss: 0.747829, acc.: 53.22%] [G loss: 0.694391]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 35/86 [loss: 0.737263, acc.: 56.40%] [G loss: 0.693066]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 36/86 [loss: 0.715088, acc.: 56.64%] [G loss: 0.699684]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 37/86 [loss: 0.742772, acc.: 54.54%] [G loss: 0.680796]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 38/86 [loss: 0.723820, acc.: 56.45%] [G loss: 0.700758]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 39/86 [loss: 0.717143, acc.: 57.76%] [G loss: 0.690065]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 40/86 [loss: 0.723971, acc.: 56.15%] [G loss: 0.685280]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 41/86 [loss: 0.706343, acc.: 57.67%] [G loss: 0.708131]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 42/86 [loss: 0.716187, acc.: 56.93%] [G loss: 0.707829]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 43/86 [loss: 0.704423, acc.: 57.67%] [G loss: 0.685748]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 44/86 [loss: 0.689384, acc.: 59.28%] [G loss: 0.685225]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 45/86 [loss: 0.668426, acc.: 61.82%] [G loss: 0.708528]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 46/86 [loss: 0.696615, acc.: 58.45%] [G loss: 0.702761]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 47/86 [loss: 0.687837, acc.: 59.28%] [G loss: 0.705907]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 48/86 [loss: 0.685069, acc.: 59.28%] [G loss: 0.706944]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 49/86 [loss: 0.673126, acc.: 61.52%] [G loss: 0.704693]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 50/86 [loss: 0.685715, acc.: 60.16%] [G loss: 0.699089]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 51/86 [loss: 0.672314, acc.: 62.01%] [G loss: 0.705927]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 52/86 [loss: 0.652720, acc.: 63.62%] [G loss: 0.716765]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 53/86 [loss: 0.667245, acc.: 61.43%] [G loss: 0.699448]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 54/86 [loss: 0.660448, acc.: 62.01%] [G loss: 0.709484]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 55/86 [loss: 0.652636, acc.: 62.89%] [G loss: 0.709600]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 56/86 [loss: 0.636535, acc.: 62.99%] [G loss: 0.715019]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 57/86 [loss: 0.626767, acc.: 66.85%] [G loss: 0.716788]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 58/86 [loss: 0.651101, acc.: 62.35%] [G loss: 0.703378]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 59/86 [loss: 0.643923, acc.: 63.77%] [G loss: 0.698837]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 60/86 [loss: 0.639886, acc.: 64.36%] [G loss: 0.691515]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 61/86 [loss: 0.641668, acc.: 63.18%] [G loss: 0.700621]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 62/86 [loss: 0.628880, acc.: 65.19%] [G loss: 0.713505]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 63/86 [loss: 0.633467, acc.: 65.23%] [G loss: 0.708870]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 64/86 [loss: 0.649389, acc.: 63.23%] [G loss: 0.694520]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 65/86 [loss: 0.633519, acc.: 63.67%] [G loss: 0.709880]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 66/86 [loss: 0.628817, acc.: 64.55%] [G loss: 0.697307]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 67/86 [loss: 0.660242, acc.: 61.04%] [G loss: 0.699364]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 68/86 [loss: 0.640759, acc.: 63.38%] [G loss: 0.685279]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 69/86 [loss: 0.644921, acc.: 63.13%] [G loss: 0.687998]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 70/86 [loss: 0.643849, acc.: 63.77%] [G loss: 0.703471]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 71/86 [loss: 0.644779, acc.: 64.31%] [G loss: 0.693419]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 72/86 [loss: 0.671960, acc.: 61.13%] [G loss: 0.693092]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 73/86 [loss: 0.645186, acc.: 62.11%] [G loss: 0.701998]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 74/86 [loss: 0.658630, acc.: 61.87%] [G loss: 0.683788]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 75/86 [loss: 0.671030, acc.: 61.23%] [G loss: 0.679798]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 76/86 [loss: 0.682573, acc.: 58.54%] [G loss: 0.686744]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 77/86 [loss: 0.679471, acc.: 60.69%] [G loss: 0.674435]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 78/86 [loss: 0.697299, acc.: 57.91%] [G loss: 0.667610]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 79/86 [loss: 0.684781, acc.: 59.38%] [G loss: 0.672532]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 80/86 [loss: 0.682398, acc.: 58.98%] [G loss: 0.680197]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 81/86 [loss: 0.679557, acc.: 59.67%] [G loss: 0.678058]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 82/86 [loss: 0.708071, acc.: 58.06%] [G loss: 0.666944]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 83/86 [loss: 0.699678, acc.: 58.74%] [G loss: 0.666610]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 84/86 [loss: 0.698560, acc.: 58.45%] [G loss: 0.660249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 85/86 [loss: 0.698159, acc.: 57.52%] [G loss: 0.678342]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 0/86 [loss: 0.708103, acc.: 58.45%] [G loss: 0.650205]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 1/86 [loss: 0.711444, acc.: 57.32%] [G loss: 0.645225]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 2/86 [loss: 0.704900, acc.: 57.13%] [G loss: 0.656098]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 3/86 [loss: 0.720425, acc.: 56.84%] [G loss: 0.661668]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 4/86 [loss: 0.735423, acc.: 55.03%] [G loss: 0.651434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 5/86 [loss: 0.730087, acc.: 54.83%] [G loss: 0.659293]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 6/86 [loss: 0.725775, acc.: 54.59%] [G loss: 0.664023]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 7/86 [loss: 0.725367, acc.: 56.01%] [G loss: 0.659504]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 8/86 [loss: 0.741724, acc.: 54.98%] [G loss: 0.672110]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 9/86 [loss: 0.738863, acc.: 53.03%] [G loss: 0.669123]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 10/86 [loss: 0.749098, acc.: 52.64%] [G loss: 0.671198]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 11/86 [loss: 0.739058, acc.: 55.57%] [G loss: 0.660285]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 12/86 [loss: 0.742294, acc.: 54.88%] [G loss: 0.664314]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 13/86 [loss: 0.737833, acc.: 53.17%] [G loss: 0.662698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 14/86 [loss: 0.755595, acc.: 51.22%] [G loss: 0.670982]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 15/86 [loss: 0.726944, acc.: 56.45%] [G loss: 0.672863]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 16/86 [loss: 0.731191, acc.: 54.25%] [G loss: 0.670073]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 17/86 [loss: 0.725318, acc.: 54.93%] [G loss: 0.677029]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 18/86 [loss: 0.717303, acc.: 56.40%] [G loss: 0.687905]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 19/86 [loss: 0.733504, acc.: 54.93%] [G loss: 0.679360]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 20/86 [loss: 0.734262, acc.: 55.03%] [G loss: 0.690900]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 21/86 [loss: 0.697555, acc.: 57.71%] [G loss: 0.706069]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 22/86 [loss: 0.707309, acc.: 55.81%] [G loss: 0.692600]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 23/86 [loss: 0.711949, acc.: 56.15%] [G loss: 0.696497]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 24/86 [loss: 0.698418, acc.: 56.93%] [G loss: 0.700736]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 25/86 [loss: 0.687242, acc.: 59.42%] [G loss: 0.712859]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 26/86 [loss: 0.703954, acc.: 58.40%] [G loss: 0.708716]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 27/86 [loss: 0.686960, acc.: 58.94%] [G loss: 0.709269]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 28/86 [loss: 0.659652, acc.: 62.30%] [G loss: 0.706719]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 29/86 [loss: 0.675356, acc.: 60.35%] [G loss: 0.712354]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 30/86 [loss: 0.672740, acc.: 60.69%] [G loss: 0.707637]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 31/86 [loss: 0.666809, acc.: 60.64%] [G loss: 0.700767]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 32/86 [loss: 0.646157, acc.: 62.21%] [G loss: 0.718765]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 33/86 [loss: 0.664988, acc.: 60.50%] [G loss: 0.710793]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 34/86 [loss: 0.675466, acc.: 59.72%] [G loss: 0.712252]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 35/86 [loss: 0.650424, acc.: 61.47%] [G loss: 0.729903]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 36/86 [loss: 0.654785, acc.: 62.79%] [G loss: 0.731947]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 37/86 [loss: 0.654321, acc.: 62.65%] [G loss: 0.706111]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 38/86 [loss: 0.645683, acc.: 63.77%] [G loss: 0.716580]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 39/86 [loss: 0.653736, acc.: 62.40%] [G loss: 0.708590]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 40/86 [loss: 0.631697, acc.: 65.04%] [G loss: 0.721587]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 41/86 [loss: 0.647757, acc.: 62.35%] [G loss: 0.717528]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 42/86 [loss: 0.642771, acc.: 63.13%] [G loss: 0.701051]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 43/86 [loss: 0.634950, acc.: 64.84%] [G loss: 0.716780]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 44/86 [loss: 0.642349, acc.: 62.70%] [G loss: 0.716125]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 45/86 [loss: 0.629583, acc.: 64.99%] [G loss: 0.708783]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 46/86 [loss: 0.659604, acc.: 61.52%] [G loss: 0.711252]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 47/86 [loss: 0.640941, acc.: 63.33%] [G loss: 0.715712]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 48/86 [loss: 0.649848, acc.: 63.28%] [G loss: 0.714134]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 49/86 [loss: 0.658691, acc.: 61.52%] [G loss: 0.714933]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 50/86 [loss: 0.647676, acc.: 62.26%] [G loss: 0.723914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 51/86 [loss: 0.640240, acc.: 64.16%] [G loss: 0.725502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 52/86 [loss: 0.642999, acc.: 63.48%] [G loss: 0.714340]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 53/86 [loss: 0.630703, acc.: 63.77%] [G loss: 0.721226]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 54/86 [loss: 0.627983, acc.: 64.79%] [G loss: 0.718479]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 55/86 [loss: 0.641702, acc.: 63.23%] [G loss: 0.717652]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 56/86 [loss: 0.642262, acc.: 64.26%] [G loss: 0.718956]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 57/86 [loss: 0.645598, acc.: 62.60%] [G loss: 0.714889]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 58/86 [loss: 0.654072, acc.: 61.67%] [G loss: 0.710791]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 59/86 [loss: 0.631336, acc.: 64.65%] [G loss: 0.712952]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 60/86 [loss: 0.648020, acc.: 63.67%] [G loss: 0.715214]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 61/86 [loss: 0.654679, acc.: 63.13%] [G loss: 0.695438]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 62/86 [loss: 0.652676, acc.: 62.65%] [G loss: 0.712710]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 63/86 [loss: 0.638099, acc.: 63.43%] [G loss: 0.698668]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 64/86 [loss: 0.642275, acc.: 63.82%] [G loss: 0.710519]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 65/86 [loss: 0.650815, acc.: 63.57%] [G loss: 0.721379]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 66/86 [loss: 0.647692, acc.: 62.06%] [G loss: 0.708632]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 67/86 [loss: 0.636408, acc.: 63.72%] [G loss: 0.699396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 68/86 [loss: 0.638670, acc.: 64.06%] [G loss: 0.702711]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 69/86 [loss: 0.650882, acc.: 62.65%] [G loss: 0.705531]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 70/86 [loss: 0.649728, acc.: 63.57%] [G loss: 0.714360]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 71/86 [loss: 0.644676, acc.: 62.40%] [G loss: 0.714557]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 72/86 [loss: 0.645210, acc.: 63.33%] [G loss: 0.706297]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 73/86 [loss: 0.649647, acc.: 61.43%] [G loss: 0.730265]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 74/86 [loss: 0.661275, acc.: 60.45%] [G loss: 0.698608]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 75/86 [loss: 0.650523, acc.: 62.79%] [G loss: 0.705934]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 76/86 [loss: 0.641011, acc.: 64.16%] [G loss: 0.713639]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 77/86 [loss: 0.641236, acc.: 64.70%] [G loss: 0.713712]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 78/86 [loss: 0.659402, acc.: 61.52%] [G loss: 0.694924]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 79/86 [loss: 0.642897, acc.: 63.72%] [G loss: 0.706828]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 80/86 [loss: 0.648062, acc.: 62.70%] [G loss: 0.708982]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 81/86 [loss: 0.643112, acc.: 63.48%] [G loss: 0.699362]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 82/86 [loss: 0.649349, acc.: 63.72%] [G loss: 0.698077]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 83/86 [loss: 0.650199, acc.: 63.38%] [G loss: 0.696327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 84/86 [loss: 0.673529, acc.: 61.47%] [G loss: 0.688146]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 85/86 [loss: 0.645231, acc.: 61.91%] [G loss: 0.691585]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 0/86 [loss: 0.653993, acc.: 62.06%] [G loss: 0.701033]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 1/86 [loss: 0.645374, acc.: 63.62%] [G loss: 0.686242]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 2/86 [loss: 0.645301, acc.: 62.40%] [G loss: 0.695675]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 3/86 [loss: 0.656292, acc.: 62.84%] [G loss: 0.707894]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 4/86 [loss: 0.644010, acc.: 62.79%] [G loss: 0.700335]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 5/86 [loss: 0.662368, acc.: 61.72%] [G loss: 0.698226]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 6/86 [loss: 0.640300, acc.: 64.06%] [G loss: 0.700605]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 7/86 [loss: 0.652999, acc.: 62.45%] [G loss: 0.691584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 8/86 [loss: 0.645127, acc.: 62.26%] [G loss: 0.696504]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 9/86 [loss: 0.640696, acc.: 64.01%] [G loss: 0.681824]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 10/86 [loss: 0.645370, acc.: 62.01%] [G loss: 0.682741]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 11/86 [loss: 0.652237, acc.: 63.57%] [G loss: 0.689232]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 12/86 [loss: 0.625589, acc.: 64.99%] [G loss: 0.690217]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 13/86 [loss: 0.657099, acc.: 62.11%] [G loss: 0.686503]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 14/86 [loss: 0.632034, acc.: 63.87%] [G loss: 0.683278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 15/86 [loss: 0.634815, acc.: 65.38%] [G loss: 0.684025]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 16/86 [loss: 0.665856, acc.: 61.18%] [G loss: 0.701230]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 17/86 [loss: 0.647430, acc.: 62.79%] [G loss: 0.693534]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 18/86 [loss: 0.654797, acc.: 63.48%] [G loss: 0.700485]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 19/86 [loss: 0.637376, acc.: 64.84%] [G loss: 0.693179]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 20/86 [loss: 0.645341, acc.: 64.50%] [G loss: 0.685484]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 21/86 [loss: 0.655540, acc.: 62.65%] [G loss: 0.699217]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 22/86 [loss: 0.635519, acc.: 64.55%] [G loss: 0.688015]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 23/86 [loss: 0.650571, acc.: 61.47%] [G loss: 0.708704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 24/86 [loss: 0.641782, acc.: 64.75%] [G loss: 0.693434]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 25/86 [loss: 0.622985, acc.: 65.97%] [G loss: 0.686260]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 26/86 [loss: 0.641999, acc.: 62.94%] [G loss: 0.693429]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 27/86 [loss: 0.633175, acc.: 64.06%] [G loss: 0.683005]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 28/86 [loss: 0.652438, acc.: 62.60%] [G loss: 0.685847]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 29/86 [loss: 0.636506, acc.: 64.06%] [G loss: 0.681471]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 30/86 [loss: 0.653688, acc.: 62.74%] [G loss: 0.678383]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 31/86 [loss: 0.641762, acc.: 64.45%] [G loss: 0.675285]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 32/86 [loss: 0.653870, acc.: 62.30%] [G loss: 0.679330]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 33/86 [loss: 0.640062, acc.: 64.45%] [G loss: 0.684542]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 34/86 [loss: 0.632815, acc.: 63.87%] [G loss: 0.681687]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 35/86 [loss: 0.630317, acc.: 64.89%] [G loss: 0.675659]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 36/86 [loss: 0.638890, acc.: 63.04%] [G loss: 0.685541]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 37/86 [loss: 0.624583, acc.: 65.33%] [G loss: 0.671090]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 38/86 [loss: 0.638190, acc.: 63.38%] [G loss: 0.684913]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 39/86 [loss: 0.626651, acc.: 64.50%] [G loss: 0.683209]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 40/86 [loss: 0.629810, acc.: 65.58%] [G loss: 0.681639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 41/86 [loss: 0.622912, acc.: 65.04%] [G loss: 0.681109]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 42/86 [loss: 0.619363, acc.: 66.11%] [G loss: 0.684496]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 43/86 [loss: 0.612901, acc.: 67.19%] [G loss: 0.675057]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 44/86 [loss: 0.630398, acc.: 65.62%] [G loss: 0.680552]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 45/86 [loss: 0.608994, acc.: 66.85%] [G loss: 0.684115]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 46/86 [loss: 0.631580, acc.: 63.77%] [G loss: 0.681424]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 47/86 [loss: 0.618947, acc.: 66.85%] [G loss: 0.681197]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 48/86 [loss: 0.615947, acc.: 66.41%] [G loss: 0.665149]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 49/86 [loss: 0.621687, acc.: 65.38%] [G loss: 0.673983]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 50/86 [loss: 0.629279, acc.: 65.53%] [G loss: 0.672525]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 51/86 [loss: 0.615502, acc.: 66.60%] [G loss: 0.681981]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 52/86 [loss: 0.637965, acc.: 63.28%] [G loss: 0.674920]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 53/86 [loss: 0.650384, acc.: 63.38%] [G loss: 0.671108]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 54/86 [loss: 0.602642, acc.: 67.19%] [G loss: 0.674196]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 55/86 [loss: 0.610527, acc.: 65.92%] [G loss: 0.675957]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 56/86 [loss: 0.635413, acc.: 62.99%] [G loss: 0.693496]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 57/86 [loss: 0.631760, acc.: 65.77%] [G loss: 0.677186]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 58/86 [loss: 0.607632, acc.: 66.80%] [G loss: 0.673083]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 59/86 [loss: 0.615860, acc.: 66.02%] [G loss: 0.683029]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 60/86 [loss: 0.627317, acc.: 64.94%] [G loss: 0.695879]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 61/86 [loss: 0.606389, acc.: 67.58%] [G loss: 0.684569]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 62/86 [loss: 0.609487, acc.: 66.94%] [G loss: 0.691303]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 63/86 [loss: 0.611443, acc.: 66.85%] [G loss: 0.670602]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 64/86 [loss: 0.601704, acc.: 67.24%] [G loss: 0.669367]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 65/86 [loss: 0.621545, acc.: 65.23%] [G loss: 0.670837]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 66/86 [loss: 0.604346, acc.: 67.04%] [G loss: 0.684491]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 67/86 [loss: 0.609707, acc.: 66.89%] [G loss: 0.676894]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 68/86 [loss: 0.617155, acc.: 65.53%] [G loss: 0.684594]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 69/86 [loss: 0.595500, acc.: 68.95%] [G loss: 0.681498]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 70/86 [loss: 0.593600, acc.: 68.46%] [G loss: 0.687780]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 71/86 [loss: 0.612452, acc.: 65.48%] [G loss: 0.672489]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 72/86 [loss: 0.601310, acc.: 66.50%] [G loss: 0.682467]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 73/86 [loss: 0.595887, acc.: 69.78%] [G loss: 0.668539]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 74/86 [loss: 0.601913, acc.: 67.58%] [G loss: 0.690146]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 75/86 [loss: 0.599427, acc.: 68.26%] [G loss: 0.674567]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 76/86 [loss: 0.618164, acc.: 65.58%] [G loss: 0.672794]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 77/86 [loss: 0.610445, acc.: 67.68%] [G loss: 0.684998]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 78/86 [loss: 0.596755, acc.: 69.04%] [G loss: 0.684320]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 79/86 [loss: 0.585772, acc.: 68.65%] [G loss: 0.670080]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 80/86 [loss: 0.612652, acc.: 67.43%] [G loss: 0.676638]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 81/86 [loss: 0.593958, acc.: 68.55%] [G loss: 0.682480]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 82/86 [loss: 0.612014, acc.: 66.75%] [G loss: 0.681683]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 83/86 [loss: 0.612925, acc.: 66.31%] [G loss: 0.679486]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 84/86 [loss: 0.610173, acc.: 67.33%] [G loss: 0.686973]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 85/86 [loss: 0.597129, acc.: 68.46%] [G loss: 0.659483]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 0/86 [loss: 0.595186, acc.: 68.60%] [G loss: 0.679965]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 1/86 [loss: 0.581404, acc.: 71.68%] [G loss: 0.682439]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 2/86 [loss: 0.611446, acc.: 66.36%] [G loss: 0.676275]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 3/86 [loss: 0.588167, acc.: 69.78%] [G loss: 0.668191]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 4/86 [loss: 0.603045, acc.: 67.43%] [G loss: 0.674347]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 5/86 [loss: 0.603795, acc.: 66.75%] [G loss: 0.677664]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 6/86 [loss: 0.595723, acc.: 67.87%] [G loss: 0.683409]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 7/86 [loss: 0.588341, acc.: 69.68%] [G loss: 0.671758]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 8/86 [loss: 0.575253, acc.: 71.34%] [G loss: 0.668930]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 9/86 [loss: 0.596215, acc.: 68.46%] [G loss: 0.680566]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 10/86 [loss: 0.587852, acc.: 69.04%] [G loss: 0.679261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 11/86 [loss: 0.597070, acc.: 67.33%] [G loss: 0.674076]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 12/86 [loss: 0.585466, acc.: 68.85%] [G loss: 0.673069]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 13/86 [loss: 0.596448, acc.: 67.68%] [G loss: 0.665850]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 14/86 [loss: 0.602625, acc.: 68.51%] [G loss: 0.669325]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 15/86 [loss: 0.580376, acc.: 70.90%] [G loss: 0.667577]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 16/86 [loss: 0.573664, acc.: 70.17%] [G loss: 0.672503]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 17/86 [loss: 0.598263, acc.: 68.07%] [G loss: 0.664751]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 18/86 [loss: 0.590388, acc.: 68.90%] [G loss: 0.675988]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 19/86 [loss: 0.585194, acc.: 68.60%] [G loss: 0.669442]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 20/86 [loss: 0.571051, acc.: 70.80%] [G loss: 0.672962]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 21/86 [loss: 0.571190, acc.: 70.36%] [G loss: 0.682530]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 22/86 [loss: 0.590945, acc.: 68.85%] [G loss: 0.673777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 23/86 [loss: 0.590749, acc.: 69.82%] [G loss: 0.670834]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 24/86 [loss: 0.581478, acc.: 70.07%] [G loss: 0.672918]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 25/86 [loss: 0.592888, acc.: 65.77%] [G loss: 0.667731]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 26/86 [loss: 0.572614, acc.: 70.85%] [G loss: 0.686002]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 27/86 [loss: 0.584922, acc.: 69.73%] [G loss: 0.678013]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 28/86 [loss: 0.575421, acc.: 70.41%] [G loss: 0.677756]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 29/86 [loss: 0.577088, acc.: 70.26%] [G loss: 0.662755]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 30/86 [loss: 0.577980, acc.: 69.78%] [G loss: 0.693469]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 31/86 [loss: 0.561093, acc.: 72.95%] [G loss: 0.686754]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 32/86 [loss: 0.570633, acc.: 71.19%] [G loss: 0.675786]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 33/86 [loss: 0.578875, acc.: 70.70%] [G loss: 0.673935]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 34/86 [loss: 0.565358, acc.: 70.56%] [G loss: 0.675633]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 35/86 [loss: 0.544298, acc.: 73.78%] [G loss: 0.683302]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 36/86 [loss: 0.571744, acc.: 71.39%] [G loss: 0.668379]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 37/86 [loss: 0.569710, acc.: 71.09%] [G loss: 0.675460]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 38/86 [loss: 0.557266, acc.: 72.46%] [G loss: 0.672246]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 39/86 [loss: 0.569957, acc.: 71.19%] [G loss: 0.670357]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 40/86 [loss: 0.574092, acc.: 71.34%] [G loss: 0.667236]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 41/86 [loss: 0.553380, acc.: 73.05%] [G loss: 0.674332]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 42/86 [loss: 0.559665, acc.: 71.29%] [G loss: 0.664960]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 43/86 [loss: 0.564068, acc.: 71.97%] [G loss: 0.671873]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 44/86 [loss: 0.570343, acc.: 70.90%] [G loss: 0.671350]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 45/86 [loss: 0.547730, acc.: 72.75%] [G loss: 0.662221]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 46/86 [loss: 0.552468, acc.: 72.71%] [G loss: 0.679956]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 47/86 [loss: 0.558862, acc.: 71.97%] [G loss: 0.680026]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 48/86 [loss: 0.557659, acc.: 72.07%] [G loss: 0.671762]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 49/86 [loss: 0.541860, acc.: 73.78%] [G loss: 0.677326]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 50/86 [loss: 0.549720, acc.: 72.90%] [G loss: 0.671441]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 51/86 [loss: 0.552814, acc.: 72.07%] [G loss: 0.678359]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 52/86 [loss: 0.553437, acc.: 73.78%] [G loss: 0.671687]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 53/86 [loss: 0.549759, acc.: 72.90%] [G loss: 0.673838]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 54/86 [loss: 0.554164, acc.: 72.56%] [G loss: 0.703792]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 55/86 [loss: 0.540920, acc.: 74.46%] [G loss: 0.683527]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 56/86 [loss: 0.546750, acc.: 72.80%] [G loss: 0.682269]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 57/86 [loss: 0.550535, acc.: 72.46%] [G loss: 0.682717]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 58/86 [loss: 0.534158, acc.: 75.00%] [G loss: 0.674504]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 59/86 [loss: 0.530555, acc.: 75.10%] [G loss: 0.680165]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 60/86 [loss: 0.559351, acc.: 72.27%] [G loss: 0.678354]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 61/86 [loss: 0.564009, acc.: 71.44%] [G loss: 0.670082]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 62/86 [loss: 0.566709, acc.: 71.29%] [G loss: 0.685449]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 63/86 [loss: 0.541567, acc.: 74.07%] [G loss: 0.673473]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 64/86 [loss: 0.543609, acc.: 73.54%] [G loss: 0.668691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 65/86 [loss: 0.534512, acc.: 74.76%] [G loss: 0.680424]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 66/86 [loss: 0.542629, acc.: 73.78%] [G loss: 0.672372]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 67/86 [loss: 0.520677, acc.: 75.93%] [G loss: 0.670636]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 68/86 [loss: 0.519069, acc.: 75.73%] [G loss: 0.686934]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 69/86 [loss: 0.549276, acc.: 73.19%] [G loss: 0.677620]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 70/86 [loss: 0.547090, acc.: 72.51%] [G loss: 0.669301]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 71/86 [loss: 0.537767, acc.: 74.32%] [G loss: 0.666996]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 72/86 [loss: 0.556472, acc.: 72.85%] [G loss: 0.678640]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 73/86 [loss: 0.529012, acc.: 75.34%] [G loss: 0.682738]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 74/86 [loss: 0.552138, acc.: 73.78%] [G loss: 0.680715]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 75/86 [loss: 0.522168, acc.: 76.22%] [G loss: 0.669195]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 76/86 [loss: 0.521103, acc.: 76.07%] [G loss: 0.678491]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 77/86 [loss: 0.518755, acc.: 76.27%] [G loss: 0.696863]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 78/86 [loss: 0.513218, acc.: 77.49%] [G loss: 0.679746]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 79/86 [loss: 0.520356, acc.: 75.39%] [G loss: 0.694394]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 80/86 [loss: 0.521331, acc.: 75.63%] [G loss: 0.683210]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 81/86 [loss: 0.517719, acc.: 74.90%] [G loss: 0.702004]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 82/86 [loss: 0.508441, acc.: 77.54%] [G loss: 0.688333]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 83/86 [loss: 0.536835, acc.: 74.56%] [G loss: 0.699809]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 84/86 [loss: 0.509445, acc.: 77.25%] [G loss: 0.708215]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 85/86 [loss: 0.512934, acc.: 75.63%] [G loss: 0.693889]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 0/86 [loss: 0.507312, acc.: 77.10%] [G loss: 0.705888]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 1/86 [loss: 0.510442, acc.: 77.10%] [G loss: 0.700722]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 2/86 [loss: 0.502179, acc.: 77.88%] [G loss: 0.694281]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 3/86 [loss: 0.511094, acc.: 75.88%] [G loss: 0.701411]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 4/86 [loss: 0.514954, acc.: 76.17%] [G loss: 0.691193]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 5/86 [loss: 0.489179, acc.: 79.59%] [G loss: 0.678123]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 6/86 [loss: 0.524296, acc.: 74.80%] [G loss: 0.695367]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 7/86 [loss: 0.502454, acc.: 77.39%] [G loss: 0.689890]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 8/86 [loss: 0.496944, acc.: 78.71%] [G loss: 0.699817]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 9/86 [loss: 0.508333, acc.: 77.05%] [G loss: 0.697600]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 10/86 [loss: 0.499830, acc.: 77.83%] [G loss: 0.708628]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 11/86 [loss: 0.494736, acc.: 77.98%] [G loss: 0.697607]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 12/86 [loss: 0.495954, acc.: 77.59%] [G loss: 0.694563]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 13/86 [loss: 0.488160, acc.: 78.61%] [G loss: 0.700924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 14/86 [loss: 0.492427, acc.: 78.47%] [G loss: 0.686250]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 15/86 [loss: 0.502714, acc.: 77.69%] [G loss: 0.707417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 16/86 [loss: 0.484314, acc.: 79.39%] [G loss: 0.704021]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 17/86 [loss: 0.502110, acc.: 78.08%] [G loss: 0.707912]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 18/86 [loss: 0.493407, acc.: 78.52%] [G loss: 0.710827]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 19/86 [loss: 0.501190, acc.: 78.12%] [G loss: 0.698805]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 20/86 [loss: 0.488706, acc.: 78.17%] [G loss: 0.694566]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 21/86 [loss: 0.488759, acc.: 78.27%] [G loss: 0.694476]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 22/86 [loss: 0.490075, acc.: 78.61%] [G loss: 0.711208]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 23/86 [loss: 0.497659, acc.: 78.08%] [G loss: 0.709172]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 24/86 [loss: 0.500508, acc.: 77.15%] [G loss: 0.689934]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 25/86 [loss: 0.502375, acc.: 77.34%] [G loss: 0.701257]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 26/86 [loss: 0.493111, acc.: 78.71%] [G loss: 0.706262]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 27/86 [loss: 0.477353, acc.: 79.10%] [G loss: 0.703422]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 28/86 [loss: 0.495161, acc.: 77.78%] [G loss: 0.691056]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 29/86 [loss: 0.488209, acc.: 78.32%] [G loss: 0.694916]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 30/86 [loss: 0.509214, acc.: 76.76%] [G loss: 0.695537]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 31/86 [loss: 0.491761, acc.: 77.88%] [G loss: 0.709513]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 32/86 [loss: 0.483874, acc.: 79.79%] [G loss: 0.705434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 33/86 [loss: 0.482658, acc.: 79.69%] [G loss: 0.700561]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 34/86 [loss: 0.483410, acc.: 79.54%] [G loss: 0.695898]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 35/86 [loss: 0.470187, acc.: 79.88%] [G loss: 0.699007]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 36/86 [loss: 0.488801, acc.: 78.27%] [G loss: 0.702070]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 37/86 [loss: 0.496640, acc.: 77.34%] [G loss: 0.701450]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 38/86 [loss: 0.484580, acc.: 77.83%] [G loss: 0.720521]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 39/86 [loss: 0.490119, acc.: 78.37%] [G loss: 0.691395]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 40/86 [loss: 0.474834, acc.: 79.98%] [G loss: 0.709352]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 41/86 [loss: 0.460602, acc.: 81.45%] [G loss: 0.707934]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 42/86 [loss: 0.469520, acc.: 79.88%] [G loss: 0.725985]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 43/86 [loss: 0.471611, acc.: 80.71%] [G loss: 0.713258]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 44/86 [loss: 0.490306, acc.: 78.52%] [G loss: 0.709284]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 45/86 [loss: 0.468788, acc.: 80.27%] [G loss: 0.726414]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 46/86 [loss: 0.473047, acc.: 80.27%] [G loss: 0.720662]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 47/86 [loss: 0.486057, acc.: 79.15%] [G loss: 0.712563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 48/86 [loss: 0.482935, acc.: 79.15%] [G loss: 0.721302]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 49/86 [loss: 0.455884, acc.: 81.30%] [G loss: 0.718219]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 50/86 [loss: 0.477412, acc.: 78.61%] [G loss: 0.705625]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 51/86 [loss: 0.462223, acc.: 80.91%] [G loss: 0.720388]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 52/86 [loss: 0.477417, acc.: 79.10%] [G loss: 0.735810]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 53/86 [loss: 0.462329, acc.: 80.52%] [G loss: 0.714745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 54/86 [loss: 0.454651, acc.: 82.57%] [G loss: 0.735639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 55/86 [loss: 0.464582, acc.: 79.83%] [G loss: 0.708636]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 56/86 [loss: 0.473809, acc.: 78.66%] [G loss: 0.708619]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 57/86 [loss: 0.465668, acc.: 80.32%] [G loss: 0.725703]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 58/86 [loss: 0.477988, acc.: 79.10%] [G loss: 0.719482]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 59/86 [loss: 0.459081, acc.: 80.62%] [G loss: 0.734516]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 60/86 [loss: 0.459646, acc.: 81.10%] [G loss: 0.742929]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 61/86 [loss: 0.457818, acc.: 81.49%] [G loss: 0.718693]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 62/86 [loss: 0.457189, acc.: 80.86%] [G loss: 0.715140]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 63/86 [loss: 0.467916, acc.: 80.71%] [G loss: 0.711715]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 64/86 [loss: 0.470941, acc.: 78.71%] [G loss: 0.703622]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 65/86 [loss: 0.459805, acc.: 81.15%] [G loss: 0.713966]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 66/86 [loss: 0.482972, acc.: 78.52%] [G loss: 0.716430]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 67/86 [loss: 0.444996, acc.: 82.28%] [G loss: 0.718756]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 68/86 [loss: 0.443883, acc.: 83.35%] [G loss: 0.718430]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 69/86 [loss: 0.450062, acc.: 81.88%] [G loss: 0.709174]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 70/86 [loss: 0.461999, acc.: 80.37%] [G loss: 0.698221]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 71/86 [loss: 0.456963, acc.: 81.20%] [G loss: 0.736245]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 72/86 [loss: 0.469635, acc.: 79.98%] [G loss: 0.725813]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 73/86 [loss: 0.471336, acc.: 79.35%] [G loss: 0.719018]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 74/86 [loss: 0.446951, acc.: 82.47%] [G loss: 0.723864]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 75/86 [loss: 0.464412, acc.: 80.71%] [G loss: 0.734915]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 76/86 [loss: 0.475242, acc.: 79.54%] [G loss: 0.738094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 77/86 [loss: 0.450859, acc.: 80.96%] [G loss: 0.736082]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 78/86 [loss: 0.448120, acc.: 81.54%] [G loss: 0.737329]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 79/86 [loss: 0.464988, acc.: 81.35%] [G loss: 0.748731]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 80/86 [loss: 0.449192, acc.: 81.88%] [G loss: 0.732122]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 81/86 [loss: 0.442422, acc.: 82.23%] [G loss: 0.739534]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 82/86 [loss: 0.438346, acc.: 82.91%] [G loss: 0.736857]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 83/86 [loss: 0.440805, acc.: 81.84%] [G loss: 0.727862]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 84/86 [loss: 0.463477, acc.: 81.01%] [G loss: 0.717884]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 85/86 [loss: 0.457605, acc.: 81.45%] [G loss: 0.733551]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 0/86 [loss: 0.439608, acc.: 82.57%] [G loss: 0.734175]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 1/86 [loss: 0.457853, acc.: 80.86%] [G loss: 0.726006]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 2/86 [loss: 0.455624, acc.: 81.25%] [G loss: 0.725550]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 3/86 [loss: 0.426230, acc.: 83.59%] [G loss: 0.741487]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 4/86 [loss: 0.460208, acc.: 81.10%] [G loss: 0.740067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 5/86 [loss: 0.447060, acc.: 81.35%] [G loss: 0.746801]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 6/86 [loss: 0.434372, acc.: 82.86%] [G loss: 0.745282]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 7/86 [loss: 0.431105, acc.: 82.57%] [G loss: 0.745691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 8/86 [loss: 0.435131, acc.: 83.69%] [G loss: 0.736193]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 9/86 [loss: 0.430247, acc.: 83.35%] [G loss: 0.741448]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 10/86 [loss: 0.458709, acc.: 80.96%] [G loss: 0.733181]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 11/86 [loss: 0.458699, acc.: 80.57%] [G loss: 0.756185]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 12/86 [loss: 0.451867, acc.: 80.62%] [G loss: 0.716713]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 13/86 [loss: 0.445263, acc.: 81.93%] [G loss: 0.740119]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 14/86 [loss: 0.447720, acc.: 82.18%] [G loss: 0.737471]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 15/86 [loss: 0.462792, acc.: 80.32%] [G loss: 0.725433]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 16/86 [loss: 0.452539, acc.: 81.59%] [G loss: 0.742649]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 17/86 [loss: 0.449098, acc.: 81.35%] [G loss: 0.731473]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 18/86 [loss: 0.443147, acc.: 81.98%] [G loss: 0.736686]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 19/86 [loss: 0.456940, acc.: 81.15%] [G loss: 0.741676]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 20/86 [loss: 0.445562, acc.: 82.71%] [G loss: 0.744970]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 21/86 [loss: 0.438434, acc.: 82.42%] [G loss: 0.736599]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 22/86 [loss: 0.448981, acc.: 80.22%] [G loss: 0.742906]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 23/86 [loss: 0.434352, acc.: 83.20%] [G loss: 0.750389]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 24/86 [loss: 0.471401, acc.: 79.98%] [G loss: 0.755259]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 25/86 [loss: 0.453921, acc.: 81.59%] [G loss: 0.747013]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 26/86 [loss: 0.449373, acc.: 81.25%] [G loss: 0.740067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 27/86 [loss: 0.443837, acc.: 82.18%] [G loss: 0.758999]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 28/86 [loss: 0.441941, acc.: 81.84%] [G loss: 0.771077]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 29/86 [loss: 0.444833, acc.: 82.28%] [G loss: 0.768716]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 30/86 [loss: 0.422950, acc.: 82.57%] [G loss: 0.759757]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 31/86 [loss: 0.444266, acc.: 81.40%] [G loss: 0.759511]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 32/86 [loss: 0.449945, acc.: 80.96%] [G loss: 0.742796]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 33/86 [loss: 0.429534, acc.: 83.40%] [G loss: 0.773190]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 34/86 [loss: 0.423530, acc.: 83.74%] [G loss: 0.746582]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 35/86 [loss: 0.451093, acc.: 81.74%] [G loss: 0.761685]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 36/86 [loss: 0.450875, acc.: 81.10%] [G loss: 0.755461]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 37/86 [loss: 0.449644, acc.: 81.25%] [G loss: 0.765583]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 38/86 [loss: 0.454765, acc.: 80.57%] [G loss: 0.743236]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 39/86 [loss: 0.446771, acc.: 82.47%] [G loss: 0.759043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 40/86 [loss: 0.457328, acc.: 80.32%] [G loss: 0.763827]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 41/86 [loss: 0.438767, acc.: 82.32%] [G loss: 0.764615]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 42/86 [loss: 0.432961, acc.: 83.25%] [G loss: 0.765102]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 43/86 [loss: 0.448874, acc.: 81.84%] [G loss: 0.772381]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 44/86 [loss: 0.435336, acc.: 82.23%] [G loss: 0.771242]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 45/86 [loss: 0.438358, acc.: 82.42%] [G loss: 0.777231]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 46/86 [loss: 0.438895, acc.: 82.03%] [G loss: 0.770841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 47/86 [loss: 0.440732, acc.: 82.08%] [G loss: 0.755133]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 48/86 [loss: 0.448459, acc.: 81.35%] [G loss: 0.772314]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 49/86 [loss: 0.446047, acc.: 81.59%] [G loss: 0.770529]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 50/86 [loss: 0.430111, acc.: 83.54%] [G loss: 0.766704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 51/86 [loss: 0.430029, acc.: 82.91%] [G loss: 0.759839]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 52/86 [loss: 0.427275, acc.: 83.11%] [G loss: 0.792674]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 53/86 [loss: 0.458000, acc.: 80.08%] [G loss: 0.753480]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 54/86 [loss: 0.446927, acc.: 81.84%] [G loss: 0.763631]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 55/86 [loss: 0.439340, acc.: 81.88%] [G loss: 0.761765]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 56/86 [loss: 0.423362, acc.: 83.59%] [G loss: 0.783528]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 57/86 [loss: 0.433412, acc.: 83.11%] [G loss: 0.757059]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 58/86 [loss: 0.426642, acc.: 83.45%] [G loss: 0.774972]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 59/86 [loss: 0.455071, acc.: 80.66%] [G loss: 0.762738]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 60/86 [loss: 0.425869, acc.: 83.20%] [G loss: 0.780649]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 61/86 [loss: 0.434839, acc.: 82.18%] [G loss: 0.772093]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 62/86 [loss: 0.444197, acc.: 81.10%] [G loss: 0.770455]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 63/86 [loss: 0.440043, acc.: 81.88%] [G loss: 0.777423]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 64/86 [loss: 0.467582, acc.: 80.32%] [G loss: 0.764362]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 65/86 [loss: 0.427179, acc.: 83.11%] [G loss: 0.781176]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 66/86 [loss: 0.448070, acc.: 82.13%] [G loss: 0.771423]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 67/86 [loss: 0.442703, acc.: 82.42%] [G loss: 0.796775]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 68/86 [loss: 0.438996, acc.: 82.57%] [G loss: 0.771414]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 69/86 [loss: 0.454387, acc.: 81.40%] [G loss: 0.768290]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 70/86 [loss: 0.432490, acc.: 83.11%] [G loss: 0.767880]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 71/86 [loss: 0.433405, acc.: 82.86%] [G loss: 0.778179]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 72/86 [loss: 0.439757, acc.: 81.40%] [G loss: 0.805463]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 73/86 [loss: 0.424688, acc.: 83.59%] [G loss: 0.782450]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 74/86 [loss: 0.445454, acc.: 81.79%] [G loss: 0.776405]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 75/86 [loss: 0.411313, acc.: 85.21%] [G loss: 0.800789]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 76/86 [loss: 0.416779, acc.: 84.52%] [G loss: 0.788183]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 77/86 [loss: 0.450972, acc.: 80.86%] [G loss: 0.798764]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 78/86 [loss: 0.446871, acc.: 81.64%] [G loss: 0.768297]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 79/86 [loss: 0.433306, acc.: 84.08%] [G loss: 0.801339]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 80/86 [loss: 0.447331, acc.: 81.05%] [G loss: 0.785874]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 81/86 [loss: 0.441005, acc.: 82.42%] [G loss: 0.770230]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 82/86 [loss: 0.430700, acc.: 82.32%] [G loss: 0.791371]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 83/86 [loss: 0.415808, acc.: 84.81%] [G loss: 0.770370]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 84/86 [loss: 0.444897, acc.: 81.20%] [G loss: 0.768892]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 85/86 [loss: 0.431906, acc.: 82.13%] [G loss: 0.780568]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 0/86 [loss: 0.430475, acc.: 82.42%] [G loss: 0.789861]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 1/86 [loss: 0.437298, acc.: 81.45%] [G loss: 0.801353]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 2/86 [loss: 0.435527, acc.: 82.32%] [G loss: 0.773063]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 3/86 [loss: 0.436581, acc.: 80.86%] [G loss: 0.778115]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 4/86 [loss: 0.436330, acc.: 82.62%] [G loss: 0.776022]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 5/86 [loss: 0.421955, acc.: 82.91%] [G loss: 0.780648]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 6/86 [loss: 0.441702, acc.: 82.08%] [G loss: 0.784724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 7/86 [loss: 0.434077, acc.: 81.98%] [G loss: 0.764662]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 8/86 [loss: 0.439460, acc.: 81.98%] [G loss: 0.799174]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 9/86 [loss: 0.433351, acc.: 81.93%] [G loss: 0.782338]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 10/86 [loss: 0.426694, acc.: 82.71%] [G loss: 0.780873]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 11/86 [loss: 0.451771, acc.: 81.79%] [G loss: 0.786034]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 12/86 [loss: 0.434737, acc.: 82.32%] [G loss: 0.819781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 13/86 [loss: 0.427925, acc.: 82.28%] [G loss: 0.772754]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 14/86 [loss: 0.432238, acc.: 82.57%] [G loss: 0.785463]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 15/86 [loss: 0.421506, acc.: 83.06%] [G loss: 0.798153]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 16/86 [loss: 0.446300, acc.: 80.81%] [G loss: 0.794877]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 17/86 [loss: 0.429924, acc.: 81.59%] [G loss: 0.799766]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 18/86 [loss: 0.422997, acc.: 83.25%] [G loss: 0.794045]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 19/86 [loss: 0.444851, acc.: 81.54%] [G loss: 0.808569]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 20/86 [loss: 0.428684, acc.: 83.40%] [G loss: 0.805918]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 21/86 [loss: 0.459266, acc.: 80.37%] [G loss: 0.803330]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 22/86 [loss: 0.422530, acc.: 83.54%] [G loss: 0.782259]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 23/86 [loss: 0.447247, acc.: 80.76%] [G loss: 0.801610]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 24/86 [loss: 0.442597, acc.: 81.45%] [G loss: 0.796279]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 25/86 [loss: 0.440123, acc.: 80.76%] [G loss: 0.780927]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 26/86 [loss: 0.432334, acc.: 82.13%] [G loss: 0.801809]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 27/86 [loss: 0.438253, acc.: 81.59%] [G loss: 0.808712]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 28/86 [loss: 0.435163, acc.: 81.15%] [G loss: 0.816698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 29/86 [loss: 0.429743, acc.: 83.11%] [G loss: 0.783457]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 30/86 [loss: 0.431739, acc.: 82.32%] [G loss: 0.782192]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 31/86 [loss: 0.432373, acc.: 82.18%] [G loss: 0.814819]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 32/86 [loss: 0.457435, acc.: 80.08%] [G loss: 0.807910]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 33/86 [loss: 0.419868, acc.: 84.13%] [G loss: 0.803856]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 34/86 [loss: 0.441491, acc.: 82.32%] [G loss: 0.793880]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 35/86 [loss: 0.450388, acc.: 81.25%] [G loss: 0.814117]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 36/86 [loss: 0.432868, acc.: 82.57%] [G loss: 0.820807]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 37/86 [loss: 0.433052, acc.: 81.84%] [G loss: 0.798982]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 38/86 [loss: 0.462506, acc.: 80.18%] [G loss: 0.800175]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 39/86 [loss: 0.426061, acc.: 83.74%] [G loss: 0.801191]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 40/86 [loss: 0.432750, acc.: 82.23%] [G loss: 0.801926]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 41/86 [loss: 0.449429, acc.: 81.05%] [G loss: 0.792488]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 42/86 [loss: 0.422444, acc.: 82.81%] [G loss: 0.811897]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 43/86 [loss: 0.419890, acc.: 83.35%] [G loss: 0.824322]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 44/86 [loss: 0.468179, acc.: 79.15%] [G loss: 0.814300]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 45/86 [loss: 0.424629, acc.: 83.20%] [G loss: 0.801067]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 46/86 [loss: 0.447616, acc.: 81.35%] [G loss: 0.820625]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 47/86 [loss: 0.440625, acc.: 81.59%] [G loss: 0.823431]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 48/86 [loss: 0.470724, acc.: 78.17%] [G loss: 0.799433]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 49/86 [loss: 0.436584, acc.: 82.71%] [G loss: 0.827915]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 50/86 [loss: 0.437029, acc.: 82.57%] [G loss: 0.813572]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 51/86 [loss: 0.415936, acc.: 84.08%] [G loss: 0.819093]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 52/86 [loss: 0.468640, acc.: 78.76%] [G loss: 0.800247]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 53/86 [loss: 0.467548, acc.: 79.30%] [G loss: 0.826502]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 54/86 [loss: 0.442675, acc.: 82.13%] [G loss: 0.818925]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 55/86 [loss: 0.453671, acc.: 81.10%] [G loss: 0.819757]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 56/86 [loss: 0.432371, acc.: 82.96%] [G loss: 0.804071]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 57/86 [loss: 0.425565, acc.: 82.23%] [G loss: 0.801523]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 58/86 [loss: 0.450097, acc.: 80.66%] [G loss: 0.804683]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 59/86 [loss: 0.436308, acc.: 82.03%] [G loss: 0.806807]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 60/86 [loss: 0.454376, acc.: 80.86%] [G loss: 0.796676]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 61/86 [loss: 0.424926, acc.: 83.20%] [G loss: 0.799034]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 62/86 [loss: 0.459943, acc.: 81.25%] [G loss: 0.829368]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 63/86 [loss: 0.432595, acc.: 82.67%] [G loss: 0.846406]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 64/86 [loss: 0.442516, acc.: 81.64%] [G loss: 0.822914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 65/86 [loss: 0.445977, acc.: 81.05%] [G loss: 0.840627]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 66/86 [loss: 0.436100, acc.: 82.08%] [G loss: 0.795265]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 67/86 [loss: 0.448700, acc.: 81.49%] [G loss: 0.834816]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 68/86 [loss: 0.433473, acc.: 82.47%] [G loss: 0.822647]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 69/86 [loss: 0.434540, acc.: 82.42%] [G loss: 0.828459]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 70/86 [loss: 0.460438, acc.: 80.96%] [G loss: 0.814248]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 71/86 [loss: 0.434087, acc.: 82.57%] [G loss: 0.832808]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 72/86 [loss: 0.433158, acc.: 82.71%] [G loss: 0.825300]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 73/86 [loss: 0.460913, acc.: 80.22%] [G loss: 0.827684]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 74/86 [loss: 0.444072, acc.: 81.64%] [G loss: 0.841362]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 75/86 [loss: 0.456425, acc.: 80.13%] [G loss: 0.844294]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 76/86 [loss: 0.436132, acc.: 81.74%] [G loss: 0.815590]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 77/86 [loss: 0.466371, acc.: 80.08%] [G loss: 0.821418]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 78/86 [loss: 0.449317, acc.: 80.96%] [G loss: 0.828396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 79/86 [loss: 0.452654, acc.: 81.35%] [G loss: 0.802694]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 80/86 [loss: 0.451750, acc.: 81.10%] [G loss: 0.797011]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 81/86 [loss: 0.420400, acc.: 82.28%] [G loss: 0.829868]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 82/86 [loss: 0.459563, acc.: 80.62%] [G loss: 0.821463]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 83/86 [loss: 0.461274, acc.: 80.42%] [G loss: 0.829327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 84/86 [loss: 0.442673, acc.: 81.54%] [G loss: 0.801520]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 85/86 [loss: 0.430470, acc.: 82.86%] [G loss: 0.814965]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 0/86 [loss: 0.428263, acc.: 82.28%] [G loss: 0.786292]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 1/86 [loss: 0.479626, acc.: 78.32%] [G loss: 0.823330]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 2/86 [loss: 0.466404, acc.: 79.44%] [G loss: 0.799603]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 3/86 [loss: 0.436894, acc.: 81.98%] [G loss: 0.817576]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 4/86 [loss: 0.433311, acc.: 81.88%] [G loss: 0.814955]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 5/86 [loss: 0.452805, acc.: 79.69%] [G loss: 0.810894]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 6/86 [loss: 0.465134, acc.: 79.25%] [G loss: 0.824292]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 7/86 [loss: 0.440128, acc.: 80.96%] [G loss: 0.798879]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 8/86 [loss: 0.452962, acc.: 80.42%] [G loss: 0.822957]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 9/86 [loss: 0.448466, acc.: 80.71%] [G loss: 0.807469]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 10/86 [loss: 0.441676, acc.: 80.76%] [G loss: 0.836812]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 11/86 [loss: 0.424637, acc.: 83.45%] [G loss: 0.829384]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 12/86 [loss: 0.445482, acc.: 81.45%] [G loss: 0.819483]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 13/86 [loss: 0.440071, acc.: 80.86%] [G loss: 0.818188]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 14/86 [loss: 0.431709, acc.: 82.03%] [G loss: 0.808841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 15/86 [loss: 0.437074, acc.: 81.93%] [G loss: 0.830747]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 16/86 [loss: 0.436451, acc.: 82.28%] [G loss: 0.819531]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 17/86 [loss: 0.438138, acc.: 82.28%] [G loss: 0.799244]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 18/86 [loss: 0.437308, acc.: 81.59%] [G loss: 0.820455]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 19/86 [loss: 0.452878, acc.: 81.01%] [G loss: 0.820322]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 20/86 [loss: 0.453007, acc.: 80.81%] [G loss: 0.819935]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 21/86 [loss: 0.436466, acc.: 81.49%] [G loss: 0.823882]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 22/86 [loss: 0.418124, acc.: 83.64%] [G loss: 0.841125]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 23/86 [loss: 0.461175, acc.: 79.69%] [G loss: 0.807870]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 24/86 [loss: 0.442793, acc.: 81.15%] [G loss: 0.830547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 25/86 [loss: 0.436557, acc.: 82.18%] [G loss: 0.820909]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 26/86 [loss: 0.455215, acc.: 80.03%] [G loss: 0.806945]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 27/86 [loss: 0.457320, acc.: 80.42%] [G loss: 0.833200]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 28/86 [loss: 0.459982, acc.: 79.74%] [G loss: 0.846966]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 29/86 [loss: 0.440805, acc.: 81.84%] [G loss: 0.835264]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 30/86 [loss: 0.446767, acc.: 81.35%] [G loss: 0.852698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 31/86 [loss: 0.443010, acc.: 81.05%] [G loss: 0.849904]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 32/86 [loss: 0.450869, acc.: 79.79%] [G loss: 0.818210]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 33/86 [loss: 0.447225, acc.: 81.10%] [G loss: 0.843759]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 34/86 [loss: 0.446828, acc.: 81.59%] [G loss: 0.810548]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 35/86 [loss: 0.447647, acc.: 80.62%] [G loss: 0.824244]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 36/86 [loss: 0.445835, acc.: 81.93%] [G loss: 0.855203]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 37/86 [loss: 0.464955, acc.: 79.39%] [G loss: 0.828422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 38/86 [loss: 0.476986, acc.: 77.83%] [G loss: 0.828750]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 39/86 [loss: 0.469599, acc.: 79.54%] [G loss: 0.831473]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 40/86 [loss: 0.447741, acc.: 81.49%] [G loss: 0.825238]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 41/86 [loss: 0.449122, acc.: 80.52%] [G loss: 0.829669]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 42/86 [loss: 0.433913, acc.: 82.32%] [G loss: 0.835405]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 43/86 [loss: 0.470929, acc.: 79.35%] [G loss: 0.825791]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 44/86 [loss: 0.448740, acc.: 80.86%] [G loss: 0.830009]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 45/86 [loss: 0.428554, acc.: 83.45%] [G loss: 0.812409]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 46/86 [loss: 0.464507, acc.: 79.30%] [G loss: 0.832065]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 47/86 [loss: 0.430365, acc.: 81.98%] [G loss: 0.826356]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 48/86 [loss: 0.468857, acc.: 78.56%] [G loss: 0.829925]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 49/86 [loss: 0.447572, acc.: 80.22%] [G loss: 0.854725]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 50/86 [loss: 0.460525, acc.: 80.91%] [G loss: 0.842375]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 51/86 [loss: 0.457032, acc.: 79.49%] [G loss: 0.821165]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 52/86 [loss: 0.452522, acc.: 80.52%] [G loss: 0.872223]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 53/86 [loss: 0.442295, acc.: 81.49%] [G loss: 0.842134]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 54/86 [loss: 0.454443, acc.: 79.74%] [G loss: 0.859068]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 55/86 [loss: 0.450758, acc.: 80.81%] [G loss: 0.871457]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 56/86 [loss: 0.470706, acc.: 78.86%] [G loss: 0.823417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 57/86 [loss: 0.458837, acc.: 79.49%] [G loss: 0.821333]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 58/86 [loss: 0.453323, acc.: 81.49%] [G loss: 0.853446]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 59/86 [loss: 0.439950, acc.: 81.15%] [G loss: 0.834107]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 60/86 [loss: 0.467272, acc.: 79.39%] [G loss: 0.841497]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 61/86 [loss: 0.447635, acc.: 80.08%] [G loss: 0.830142]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 62/86 [loss: 0.458379, acc.: 80.57%] [G loss: 0.862770]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 63/86 [loss: 0.458791, acc.: 80.66%] [G loss: 0.835978]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 64/86 [loss: 0.432821, acc.: 82.32%] [G loss: 0.841065]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 65/86 [loss: 0.436822, acc.: 82.32%] [G loss: 0.835496]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 66/86 [loss: 0.454452, acc.: 79.59%] [G loss: 0.842198]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 67/86 [loss: 0.462067, acc.: 79.59%] [G loss: 0.865937]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 7/200  Batch Size: 68/86 [loss: 0.447757, acc.: 81.79%] [G loss: 0.863316]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 69/86 [loss: 0.458677, acc.: 80.13%] [G loss: 0.864222]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 70/86 [loss: 0.442035, acc.: 81.54%] [G loss: 0.846557]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 71/86 [loss: 0.445032, acc.: 80.42%] [G loss: 0.857864]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 72/86 [loss: 0.439451, acc.: 81.74%] [G loss: 0.842959]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 73/86 [loss: 0.469053, acc.: 78.91%] [G loss: 0.833706]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 74/86 [loss: 0.449627, acc.: 80.66%] [G loss: 0.850129]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 75/86 [loss: 0.471033, acc.: 78.61%] [G loss: 0.844445]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 76/86 [loss: 0.427193, acc.: 82.76%] [G loss: 0.893003]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 77/86 [loss: 0.431305, acc.: 82.76%] [G loss: 0.863456]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 78/86 [loss: 0.468499, acc.: 79.54%] [G loss: 0.869512]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 79/86 [loss: 0.445905, acc.: 81.25%] [G loss: 0.837922]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 80/86 [loss: 0.467751, acc.: 78.96%] [G loss: 0.831564]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 81/86 [loss: 0.458017, acc.: 80.22%] [G loss: 0.848074]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 82/86 [loss: 0.445820, acc.: 82.28%] [G loss: 0.853260]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 83/86 [loss: 0.478013, acc.: 79.05%] [G loss: 0.858956]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 84/86 [loss: 0.430514, acc.: 81.84%] [G loss: 0.856391]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 7/200  Batch Size: 85/86 [loss: 0.469417, acc.: 78.56%] [G loss: 0.869290]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 0/86 [loss: 0.463553, acc.: 79.74%] [G loss: 0.854682]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 1/86 [loss: 0.464102, acc.: 79.79%] [G loss: 0.854008]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 2/86 [loss: 0.445762, acc.: 80.81%] [G loss: 0.847986]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 3/86 [loss: 0.462731, acc.: 80.03%] [G loss: 0.875913]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 4/86 [loss: 0.481957, acc.: 77.10%] [G loss: 0.829530]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 5/86 [loss: 0.445928, acc.: 81.25%] [G loss: 0.853506]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 6/86 [loss: 0.428579, acc.: 82.71%] [G loss: 0.855578]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 7/86 [loss: 0.478877, acc.: 78.47%] [G loss: 0.851941]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 8/86 [loss: 0.442983, acc.: 81.49%] [G loss: 0.840762]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 9/86 [loss: 0.476456, acc.: 78.96%] [G loss: 0.838741]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 10/86 [loss: 0.451215, acc.: 80.66%] [G loss: 0.849241]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 11/86 [loss: 0.433036, acc.: 81.84%] [G loss: 0.851651]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 12/86 [loss: 0.457830, acc.: 79.69%] [G loss: 0.847700]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 13/86 [loss: 0.436551, acc.: 82.76%] [G loss: 0.859129]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 14/86 [loss: 0.442594, acc.: 80.86%] [G loss: 0.871733]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 15/86 [loss: 0.456087, acc.: 81.01%] [G loss: 0.841614]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 16/86 [loss: 0.448543, acc.: 81.59%] [G loss: 0.822903]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 17/86 [loss: 0.482893, acc.: 77.25%] [G loss: 0.844784]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 18/86 [loss: 0.440671, acc.: 81.20%] [G loss: 0.860886]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 19/86 [loss: 0.459207, acc.: 80.66%] [G loss: 0.823624]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 20/86 [loss: 0.458515, acc.: 80.27%] [G loss: 0.868923]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 21/86 [loss: 0.473499, acc.: 78.81%] [G loss: 0.861338]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 22/86 [loss: 0.487593, acc.: 77.83%] [G loss: 0.839811]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 23/86 [loss: 0.478870, acc.: 78.61%] [G loss: 0.830240]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 24/86 [loss: 0.449018, acc.: 80.37%] [G loss: 0.816939]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 25/86 [loss: 0.458061, acc.: 79.64%] [G loss: 0.836858]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 26/86 [loss: 0.475083, acc.: 78.96%] [G loss: 0.852896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 27/86 [loss: 0.471799, acc.: 79.39%] [G loss: 0.830085]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 28/86 [loss: 0.466751, acc.: 79.39%] [G loss: 0.862279]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 29/86 [loss: 0.473159, acc.: 78.08%] [G loss: 0.836870]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 30/86 [loss: 0.472102, acc.: 78.52%] [G loss: 0.835239]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 31/86 [loss: 0.432131, acc.: 82.03%] [G loss: 0.824832]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 32/86 [loss: 0.474769, acc.: 77.73%] [G loss: 0.829994]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 33/86 [loss: 0.491202, acc.: 77.69%] [G loss: 0.823036]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 34/86 [loss: 0.462734, acc.: 80.13%] [G loss: 0.857502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 35/86 [loss: 0.453840, acc.: 80.37%] [G loss: 0.846300]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 36/86 [loss: 0.449393, acc.: 80.37%] [G loss: 0.836281]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 37/86 [loss: 0.447984, acc.: 81.20%] [G loss: 0.832410]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 38/86 [loss: 0.473674, acc.: 78.56%] [G loss: 0.842932]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 39/86 [loss: 0.470348, acc.: 77.98%] [G loss: 0.859563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 40/86 [loss: 0.481357, acc.: 78.03%] [G loss: 0.844447]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 41/86 [loss: 0.454975, acc.: 78.91%] [G loss: 0.864247]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 42/86 [loss: 0.467218, acc.: 79.25%] [G loss: 0.841810]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 43/86 [loss: 0.454337, acc.: 80.18%] [G loss: 0.899414]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 44/86 [loss: 0.447009, acc.: 80.57%] [G loss: 0.876879]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 45/86 [loss: 0.451541, acc.: 79.44%] [G loss: 0.858942]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 46/86 [loss: 0.453317, acc.: 81.54%] [G loss: 0.887138]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 47/86 [loss: 0.447945, acc.: 80.52%] [G loss: 0.864284]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 48/86 [loss: 0.489822, acc.: 76.90%] [G loss: 0.860981]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 49/86 [loss: 0.485815, acc.: 77.00%] [G loss: 0.807312]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 50/86 [loss: 0.451491, acc.: 79.83%] [G loss: 0.868909]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 51/86 [loss: 0.476868, acc.: 77.93%] [G loss: 0.854568]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 52/86 [loss: 0.453599, acc.: 80.32%] [G loss: 0.842616]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 53/86 [loss: 0.497861, acc.: 76.86%] [G loss: 0.816326]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 54/86 [loss: 0.465191, acc.: 79.59%] [G loss: 0.834026]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 55/86 [loss: 0.425385, acc.: 82.81%] [G loss: 0.851387]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 56/86 [loss: 0.466776, acc.: 79.35%] [G loss: 0.855067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 57/86 [loss: 0.436530, acc.: 82.18%] [G loss: 0.846482]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 58/86 [loss: 0.458294, acc.: 80.27%] [G loss: 0.866590]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 59/86 [loss: 0.453794, acc.: 80.42%] [G loss: 0.874172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 60/86 [loss: 0.436424, acc.: 81.69%] [G loss: 0.856083]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 61/86 [loss: 0.486642, acc.: 77.69%] [G loss: 0.846682]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 62/86 [loss: 0.458679, acc.: 79.15%] [G loss: 0.853432]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 63/86 [loss: 0.454459, acc.: 80.27%] [G loss: 0.855927]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 64/86 [loss: 0.479260, acc.: 78.17%] [G loss: 0.861736]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 65/86 [loss: 0.481735, acc.: 78.03%] [G loss: 0.850038]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 66/86 [loss: 0.474396, acc.: 77.83%] [G loss: 0.866368]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 67/86 [loss: 0.466623, acc.: 78.52%] [G loss: 0.829694]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 68/86 [loss: 0.489834, acc.: 77.49%] [G loss: 0.856491]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 69/86 [loss: 0.459613, acc.: 79.44%] [G loss: 0.860115]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 70/86 [loss: 0.457724, acc.: 79.93%] [G loss: 0.831835]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 71/86 [loss: 0.463626, acc.: 79.79%] [G loss: 0.890987]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 72/86 [loss: 0.450303, acc.: 81.88%] [G loss: 0.864912]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 73/86 [loss: 0.465540, acc.: 79.69%] [G loss: 0.855594]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 74/86 [loss: 0.447742, acc.: 80.96%] [G loss: 0.859178]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 75/86 [loss: 0.439012, acc.: 81.45%] [G loss: 0.849565]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 76/86 [loss: 0.464573, acc.: 78.32%] [G loss: 0.842765]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 77/86 [loss: 0.477898, acc.: 77.10%] [G loss: 0.860738]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 78/86 [loss: 0.447190, acc.: 80.57%] [G loss: 0.833434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 79/86 [loss: 0.484445, acc.: 78.47%] [G loss: 0.832374]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 80/86 [loss: 0.460244, acc.: 80.03%] [G loss: 0.827915]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 81/86 [loss: 0.478270, acc.: 78.17%] [G loss: 0.846708]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 82/86 [loss: 0.467087, acc.: 79.54%] [G loss: 0.846809]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 83/86 [loss: 0.469630, acc.: 78.76%] [G loss: 0.809364]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 84/86 [loss: 0.454178, acc.: 80.76%] [G loss: 0.871637]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 8/200  Batch Size: 85/86 [loss: 0.458593, acc.: 80.47%] [G loss: 0.883153]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 0/86 [loss: 0.502552, acc.: 76.17%] [G loss: 0.878199]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 1/86 [loss: 0.505254, acc.: 75.44%] [G loss: 0.843916]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 2/86 [loss: 0.460468, acc.: 79.69%] [G loss: 0.861777]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 3/86 [loss: 0.458070, acc.: 80.32%] [G loss: 0.876807]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 4/86 [loss: 0.490401, acc.: 76.37%] [G loss: 0.867975]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 5/86 [loss: 0.473860, acc.: 78.91%] [G loss: 0.838731]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 6/86 [loss: 0.439184, acc.: 81.88%] [G loss: 0.821742]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 7/86 [loss: 0.459803, acc.: 79.59%] [G loss: 0.864497]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 8/86 [loss: 0.474173, acc.: 78.81%] [G loss: 0.864906]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 9/86 [loss: 0.489131, acc.: 78.12%] [G loss: 0.875070]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 10/86 [loss: 0.469172, acc.: 78.66%] [G loss: 0.857022]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 11/86 [loss: 0.457379, acc.: 80.42%] [G loss: 0.872550]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 12/86 [loss: 0.423799, acc.: 82.42%] [G loss: 0.893966]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 9/200  Batch Size: 13/86 [loss: 0.466086, acc.: 78.96%] [G loss: 0.925499]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 9/200  Batch Size: 14/86 [loss: 0.451977, acc.: 80.96%] [G loss: 0.860241]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 15/86 [loss: 0.427785, acc.: 81.54%] [G loss: 0.874572]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 16/86 [loss: 0.479260, acc.: 78.56%] [G loss: 0.846891]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 17/86 [loss: 0.466390, acc.: 78.91%] [G loss: 0.846742]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 18/86 [loss: 0.465536, acc.: 79.44%] [G loss: 0.883740]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 9/200  Batch Size: 19/86 [loss: 0.458631, acc.: 79.74%] [G loss: 0.843548]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 20/86 [loss: 0.480567, acc.: 77.83%] [G loss: 0.852448]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 21/86 [loss: 0.480955, acc.: 77.69%] [G loss: 0.873515]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 22/86 [loss: 0.440015, acc.: 81.25%] [G loss: 0.845640]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 23/86 [loss: 0.462790, acc.: 79.39%] [G loss: 0.885497]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 24/86 [loss: 0.432854, acc.: 82.03%] [G loss: 0.848654]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 25/86 [loss: 0.464262, acc.: 79.39%] [G loss: 0.875565]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 26/86 [loss: 0.462266, acc.: 79.64%] [G loss: 0.876618]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 27/86 [loss: 0.442684, acc.: 81.59%] [G loss: 0.870951]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 9/200  Batch Size: 28/86 [loss: 0.461600, acc.: 79.15%] [G loss: 0.885594]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 29/86 [loss: 0.474267, acc.: 78.86%] [G loss: 0.856799]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 30/86 [loss: 0.476155, acc.: 79.39%] [G loss: 0.867114]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 9/200  Batch Size: 31/86 [loss: 0.466353, acc.: 79.49%] [G loss: 0.860910]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 32/86 [loss: 0.453801, acc.: 80.57%] [G loss: 0.835990]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 9/200  Batch Size: 33/86 [loss: 0.448104, acc.: 80.32%] [G loss: 0.849924]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 34/86 [loss: 0.465176, acc.: 78.56%] [G loss: 0.856496]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 35/86 [loss: 0.447197, acc.: 80.96%] [G loss: 0.836190]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 36/86 [loss: 0.482576, acc.: 77.10%] [G loss: 0.858571]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 37/86 [loss: 0.440204, acc.: 80.57%] [G loss: 0.860879]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 38/86 [loss: 0.439992, acc.: 82.18%] [G loss: 0.826735]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 39/86 [loss: 0.450293, acc.: 80.57%] [G loss: 0.840331]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 40/86 [loss: 0.472529, acc.: 78.47%] [G loss: 0.875589]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 41/86 [loss: 0.468305, acc.: 79.10%] [G loss: 0.848371]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 42/86 [loss: 0.439644, acc.: 81.25%] [G loss: 0.872801]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 43/86 [loss: 0.456422, acc.: 79.98%] [G loss: 0.887089]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 44/86 [loss: 0.463003, acc.: 79.64%] [G loss: 0.857702]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 45/86 [loss: 0.441230, acc.: 82.13%] [G loss: 0.875782]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 46/86 [loss: 0.485974, acc.: 77.69%] [G loss: 0.835634]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 9/200  Batch Size: 47/86 [loss: 0.449091, acc.: 80.66%] [G loss: 0.851925]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 9/200  Batch Size: 48/86 [loss: 0.471795, acc.: 79.20%] [G loss: 0.859290]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 49/86 [loss: 0.441297, acc.: 82.42%] [G loss: 0.866823]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 50/86 [loss: 0.477061, acc.: 78.66%] [G loss: 0.864008]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 51/86 [loss: 0.449554, acc.: 81.05%] [G loss: 0.861053]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 52/86 [loss: 0.450552, acc.: 81.15%] [G loss: 0.869100]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 53/86 [loss: 0.457691, acc.: 79.88%] [G loss: 0.866889]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 54/86 [loss: 0.444571, acc.: 81.84%] [G loss: 0.894706]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 55/86 [loss: 0.446884, acc.: 80.62%] [G loss: 0.879646]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 56/86 [loss: 0.482823, acc.: 77.05%] [G loss: 0.892501]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 57/86 [loss: 0.448396, acc.: 80.76%] [G loss: 0.909480]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 58/86 [loss: 0.471702, acc.: 78.86%] [G loss: 0.875814]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 59/86 [loss: 0.465067, acc.: 79.25%] [G loss: 0.894939]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 9/200  Batch Size: 60/86 [loss: 0.453696, acc.: 81.01%] [G loss: 0.881128]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 61/86 [loss: 0.478225, acc.: 78.32%] [G loss: 0.858008]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 62/86 [loss: 0.461971, acc.: 79.69%] [G loss: 0.859328]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 63/86 [loss: 0.443604, acc.: 81.64%] [G loss: 0.865345]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 9/200  Batch Size: 64/86 [loss: 0.451747, acc.: 80.47%] [G loss: 0.868713]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 65/86 [loss: 0.490412, acc.: 77.29%] [G loss: 0.858014]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 9/200  Batch Size: 66/86 [loss: 0.472180, acc.: 79.64%] [G loss: 0.897638]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 67/86 [loss: 0.441371, acc.: 81.35%] [G loss: 0.846825]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 68/86 [loss: 0.484771, acc.: 78.81%] [G loss: 0.867793]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 9/200  Batch Size: 69/86 [loss: 0.480496, acc.: 77.49%] [G loss: 0.863233]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 70/86 [loss: 0.468379, acc.: 79.20%] [G loss: 0.851332]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 9/200  Batch Size: 71/86 [loss: 0.452488, acc.: 80.86%] [G loss: 0.819606]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 72/86 [loss: 0.455156, acc.: 81.45%] [G loss: 0.860942]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 73/86 [loss: 0.467243, acc.: 79.15%] [G loss: 0.872137]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 74/86 [loss: 0.441173, acc.: 82.18%] [G loss: 0.869658]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 75/86 [loss: 0.417937, acc.: 84.08%] [G loss: 0.840397]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 76/86 [loss: 0.471619, acc.: 80.08%] [G loss: 0.868743]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 77/86 [loss: 0.457064, acc.: 79.93%] [G loss: 0.830591]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 78/86 [loss: 0.456540, acc.: 81.35%] [G loss: 0.865613]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 9/200  Batch Size: 79/86 [loss: 0.468911, acc.: 80.52%] [G loss: 0.871658]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 80/86 [loss: 0.469011, acc.: 79.39%] [G loss: 0.855701]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 81/86 [loss: 0.478807, acc.: 79.39%] [G loss: 0.853319]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 9/200  Batch Size: 82/86 [loss: 0.444572, acc.: 81.35%] [G loss: 0.853067]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 83/86 [loss: 0.423926, acc.: 83.40%] [G loss: 0.865571]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 9/200  Batch Size: 84/86 [loss: 0.453365, acc.: 80.52%] [G loss: 0.909272]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 9/200  Batch Size: 85/86 [loss: 0.475858, acc.: 78.86%] [G loss: 0.857758]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 0/86 [loss: 0.445597, acc.: 80.37%] [G loss: 0.874562]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 1/86 [loss: 0.449708, acc.: 81.20%] [G loss: 0.866498]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 2/86 [loss: 0.468370, acc.: 79.30%] [G loss: 0.861172]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 3/86 [loss: 0.464200, acc.: 79.35%] [G loss: 0.868709]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 4/86 [loss: 0.485372, acc.: 77.59%] [G loss: 0.846792]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 5/86 [loss: 0.475118, acc.: 78.47%] [G loss: 0.847714]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 6/86 [loss: 0.461520, acc.: 80.52%] [G loss: 0.857210]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 7/86 [loss: 0.466717, acc.: 78.42%] [G loss: 0.843472]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 8/86 [loss: 0.442445, acc.: 81.20%] [G loss: 0.870981]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 9/86 [loss: 0.464315, acc.: 80.08%] [G loss: 0.859810]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 10/86 [loss: 0.450669, acc.: 80.18%] [G loss: 0.846872]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 11/86 [loss: 0.449518, acc.: 80.66%] [G loss: 0.873854]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 12/86 [loss: 0.464388, acc.: 79.35%] [G loss: 0.873034]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 13/86 [loss: 0.467209, acc.: 78.56%] [G loss: 0.876693]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 14/86 [loss: 0.449771, acc.: 81.30%] [G loss: 0.862170]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 15/86 [loss: 0.441590, acc.: 82.37%] [G loss: 0.886566]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 16/86 [loss: 0.492222, acc.: 76.51%] [G loss: 0.895860]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 17/86 [loss: 0.469085, acc.: 79.83%] [G loss: 0.865779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 18/86 [loss: 0.488860, acc.: 77.59%] [G loss: 0.868291]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 19/86 [loss: 0.458671, acc.: 80.13%] [G loss: 0.868737]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 20/86 [loss: 0.486310, acc.: 76.71%] [G loss: 0.863143]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 21/86 [loss: 0.486721, acc.: 77.73%] [G loss: 0.889355]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 22/86 [loss: 0.484036, acc.: 77.83%] [G loss: 0.837492]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 23/86 [loss: 0.456257, acc.: 79.49%] [G loss: 0.825751]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 24/86 [loss: 0.461607, acc.: 79.64%] [G loss: 0.843333]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 25/86 [loss: 0.461805, acc.: 79.00%] [G loss: 0.848074]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 26/86 [loss: 0.468054, acc.: 79.93%] [G loss: 0.839590]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 27/86 [loss: 0.443869, acc.: 81.25%] [G loss: 0.869644]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 28/86 [loss: 0.464882, acc.: 79.88%] [G loss: 0.840851]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 29/86 [loss: 0.473791, acc.: 79.00%] [G loss: 0.869701]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 30/86 [loss: 0.474448, acc.: 78.52%] [G loss: 0.866363]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 31/86 [loss: 0.480560, acc.: 78.22%] [G loss: 0.838496]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 32/86 [loss: 0.481588, acc.: 78.47%] [G loss: 0.863823]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 33/86 [loss: 0.452550, acc.: 80.27%] [G loss: 0.840484]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 34/86 [loss: 0.443051, acc.: 82.37%] [G loss: 0.843072]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 35/86 [loss: 0.449960, acc.: 81.49%] [G loss: 0.851845]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 36/86 [loss: 0.447884, acc.: 80.86%] [G loss: 0.844432]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 37/86 [loss: 0.477804, acc.: 79.10%] [G loss: 0.849056]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 38/86 [loss: 0.445962, acc.: 80.66%] [G loss: 0.846713]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 39/86 [loss: 0.433884, acc.: 82.03%] [G loss: 0.869604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 40/86 [loss: 0.472950, acc.: 79.00%] [G loss: 0.838682]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 41/86 [loss: 0.482473, acc.: 76.71%] [G loss: 0.843321]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 42/86 [loss: 0.455432, acc.: 80.76%] [G loss: 0.863595]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 43/86 [loss: 0.472938, acc.: 79.44%] [G loss: 0.863020]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 44/86 [loss: 0.459109, acc.: 79.74%] [G loss: 0.861206]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 45/86 [loss: 0.419600, acc.: 83.89%] [G loss: 0.829822]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 46/86 [loss: 0.477808, acc.: 79.00%] [G loss: 0.870124]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 47/86 [loss: 0.448624, acc.: 81.59%] [G loss: 0.854774]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 48/86 [loss: 0.450928, acc.: 79.88%] [G loss: 0.874826]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 49/86 [loss: 0.439116, acc.: 81.20%] [G loss: 0.861670]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 50/86 [loss: 0.448764, acc.: 80.57%] [G loss: 0.843345]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 51/86 [loss: 0.486550, acc.: 78.03%] [G loss: 0.867414]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 52/86 [loss: 0.432316, acc.: 81.98%] [G loss: 0.825381]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 53/86 [loss: 0.428238, acc.: 82.42%] [G loss: 0.850921]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 54/86 [loss: 0.455915, acc.: 80.42%] [G loss: 0.883237]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 55/86 [loss: 0.445101, acc.: 81.40%] [G loss: 0.856591]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 56/86 [loss: 0.465394, acc.: 79.54%] [G loss: 0.848853]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 57/86 [loss: 0.483169, acc.: 77.93%] [G loss: 0.872773]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 58/86 [loss: 0.507085, acc.: 75.63%] [G loss: 0.863979]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 59/86 [loss: 0.441288, acc.: 80.91%] [G loss: 0.859253]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 60/86 [loss: 0.511166, acc.: 75.73%] [G loss: 0.856559]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 61/86 [loss: 0.459861, acc.: 80.76%] [G loss: 0.867297]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 62/86 [loss: 0.450325, acc.: 80.57%] [G loss: 0.834406]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 63/86 [loss: 0.449025, acc.: 80.81%] [G loss: 0.870671]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 64/86 [loss: 0.469189, acc.: 78.17%] [G loss: 0.846537]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 65/86 [loss: 0.440208, acc.: 81.74%] [G loss: 0.847425]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 66/86 [loss: 0.458871, acc.: 80.03%] [G loss: 0.865369]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 67/86 [loss: 0.442063, acc.: 81.01%] [G loss: 0.884795]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 68/86 [loss: 0.440157, acc.: 80.81%] [G loss: 0.889192]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 69/86 [loss: 0.453638, acc.: 80.66%] [G loss: 0.859436]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 70/86 [loss: 0.454776, acc.: 79.79%] [G loss: 0.830300]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 71/86 [loss: 0.453823, acc.: 80.37%] [G loss: 0.861482]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 72/86 [loss: 0.488888, acc.: 77.34%] [G loss: 0.837387]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 73/86 [loss: 0.453131, acc.: 81.15%] [G loss: 0.876356]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 74/86 [loss: 0.478083, acc.: 78.76%] [G loss: 0.878580]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 75/86 [loss: 0.464815, acc.: 80.03%] [G loss: 0.870009]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 76/86 [loss: 0.509266, acc.: 75.24%] [G loss: 0.857328]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 77/86 [loss: 0.450866, acc.: 79.83%] [G loss: 0.865411]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 10/200  Batch Size: 78/86 [loss: 0.461629, acc.: 79.20%] [G loss: 0.866330]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 79/86 [loss: 0.455591, acc.: 79.64%] [G loss: 0.845279]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 10/200  Batch Size: 80/86 [loss: 0.428934, acc.: 82.67%] [G loss: 0.842758]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 10/200  Batch Size: 81/86 [loss: 0.442245, acc.: 81.35%] [G loss: 0.890783]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 82/86 [loss: 0.459277, acc.: 79.69%] [G loss: 0.848893]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 10/200  Batch Size: 83/86 [loss: 0.438892, acc.: 81.84%] [G loss: 0.871714]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 10/200  Batch Size: 84/86 [loss: 0.492706, acc.: 76.95%] [G loss: 0.841874]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 10/200  Batch Size: 85/86 [loss: 0.473540, acc.: 79.10%] [G loss: 0.856806]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 0/86 [loss: 0.446173, acc.: 80.62%] [G loss: 0.857290]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 1/86 [loss: 0.440452, acc.: 81.69%] [G loss: 0.867263]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 2/86 [loss: 0.442432, acc.: 82.13%] [G loss: 0.853847]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 3/86 [loss: 0.421862, acc.: 82.67%] [G loss: 0.851927]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 4/86 [loss: 0.445396, acc.: 82.13%] [G loss: 0.857590]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 5/86 [loss: 0.464599, acc.: 80.18%] [G loss: 0.827572]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 6/86 [loss: 0.454454, acc.: 80.42%] [G loss: 0.829093]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 7/86 [loss: 0.442375, acc.: 81.45%] [G loss: 0.847975]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 8/86 [loss: 0.439249, acc.: 81.45%] [G loss: 0.867750]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 9/86 [loss: 0.460845, acc.: 79.15%] [G loss: 0.833999]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 10/86 [loss: 0.440663, acc.: 82.18%] [G loss: 0.891247]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 11/200  Batch Size: 11/86 [loss: 0.429029, acc.: 82.67%] [G loss: 0.820987]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 12/86 [loss: 0.441850, acc.: 82.13%] [G loss: 0.860777]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 11/200  Batch Size: 13/86 [loss: 0.456441, acc.: 79.74%] [G loss: 0.859159]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 14/86 [loss: 0.454442, acc.: 80.13%] [G loss: 0.882641]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 15/86 [loss: 0.467568, acc.: 78.47%] [G loss: 0.853224]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 16/86 [loss: 0.441889, acc.: 81.05%] [G loss: 0.866933]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 17/86 [loss: 0.445567, acc.: 81.40%] [G loss: 0.840063]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 18/86 [loss: 0.446297, acc.: 81.30%] [G loss: 0.859177]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 11/200  Batch Size: 19/86 [loss: 0.474276, acc.: 78.86%] [G loss: 0.848757]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 20/86 [loss: 0.436265, acc.: 81.64%] [G loss: 0.849260]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 21/86 [loss: 0.475969, acc.: 79.05%] [G loss: 0.823084]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 22/86 [loss: 0.459112, acc.: 79.98%] [G loss: 0.842785]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 23/86 [loss: 0.477149, acc.: 78.37%] [G loss: 0.864909]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 24/86 [loss: 0.471602, acc.: 78.91%] [G loss: 0.876507]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 25/86 [loss: 0.411460, acc.: 84.28%] [G loss: 0.874517]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 26/86 [loss: 0.438159, acc.: 81.98%] [G loss: 0.886252]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 27/86 [loss: 0.452906, acc.: 80.86%] [G loss: 0.850479]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 28/86 [loss: 0.439284, acc.: 82.62%] [G loss: 0.872429]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 29/86 [loss: 0.477631, acc.: 78.27%] [G loss: 0.866541]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 30/86 [loss: 0.492113, acc.: 77.00%] [G loss: 0.884243]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 31/86 [loss: 0.416270, acc.: 83.50%] [G loss: 0.878369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 32/86 [loss: 0.436623, acc.: 81.35%] [G loss: 0.869914]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 33/86 [loss: 0.453697, acc.: 80.76%] [G loss: 0.875503]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 11/200  Batch Size: 34/86 [loss: 0.483571, acc.: 77.39%] [G loss: 0.852712]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 11/200  Batch Size: 35/86 [loss: 0.433702, acc.: 82.13%] [G loss: 0.860834]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 11/200  Batch Size: 36/86 [loss: 0.470981, acc.: 77.73%] [G loss: 0.844724]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 37/86 [loss: 0.469091, acc.: 79.93%] [G loss: 0.845093]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 38/86 [loss: 0.473120, acc.: 78.61%] [G loss: 0.843192]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 39/86 [loss: 0.460572, acc.: 79.69%] [G loss: 0.866270]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 40/86 [loss: 0.437255, acc.: 82.62%] [G loss: 0.858103]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 41/86 [loss: 0.480568, acc.: 77.88%] [G loss: 0.849134]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 42/86 [loss: 0.459598, acc.: 79.83%] [G loss: 0.857575]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 43/86 [loss: 0.461871, acc.: 80.13%] [G loss: 0.856678]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 44/86 [loss: 0.450490, acc.: 81.40%] [G loss: 0.856459]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 45/86 [loss: 0.443124, acc.: 82.23%] [G loss: 0.857491]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 46/86 [loss: 0.439106, acc.: 82.37%] [G loss: 0.860140]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 47/86 [loss: 0.436099, acc.: 82.32%] [G loss: 0.847093]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 11/200  Batch Size: 48/86 [loss: 0.444501, acc.: 81.88%] [G loss: 0.859309]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 49/86 [loss: 0.455733, acc.: 79.69%] [G loss: 0.822987]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 50/86 [loss: 0.448394, acc.: 81.64%] [G loss: 0.853346]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 51/86 [loss: 0.467352, acc.: 80.03%] [G loss: 0.856254]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 52/86 [loss: 0.423049, acc.: 83.25%] [G loss: 0.860603]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 53/86 [loss: 0.449442, acc.: 80.81%] [G loss: 0.830243]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 54/86 [loss: 0.458676, acc.: 80.57%] [G loss: 0.835005]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 55/86 [loss: 0.461698, acc.: 79.83%] [G loss: 0.854113]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 56/86 [loss: 0.418527, acc.: 82.81%] [G loss: 0.862967]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 11/200  Batch Size: 57/86 [loss: 0.467959, acc.: 79.35%] [G loss: 0.850642]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 58/86 [loss: 0.448387, acc.: 80.37%] [G loss: 0.865779]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 59/86 [loss: 0.461887, acc.: 80.42%] [G loss: 0.874390]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 60/86 [loss: 0.443213, acc.: 80.52%] [G loss: 0.871291]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 61/86 [loss: 0.420077, acc.: 83.30%] [G loss: 0.845740]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 62/86 [loss: 0.448493, acc.: 81.15%] [G loss: 0.873624]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 63/86 [loss: 0.444994, acc.: 81.40%] [G loss: 0.883612]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 64/86 [loss: 0.438631, acc.: 82.76%] [G loss: 0.853163]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 65/86 [loss: 0.431367, acc.: 82.28%] [G loss: 0.882638]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 66/86 [loss: 0.424671, acc.: 82.28%] [G loss: 0.864323]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 67/86 [loss: 0.433477, acc.: 82.81%] [G loss: 0.855706]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 11/200  Batch Size: 68/86 [loss: 0.465808, acc.: 79.35%] [G loss: 0.862363]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 69/86 [loss: 0.448111, acc.: 81.20%] [G loss: 0.842327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 70/86 [loss: 0.425418, acc.: 83.30%] [G loss: 0.866226]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 71/86 [loss: 0.439635, acc.: 81.10%] [G loss: 0.856969]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 11/200  Batch Size: 72/86 [loss: 0.423059, acc.: 84.47%] [G loss: 0.851789]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 11/200  Batch Size: 73/86 [loss: 0.441007, acc.: 81.74%] [G loss: 0.840018]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 74/86 [loss: 0.433250, acc.: 82.28%] [G loss: 0.863370]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 11/200  Batch Size: 75/86 [loss: 0.462431, acc.: 80.27%] [G loss: 0.838850]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 76/86 [loss: 0.437370, acc.: 81.69%] [G loss: 0.842949]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 77/86 [loss: 0.426033, acc.: 82.91%] [G loss: 0.847631]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 78/86 [loss: 0.457578, acc.: 81.05%] [G loss: 0.795115]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 79/86 [loss: 0.421497, acc.: 83.94%] [G loss: 0.856087]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 11/200  Batch Size: 80/86 [loss: 0.454081, acc.: 80.18%] [G loss: 0.851792]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 81/86 [loss: 0.438494, acc.: 82.28%] [G loss: 0.843852]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 11/200  Batch Size: 82/86 [loss: 0.479515, acc.: 78.32%] [G loss: 0.857333]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 11/200  Batch Size: 83/86 [loss: 0.462360, acc.: 78.61%] [G loss: 0.830630]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 84/86 [loss: 0.459150, acc.: 79.88%] [G loss: 0.807077]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 11/200  Batch Size: 85/86 [loss: 0.456406, acc.: 80.08%] [G loss: 0.859073]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 12/200  Batch Size: 0/86 [loss: 0.483575, acc.: 77.39%] [G loss: 0.817510]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 1/86 [loss: 0.449563, acc.: 80.32%] [G loss: 0.868100]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 12/200  Batch Size: 2/86 [loss: 0.452724, acc.: 81.74%] [G loss: 0.853633]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 3/86 [loss: 0.463208, acc.: 79.05%] [G loss: 0.842610]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 4/86 [loss: 0.457919, acc.: 81.69%] [G loss: 0.835210]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 5/86 [loss: 0.449777, acc.: 80.76%] [G loss: 0.872710]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 6/86 [loss: 0.448977, acc.: 81.15%] [G loss: 0.844436]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 7/86 [loss: 0.417082, acc.: 83.98%] [G loss: 0.856810]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 8/86 [loss: 0.434182, acc.: 82.13%] [G loss: 0.852239]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 9/86 [loss: 0.424498, acc.: 82.62%] [G loss: 0.858877]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 10/86 [loss: 0.487251, acc.: 77.93%] [G loss: 0.828357]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 11/86 [loss: 0.470363, acc.: 79.74%] [G loss: 0.840063]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 12/86 [loss: 0.437677, acc.: 82.18%] [G loss: 0.863599]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 12/200  Batch Size: 13/86 [loss: 0.438701, acc.: 82.18%] [G loss: 0.824287]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 14/86 [loss: 0.467001, acc.: 79.00%] [G loss: 0.856038]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 12/200  Batch Size: 15/86 [loss: 0.484814, acc.: 78.08%] [G loss: 0.860943]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 16/86 [loss: 0.459200, acc.: 80.18%] [G loss: 0.832785]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 17/86 [loss: 0.435151, acc.: 82.28%] [G loss: 0.875159]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 18/86 [loss: 0.469695, acc.: 78.61%] [G loss: 0.828416]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 12/200  Batch Size: 19/86 [loss: 0.447215, acc.: 80.66%] [G loss: 0.846701]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 12/200  Batch Size: 20/86 [loss: 0.444623, acc.: 81.20%] [G loss: 0.875851]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 21/86 [loss: 0.442767, acc.: 81.74%] [G loss: 0.840709]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 22/86 [loss: 0.432940, acc.: 81.74%] [G loss: 0.813449]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 23/86 [loss: 0.425635, acc.: 83.01%] [G loss: 0.860752]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 24/86 [loss: 0.463587, acc.: 80.47%] [G loss: 0.860717]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 25/86 [loss: 0.420384, acc.: 82.62%] [G loss: 0.840177]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 12/200  Batch Size: 26/86 [loss: 0.433369, acc.: 82.28%] [G loss: 0.867839]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 27/86 [loss: 0.461951, acc.: 80.08%] [G loss: 0.850267]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 28/86 [loss: 0.456845, acc.: 81.25%] [G loss: 0.862254]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 12/200  Batch Size: 29/86 [loss: 0.473440, acc.: 78.52%] [G loss: 0.832347]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 30/86 [loss: 0.477649, acc.: 79.59%] [G loss: 0.868523]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 31/86 [loss: 0.440091, acc.: 81.64%] [G loss: 0.825974]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 32/86 [loss: 0.432525, acc.: 82.52%] [G loss: 0.806166]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 33/86 [loss: 0.411162, acc.: 84.08%] [G loss: 0.850623]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 34/86 [loss: 0.430311, acc.: 83.40%] [G loss: 0.860707]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 12/200  Batch Size: 35/86 [loss: 0.432317, acc.: 82.13%] [G loss: 0.872187]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 36/86 [loss: 0.420184, acc.: 83.35%] [G loss: 0.855020]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 37/86 [loss: 0.422943, acc.: 83.06%] [G loss: 0.848948]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 38/86 [loss: 0.446568, acc.: 81.64%] [G loss: 0.881577]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 39/86 [loss: 0.440939, acc.: 81.93%] [G loss: 0.864781]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 40/86 [loss: 0.402947, acc.: 85.06%] [G loss: 0.872955]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 41/86 [loss: 0.446507, acc.: 81.35%] [G loss: 0.877949]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 42/86 [loss: 0.418587, acc.: 83.35%] [G loss: 0.843681]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 43/86 [loss: 0.413737, acc.: 83.45%] [G loss: 0.900603]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 44/86 [loss: 0.408163, acc.: 85.35%] [G loss: 0.891005]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 45/86 [loss: 0.432956, acc.: 82.28%] [G loss: 0.861894]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 46/86 [loss: 0.392306, acc.: 85.30%] [G loss: 0.831687]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 47/86 [loss: 0.416572, acc.: 83.64%] [G loss: 0.844452]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 48/86 [loss: 0.412302, acc.: 84.81%] [G loss: 0.862265]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 49/86 [loss: 0.468830, acc.: 78.71%] [G loss: 0.868469]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 12/200  Batch Size: 50/86 [loss: 0.475586, acc.: 79.30%] [G loss: 0.822270]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 51/86 [loss: 0.433947, acc.: 82.81%] [G loss: 0.862960]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 52/86 [loss: 0.445984, acc.: 81.25%] [G loss: 0.852015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 53/86 [loss: 0.435684, acc.: 83.01%] [G loss: 0.845838]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 54/86 [loss: 0.426150, acc.: 82.62%] [G loss: 0.855906]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 55/86 [loss: 0.433893, acc.: 82.76%] [G loss: 0.830279]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 56/86 [loss: 0.443922, acc.: 82.03%] [G loss: 0.874884]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 57/86 [loss: 0.443236, acc.: 82.13%] [G loss: 0.847334]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 58/86 [loss: 0.440897, acc.: 82.42%] [G loss: 0.877200]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 59/86 [loss: 0.402470, acc.: 84.18%] [G loss: 0.852156]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 60/86 [loss: 0.428522, acc.: 82.57%] [G loss: 0.848590]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 61/86 [loss: 0.485602, acc.: 78.42%] [G loss: 0.871526]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 62/86 [loss: 0.425121, acc.: 82.67%] [G loss: 0.905821]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 63/86 [loss: 0.424352, acc.: 83.45%] [G loss: 0.846945]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 12/200  Batch Size: 64/86 [loss: 0.418803, acc.: 83.11%] [G loss: 0.881004]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 65/86 [loss: 0.429671, acc.: 83.20%] [G loss: 0.893341]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 66/86 [loss: 0.443717, acc.: 81.40%] [G loss: 0.849425]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 67/86 [loss: 0.460616, acc.: 80.18%] [G loss: 0.835768]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 12/200  Batch Size: 68/86 [loss: 0.397210, acc.: 85.30%] [G loss: 0.849811]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 69/86 [loss: 0.425343, acc.: 83.01%] [G loss: 0.833336]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 12/200  Batch Size: 70/86 [loss: 0.444478, acc.: 81.74%] [G loss: 0.865382]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 71/86 [loss: 0.403005, acc.: 84.62%] [G loss: 0.866333]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 72/86 [loss: 0.400599, acc.: 84.72%] [G loss: 0.848838]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 73/86 [loss: 0.467250, acc.: 79.69%] [G loss: 0.864662]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 74/86 [loss: 0.440123, acc.: 82.71%] [G loss: 0.841218]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 12/200  Batch Size: 75/86 [loss: 0.434149, acc.: 81.59%] [G loss: 0.847757]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 76/86 [loss: 0.427417, acc.: 83.64%] [G loss: 0.846057]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 12/200  Batch Size: 77/86 [loss: 0.420551, acc.: 83.74%] [G loss: 0.846194]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 78/86 [loss: 0.448839, acc.: 80.62%] [G loss: 0.868669]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 79/86 [loss: 0.448483, acc.: 81.30%] [G loss: 0.837124]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 12/200  Batch Size: 80/86 [loss: 0.445485, acc.: 81.64%] [G loss: 0.850683]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 12/200  Batch Size: 81/86 [loss: 0.432327, acc.: 82.47%] [G loss: 0.842939]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 82/86 [loss: 0.452612, acc.: 80.42%] [G loss: 0.840335]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 83/86 [loss: 0.413029, acc.: 83.89%] [G loss: 0.829207]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 12/200  Batch Size: 84/86 [loss: 0.418109, acc.: 83.74%] [G loss: 0.859574]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 12/200  Batch Size: 85/86 [loss: 0.442221, acc.: 82.23%] [G loss: 0.841872]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 13/200  Batch Size: 0/86 [loss: 0.421324, acc.: 83.25%] [G loss: 0.866029]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 1/86 [loss: 0.437823, acc.: 82.28%] [G loss: 0.815115]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 2/86 [loss: 0.427837, acc.: 83.15%] [G loss: 0.842214]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 3/86 [loss: 0.383506, acc.: 87.16%] [G loss: 0.830637]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 4/86 [loss: 0.432078, acc.: 81.40%] [G loss: 0.863736]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 5/86 [loss: 0.425392, acc.: 83.40%] [G loss: 0.824168]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 6/86 [loss: 0.426003, acc.: 83.11%] [G loss: 0.869927]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 13/200  Batch Size: 7/86 [loss: 0.421013, acc.: 83.01%] [G loss: 0.901398]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 8/86 [loss: 0.459394, acc.: 79.59%] [G loss: 0.866423]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 9/86 [loss: 0.453866, acc.: 80.52%] [G loss: 0.830314]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 10/86 [loss: 0.450901, acc.: 79.88%] [G loss: 0.830303]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 11/86 [loss: 0.441403, acc.: 82.71%] [G loss: 0.830124]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 12/86 [loss: 0.355469, acc.: 88.77%] [G loss: 0.853613]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 13/200  Batch Size: 13/86 [loss: 0.424661, acc.: 83.20%] [G loss: 0.863535]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 13/200  Batch Size: 14/86 [loss: 0.412477, acc.: 83.84%] [G loss: 0.850216]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 15/86 [loss: 0.421088, acc.: 84.18%] [G loss: 0.874443]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 16/86 [loss: 0.407854, acc.: 84.33%] [G loss: 0.855714]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 17/86 [loss: 0.454851, acc.: 81.20%] [G loss: 0.869550]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 18/86 [loss: 0.499828, acc.: 76.22%] [G loss: 0.830902]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 19/86 [loss: 0.404185, acc.: 85.40%] [G loss: 0.886565]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 20/86 [loss: 0.474937, acc.: 78.71%] [G loss: 0.870775]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 21/86 [loss: 0.411196, acc.: 84.42%] [G loss: 0.869046]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 22/86 [loss: 0.421197, acc.: 83.45%] [G loss: 0.841617]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 23/86 [loss: 0.413008, acc.: 83.01%] [G loss: 0.860391]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 24/86 [loss: 0.407934, acc.: 84.08%] [G loss: 0.847247]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 25/86 [loss: 0.412050, acc.: 83.69%] [G loss: 0.854231]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 26/86 [loss: 0.428143, acc.: 82.28%] [G loss: 0.850472]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 27/86 [loss: 0.383527, acc.: 86.82%] [G loss: 0.849495]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 13/200  Batch Size: 28/86 [loss: 0.437088, acc.: 82.18%] [G loss: 0.881679]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 29/86 [loss: 0.419703, acc.: 83.74%] [G loss: 0.863767]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 30/86 [loss: 0.423878, acc.: 84.13%] [G loss: 0.848285]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 31/86 [loss: 0.389401, acc.: 86.62%] [G loss: 0.878183]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 32/86 [loss: 0.414671, acc.: 83.40%] [G loss: 0.853732]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 33/86 [loss: 0.411747, acc.: 84.67%] [G loss: 0.844261]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 34/86 [loss: 0.426579, acc.: 81.98%] [G loss: 0.885348]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 35/86 [loss: 0.405847, acc.: 84.28%] [G loss: 0.877058]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 36/86 [loss: 0.489591, acc.: 77.39%] [G loss: 0.875588]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 37/86 [loss: 0.401082, acc.: 84.72%] [G loss: 0.851960]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 38/86 [loss: 0.422549, acc.: 83.89%] [G loss: 0.863704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 39/86 [loss: 0.454136, acc.: 80.66%] [G loss: 0.856081]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 40/86 [loss: 0.431466, acc.: 81.79%] [G loss: 0.866480]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 41/86 [loss: 0.395974, acc.: 85.35%] [G loss: 0.874738]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 42/86 [loss: 0.403007, acc.: 84.96%] [G loss: 0.869665]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 43/86 [loss: 0.416406, acc.: 84.23%] [G loss: 0.878127]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 44/86 [loss: 0.390156, acc.: 85.99%] [G loss: 0.874404]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 45/86 [loss: 0.401204, acc.: 84.96%] [G loss: 0.875482]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 46/86 [loss: 0.429677, acc.: 82.52%] [G loss: 0.840580]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 47/86 [loss: 0.403593, acc.: 85.35%] [G loss: 0.883603]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 48/86 [loss: 0.404715, acc.: 84.47%] [G loss: 0.850295]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 49/86 [loss: 0.399430, acc.: 85.74%] [G loss: 0.834110]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 50/86 [loss: 0.396709, acc.: 85.74%] [G loss: 0.844870]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 51/86 [loss: 0.420377, acc.: 83.50%] [G loss: 0.852701]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 52/86 [loss: 0.459689, acc.: 79.74%] [G loss: 0.846412]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 53/86 [loss: 0.384077, acc.: 85.64%] [G loss: 0.845125]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 54/86 [loss: 0.421878, acc.: 83.50%] [G loss: 0.868702]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 55/86 [loss: 0.406202, acc.: 84.33%] [G loss: 0.867838]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 56/86 [loss: 0.403412, acc.: 84.77%] [G loss: 0.804334]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 57/86 [loss: 0.401793, acc.: 84.91%] [G loss: 0.888155]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 58/86 [loss: 0.429006, acc.: 82.62%] [G loss: 0.879525]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 59/86 [loss: 0.378478, acc.: 86.28%] [G loss: 0.853261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 60/86 [loss: 0.423673, acc.: 82.81%] [G loss: 0.855077]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 61/86 [loss: 0.417769, acc.: 84.03%] [G loss: 0.835093]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 13/200  Batch Size: 62/86 [loss: 0.407593, acc.: 84.28%] [G loss: 0.837427]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 63/86 [loss: 0.424028, acc.: 83.20%] [G loss: 0.853157]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 64/86 [loss: 0.429315, acc.: 82.57%] [G loss: 0.838621]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 65/86 [loss: 0.435123, acc.: 82.67%] [G loss: 0.885021]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 66/86 [loss: 0.433808, acc.: 82.13%] [G loss: 0.881682]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 13/200  Batch Size: 67/86 [loss: 0.402033, acc.: 84.91%] [G loss: 0.881623]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 68/86 [loss: 0.426867, acc.: 82.71%] [G loss: 0.841010]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 69/86 [loss: 0.421965, acc.: 83.64%] [G loss: 0.847938]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 70/86 [loss: 0.420719, acc.: 83.89%] [G loss: 0.865107]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 71/86 [loss: 0.423329, acc.: 82.81%] [G loss: 0.839464]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 72/86 [loss: 0.414206, acc.: 84.52%] [G loss: 0.841783]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 73/86 [loss: 0.433147, acc.: 83.20%] [G loss: 0.803102]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 13/200  Batch Size: 74/86 [loss: 0.403370, acc.: 84.67%] [G loss: 0.865820]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 75/86 [loss: 0.431732, acc.: 81.40%] [G loss: 0.875754]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 76/86 [loss: 0.432549, acc.: 81.35%] [G loss: 0.864344]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 77/86 [loss: 0.398017, acc.: 86.08%] [G loss: 0.845228]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 78/86 [loss: 0.408921, acc.: 83.98%] [G loss: 0.830480]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 79/86 [loss: 0.405943, acc.: 84.77%] [G loss: 0.859467]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 13/200  Batch Size: 80/86 [loss: 0.418960, acc.: 83.69%] [G loss: 0.870894]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 13/200  Batch Size: 81/86 [loss: 0.417456, acc.: 83.84%] [G loss: 0.863728]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 13/200  Batch Size: 82/86 [loss: 0.417790, acc.: 83.11%] [G loss: 0.885346]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 13/200  Batch Size: 83/86 [loss: 0.388670, acc.: 86.08%] [G loss: 0.844590]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 84/86 [loss: 0.418036, acc.: 83.15%] [G loss: 0.862396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 13/200  Batch Size: 85/86 [loss: 0.420202, acc.: 83.79%] [G loss: 0.877062]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 0/86 [loss: 0.406666, acc.: 84.57%] [G loss: 0.884934]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 1/86 [loss: 0.365915, acc.: 88.23%] [G loss: 0.852775]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 2/86 [loss: 0.420753, acc.: 84.38%] [G loss: 0.862881]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 3/86 [loss: 0.406019, acc.: 85.16%] [G loss: 0.828953]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 4/86 [loss: 0.414142, acc.: 83.94%] [G loss: 0.839802]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 5/86 [loss: 0.382933, acc.: 86.38%] [G loss: 0.882674]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 6/86 [loss: 0.409820, acc.: 84.96%] [G loss: 0.852631]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 7/86 [loss: 0.411103, acc.: 84.81%] [G loss: 0.851760]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 8/86 [loss: 0.400961, acc.: 85.30%] [G loss: 0.882143]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 9/86 [loss: 0.417514, acc.: 84.23%] [G loss: 0.874039]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 14/200  Batch Size: 10/86 [loss: 0.401870, acc.: 85.74%] [G loss: 0.842716]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 11/86 [loss: 0.374959, acc.: 87.65%] [G loss: 0.848502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 12/86 [loss: 0.413453, acc.: 83.45%] [G loss: 0.864550]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 13/86 [loss: 0.398093, acc.: 86.28%] [G loss: 0.908190]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 14/86 [loss: 0.435823, acc.: 82.23%] [G loss: 0.862622]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 15/86 [loss: 0.387544, acc.: 85.30%] [G loss: 0.863917]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 16/86 [loss: 0.386052, acc.: 86.18%] [G loss: 0.871039]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 17/86 [loss: 0.425470, acc.: 83.64%] [G loss: 0.852286]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 18/86 [loss: 0.414745, acc.: 82.71%] [G loss: 0.832636]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 19/86 [loss: 0.348551, acc.: 88.13%] [G loss: 0.849597]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 20/86 [loss: 0.380489, acc.: 86.18%] [G loss: 0.872651]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 21/86 [loss: 0.399414, acc.: 84.96%] [G loss: 0.858515]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 22/86 [loss: 0.410509, acc.: 83.59%] [G loss: 0.832248]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 23/86 [loss: 0.360835, acc.: 88.53%] [G loss: 0.846336]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 24/86 [loss: 0.364387, acc.: 87.50%] [G loss: 0.887575]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 25/86 [loss: 0.418757, acc.: 83.89%] [G loss: 0.845098]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 26/86 [loss: 0.414669, acc.: 84.67%] [G loss: 0.847431]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 27/86 [loss: 0.414585, acc.: 84.62%] [G loss: 0.858688]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 28/86 [loss: 0.400971, acc.: 84.57%] [G loss: 0.857339]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 29/86 [loss: 0.396656, acc.: 84.81%] [G loss: 0.868511]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 30/86 [loss: 0.422076, acc.: 83.54%] [G loss: 0.856570]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 31/86 [loss: 0.406130, acc.: 84.86%] [G loss: 0.898148]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 32/86 [loss: 0.396698, acc.: 85.21%] [G loss: 0.868904]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 33/86 [loss: 0.420761, acc.: 82.37%] [G loss: 0.851700]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 34/86 [loss: 0.428444, acc.: 82.67%] [G loss: 0.878410]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 35/86 [loss: 0.419533, acc.: 84.13%] [G loss: 0.892601]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 36/86 [loss: 0.400132, acc.: 84.18%] [G loss: 0.879890]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 37/86 [loss: 0.405475, acc.: 84.62%] [G loss: 0.847405]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 38/86 [loss: 0.385245, acc.: 85.55%] [G loss: 0.893252]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 39/86 [loss: 0.377628, acc.: 86.04%] [G loss: 0.897103]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 40/86 [loss: 0.374003, acc.: 87.79%] [G loss: 0.849339]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 41/86 [loss: 0.377994, acc.: 87.30%] [G loss: 0.856794]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 42/86 [loss: 0.383028, acc.: 86.96%] [G loss: 0.855093]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 43/86 [loss: 0.379543, acc.: 87.06%] [G loss: 0.842198]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 44/86 [loss: 0.378382, acc.: 86.57%] [G loss: 0.890317]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 45/86 [loss: 0.387441, acc.: 85.45%] [G loss: 0.882556]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 46/86 [loss: 0.376020, acc.: 86.28%] [G loss: 0.870983]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 47/86 [loss: 0.377100, acc.: 87.06%] [G loss: 0.868939]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 48/86 [loss: 0.404195, acc.: 84.91%] [G loss: 0.868781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 49/86 [loss: 0.367693, acc.: 87.94%] [G loss: 0.854599]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 50/86 [loss: 0.391031, acc.: 86.33%] [G loss: 0.828257]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 51/86 [loss: 0.382689, acc.: 86.57%] [G loss: 0.842956]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 52/86 [loss: 0.402885, acc.: 84.62%] [G loss: 0.844583]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 53/86 [loss: 0.379371, acc.: 86.77%] [G loss: 0.854238]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 14/200  Batch Size: 54/86 [loss: 0.404911, acc.: 85.35%] [G loss: 0.858104]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 55/86 [loss: 0.335819, acc.: 89.99%] [G loss: 0.844169]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 56/86 [loss: 0.394174, acc.: 85.45%] [G loss: 0.856123]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 57/86 [loss: 0.407012, acc.: 84.77%] [G loss: 0.855718]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 58/86 [loss: 0.392189, acc.: 86.08%] [G loss: 0.865072]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 59/86 [loss: 0.452918, acc.: 80.52%] [G loss: 0.867262]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 60/86 [loss: 0.419108, acc.: 83.59%] [G loss: 0.852751]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 14/200  Batch Size: 61/86 [loss: 0.376004, acc.: 87.26%] [G loss: 0.831681]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 62/86 [loss: 0.406922, acc.: 84.52%] [G loss: 0.868967]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 63/86 [loss: 0.397948, acc.: 85.79%] [G loss: 0.891368]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 64/86 [loss: 0.382310, acc.: 86.72%] [G loss: 0.870439]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 65/86 [loss: 0.407823, acc.: 84.91%] [G loss: 0.843749]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 66/86 [loss: 0.388226, acc.: 86.04%] [G loss: 0.870992]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 14/200  Batch Size: 67/86 [loss: 0.372830, acc.: 88.28%] [G loss: 0.857689]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 68/86 [loss: 0.372854, acc.: 87.30%] [G loss: 0.848054]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 69/86 [loss: 0.365884, acc.: 87.74%] [G loss: 0.876910]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 70/86 [loss: 0.363538, acc.: 87.84%] [G loss: 0.840057]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 71/86 [loss: 0.397289, acc.: 85.60%] [G loss: 0.870741]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 72/86 [loss: 0.398952, acc.: 85.60%] [G loss: 0.840481]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 73/86 [loss: 0.392868, acc.: 84.67%] [G loss: 0.847410]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 74/86 [loss: 0.436966, acc.: 82.13%] [G loss: 0.891836]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 75/86 [loss: 0.367479, acc.: 87.74%] [G loss: 0.855342]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 76/86 [loss: 0.377465, acc.: 87.26%] [G loss: 0.871034]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 77/86 [loss: 0.406988, acc.: 84.72%] [G loss: 0.866721]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 78/86 [loss: 0.426814, acc.: 82.42%] [G loss: 0.843330]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 14/200  Batch Size: 79/86 [loss: 0.408374, acc.: 84.28%] [G loss: 0.840455]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 14/200  Batch Size: 80/86 [loss: 0.393964, acc.: 85.89%] [G loss: 0.868707]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 81/86 [loss: 0.399872, acc.: 85.40%] [G loss: 0.866060]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 14/200  Batch Size: 82/86 [loss: 0.403239, acc.: 85.01%] [G loss: 0.842519]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 14/200  Batch Size: 83/86 [loss: 0.412658, acc.: 84.28%] [G loss: 0.853445]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 84/86 [loss: 0.419214, acc.: 83.98%] [G loss: 0.847941]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 14/200  Batch Size: 85/86 [loss: 0.388735, acc.: 85.11%] [G loss: 0.896243]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 0/86 [loss: 0.390350, acc.: 84.57%] [G loss: 0.854964]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 1/86 [loss: 0.400419, acc.: 86.23%] [G loss: 0.864366]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 2/86 [loss: 0.416510, acc.: 83.89%] [G loss: 0.833014]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 3/86 [loss: 0.357340, acc.: 88.82%] [G loss: 0.866206]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 4/86 [loss: 0.403612, acc.: 84.67%] [G loss: 0.859500]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 5/86 [loss: 0.390036, acc.: 86.04%] [G loss: 0.879107]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 6/86 [loss: 0.439170, acc.: 82.71%] [G loss: 0.884796]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 7/86 [loss: 0.423041, acc.: 83.98%] [G loss: 0.849199]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 8/86 [loss: 0.406370, acc.: 84.18%] [G loss: 0.835805]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 9/86 [loss: 0.392488, acc.: 85.84%] [G loss: 0.858503]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 10/86 [loss: 0.367491, acc.: 88.18%] [G loss: 0.843192]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 11/86 [loss: 0.383674, acc.: 86.72%] [G loss: 0.864633]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 12/86 [loss: 0.370865, acc.: 87.50%] [G loss: 0.849485]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 13/86 [loss: 0.428071, acc.: 81.98%] [G loss: 0.851919]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 14/86 [loss: 0.382308, acc.: 86.47%] [G loss: 0.844627]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 15/86 [loss: 0.399419, acc.: 84.47%] [G loss: 0.812293]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 16/86 [loss: 0.381301, acc.: 86.82%] [G loss: 0.860731]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 15/200  Batch Size: 17/86 [loss: 0.411711, acc.: 84.28%] [G loss: 0.878559]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 18/86 [loss: 0.431666, acc.: 82.13%] [G loss: 0.872550]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 19/86 [loss: 0.362037, acc.: 88.09%] [G loss: 0.852183]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 20/86 [loss: 0.380245, acc.: 86.82%] [G loss: 0.857811]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 21/86 [loss: 0.370819, acc.: 87.16%] [G loss: 0.860983]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 22/86 [loss: 0.384131, acc.: 85.89%] [G loss: 0.803156]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 23/86 [loss: 0.399156, acc.: 85.35%] [G loss: 0.878933]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 24/86 [loss: 0.428124, acc.: 82.86%] [G loss: 0.841132]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 25/86 [loss: 0.356985, acc.: 87.99%] [G loss: 0.859767]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 26/86 [loss: 0.359497, acc.: 88.23%] [G loss: 0.865845]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 27/86 [loss: 0.414423, acc.: 83.89%] [G loss: 0.871727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 28/86 [loss: 0.380867, acc.: 87.11%] [G loss: 0.869973]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 29/86 [loss: 0.387647, acc.: 84.81%] [G loss: 0.857845]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 30/86 [loss: 0.380692, acc.: 87.16%] [G loss: 0.831763]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 31/86 [loss: 0.370311, acc.: 87.45%] [G loss: 0.860485]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 32/86 [loss: 0.422079, acc.: 83.50%] [G loss: 0.843800]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 33/86 [loss: 0.405856, acc.: 84.67%] [G loss: 0.826770]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 34/86 [loss: 0.385736, acc.: 85.30%] [G loss: 0.826379]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 35/86 [loss: 0.400370, acc.: 85.06%] [G loss: 0.844538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 36/86 [loss: 0.412063, acc.: 83.64%] [G loss: 0.845600]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 37/86 [loss: 0.410733, acc.: 83.98%] [G loss: 0.902846]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 15/200  Batch Size: 38/86 [loss: 0.383147, acc.: 87.35%] [G loss: 0.849608]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 39/86 [loss: 0.379030, acc.: 87.21%] [G loss: 0.852902]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 40/86 [loss: 0.408352, acc.: 84.08%] [G loss: 0.854153]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 41/86 [loss: 0.370543, acc.: 87.40%] [G loss: 0.852531]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 42/86 [loss: 0.384464, acc.: 87.26%] [G loss: 0.822958]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 15/200  Batch Size: 43/86 [loss: 0.394899, acc.: 84.67%] [G loss: 0.856615]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 44/86 [loss: 0.369771, acc.: 87.50%] [G loss: 0.865046]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 45/86 [loss: 0.411657, acc.: 83.20%] [G loss: 0.843480]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 15/200  Batch Size: 46/86 [loss: 0.359419, acc.: 88.33%] [G loss: 0.898480]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 15/200  Batch Size: 47/86 [loss: 0.444474, acc.: 81.49%] [G loss: 0.829756]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 48/86 [loss: 0.392951, acc.: 85.84%] [G loss: 0.865694]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 49/86 [loss: 0.420675, acc.: 83.59%] [G loss: 0.840114]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 50/86 [loss: 0.362609, acc.: 88.87%] [G loss: 0.847658]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 51/86 [loss: 0.353583, acc.: 88.87%] [G loss: 0.867727]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 52/86 [loss: 0.378005, acc.: 87.70%] [G loss: 0.892449]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 53/86 [loss: 0.381764, acc.: 86.72%] [G loss: 0.854964]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 54/86 [loss: 0.395645, acc.: 86.33%] [G loss: 0.851333]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 55/86 [loss: 0.386604, acc.: 85.64%] [G loss: 0.870653]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 56/86 [loss: 0.418282, acc.: 84.18%] [G loss: 0.874612]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 57/86 [loss: 0.414812, acc.: 83.54%] [G loss: 0.845294]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 58/86 [loss: 0.369674, acc.: 86.72%] [G loss: 0.865215]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 59/86 [loss: 0.373182, acc.: 87.50%] [G loss: 0.862252]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 60/86 [loss: 0.415400, acc.: 83.69%] [G loss: 0.832621]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 61/86 [loss: 0.452826, acc.: 80.42%] [G loss: 0.850866]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 62/86 [loss: 0.395822, acc.: 85.40%] [G loss: 0.868578]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 63/86 [loss: 0.373088, acc.: 87.01%] [G loss: 0.860903]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 64/86 [loss: 0.381733, acc.: 86.62%] [G loss: 0.867145]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 65/86 [loss: 0.387552, acc.: 86.57%] [G loss: 0.851531]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 66/86 [loss: 0.396617, acc.: 84.91%] [G loss: 0.832675]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 67/86 [loss: 0.437473, acc.: 83.25%] [G loss: 0.878377]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 68/86 [loss: 0.355526, acc.: 88.23%] [G loss: 0.843164]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 69/86 [loss: 0.427052, acc.: 83.69%] [G loss: 0.860329]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 70/86 [loss: 0.373784, acc.: 87.01%] [G loss: 0.878024]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 15/200  Batch Size: 71/86 [loss: 0.413121, acc.: 83.64%] [G loss: 0.851732]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 72/86 [loss: 0.372054, acc.: 87.35%] [G loss: 0.848262]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 73/86 [loss: 0.348727, acc.: 88.62%] [G loss: 0.845417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 74/86 [loss: 0.370360, acc.: 87.45%] [G loss: 0.848514]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 75/86 [loss: 0.428709, acc.: 82.08%] [G loss: 0.867730]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 76/86 [loss: 0.377523, acc.: 86.96%] [G loss: 0.866386]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 77/86 [loss: 0.371925, acc.: 87.45%] [G loss: 0.880157]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 15/200  Batch Size: 78/86 [loss: 0.374271, acc.: 87.01%] [G loss: 0.845793]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 15/200  Batch Size: 79/86 [loss: 0.424083, acc.: 83.40%] [G loss: 0.820844]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 15/200  Batch Size: 80/86 [loss: 0.387460, acc.: 86.82%] [G loss: 0.846254]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 81/86 [loss: 0.410365, acc.: 84.13%] [G loss: 0.861619]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 15/200  Batch Size: 82/86 [loss: 0.341762, acc.: 89.55%] [G loss: 0.859437]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 15/200  Batch Size: 83/86 [loss: 0.410031, acc.: 84.28%] [G loss: 0.868330]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 84/86 [loss: 0.381279, acc.: 85.74%] [G loss: 0.858531]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 15/200  Batch Size: 85/86 [loss: 0.395516, acc.: 85.94%] [G loss: 0.875903]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 0/86 [loss: 0.356077, acc.: 88.48%] [G loss: 0.878615]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 1/86 [loss: 0.383169, acc.: 86.43%] [G loss: 0.873180]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 2/86 [loss: 0.370535, acc.: 88.13%] [G loss: 0.874177]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 3/86 [loss: 0.429758, acc.: 82.37%] [G loss: 0.871348]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 16/200  Batch Size: 4/86 [loss: 0.365989, acc.: 88.28%] [G loss: 0.819446]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 16/200  Batch Size: 5/86 [loss: 0.399378, acc.: 85.69%] [G loss: 0.856827]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 6/86 [loss: 0.390490, acc.: 85.30%] [G loss: 0.864459]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 7/86 [loss: 0.389413, acc.: 85.55%] [G loss: 0.875632]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 8/86 [loss: 0.401393, acc.: 84.96%] [G loss: 0.858564]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 16/200  Batch Size: 9/86 [loss: 0.364371, acc.: 87.01%] [G loss: 0.871447]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 10/86 [loss: 0.396666, acc.: 84.72%] [G loss: 0.884010]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 11/86 [loss: 0.396939, acc.: 86.38%] [G loss: 0.858124]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 16/200  Batch Size: 12/86 [loss: 0.360871, acc.: 88.09%] [G loss: 0.876901]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 13/86 [loss: 0.413975, acc.: 83.84%] [G loss: 0.857708]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 16/200  Batch Size: 14/86 [loss: 0.367712, acc.: 87.70%] [G loss: 0.831139]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 15/86 [loss: 0.367936, acc.: 87.16%] [G loss: 0.896985]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 16/86 [loss: 0.358951, acc.: 88.82%] [G loss: 0.893180]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 17/86 [loss: 0.361014, acc.: 88.43%] [G loss: 0.852072]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 18/86 [loss: 0.338757, acc.: 89.89%] [G loss: 0.867377]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 16/200  Batch Size: 19/86 [loss: 0.417453, acc.: 83.94%] [G loss: 0.864342]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 20/86 [loss: 0.351405, acc.: 88.77%] [G loss: 0.853553]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 21/86 [loss: 0.365976, acc.: 87.65%] [G loss: 0.844140]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 22/86 [loss: 0.365691, acc.: 87.84%] [G loss: 0.849711]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 16/200  Batch Size: 23/86 [loss: 0.380279, acc.: 87.50%] [G loss: 0.890090]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 16/200  Batch Size: 24/86 [loss: 0.380284, acc.: 86.18%] [G loss: 0.843243]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 16/200  Batch Size: 25/86 [loss: 0.385510, acc.: 86.87%] [G loss: 0.809717]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 26/86 [loss: 0.403175, acc.: 84.91%] [G loss: 0.856132]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 16/200  Batch Size: 27/86 [loss: 0.431306, acc.: 83.35%] [G loss: 0.851240]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 28/86 [loss: 0.392782, acc.: 87.16%] [G loss: 0.869671]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 29/86 [loss: 0.375869, acc.: 85.89%] [G loss: 0.880438]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 30/86 [loss: 0.393407, acc.: 85.69%] [G loss: 0.862516]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 31/86 [loss: 0.360551, acc.: 88.43%] [G loss: 0.875243]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 16/200  Batch Size: 32/86 [loss: 0.441164, acc.: 81.79%] [G loss: 0.842096]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 16/200  Batch Size: 33/86 [loss: 0.412791, acc.: 84.03%] [G loss: 0.844315]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 16/200  Batch Size: 34/86 [loss: 0.414662, acc.: 84.57%] [G loss: 0.851977]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 35/86 [loss: 0.400233, acc.: 85.64%] [G loss: 0.867291]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 36/86 [loss: 0.381309, acc.: 86.96%] [G loss: 0.859834]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 37/86 [loss: 0.381804, acc.: 86.87%] [G loss: 0.867702]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 38/86 [loss: 0.360346, acc.: 87.99%] [G loss: 0.873921]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 39/86 [loss: 0.400412, acc.: 85.69%] [G loss: 0.873756]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 16/200  Batch Size: 40/86 [loss: 0.389808, acc.: 86.23%] [G loss: 0.867288]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 16/200  Batch Size: 41/86 [loss: 0.428030, acc.: 83.45%] [G loss: 0.848748]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 42/86 [loss: 0.386646, acc.: 85.99%] [G loss: 0.859439]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 43/86 [loss: 0.382677, acc.: 85.99%] [G loss: 0.874753]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 44/86 [loss: 0.397048, acc.: 85.89%] [G loss: 0.853652]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 45/86 [loss: 0.375828, acc.: 87.60%] [G loss: 0.845008]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 46/86 [loss: 0.377576, acc.: 87.01%] [G loss: 0.849431]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 47/86 [loss: 0.421349, acc.: 83.45%] [G loss: 0.829118]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 48/86 [loss: 0.373373, acc.: 88.18%] [G loss: 0.850900]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 16/200  Batch Size: 49/86 [loss: 0.338261, acc.: 89.94%] [G loss: 0.864056]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 50/86 [loss: 0.401418, acc.: 85.60%] [G loss: 0.878984]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 51/86 [loss: 0.415389, acc.: 83.01%] [G loss: 0.858805]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 52/86 [loss: 0.384367, acc.: 85.99%] [G loss: 0.839476]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 16/200  Batch Size: 53/86 [loss: 0.364561, acc.: 88.09%] [G loss: 0.870687]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 54/86 [loss: 0.405413, acc.: 85.69%] [G loss: 0.883972]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 16/200  Batch Size: 55/86 [loss: 0.408023, acc.: 84.03%] [G loss: 0.876457]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 16/200  Batch Size: 56/86 [loss: 0.400984, acc.: 84.96%] [G loss: 0.825776]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 57/86 [loss: 0.386028, acc.: 86.52%] [G loss: 0.836807]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 58/86 [loss: 0.389997, acc.: 85.01%] [G loss: 0.843895]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 59/86 [loss: 0.386685, acc.: 85.74%] [G loss: 0.839597]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 60/86 [loss: 0.391514, acc.: 85.55%] [G loss: 0.842467]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 61/86 [loss: 0.381126, acc.: 85.99%] [G loss: 0.817931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 62/86 [loss: 0.391159, acc.: 85.89%] [G loss: 0.841451]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 63/86 [loss: 0.395758, acc.: 85.60%] [G loss: 0.890564]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 64/86 [loss: 0.438666, acc.: 81.84%] [G loss: 0.887739]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 65/86 [loss: 0.371259, acc.: 87.74%] [G loss: 0.844562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 66/86 [loss: 0.409769, acc.: 84.52%] [G loss: 0.881360]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 67/86 [loss: 0.365324, acc.: 87.26%] [G loss: 0.858349]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 68/86 [loss: 0.423814, acc.: 82.47%] [G loss: 0.833605]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 69/86 [loss: 0.383367, acc.: 85.79%] [G loss: 0.869141]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 70/86 [loss: 0.378764, acc.: 86.72%] [G loss: 0.870291]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 71/86 [loss: 0.415726, acc.: 83.50%] [G loss: 0.841135]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 72/86 [loss: 0.382691, acc.: 86.28%] [G loss: 0.856759]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 73/86 [loss: 0.434140, acc.: 82.47%] [G loss: 0.849696]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 74/86 [loss: 0.404855, acc.: 85.60%] [G loss: 0.843852]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 75/86 [loss: 0.385882, acc.: 86.08%] [G loss: 0.867477]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 76/86 [loss: 0.364751, acc.: 87.45%] [G loss: 0.858630]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 77/86 [loss: 0.418273, acc.: 84.03%] [G loss: 0.848695]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 78/86 [loss: 0.391813, acc.: 86.72%] [G loss: 0.862949]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 79/86 [loss: 0.371912, acc.: 86.87%] [G loss: 0.867393]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 80/86 [loss: 0.382850, acc.: 86.23%] [G loss: 0.862215]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 81/86 [loss: 0.406992, acc.: 85.74%] [G loss: 0.861659]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 82/86 [loss: 0.329576, acc.: 90.87%] [G loss: 0.862149]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 83/86 [loss: 0.370022, acc.: 87.89%] [G loss: 0.881327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 84/86 [loss: 0.362556, acc.: 88.48%] [G loss: 0.881429]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 16/200  Batch Size: 85/86 [loss: 0.365667, acc.: 88.43%] [G loss: 0.863321]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 0/86 [loss: 0.364880, acc.: 87.74%] [G loss: 0.857985]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 1/86 [loss: 0.360776, acc.: 88.23%] [G loss: 0.858501]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 2/86 [loss: 0.363327, acc.: 87.60%] [G loss: 0.851892]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 3/86 [loss: 0.350631, acc.: 89.60%] [G loss: 0.885668]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 4/86 [loss: 0.400571, acc.: 84.13%] [G loss: 0.866863]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 5/86 [loss: 0.400300, acc.: 85.21%] [G loss: 0.867806]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 6/86 [loss: 0.378242, acc.: 87.01%] [G loss: 0.830624]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 7/86 [loss: 0.363623, acc.: 87.65%] [G loss: 0.844395]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 17/200  Batch Size: 8/86 [loss: 0.380174, acc.: 86.67%] [G loss: 0.861258]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 9/86 [loss: 0.401146, acc.: 85.69%] [G loss: 0.888946]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 10/86 [loss: 0.398207, acc.: 85.55%] [G loss: 0.857532]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 11/86 [loss: 0.376793, acc.: 86.62%] [G loss: 0.867059]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 12/86 [loss: 0.414689, acc.: 83.84%] [G loss: 0.842984]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 17/200  Batch Size: 13/86 [loss: 0.351961, acc.: 89.84%] [G loss: 0.834248]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 17/200  Batch Size: 14/86 [loss: 0.392067, acc.: 86.04%] [G loss: 0.874164]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 17/200  Batch Size: 15/86 [loss: 0.382958, acc.: 86.38%] [G loss: 0.867469]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 16/86 [loss: 0.375625, acc.: 86.43%] [G loss: 0.864375]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 17/86 [loss: 0.390040, acc.: 85.79%] [G loss: 0.864674]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 18/86 [loss: 0.372501, acc.: 87.65%] [G loss: 0.860126]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 17/200  Batch Size: 19/86 [loss: 0.394328, acc.: 85.25%] [G loss: 0.870080]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 17/200  Batch Size: 20/86 [loss: 0.375761, acc.: 87.01%] [G loss: 0.874018]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 21/86 [loss: 0.381533, acc.: 86.43%] [G loss: 0.883867]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 22/86 [loss: 0.370005, acc.: 87.89%] [G loss: 0.891771]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 23/86 [loss: 0.379632, acc.: 86.67%] [G loss: 0.853201]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 24/86 [loss: 0.387229, acc.: 87.35%] [G loss: 0.876984]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 25/86 [loss: 0.409531, acc.: 84.23%] [G loss: 0.871497]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 17/200  Batch Size: 26/86 [loss: 0.385421, acc.: 86.08%] [G loss: 0.875467]\n",
      "32/32 [==============================] - 0s 8ms/step\n",
      "Epoch: 17/200  Batch Size: 27/86 [loss: 0.372173, acc.: 88.13%] [G loss: 0.881317]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 28/86 [loss: 0.385122, acc.: 85.40%] [G loss: 0.838556]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 29/86 [loss: 0.350204, acc.: 88.38%] [G loss: 0.855781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 30/86 [loss: 0.401461, acc.: 84.86%] [G loss: 0.877850]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 31/86 [loss: 0.376535, acc.: 86.67%] [G loss: 0.871729]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 32/86 [loss: 0.380932, acc.: 87.16%] [G loss: 0.868337]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 33/86 [loss: 0.454877, acc.: 81.05%] [G loss: 0.829863]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 34/86 [loss: 0.371369, acc.: 87.35%] [G loss: 0.858213]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 35/86 [loss: 0.385392, acc.: 86.62%] [G loss: 0.861095]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 36/86 [loss: 0.364767, acc.: 87.94%] [G loss: 0.860958]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 37/86 [loss: 0.352837, acc.: 89.01%] [G loss: 0.853013]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 38/86 [loss: 0.394112, acc.: 85.40%] [G loss: 0.855863]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 39/86 [loss: 0.382320, acc.: 86.87%] [G loss: 0.841585]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 40/86 [loss: 0.418436, acc.: 83.20%] [G loss: 0.883901]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 41/86 [loss: 0.359184, acc.: 89.21%] [G loss: 0.849449]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 42/86 [loss: 0.413020, acc.: 83.54%] [G loss: 0.912694]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 43/86 [loss: 0.402488, acc.: 85.11%] [G loss: 0.874146]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 44/86 [loss: 0.384988, acc.: 86.33%] [G loss: 0.878841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 45/86 [loss: 0.394436, acc.: 86.04%] [G loss: 0.876664]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 46/86 [loss: 0.389077, acc.: 86.23%] [G loss: 0.909150]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 47/86 [loss: 0.327984, acc.: 90.38%] [G loss: 0.897874]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 48/86 [loss: 0.384822, acc.: 86.91%] [G loss: 0.825309]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 49/86 [loss: 0.346670, acc.: 89.40%] [G loss: 0.871338]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 50/86 [loss: 0.377977, acc.: 87.26%] [G loss: 0.862518]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 51/86 [loss: 0.421894, acc.: 84.18%] [G loss: 0.886114]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 52/86 [loss: 0.392169, acc.: 86.87%] [G loss: 0.911643]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 53/86 [loss: 0.392960, acc.: 84.96%] [G loss: 0.876874]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 54/86 [loss: 0.404481, acc.: 84.77%] [G loss: 0.859658]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 55/86 [loss: 0.392418, acc.: 85.55%] [G loss: 0.843857]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 56/86 [loss: 0.384314, acc.: 86.72%] [G loss: 0.869463]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 57/86 [loss: 0.406855, acc.: 85.21%] [G loss: 0.901082]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 58/86 [loss: 0.404550, acc.: 85.30%] [G loss: 0.864846]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 59/86 [loss: 0.352941, acc.: 88.57%] [G loss: 0.888585]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 60/86 [loss: 0.383017, acc.: 86.91%] [G loss: 0.850266]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 61/86 [loss: 0.374185, acc.: 87.01%] [G loss: 0.900703]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 62/86 [loss: 0.388752, acc.: 86.28%] [G loss: 0.863329]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 63/86 [loss: 0.365346, acc.: 88.82%] [G loss: 0.881635]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 64/86 [loss: 0.405184, acc.: 85.45%] [G loss: 0.880444]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 65/86 [loss: 0.406145, acc.: 84.62%] [G loss: 0.898548]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 66/86 [loss: 0.410561, acc.: 84.86%] [G loss: 0.889022]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 67/86 [loss: 0.391471, acc.: 86.13%] [G loss: 0.892278]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 68/86 [loss: 0.408094, acc.: 84.47%] [G loss: 0.863298]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 69/86 [loss: 0.413403, acc.: 83.01%] [G loss: 0.839625]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 70/86 [loss: 0.434975, acc.: 81.93%] [G loss: 0.832927]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 17/200  Batch Size: 71/86 [loss: 0.341308, acc.: 89.40%] [G loss: 0.885830]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 72/86 [loss: 0.390942, acc.: 85.74%] [G loss: 0.891410]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 73/86 [loss: 0.368173, acc.: 87.55%] [G loss: 0.933568]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 74/86 [loss: 0.378847, acc.: 87.45%] [G loss: 0.899812]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 75/86 [loss: 0.436420, acc.: 83.30%] [G loss: 0.865677]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 76/86 [loss: 0.419743, acc.: 83.30%] [G loss: 0.864257]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 77/86 [loss: 0.362556, acc.: 87.50%] [G loss: 0.866724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 78/86 [loss: 0.343031, acc.: 88.92%] [G loss: 0.864203]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 79/86 [loss: 0.393403, acc.: 85.40%] [G loss: 0.867312]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 80/86 [loss: 0.429147, acc.: 82.86%] [G loss: 0.881199]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 81/86 [loss: 0.385244, acc.: 86.72%] [G loss: 0.843110]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 82/86 [loss: 0.437008, acc.: 82.32%] [G loss: 0.846651]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 17/200  Batch Size: 83/86 [loss: 0.396852, acc.: 85.45%] [G loss: 0.873610]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 17/200  Batch Size: 84/86 [loss: 0.374503, acc.: 87.40%] [G loss: 0.877768]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 17/200  Batch Size: 85/86 [loss: 0.428953, acc.: 83.45%] [G loss: 0.839019]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 0/86 [loss: 0.372110, acc.: 86.72%] [G loss: 0.860488]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 1/86 [loss: 0.363675, acc.: 88.96%] [G loss: 0.818432]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 2/86 [loss: 0.393272, acc.: 84.81%] [G loss: 0.853695]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 3/86 [loss: 0.391329, acc.: 86.67%] [G loss: 0.858574]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 4/86 [loss: 0.365007, acc.: 87.84%] [G loss: 0.862203]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 5/86 [loss: 0.415887, acc.: 83.94%] [G loss: 0.862499]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 6/86 [loss: 0.391713, acc.: 86.13%] [G loss: 0.855536]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 7/86 [loss: 0.362276, acc.: 88.82%] [G loss: 0.855567]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 8/86 [loss: 0.346404, acc.: 88.62%] [G loss: 0.855024]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 9/86 [loss: 0.389278, acc.: 86.43%] [G loss: 0.840046]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 10/86 [loss: 0.395142, acc.: 85.89%] [G loss: 0.879245]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 11/86 [loss: 0.378958, acc.: 87.45%] [G loss: 0.873082]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 12/86 [loss: 0.416594, acc.: 83.84%] [G loss: 0.871860]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 13/86 [loss: 0.378854, acc.: 86.28%] [G loss: 0.900634]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 14/86 [loss: 0.423371, acc.: 82.76%] [G loss: 0.885938]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 15/86 [loss: 0.416177, acc.: 83.54%] [G loss: 0.855159]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 16/86 [loss: 0.396703, acc.: 85.89%] [G loss: 0.883738]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 17/86 [loss: 0.384477, acc.: 86.62%] [G loss: 0.895679]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 18/86 [loss: 0.424211, acc.: 82.62%] [G loss: 0.868718]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 19/86 [loss: 0.399689, acc.: 85.69%] [G loss: 0.877848]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 20/86 [loss: 0.423953, acc.: 83.45%] [G loss: 0.877932]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 21/86 [loss: 0.412113, acc.: 84.77%] [G loss: 0.891216]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 22/86 [loss: 0.400912, acc.: 84.62%] [G loss: 0.889015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 23/86 [loss: 0.347855, acc.: 88.77%] [G loss: 0.896567]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 24/86 [loss: 0.377065, acc.: 86.91%] [G loss: 0.868308]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 25/86 [loss: 0.383248, acc.: 86.77%] [G loss: 0.887477]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 26/86 [loss: 0.390167, acc.: 86.04%] [G loss: 0.819624]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 27/86 [loss: 0.368347, acc.: 88.72%] [G loss: 0.863484]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 28/86 [loss: 0.390648, acc.: 85.89%] [G loss: 0.864942]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 29/86 [loss: 0.432953, acc.: 83.11%] [G loss: 0.830902]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 30/86 [loss: 0.367079, acc.: 88.04%] [G loss: 0.852427]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 31/86 [loss: 0.369977, acc.: 88.04%] [G loss: 0.850954]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 32/86 [loss: 0.390781, acc.: 86.23%] [G loss: 0.838055]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 33/86 [loss: 0.422172, acc.: 83.30%] [G loss: 0.846023]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 34/86 [loss: 0.386855, acc.: 86.67%] [G loss: 0.890490]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 35/86 [loss: 0.369424, acc.: 87.06%] [G loss: 0.887702]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 36/86 [loss: 0.426463, acc.: 83.15%] [G loss: 0.856579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 37/86 [loss: 0.405659, acc.: 85.11%] [G loss: 0.853197]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 38/86 [loss: 0.391430, acc.: 86.13%] [G loss: 0.864505]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 39/86 [loss: 0.419275, acc.: 84.18%] [G loss: 0.852177]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 40/86 [loss: 0.383352, acc.: 87.70%] [G loss: 0.851005]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 41/86 [loss: 0.414058, acc.: 83.98%] [G loss: 0.855947]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 42/86 [loss: 0.396451, acc.: 85.84%] [G loss: 0.849214]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 43/86 [loss: 0.375320, acc.: 86.23%] [G loss: 0.863104]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 44/86 [loss: 0.414061, acc.: 84.28%] [G loss: 0.870467]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 45/86 [loss: 0.384158, acc.: 86.28%] [G loss: 0.841544]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 46/86 [loss: 0.397868, acc.: 85.16%] [G loss: 0.852607]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 47/86 [loss: 0.415898, acc.: 84.08%] [G loss: 0.832170]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 48/86 [loss: 0.365374, acc.: 87.45%] [G loss: 0.885685]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 49/86 [loss: 0.389902, acc.: 85.79%] [G loss: 0.867497]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 50/86 [loss: 0.374549, acc.: 87.70%] [G loss: 0.848886]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 51/86 [loss: 0.368723, acc.: 87.50%] [G loss: 0.861767]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 52/86 [loss: 0.380786, acc.: 86.43%] [G loss: 0.857810]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 53/86 [loss: 0.336774, acc.: 89.89%] [G loss: 0.890664]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 54/86 [loss: 0.441733, acc.: 81.79%] [G loss: 0.881691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 55/86 [loss: 0.360936, acc.: 88.28%] [G loss: 0.852464]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 56/86 [loss: 0.421099, acc.: 84.03%] [G loss: 0.873260]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 57/86 [loss: 0.417377, acc.: 84.13%] [G loss: 0.886677]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 58/86 [loss: 0.397961, acc.: 85.30%] [G loss: 0.843108]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 59/86 [loss: 0.389715, acc.: 86.08%] [G loss: 0.847006]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 60/86 [loss: 0.408482, acc.: 84.57%] [G loss: 0.851107]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 61/86 [loss: 0.398810, acc.: 85.50%] [G loss: 0.865125]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 62/86 [loss: 0.418576, acc.: 83.20%] [G loss: 0.826670]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 63/86 [loss: 0.396314, acc.: 85.84%] [G loss: 0.859677]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 64/86 [loss: 0.405001, acc.: 85.25%] [G loss: 0.870385]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 65/86 [loss: 0.388946, acc.: 85.69%] [G loss: 0.868656]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 66/86 [loss: 0.401525, acc.: 85.55%] [G loss: 0.835622]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 67/86 [loss: 0.410588, acc.: 84.72%] [G loss: 0.866453]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 68/86 [loss: 0.393765, acc.: 85.45%] [G loss: 0.815973]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 69/86 [loss: 0.399746, acc.: 85.01%] [G loss: 0.885943]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 70/86 [loss: 0.360458, acc.: 88.18%] [G loss: 0.868126]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 71/86 [loss: 0.380948, acc.: 85.99%] [G loss: 0.857050]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 72/86 [loss: 0.354815, acc.: 89.31%] [G loss: 0.916854]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 73/86 [loss: 0.437260, acc.: 82.23%] [G loss: 0.870081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 74/86 [loss: 0.392581, acc.: 86.08%] [G loss: 0.880717]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 75/86 [loss: 0.377098, acc.: 88.18%] [G loss: 0.867996]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 76/86 [loss: 0.413334, acc.: 84.08%] [G loss: 0.866446]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 77/86 [loss: 0.388611, acc.: 86.82%] [G loss: 0.870380]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 78/86 [loss: 0.395034, acc.: 85.84%] [G loss: 0.867631]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 18/200  Batch Size: 79/86 [loss: 0.378862, acc.: 86.57%] [G loss: 0.861858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 80/86 [loss: 0.332623, acc.: 90.58%] [G loss: 0.865573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 81/86 [loss: 0.411694, acc.: 83.84%] [G loss: 0.862734]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 82/86 [loss: 0.398780, acc.: 85.60%] [G loss: 0.859294]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 18/200  Batch Size: 83/86 [loss: 0.354346, acc.: 89.21%] [G loss: 0.870145]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 18/200  Batch Size: 84/86 [loss: 0.354826, acc.: 88.04%] [G loss: 0.850092]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 18/200  Batch Size: 85/86 [loss: 0.356471, acc.: 89.26%] [G loss: 0.895365]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 0/86 [loss: 0.369458, acc.: 88.33%] [G loss: 0.901059]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 1/86 [loss: 0.368349, acc.: 87.65%] [G loss: 0.855546]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 2/86 [loss: 0.428262, acc.: 82.47%] [G loss: 0.858077]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 3/86 [loss: 0.378587, acc.: 86.28%] [G loss: 0.849422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 4/86 [loss: 0.424077, acc.: 83.64%] [G loss: 0.837373]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 5/86 [loss: 0.430249, acc.: 83.06%] [G loss: 0.854170]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 6/86 [loss: 0.385194, acc.: 87.55%] [G loss: 0.855327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 7/86 [loss: 0.414111, acc.: 84.57%] [G loss: 0.857697]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 8/86 [loss: 0.396863, acc.: 85.45%] [G loss: 0.846774]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 9/86 [loss: 0.375501, acc.: 87.01%] [G loss: 0.888691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 10/86 [loss: 0.404054, acc.: 84.91%] [G loss: 0.914405]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 11/86 [loss: 0.397889, acc.: 85.25%] [G loss: 0.885883]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 12/86 [loss: 0.402962, acc.: 85.40%] [G loss: 0.900923]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 13/86 [loss: 0.348838, acc.: 89.16%] [G loss: 0.895323]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 14/86 [loss: 0.408996, acc.: 84.86%] [G loss: 0.850236]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 15/86 [loss: 0.421620, acc.: 83.79%] [G loss: 0.821767]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 16/86 [loss: 0.392266, acc.: 86.18%] [G loss: 0.878984]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 17/86 [loss: 0.390844, acc.: 86.08%] [G loss: 0.893855]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 18/86 [loss: 0.374990, acc.: 87.89%] [G loss: 0.843446]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 19/86 [loss: 0.349194, acc.: 89.06%] [G loss: 0.921336]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 20/86 [loss: 0.370314, acc.: 87.55%] [G loss: 0.847495]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 21/86 [loss: 0.432434, acc.: 82.18%] [G loss: 0.876841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 22/86 [loss: 0.377924, acc.: 86.82%] [G loss: 0.839888]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 23/86 [loss: 0.330361, acc.: 90.62%] [G loss: 0.884760]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 24/86 [loss: 0.368811, acc.: 87.30%] [G loss: 0.861044]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 25/86 [loss: 0.371813, acc.: 87.30%] [G loss: 0.880986]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 26/86 [loss: 0.404272, acc.: 85.45%] [G loss: 0.857780]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 27/86 [loss: 0.346855, acc.: 89.26%] [G loss: 0.841110]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 28/86 [loss: 0.346770, acc.: 89.50%] [G loss: 0.887126]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 29/86 [loss: 0.384852, acc.: 86.43%] [G loss: 0.845730]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 30/86 [loss: 0.346149, acc.: 88.87%] [G loss: 0.891700]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 31/86 [loss: 0.440102, acc.: 81.69%] [G loss: 0.923886]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 32/86 [loss: 0.398149, acc.: 85.06%] [G loss: 0.866015]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 33/86 [loss: 0.394569, acc.: 85.64%] [G loss: 0.878723]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 34/86 [loss: 0.410346, acc.: 84.13%] [G loss: 0.844679]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 35/86 [loss: 0.404582, acc.: 85.16%] [G loss: 0.858188]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 36/86 [loss: 0.449055, acc.: 80.66%] [G loss: 0.853772]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 37/86 [loss: 0.324932, acc.: 91.31%] [G loss: 0.846942]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 38/86 [loss: 0.388213, acc.: 85.94%] [G loss: 0.864409]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 39/86 [loss: 0.381124, acc.: 86.47%] [G loss: 0.881511]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 40/86 [loss: 0.357170, acc.: 87.79%] [G loss: 0.921388]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 41/86 [loss: 0.424994, acc.: 83.74%] [G loss: 0.882358]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 42/86 [loss: 0.365084, acc.: 88.48%] [G loss: 0.876738]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 43/86 [loss: 0.384046, acc.: 87.26%] [G loss: 0.849779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 44/86 [loss: 0.376303, acc.: 87.16%] [G loss: 0.870771]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 45/86 [loss: 0.378511, acc.: 86.38%] [G loss: 0.855906]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 46/86 [loss: 0.427819, acc.: 83.06%] [G loss: 0.855231]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 47/86 [loss: 0.362766, acc.: 88.38%] [G loss: 0.859186]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 48/86 [loss: 0.418419, acc.: 83.79%] [G loss: 0.849963]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 49/86 [loss: 0.399226, acc.: 84.91%] [G loss: 0.888256]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 50/86 [loss: 0.360137, acc.: 88.92%] [G loss: 0.913490]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 51/86 [loss: 0.339792, acc.: 89.70%] [G loss: 0.876804]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 52/86 [loss: 0.424994, acc.: 83.25%] [G loss: 0.863564]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 53/86 [loss: 0.377350, acc.: 87.35%] [G loss: 0.859750]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 54/86 [loss: 0.379901, acc.: 86.77%] [G loss: 0.880162]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 55/86 [loss: 0.447026, acc.: 81.74%] [G loss: 0.881505]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 56/86 [loss: 0.409689, acc.: 84.52%] [G loss: 0.857251]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 57/86 [loss: 0.430487, acc.: 83.40%] [G loss: 0.856777]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 58/86 [loss: 0.407829, acc.: 83.98%] [G loss: 0.857704]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 59/86 [loss: 0.388612, acc.: 85.64%] [G loss: 0.858337]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 60/86 [loss: 0.392245, acc.: 86.04%] [G loss: 0.854121]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 61/86 [loss: 0.444844, acc.: 81.64%] [G loss: 0.841256]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 62/86 [loss: 0.408893, acc.: 83.89%] [G loss: 0.828157]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 63/86 [loss: 0.360146, acc.: 88.38%] [G loss: 0.869999]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 64/86 [loss: 0.368059, acc.: 87.84%] [G loss: 0.882373]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 65/86 [loss: 0.375956, acc.: 86.96%] [G loss: 0.877499]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 66/86 [loss: 0.391272, acc.: 85.45%] [G loss: 0.878733]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 67/86 [loss: 0.378307, acc.: 87.21%] [G loss: 0.878053]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 68/86 [loss: 0.422113, acc.: 83.50%] [G loss: 0.858162]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 69/86 [loss: 0.349217, acc.: 90.19%] [G loss: 0.854807]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 70/86 [loss: 0.412356, acc.: 84.08%] [G loss: 0.851748]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 71/86 [loss: 0.354289, acc.: 89.21%] [G loss: 0.882684]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 72/86 [loss: 0.410943, acc.: 84.23%] [G loss: 0.833936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 73/86 [loss: 0.424297, acc.: 83.20%] [G loss: 0.855905]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 74/86 [loss: 0.344815, acc.: 89.60%] [G loss: 0.866417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 75/86 [loss: 0.297308, acc.: 92.48%] [G loss: 0.867785]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 76/86 [loss: 0.384326, acc.: 86.04%] [G loss: 0.850526]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 77/86 [loss: 0.366913, acc.: 88.43%] [G loss: 0.892981]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 19/200  Batch Size: 78/86 [loss: 0.428391, acc.: 82.71%] [G loss: 0.873618]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 19/200  Batch Size: 79/86 [loss: 0.342783, acc.: 89.31%] [G loss: 0.829066]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 19/200  Batch Size: 80/86 [loss: 0.424988, acc.: 83.69%] [G loss: 0.879820]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 81/86 [loss: 0.386937, acc.: 86.77%] [G loss: 0.862697]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 82/86 [loss: 0.412526, acc.: 84.52%] [G loss: 0.852369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 19/200  Batch Size: 83/86 [loss: 0.442672, acc.: 80.96%] [G loss: 0.855798]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 19/200  Batch Size: 84/86 [loss: 0.394443, acc.: 85.50%] [G loss: 0.864915]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 19/200  Batch Size: 85/86 [loss: 0.400444, acc.: 85.60%] [G loss: 0.858667]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 0/86 [loss: 0.378808, acc.: 86.96%] [G loss: 0.892710]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 1/86 [loss: 0.354928, acc.: 89.16%] [G loss: 0.893622]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 2/86 [loss: 0.362434, acc.: 87.84%] [G loss: 0.902537]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 3/86 [loss: 0.439255, acc.: 82.47%] [G loss: 0.844015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 4/86 [loss: 0.397524, acc.: 85.79%] [G loss: 0.862054]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 5/86 [loss: 0.370996, acc.: 88.13%] [G loss: 0.864796]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 6/86 [loss: 0.351795, acc.: 89.16%] [G loss: 0.895952]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 7/86 [loss: 0.379975, acc.: 86.52%] [G loss: 0.887750]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 8/86 [loss: 0.375744, acc.: 87.65%] [G loss: 0.851648]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 9/86 [loss: 0.397294, acc.: 86.28%] [G loss: 0.851732]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 10/86 [loss: 0.375959, acc.: 86.91%] [G loss: 0.861638]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 11/86 [loss: 0.375309, acc.: 86.77%] [G loss: 0.865566]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 12/86 [loss: 0.373458, acc.: 87.55%] [G loss: 0.819549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 13/86 [loss: 0.385573, acc.: 86.57%] [G loss: 0.840067]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 14/86 [loss: 0.334676, acc.: 89.75%] [G loss: 0.843457]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 15/86 [loss: 0.393498, acc.: 86.77%] [G loss: 0.823859]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 16/86 [loss: 0.381954, acc.: 85.89%] [G loss: 0.872756]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 17/86 [loss: 0.379041, acc.: 86.77%] [G loss: 0.895546]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 18/86 [loss: 0.373371, acc.: 87.50%] [G loss: 0.852759]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 19/86 [loss: 0.388584, acc.: 86.57%] [G loss: 0.856681]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 20/86 [loss: 0.431900, acc.: 82.76%] [G loss: 0.838488]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 21/86 [loss: 0.367505, acc.: 87.79%] [G loss: 0.857430]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 22/86 [loss: 0.400527, acc.: 85.45%] [G loss: 0.864153]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 23/86 [loss: 0.389377, acc.: 86.18%] [G loss: 0.861560]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 24/86 [loss: 0.390275, acc.: 86.62%] [G loss: 0.865328]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 25/86 [loss: 0.379784, acc.: 86.67%] [G loss: 0.877312]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 26/86 [loss: 0.399786, acc.: 84.81%] [G loss: 0.847991]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 27/86 [loss: 0.383966, acc.: 86.57%] [G loss: 0.871242]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 28/86 [loss: 0.399091, acc.: 85.35%] [G loss: 0.879832]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 29/86 [loss: 0.339264, acc.: 89.75%] [G loss: 0.888095]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 30/86 [loss: 0.404981, acc.: 84.86%] [G loss: 0.852462]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 31/86 [loss: 0.373168, acc.: 87.11%] [G loss: 0.865286]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 32/86 [loss: 0.395013, acc.: 85.79%] [G loss: 0.848410]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 33/86 [loss: 0.368619, acc.: 88.13%] [G loss: 0.845261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 34/86 [loss: 0.376553, acc.: 86.87%] [G loss: 0.829897]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 35/86 [loss: 0.401314, acc.: 84.86%] [G loss: 0.874478]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 36/86 [loss: 0.397519, acc.: 85.50%] [G loss: 0.871633]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 37/86 [loss: 0.351341, acc.: 89.06%] [G loss: 0.894632]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 38/86 [loss: 0.377582, acc.: 86.96%] [G loss: 0.837721]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 39/86 [loss: 0.377826, acc.: 86.38%] [G loss: 0.886267]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 40/86 [loss: 0.358500, acc.: 88.48%] [G loss: 0.899425]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 41/86 [loss: 0.397018, acc.: 85.60%] [G loss: 0.886993]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 42/86 [loss: 0.443821, acc.: 81.98%] [G loss: 0.843879]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 43/86 [loss: 0.431807, acc.: 83.06%] [G loss: 0.888779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 44/86 [loss: 0.399594, acc.: 85.01%] [G loss: 0.847578]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 45/86 [loss: 0.373576, acc.: 87.30%] [G loss: 0.894809]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 46/86 [loss: 0.381915, acc.: 87.50%] [G loss: 0.872664]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 47/86 [loss: 0.402495, acc.: 85.06%] [G loss: 0.874909]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 48/86 [loss: 0.370401, acc.: 87.70%] [G loss: 0.873565]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 49/86 [loss: 0.375486, acc.: 87.45%] [G loss: 0.893494]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 50/86 [loss: 0.366765, acc.: 88.13%] [G loss: 0.857574]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 51/86 [loss: 0.336628, acc.: 89.94%] [G loss: 0.853710]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 52/86 [loss: 0.395964, acc.: 86.13%] [G loss: 0.852819]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 53/86 [loss: 0.382090, acc.: 87.35%] [G loss: 0.866131]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 54/86 [loss: 0.383913, acc.: 85.79%] [G loss: 0.861089]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 55/86 [loss: 0.385592, acc.: 86.04%] [G loss: 0.866047]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 56/86 [loss: 0.429219, acc.: 82.67%] [G loss: 0.877010]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 57/86 [loss: 0.412917, acc.: 83.84%] [G loss: 0.900285]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 58/86 [loss: 0.395275, acc.: 86.04%] [G loss: 0.865256]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 59/86 [loss: 0.357282, acc.: 88.77%] [G loss: 0.860795]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 60/86 [loss: 0.387731, acc.: 85.60%] [G loss: 0.863431]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 61/86 [loss: 0.368128, acc.: 88.13%] [G loss: 0.870164]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 62/86 [loss: 0.381333, acc.: 87.16%] [G loss: 0.845355]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 63/86 [loss: 0.411134, acc.: 84.72%] [G loss: 0.853914]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 64/86 [loss: 0.417496, acc.: 83.74%] [G loss: 0.869761]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 65/86 [loss: 0.375518, acc.: 87.26%] [G loss: 0.849462]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 66/86 [loss: 0.446662, acc.: 81.88%] [G loss: 0.824060]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 67/86 [loss: 0.386297, acc.: 85.99%] [G loss: 0.869035]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 68/86 [loss: 0.389868, acc.: 86.96%] [G loss: 0.849393]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 69/86 [loss: 0.350158, acc.: 89.11%] [G loss: 0.852082]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 20/200  Batch Size: 70/86 [loss: 0.314884, acc.: 91.99%] [G loss: 0.876943]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 71/86 [loss: 0.368633, acc.: 88.72%] [G loss: 0.847634]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 72/86 [loss: 0.367856, acc.: 87.30%] [G loss: 0.865750]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 73/86 [loss: 0.390803, acc.: 86.13%] [G loss: 0.856963]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 74/86 [loss: 0.323244, acc.: 91.41%] [G loss: 0.892147]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 75/86 [loss: 0.359448, acc.: 88.33%] [G loss: 0.877520]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 76/86 [loss: 0.406628, acc.: 84.67%] [G loss: 0.878480]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 77/86 [loss: 0.388254, acc.: 85.64%] [G loss: 0.894783]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 78/86 [loss: 0.320286, acc.: 91.36%] [G loss: 0.885461]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 79/86 [loss: 0.380712, acc.: 86.52%] [G loss: 0.933425]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 80/86 [loss: 0.356949, acc.: 89.50%] [G loss: 0.849278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 81/86 [loss: 0.362582, acc.: 88.13%] [G loss: 0.872144]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 82/86 [loss: 0.365033, acc.: 88.09%] [G loss: 0.876650]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 20/200  Batch Size: 83/86 [loss: 0.431942, acc.: 83.20%] [G loss: 0.826460]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 20/200  Batch Size: 84/86 [loss: 0.375592, acc.: 86.47%] [G loss: 0.889128]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 20/200  Batch Size: 85/86 [loss: 0.353524, acc.: 89.36%] [G loss: 0.885576]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 0/86 [loss: 0.422173, acc.: 84.03%] [G loss: 0.872984]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 1/86 [loss: 0.401433, acc.: 84.72%] [G loss: 0.855982]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 2/86 [loss: 0.349014, acc.: 89.89%] [G loss: 0.856802]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 3/86 [loss: 0.412116, acc.: 85.74%] [G loss: 0.843904]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 4/86 [loss: 0.329669, acc.: 90.58%] [G loss: 0.887386]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 5/86 [loss: 0.341220, acc.: 89.50%] [G loss: 0.883739]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 6/86 [loss: 0.436076, acc.: 82.52%] [G loss: 0.872861]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 7/86 [loss: 0.377008, acc.: 87.50%] [G loss: 0.878462]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 8/86 [loss: 0.401602, acc.: 86.23%] [G loss: 0.870740]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 9/86 [loss: 0.355335, acc.: 88.43%] [G loss: 0.850170]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 10/86 [loss: 0.396408, acc.: 85.21%] [G loss: 0.885745]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 11/86 [loss: 0.414224, acc.: 84.47%] [G loss: 0.856502]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 12/86 [loss: 0.407413, acc.: 85.25%] [G loss: 0.875559]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 13/86 [loss: 0.455456, acc.: 80.13%] [G loss: 0.889131]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 14/86 [loss: 0.418039, acc.: 83.64%] [G loss: 0.841165]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 15/86 [loss: 0.357549, acc.: 88.87%] [G loss: 0.853491]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 16/86 [loss: 0.423652, acc.: 83.40%] [G loss: 0.858279]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 17/86 [loss: 0.421950, acc.: 83.25%] [G loss: 0.829323]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 18/86 [loss: 0.353181, acc.: 88.28%] [G loss: 0.843501]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 19/86 [loss: 0.378266, acc.: 87.11%] [G loss: 0.871877]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 20/86 [loss: 0.400754, acc.: 85.30%] [G loss: 0.881262]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 21/86 [loss: 0.379343, acc.: 87.35%] [G loss: 0.854310]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 22/86 [loss: 0.448025, acc.: 81.20%] [G loss: 0.840458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 23/86 [loss: 0.355660, acc.: 88.77%] [G loss: 0.874779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 24/86 [loss: 0.369395, acc.: 88.96%] [G loss: 0.849748]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 25/86 [loss: 0.421580, acc.: 83.59%] [G loss: 0.864371]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 26/86 [loss: 0.398250, acc.: 85.55%] [G loss: 0.891061]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 27/86 [loss: 0.400695, acc.: 85.11%] [G loss: 0.876449]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 28/86 [loss: 0.361066, acc.: 88.77%] [G loss: 0.855676]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 29/86 [loss: 0.373787, acc.: 87.40%] [G loss: 0.859441]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 30/86 [loss: 0.400401, acc.: 85.01%] [G loss: 0.884493]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 31/86 [loss: 0.364488, acc.: 88.13%] [G loss: 0.885707]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 32/86 [loss: 0.351629, acc.: 89.16%] [G loss: 0.880849]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 33/86 [loss: 0.369240, acc.: 87.74%] [G loss: 0.891246]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 34/86 [loss: 0.360526, acc.: 87.89%] [G loss: 0.875467]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 35/86 [loss: 0.356346, acc.: 88.43%] [G loss: 0.867735]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 36/86 [loss: 0.422883, acc.: 84.33%] [G loss: 0.862232]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 37/86 [loss: 0.369530, acc.: 88.28%] [G loss: 0.875307]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 38/86 [loss: 0.345140, acc.: 89.16%] [G loss: 0.869057]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 39/86 [loss: 0.387843, acc.: 86.04%] [G loss: 0.890119]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 40/86 [loss: 0.404689, acc.: 84.62%] [G loss: 0.866877]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 41/86 [loss: 0.360476, acc.: 88.48%] [G loss: 0.882631]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 42/86 [loss: 0.440266, acc.: 82.18%] [G loss: 0.858302]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 43/86 [loss: 0.384623, acc.: 87.45%] [G loss: 0.889426]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 44/86 [loss: 0.429572, acc.: 82.86%] [G loss: 0.869573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 45/86 [loss: 0.469314, acc.: 79.54%] [G loss: 0.836772]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 46/86 [loss: 0.400094, acc.: 85.50%] [G loss: 0.830934]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 47/86 [loss: 0.378372, acc.: 87.79%] [G loss: 0.877888]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 48/86 [loss: 0.377318, acc.: 87.06%] [G loss: 0.901896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 49/86 [loss: 0.380864, acc.: 86.72%] [G loss: 0.891384]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 50/86 [loss: 0.408448, acc.: 85.01%] [G loss: 0.920030]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 51/86 [loss: 0.422691, acc.: 83.79%] [G loss: 0.843269]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 52/86 [loss: 0.354133, acc.: 89.60%] [G loss: 0.856752]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 53/86 [loss: 0.401355, acc.: 85.45%] [G loss: 0.866545]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 54/86 [loss: 0.439209, acc.: 82.62%] [G loss: 0.865951]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 55/86 [loss: 0.397347, acc.: 86.13%] [G loss: 0.859762]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 56/86 [loss: 0.433836, acc.: 82.23%] [G loss: 0.849274]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 57/86 [loss: 0.380601, acc.: 87.11%] [G loss: 0.816007]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 58/86 [loss: 0.404197, acc.: 84.91%] [G loss: 0.847512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 59/86 [loss: 0.372653, acc.: 87.74%] [G loss: 0.869384]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 60/86 [loss: 0.387056, acc.: 86.33%] [G loss: 0.899487]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 61/86 [loss: 0.407448, acc.: 84.33%] [G loss: 0.885468]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 62/86 [loss: 0.452405, acc.: 80.66%] [G loss: 0.886466]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 63/86 [loss: 0.424491, acc.: 83.06%] [G loss: 0.865322]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 64/86 [loss: 0.389340, acc.: 87.30%] [G loss: 0.820931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 65/86 [loss: 0.381562, acc.: 86.47%] [G loss: 0.858111]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 66/86 [loss: 0.369342, acc.: 87.21%] [G loss: 0.834724]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 67/86 [loss: 0.410396, acc.: 84.13%] [G loss: 0.880740]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 68/86 [loss: 0.329416, acc.: 90.77%] [G loss: 0.875686]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 69/86 [loss: 0.357169, acc.: 89.06%] [G loss: 0.859780]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 70/86 [loss: 0.390649, acc.: 85.35%] [G loss: 0.899775]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 71/86 [loss: 0.369840, acc.: 88.53%] [G loss: 0.873247]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 72/86 [loss: 0.383115, acc.: 86.13%] [G loss: 0.871240]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 73/86 [loss: 0.352811, acc.: 89.70%] [G loss: 0.876910]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 74/86 [loss: 0.354707, acc.: 88.96%] [G loss: 0.898380]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 75/86 [loss: 0.374018, acc.: 87.40%] [G loss: 0.880383]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 76/86 [loss: 0.404624, acc.: 84.23%] [G loss: 0.891248]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 77/86 [loss: 0.349456, acc.: 89.31%] [G loss: 0.878496]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 78/86 [loss: 0.417621, acc.: 84.42%] [G loss: 0.867643]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 79/86 [loss: 0.374768, acc.: 87.21%] [G loss: 0.845912]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 80/86 [loss: 0.418359, acc.: 84.23%] [G loss: 0.865596]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 21/200  Batch Size: 81/86 [loss: 0.334382, acc.: 90.19%] [G loss: 0.871413]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 21/200  Batch Size: 82/86 [loss: 0.375587, acc.: 87.16%] [G loss: 0.868120]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 83/86 [loss: 0.353859, acc.: 88.53%] [G loss: 0.844805]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 21/200  Batch Size: 84/86 [loss: 0.364075, acc.: 87.65%] [G loss: 0.903103]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 21/200  Batch Size: 85/86 [loss: 0.405457, acc.: 85.89%] [G loss: 0.898126]\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 0/86 [loss: 0.385217, acc.: 86.96%] [G loss: 0.877257]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 1/86 [loss: 0.382214, acc.: 87.01%] [G loss: 0.893331]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 2/86 [loss: 0.336609, acc.: 90.58%] [G loss: 0.876938]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 3/86 [loss: 0.350873, acc.: 89.01%] [G loss: 0.848056]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 4/86 [loss: 0.383813, acc.: 87.35%] [G loss: 0.841228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 5/86 [loss: 0.373448, acc.: 87.50%] [G loss: 0.848442]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 6/86 [loss: 0.346882, acc.: 89.79%] [G loss: 0.872648]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 7/86 [loss: 0.347121, acc.: 88.48%] [G loss: 0.873101]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 8/86 [loss: 0.389503, acc.: 85.55%] [G loss: 0.829676]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 9/86 [loss: 0.460432, acc.: 81.01%] [G loss: 0.860421]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 10/86 [loss: 0.408550, acc.: 85.30%] [G loss: 0.863994]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 11/86 [loss: 0.435238, acc.: 83.15%] [G loss: 0.867697]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 12/86 [loss: 0.408976, acc.: 84.33%] [G loss: 0.893186]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 13/86 [loss: 0.465086, acc.: 79.20%] [G loss: 0.873097]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 14/86 [loss: 0.351613, acc.: 89.60%] [G loss: 0.826251]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 15/86 [loss: 0.390089, acc.: 85.74%] [G loss: 0.887091]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 16/86 [loss: 0.399400, acc.: 84.38%] [G loss: 0.885278]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 17/86 [loss: 0.409493, acc.: 85.30%] [G loss: 0.873541]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 18/86 [loss: 0.426516, acc.: 83.50%] [G loss: 0.883906]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 19/86 [loss: 0.393770, acc.: 85.69%] [G loss: 0.849330]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 20/86 [loss: 0.368750, acc.: 87.84%] [G loss: 0.865686]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 21/86 [loss: 0.352450, acc.: 88.72%] [G loss: 0.877431]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 22/86 [loss: 0.380558, acc.: 87.21%] [G loss: 0.871858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 23/86 [loss: 0.405889, acc.: 84.91%] [G loss: 0.891211]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 24/86 [loss: 0.380722, acc.: 87.06%] [G loss: 0.867477]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 25/86 [loss: 0.403541, acc.: 85.45%] [G loss: 0.842913]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 26/86 [loss: 0.424168, acc.: 83.74%] [G loss: 0.870604]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 27/86 [loss: 0.445011, acc.: 80.81%] [G loss: 0.855389]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 28/86 [loss: 0.450698, acc.: 81.40%] [G loss: 0.885744]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 22/200  Batch Size: 29/86 [loss: 0.397728, acc.: 85.79%] [G loss: 0.847901]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 30/86 [loss: 0.414235, acc.: 83.98%] [G loss: 0.855342]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 31/86 [loss: 0.378396, acc.: 87.16%] [G loss: 0.831426]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 32/86 [loss: 0.375668, acc.: 87.50%] [G loss: 0.865053]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 33/86 [loss: 0.380512, acc.: 87.35%] [G loss: 0.868369]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 34/86 [loss: 0.416916, acc.: 83.64%] [G loss: 0.905446]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 35/86 [loss: 0.359073, acc.: 88.04%] [G loss: 0.847253]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 36/86 [loss: 0.401526, acc.: 84.96%] [G loss: 0.858870]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 37/86 [loss: 0.421678, acc.: 84.28%] [G loss: 0.835147]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 38/86 [loss: 0.333035, acc.: 90.92%] [G loss: 0.853510]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 39/86 [loss: 0.388442, acc.: 86.57%] [G loss: 0.872334]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 40/86 [loss: 0.392182, acc.: 86.04%] [G loss: 0.867041]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 41/86 [loss: 0.324869, acc.: 91.85%] [G loss: 0.889151]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 42/86 [loss: 0.371376, acc.: 87.99%] [G loss: 0.870458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 43/86 [loss: 0.407863, acc.: 84.28%] [G loss: 0.880439]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 44/86 [loss: 0.340787, acc.: 89.79%] [G loss: 0.880842]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 45/86 [loss: 0.407931, acc.: 85.06%] [G loss: 0.876528]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 46/86 [loss: 0.430192, acc.: 83.50%] [G loss: 0.851596]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 47/86 [loss: 0.326742, acc.: 90.77%] [G loss: 0.911412]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 48/86 [loss: 0.449356, acc.: 82.13%] [G loss: 0.859043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 49/86 [loss: 0.450853, acc.: 80.86%] [G loss: 0.887457]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 50/86 [loss: 0.363369, acc.: 87.40%] [G loss: 0.846512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 51/86 [loss: 0.388649, acc.: 87.01%] [G loss: 0.867994]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 52/86 [loss: 0.330845, acc.: 90.43%] [G loss: 0.850083]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 53/86 [loss: 0.369451, acc.: 87.16%] [G loss: 0.892287]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 54/86 [loss: 0.364572, acc.: 88.82%] [G loss: 0.836274]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 55/86 [loss: 0.357804, acc.: 88.92%] [G loss: 0.882652]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 56/86 [loss: 0.432237, acc.: 83.50%] [G loss: 0.843227]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 57/86 [loss: 0.387945, acc.: 86.13%] [G loss: 0.849845]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 58/86 [loss: 0.334116, acc.: 90.23%] [G loss: 0.895072]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 59/86 [loss: 0.346825, acc.: 89.06%] [G loss: 0.876843]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 60/86 [loss: 0.338407, acc.: 90.28%] [G loss: 0.880068]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 61/86 [loss: 0.341439, acc.: 90.33%] [G loss: 0.881413]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 62/86 [loss: 0.411836, acc.: 85.01%] [G loss: 0.830230]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 63/86 [loss: 0.363073, acc.: 88.72%] [G loss: 0.873749]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 64/86 [loss: 0.359270, acc.: 88.18%] [G loss: 0.886140]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 65/86 [loss: 0.321699, acc.: 91.55%] [G loss: 0.856094]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 66/86 [loss: 0.331678, acc.: 90.67%] [G loss: 0.848298]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 67/86 [loss: 0.360161, acc.: 88.48%] [G loss: 0.846659]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 68/86 [loss: 0.377978, acc.: 87.21%] [G loss: 0.856864]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 69/86 [loss: 0.348520, acc.: 88.72%] [G loss: 0.874836]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 70/86 [loss: 0.403122, acc.: 86.08%] [G loss: 0.868501]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 71/86 [loss: 0.397351, acc.: 85.69%] [G loss: 0.886167]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 72/86 [loss: 0.344871, acc.: 88.96%] [G loss: 0.874825]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 73/86 [loss: 0.394864, acc.: 84.86%] [G loss: 0.872658]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 22/200  Batch Size: 74/86 [loss: 0.373256, acc.: 87.40%] [G loss: 0.830930]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 75/86 [loss: 0.353431, acc.: 89.70%] [G loss: 0.831836]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 76/86 [loss: 0.380628, acc.: 86.72%] [G loss: 0.857421]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 77/86 [loss: 0.383137, acc.: 86.91%] [G loss: 0.856429]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 78/86 [loss: 0.331257, acc.: 90.19%] [G loss: 0.881264]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 79/86 [loss: 0.391566, acc.: 85.35%] [G loss: 0.848725]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 22/200  Batch Size: 80/86 [loss: 0.368686, acc.: 88.72%] [G loss: 0.875653]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 81/86 [loss: 0.327469, acc.: 90.48%] [G loss: 0.859136]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 82/86 [loss: 0.380149, acc.: 86.28%] [G loss: 0.838264]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 83/86 [loss: 0.400899, acc.: 85.89%] [G loss: 0.834115]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 22/200  Batch Size: 84/86 [loss: 0.383952, acc.: 87.11%] [G loss: 0.867516]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 22/200  Batch Size: 85/86 [loss: 0.391281, acc.: 86.04%] [G loss: 0.865246]\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 0/86 [loss: 0.402994, acc.: 85.35%] [G loss: 0.891103]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 1/86 [loss: 0.393948, acc.: 86.18%] [G loss: 0.865547]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 2/86 [loss: 0.365522, acc.: 88.33%] [G loss: 0.850642]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 3/86 [loss: 0.346736, acc.: 89.36%] [G loss: 0.893672]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 4/86 [loss: 0.358034, acc.: 88.96%] [G loss: 0.873982]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 5/86 [loss: 0.360018, acc.: 88.28%] [G loss: 0.849302]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 6/86 [loss: 0.401843, acc.: 85.45%] [G loss: 0.877798]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 7/86 [loss: 0.404693, acc.: 85.01%] [G loss: 0.889551]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 8/86 [loss: 0.375648, acc.: 87.89%] [G loss: 0.857791]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 9/86 [loss: 0.346915, acc.: 89.26%] [G loss: 0.856422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 10/86 [loss: 0.392422, acc.: 86.38%] [G loss: 0.866613]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 11/86 [loss: 0.402271, acc.: 85.55%] [G loss: 0.889150]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 12/86 [loss: 0.345551, acc.: 88.82%] [G loss: 0.880703]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 13/86 [loss: 0.404631, acc.: 84.23%] [G loss: 0.865222]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 14/86 [loss: 0.357872, acc.: 88.77%] [G loss: 0.848556]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 15/86 [loss: 0.380006, acc.: 86.62%] [G loss: 0.852944]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 16/86 [loss: 0.387583, acc.: 85.84%] [G loss: 0.899840]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 17/86 [loss: 0.380738, acc.: 86.62%] [G loss: 0.874452]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 18/86 [loss: 0.395650, acc.: 84.96%] [G loss: 0.848471]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 19/86 [loss: 0.397023, acc.: 86.04%] [G loss: 0.859454]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 20/86 [loss: 0.366554, acc.: 87.89%] [G loss: 0.871892]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 21/86 [loss: 0.370375, acc.: 87.79%] [G loss: 0.878792]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 22/86 [loss: 0.384101, acc.: 86.57%] [G loss: 0.851482]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 23/86 [loss: 0.417077, acc.: 83.79%] [G loss: 0.867991]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 24/86 [loss: 0.360335, acc.: 88.53%] [G loss: 0.876715]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 25/86 [loss: 0.352966, acc.: 88.67%] [G loss: 0.874672]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 26/86 [loss: 0.333712, acc.: 90.82%] [G loss: 0.860383]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 27/86 [loss: 0.374943, acc.: 88.13%] [G loss: 0.868879]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 28/86 [loss: 0.357366, acc.: 89.16%] [G loss: 0.858983]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 29/86 [loss: 0.354013, acc.: 88.72%] [G loss: 0.891134]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 30/86 [loss: 0.375205, acc.: 87.35%] [G loss: 0.863759]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 31/86 [loss: 0.328627, acc.: 90.77%] [G loss: 0.854694]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 32/86 [loss: 0.387639, acc.: 86.87%] [G loss: 0.859011]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 33/86 [loss: 0.378608, acc.: 87.50%] [G loss: 0.898855]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 34/86 [loss: 0.365265, acc.: 88.13%] [G loss: 0.920806]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 35/86 [loss: 0.425392, acc.: 83.94%] [G loss: 0.859075]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 36/86 [loss: 0.351158, acc.: 89.40%] [G loss: 0.858357]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 37/86 [loss: 0.372611, acc.: 87.70%] [G loss: 0.868835]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 38/86 [loss: 0.333878, acc.: 90.87%] [G loss: 0.865998]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 39/86 [loss: 0.356335, acc.: 88.62%] [G loss: 0.882086]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 40/86 [loss: 0.405205, acc.: 85.35%] [G loss: 0.883241]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 41/86 [loss: 0.374062, acc.: 87.65%] [G loss: 0.852249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 42/86 [loss: 0.387979, acc.: 87.01%] [G loss: 0.851939]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 43/86 [loss: 0.347497, acc.: 89.60%] [G loss: 0.865999]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 44/86 [loss: 0.366743, acc.: 88.43%] [G loss: 0.887263]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 45/86 [loss: 0.349208, acc.: 89.01%] [G loss: 0.888060]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 46/86 [loss: 0.335369, acc.: 91.36%] [G loss: 0.883733]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 47/86 [loss: 0.414286, acc.: 83.79%] [G loss: 0.847416]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 48/86 [loss: 0.408036, acc.: 84.72%] [G loss: 0.843665]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 49/86 [loss: 0.365961, acc.: 88.67%] [G loss: 0.838482]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 50/86 [loss: 0.435620, acc.: 82.76%] [G loss: 0.822049]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 51/86 [loss: 0.381506, acc.: 87.55%] [G loss: 0.843886]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 52/86 [loss: 0.404611, acc.: 84.96%] [G loss: 0.860262]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 53/86 [loss: 0.357846, acc.: 88.67%] [G loss: 0.900498]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 54/86 [loss: 0.368036, acc.: 87.89%] [G loss: 0.880989]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 55/86 [loss: 0.418981, acc.: 83.69%] [G loss: 0.880676]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 56/86 [loss: 0.418970, acc.: 84.62%] [G loss: 0.884145]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 57/86 [loss: 0.387850, acc.: 85.99%] [G loss: 0.849058]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 58/86 [loss: 0.337084, acc.: 90.43%] [G loss: 0.849680]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 59/86 [loss: 0.338069, acc.: 90.09%] [G loss: 0.887060]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 60/86 [loss: 0.458009, acc.: 80.91%] [G loss: 0.864834]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 61/86 [loss: 0.319561, acc.: 91.11%] [G loss: 0.942817]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 62/86 [loss: 0.353806, acc.: 89.36%] [G loss: 0.867219]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 63/86 [loss: 0.395041, acc.: 85.45%] [G loss: 0.842730]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 64/86 [loss: 0.319311, acc.: 91.85%] [G loss: 0.843052]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 65/86 [loss: 0.382788, acc.: 87.35%] [G loss: 0.867410]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 66/86 [loss: 0.378083, acc.: 86.87%] [G loss: 0.873879]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 67/86 [loss: 0.374744, acc.: 87.40%] [G loss: 0.870712]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 68/86 [loss: 0.465056, acc.: 80.91%] [G loss: 0.866247]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 69/86 [loss: 0.363207, acc.: 88.92%] [G loss: 0.842567]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 70/86 [loss: 0.352027, acc.: 88.04%] [G loss: 0.818442]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 71/86 [loss: 0.350326, acc.: 89.60%] [G loss: 0.832210]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 72/86 [loss: 0.470774, acc.: 79.79%] [G loss: 0.861822]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 73/86 [loss: 0.419379, acc.: 83.98%] [G loss: 0.883278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 74/86 [loss: 0.391275, acc.: 85.25%] [G loss: 0.852574]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 75/86 [loss: 0.359408, acc.: 88.67%] [G loss: 0.870727]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 76/86 [loss: 0.352276, acc.: 89.06%] [G loss: 0.852422]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 77/86 [loss: 0.392201, acc.: 85.99%] [G loss: 0.862248]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 78/86 [loss: 0.446061, acc.: 81.69%] [G loss: 0.874951]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 23/200  Batch Size: 79/86 [loss: 0.342255, acc.: 90.09%] [G loss: 0.873634]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 23/200  Batch Size: 80/86 [loss: 0.355000, acc.: 88.92%] [G loss: 0.810201]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 23/200  Batch Size: 81/86 [loss: 0.336980, acc.: 89.84%] [G loss: 0.894485]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 82/86 [loss: 0.417115, acc.: 84.67%] [G loss: 0.866427]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 83/86 [loss: 0.412519, acc.: 84.52%] [G loss: 0.838822]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 84/86 [loss: 0.445392, acc.: 81.84%] [G loss: 0.872661]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 23/200  Batch Size: 85/86 [loss: 0.409161, acc.: 85.74%] [G loss: 0.838472]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 0/86 [loss: 0.365448, acc.: 88.43%] [G loss: 0.824151]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 1/86 [loss: 0.383011, acc.: 86.28%] [G loss: 0.857459]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 2/86 [loss: 0.429180, acc.: 82.81%] [G loss: 0.846326]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 3/86 [loss: 0.330795, acc.: 90.72%] [G loss: 0.861523]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 4/86 [loss: 0.441225, acc.: 83.11%] [G loss: 0.861924]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 5/86 [loss: 0.362143, acc.: 87.99%] [G loss: 0.868049]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 6/86 [loss: 0.342491, acc.: 90.33%] [G loss: 0.847913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 7/86 [loss: 0.362725, acc.: 87.94%] [G loss: 0.869041]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 8/86 [loss: 0.392984, acc.: 86.33%] [G loss: 0.880099]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 9/86 [loss: 0.351909, acc.: 88.33%] [G loss: 0.883214]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 10/86 [loss: 0.390154, acc.: 86.62%] [G loss: 0.889140]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 11/86 [loss: 0.337359, acc.: 90.14%] [G loss: 0.855506]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 12/86 [loss: 0.375958, acc.: 87.55%] [G loss: 0.857465]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 13/86 [loss: 0.394476, acc.: 84.72%] [G loss: 0.839926]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 14/86 [loss: 0.365059, acc.: 88.67%] [G loss: 0.861670]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 15/86 [loss: 0.378641, acc.: 86.38%] [G loss: 0.828560]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 16/86 [loss: 0.399318, acc.: 85.79%] [G loss: 0.846528]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 17/86 [loss: 0.381972, acc.: 87.84%] [G loss: 0.875987]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 18/86 [loss: 0.383512, acc.: 86.96%] [G loss: 0.858858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 19/86 [loss: 0.320381, acc.: 91.41%] [G loss: 0.865117]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 20/86 [loss: 0.364771, acc.: 88.43%] [G loss: 0.870456]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 21/86 [loss: 0.464125, acc.: 79.39%] [G loss: 0.866325]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 22/86 [loss: 0.354181, acc.: 87.79%] [G loss: 0.875460]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 23/86 [loss: 0.430536, acc.: 83.06%] [G loss: 0.874620]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 24/86 [loss: 0.332137, acc.: 90.77%] [G loss: 0.877907]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 25/86 [loss: 0.444881, acc.: 81.54%] [G loss: 0.878028]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 26/86 [loss: 0.374405, acc.: 87.65%] [G loss: 0.885037]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 27/86 [loss: 0.340763, acc.: 90.43%] [G loss: 0.887017]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 28/86 [loss: 0.408346, acc.: 85.11%] [G loss: 0.826618]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 29/86 [loss: 0.334412, acc.: 90.04%] [G loss: 0.890698]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 30/86 [loss: 0.390868, acc.: 85.21%] [G loss: 0.852750]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 31/86 [loss: 0.328170, acc.: 91.65%] [G loss: 0.879721]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 32/86 [loss: 0.450107, acc.: 81.05%] [G loss: 0.863635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 33/86 [loss: 0.428331, acc.: 82.62%] [G loss: 0.853771]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 34/86 [loss: 0.405325, acc.: 85.16%] [G loss: 0.847408]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 35/86 [loss: 0.417939, acc.: 84.72%] [G loss: 0.851705]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 36/86 [loss: 0.431912, acc.: 83.06%] [G loss: 0.852409]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 37/86 [loss: 0.350215, acc.: 89.36%] [G loss: 0.881796]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 38/86 [loss: 0.410496, acc.: 83.98%] [G loss: 0.855324]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 39/86 [loss: 0.327373, acc.: 91.55%] [G loss: 0.833077]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 40/86 [loss: 0.369629, acc.: 87.50%] [G loss: 0.876600]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 41/86 [loss: 0.352923, acc.: 89.31%] [G loss: 0.870703]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 42/86 [loss: 0.373697, acc.: 87.99%] [G loss: 0.862413]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 43/86 [loss: 0.420083, acc.: 83.15%] [G loss: 0.877306]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 44/86 [loss: 0.374493, acc.: 86.38%] [G loss: 0.876733]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 45/86 [loss: 0.400739, acc.: 85.11%] [G loss: 0.893583]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 46/86 [loss: 0.396312, acc.: 86.08%] [G loss: 0.854380]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 47/86 [loss: 0.360938, acc.: 88.43%] [G loss: 0.849454]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 48/86 [loss: 0.336412, acc.: 90.14%] [G loss: 0.872766]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 49/86 [loss: 0.399018, acc.: 85.64%] [G loss: 0.874360]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 50/86 [loss: 0.421163, acc.: 84.03%] [G loss: 0.845908]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 51/86 [loss: 0.369308, acc.: 88.67%] [G loss: 0.876128]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 52/86 [loss: 0.401981, acc.: 85.16%] [G loss: 0.884655]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 53/86 [loss: 0.356447, acc.: 88.67%] [G loss: 0.920393]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 54/86 [loss: 0.405787, acc.: 85.01%] [G loss: 0.891009]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 55/86 [loss: 0.367532, acc.: 87.30%] [G loss: 0.914830]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 56/86 [loss: 0.323890, acc.: 90.87%] [G loss: 0.844415]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 57/86 [loss: 0.379939, acc.: 87.11%] [G loss: 0.848864]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 58/86 [loss: 0.356553, acc.: 89.60%] [G loss: 0.864503]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 59/86 [loss: 0.385008, acc.: 87.01%] [G loss: 0.865872]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 60/86 [loss: 0.396425, acc.: 85.21%] [G loss: 0.880767]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 61/86 [loss: 0.441074, acc.: 81.79%] [G loss: 0.849791]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 62/86 [loss: 0.439100, acc.: 83.06%] [G loss: 0.863224]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 63/86 [loss: 0.345101, acc.: 90.14%] [G loss: 0.873724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 64/86 [loss: 0.393113, acc.: 86.43%] [G loss: 0.846647]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 65/86 [loss: 0.334598, acc.: 90.77%] [G loss: 0.871382]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 66/86 [loss: 0.447000, acc.: 81.93%] [G loss: 0.884794]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 67/86 [loss: 0.391099, acc.: 86.43%] [G loss: 0.857408]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 68/86 [loss: 0.389253, acc.: 86.62%] [G loss: 0.902914]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 69/86 [loss: 0.388627, acc.: 86.38%] [G loss: 0.867631]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 70/86 [loss: 0.399673, acc.: 86.08%] [G loss: 0.884238]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 71/86 [loss: 0.374397, acc.: 87.94%] [G loss: 0.844988]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 72/86 [loss: 0.382547, acc.: 86.62%] [G loss: 0.879233]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 73/86 [loss: 0.390769, acc.: 86.33%] [G loss: 0.866910]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 74/86 [loss: 0.416355, acc.: 84.52%] [G loss: 0.861151]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 75/86 [loss: 0.432194, acc.: 82.18%] [G loss: 0.804610]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 76/86 [loss: 0.322646, acc.: 91.41%] [G loss: 0.875201]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 77/86 [loss: 0.353887, acc.: 88.96%] [G loss: 0.906917]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 78/86 [loss: 0.472801, acc.: 78.61%] [G loss: 0.937778]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 79/86 [loss: 0.404282, acc.: 84.81%] [G loss: 0.886592]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 80/86 [loss: 0.343133, acc.: 89.79%] [G loss: 0.847572]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 24/200  Batch Size: 81/86 [loss: 0.339741, acc.: 89.40%] [G loss: 0.861491]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 24/200  Batch Size: 82/86 [loss: 0.482113, acc.: 77.64%] [G loss: 0.861338]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 83/86 [loss: 0.399991, acc.: 85.35%] [G loss: 0.859079]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 24/200  Batch Size: 84/86 [loss: 0.327487, acc.: 90.58%] [G loss: 0.853859]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 24/200  Batch Size: 85/86 [loss: 0.388219, acc.: 87.06%] [G loss: 0.862933]\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 0/86 [loss: 0.333495, acc.: 90.23%] [G loss: 0.885612]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 1/86 [loss: 0.417822, acc.: 84.62%] [G loss: 0.846807]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 25/200  Batch Size: 2/86 [loss: 0.310156, acc.: 92.43%] [G loss: 0.844070]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 3/86 [loss: 0.388816, acc.: 87.74%] [G loss: 0.858959]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 4/86 [loss: 0.442004, acc.: 82.37%] [G loss: 0.860080]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 5/86 [loss: 0.417227, acc.: 84.23%] [G loss: 0.850790]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 6/86 [loss: 0.496428, acc.: 77.69%] [G loss: 0.828741]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 7/86 [loss: 0.404238, acc.: 85.55%] [G loss: 0.855749]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 8/86 [loss: 0.384992, acc.: 86.28%] [G loss: 0.860688]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 9/86 [loss: 0.443029, acc.: 82.08%] [G loss: 0.856291]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 10/86 [loss: 0.397582, acc.: 85.74%] [G loss: 0.861407]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 25/200  Batch Size: 11/86 [loss: 0.442685, acc.: 81.59%] [G loss: 0.831983]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 12/86 [loss: 0.326204, acc.: 92.04%] [G loss: 0.855502]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 13/86 [loss: 0.334349, acc.: 90.67%] [G loss: 0.883095]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 14/86 [loss: 0.376112, acc.: 87.35%] [G loss: 0.887767]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 15/86 [loss: 0.400362, acc.: 85.11%] [G loss: 0.885593]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 16/86 [loss: 0.357550, acc.: 89.50%] [G loss: 0.856693]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 17/86 [loss: 0.355549, acc.: 89.31%] [G loss: 0.857856]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 18/86 [loss: 0.407690, acc.: 84.33%] [G loss: 0.850840]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 19/86 [loss: 0.363569, acc.: 88.67%] [G loss: 0.839943]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 20/86 [loss: 0.375248, acc.: 87.65%] [G loss: 0.853633]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 21/86 [loss: 0.408359, acc.: 84.91%] [G loss: 0.875499]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 22/86 [loss: 0.415036, acc.: 84.38%] [G loss: 0.872896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 23/86 [loss: 0.393332, acc.: 86.82%] [G loss: 0.856071]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 25/200  Batch Size: 24/86 [loss: 0.372603, acc.: 87.89%] [G loss: 0.860914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 25/86 [loss: 0.443404, acc.: 81.69%] [G loss: 0.858123]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 26/86 [loss: 0.331754, acc.: 91.11%] [G loss: 0.871549]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 27/86 [loss: 0.400835, acc.: 84.67%] [G loss: 0.925048]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 28/86 [loss: 0.380219, acc.: 87.16%] [G loss: 0.828386]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 29/86 [loss: 0.359774, acc.: 88.33%] [G loss: 0.900495]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 30/86 [loss: 0.447130, acc.: 81.93%] [G loss: 0.880760]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 31/86 [loss: 0.399124, acc.: 86.04%] [G loss: 0.862592]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 32/86 [loss: 0.372708, acc.: 87.06%] [G loss: 0.843142]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 33/86 [loss: 0.355163, acc.: 88.48%] [G loss: 0.871502]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 34/86 [loss: 0.392868, acc.: 85.60%] [G loss: 0.913021]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 35/86 [loss: 0.400857, acc.: 85.21%] [G loss: 0.895614]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 36/86 [loss: 0.388204, acc.: 87.01%] [G loss: 0.861522]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 37/86 [loss: 0.380117, acc.: 86.87%] [G loss: 0.816601]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 38/86 [loss: 0.366489, acc.: 88.53%] [G loss: 0.877127]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 39/86 [loss: 0.419456, acc.: 84.52%] [G loss: 0.862754]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 40/86 [loss: 0.347746, acc.: 89.11%] [G loss: 0.860799]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 41/86 [loss: 0.409977, acc.: 84.47%] [G loss: 0.877209]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 42/86 [loss: 0.395389, acc.: 86.52%] [G loss: 0.834599]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 43/86 [loss: 0.392660, acc.: 84.96%] [G loss: 0.867099]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 25/200  Batch Size: 44/86 [loss: 0.359143, acc.: 89.11%] [G loss: 0.869007]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 45/86 [loss: 0.382043, acc.: 86.82%] [G loss: 0.861979]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 46/86 [loss: 0.328744, acc.: 90.67%] [G loss: 0.879544]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 47/86 [loss: 0.336607, acc.: 90.04%] [G loss: 0.865639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 48/86 [loss: 0.358997, acc.: 88.33%] [G loss: 0.879022]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 49/86 [loss: 0.390553, acc.: 85.89%] [G loss: 0.851590]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 50/86 [loss: 0.296633, acc.: 92.92%] [G loss: 0.845543]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 51/86 [loss: 0.445936, acc.: 82.42%] [G loss: 0.835604]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 52/86 [loss: 0.388817, acc.: 86.87%] [G loss: 0.872509]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 53/86 [loss: 0.362431, acc.: 87.99%] [G loss: 0.835949]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 54/86 [loss: 0.386104, acc.: 86.87%] [G loss: 0.874423]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 55/86 [loss: 0.403468, acc.: 85.60%] [G loss: 0.851860]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 56/86 [loss: 0.498067, acc.: 76.90%] [G loss: 0.901977]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 57/86 [loss: 0.335136, acc.: 90.48%] [G loss: 0.893930]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 58/86 [loss: 0.338610, acc.: 90.28%] [G loss: 0.899824]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 25/200  Batch Size: 59/86 [loss: 0.356340, acc.: 89.01%] [G loss: 0.865105]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 60/86 [loss: 0.440669, acc.: 82.28%] [G loss: 0.863067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 61/86 [loss: 0.387346, acc.: 86.57%] [G loss: 0.829486]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 25/200  Batch Size: 62/86 [loss: 0.443460, acc.: 81.59%] [G loss: 0.861300]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 63/86 [loss: 0.274660, acc.: 94.14%] [G loss: 0.856350]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 64/86 [loss: 0.373710, acc.: 87.40%] [G loss: 0.865528]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 65/86 [loss: 0.382182, acc.: 86.62%] [G loss: 0.844094]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 66/86 [loss: 0.358568, acc.: 88.33%] [G loss: 0.844459]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 67/86 [loss: 0.349524, acc.: 88.92%] [G loss: 0.853365]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 68/86 [loss: 0.423468, acc.: 83.20%] [G loss: 0.835779]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 69/86 [loss: 0.368085, acc.: 87.79%] [G loss: 0.840937]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 70/86 [loss: 0.285318, acc.: 93.90%] [G loss: 0.886960]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 71/86 [loss: 0.456929, acc.: 80.57%] [G loss: 0.885904]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 72/86 [loss: 0.474875, acc.: 79.25%] [G loss: 0.861865]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 73/86 [loss: 0.393093, acc.: 86.28%] [G loss: 0.864848]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 74/86 [loss: 0.366051, acc.: 88.09%] [G loss: 0.861399]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 75/86 [loss: 0.412318, acc.: 85.11%] [G loss: 0.883450]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 76/86 [loss: 0.420922, acc.: 82.91%] [G loss: 0.857388]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 77/86 [loss: 0.345660, acc.: 89.79%] [G loss: 0.865629]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 25/200  Batch Size: 78/86 [loss: 0.421500, acc.: 84.57%] [G loss: 0.849367]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 79/86 [loss: 0.391440, acc.: 86.82%] [G loss: 0.840892]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 80/86 [loss: 0.355372, acc.: 89.01%] [G loss: 0.895588]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 81/86 [loss: 0.341623, acc.: 89.79%] [G loss: 0.903467]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 25/200  Batch Size: 82/86 [loss: 0.377382, acc.: 87.45%] [G loss: 0.857138]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 25/200  Batch Size: 83/86 [loss: 0.319429, acc.: 91.75%] [G loss: 0.887447]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 25/200  Batch Size: 84/86 [loss: 0.402775, acc.: 85.84%] [G loss: 0.856141]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 25/200  Batch Size: 85/86 [loss: 0.349759, acc.: 88.92%] [G loss: 0.905622]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 0/86 [loss: 0.348588, acc.: 89.40%] [G loss: 0.866966]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 1/86 [loss: 0.371028, acc.: 87.70%] [G loss: 0.869238]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 2/86 [loss: 0.354115, acc.: 88.92%] [G loss: 0.862402]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 3/86 [loss: 0.346157, acc.: 89.94%] [G loss: 0.873353]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 4/86 [loss: 0.323450, acc.: 91.80%] [G loss: 0.871226]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 5/86 [loss: 0.473917, acc.: 79.35%] [G loss: 0.875402]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 6/86 [loss: 0.397489, acc.: 85.40%] [G loss: 0.861780]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 7/86 [loss: 0.397230, acc.: 85.50%] [G loss: 0.876483]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 8/86 [loss: 0.398434, acc.: 85.45%] [G loss: 0.904482]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 9/86 [loss: 0.311185, acc.: 91.89%] [G loss: 0.892771]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 10/86 [loss: 0.348789, acc.: 89.65%] [G loss: 0.844942]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 11/86 [loss: 0.382328, acc.: 87.40%] [G loss: 0.892873]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 12/86 [loss: 0.332192, acc.: 90.72%] [G loss: 0.885443]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 13/86 [loss: 0.370642, acc.: 88.33%] [G loss: 0.874363]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 14/86 [loss: 0.411748, acc.: 84.86%] [G loss: 0.880241]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 15/86 [loss: 0.395623, acc.: 85.40%] [G loss: 0.893373]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 16/86 [loss: 0.356873, acc.: 89.11%] [G loss: 0.838664]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 17/86 [loss: 0.330576, acc.: 90.87%] [G loss: 0.871265]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 18/86 [loss: 0.310671, acc.: 91.70%] [G loss: 0.906102]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 19/86 [loss: 0.438658, acc.: 83.35%] [G loss: 0.892408]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 20/86 [loss: 0.367239, acc.: 88.09%] [G loss: 0.860042]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 21/86 [loss: 0.382803, acc.: 87.84%] [G loss: 0.848658]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 22/86 [loss: 0.394278, acc.: 86.62%] [G loss: 0.891209]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 23/86 [loss: 0.381621, acc.: 88.13%] [G loss: 0.855822]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 24/86 [loss: 0.368227, acc.: 88.04%] [G loss: 0.876525]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 25/86 [loss: 0.349477, acc.: 90.53%] [G loss: 0.882043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 26/86 [loss: 0.327861, acc.: 91.02%] [G loss: 0.841437]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 27/86 [loss: 0.333115, acc.: 89.99%] [G loss: 0.882105]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 28/86 [loss: 0.320872, acc.: 92.09%] [G loss: 0.885468]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 29/86 [loss: 0.375907, acc.: 87.11%] [G loss: 0.857781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 30/86 [loss: 0.346788, acc.: 89.89%] [G loss: 0.828668]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 31/86 [loss: 0.426052, acc.: 83.40%] [G loss: 0.882509]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 32/86 [loss: 0.333228, acc.: 90.28%] [G loss: 0.927034]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 33/86 [loss: 0.370242, acc.: 88.13%] [G loss: 0.856038]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 26/200  Batch Size: 34/86 [loss: 0.339370, acc.: 90.72%] [G loss: 0.846026]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 26/200  Batch Size: 35/86 [loss: 0.376539, acc.: 87.06%] [G loss: 0.873390]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 36/86 [loss: 0.422751, acc.: 83.79%] [G loss: 0.871907]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 37/86 [loss: 0.363723, acc.: 88.33%] [G loss: 0.846313]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 38/86 [loss: 0.319622, acc.: 91.70%] [G loss: 0.843981]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 39/86 [loss: 0.424500, acc.: 84.03%] [G loss: 0.841173]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 40/86 [loss: 0.412163, acc.: 84.77%] [G loss: 0.838212]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 41/86 [loss: 0.406872, acc.: 84.91%] [G loss: 0.866576]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 42/86 [loss: 0.395838, acc.: 85.16%] [G loss: 0.871114]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 43/86 [loss: 0.383257, acc.: 86.96%] [G loss: 0.856525]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 44/86 [loss: 0.357732, acc.: 88.53%] [G loss: 0.900573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 45/86 [loss: 0.330154, acc.: 89.99%] [G loss: 0.877647]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 46/86 [loss: 0.394649, acc.: 85.01%] [G loss: 0.887875]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 47/86 [loss: 0.460520, acc.: 80.22%] [G loss: 0.825433]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 48/86 [loss: 0.301862, acc.: 92.53%] [G loss: 0.831745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 49/86 [loss: 0.407167, acc.: 85.16%] [G loss: 0.856455]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 50/86 [loss: 0.352092, acc.: 89.16%] [G loss: 0.851461]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 51/86 [loss: 0.405971, acc.: 84.13%] [G loss: 0.863773]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 52/86 [loss: 0.419204, acc.: 83.98%] [G loss: 0.846165]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 53/86 [loss: 0.368783, acc.: 88.43%] [G loss: 0.865779]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 54/86 [loss: 0.412872, acc.: 84.57%] [G loss: 0.870613]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 55/86 [loss: 0.361055, acc.: 88.48%] [G loss: 0.844408]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 56/86 [loss: 0.409396, acc.: 84.23%] [G loss: 0.846097]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 57/86 [loss: 0.405212, acc.: 84.67%] [G loss: 0.874569]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 58/86 [loss: 0.369292, acc.: 89.21%] [G loss: 0.882039]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 59/86 [loss: 0.332842, acc.: 90.23%] [G loss: 0.883930]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 60/86 [loss: 0.364873, acc.: 86.72%] [G loss: 0.892354]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 61/86 [loss: 0.378131, acc.: 86.47%] [G loss: 0.856697]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 62/86 [loss: 0.311931, acc.: 91.70%] [G loss: 0.850242]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 63/86 [loss: 0.376053, acc.: 87.50%] [G loss: 0.847635]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 64/86 [loss: 0.351809, acc.: 89.45%] [G loss: 0.865583]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 65/86 [loss: 0.343426, acc.: 89.21%] [G loss: 0.866028]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 66/86 [loss: 0.349357, acc.: 89.40%] [G loss: 0.890840]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 67/86 [loss: 0.366433, acc.: 87.84%] [G loss: 0.886470]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 68/86 [loss: 0.394048, acc.: 86.72%] [G loss: 0.859715]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 69/86 [loss: 0.368185, acc.: 88.04%] [G loss: 0.824784]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 70/86 [loss: 0.383713, acc.: 87.30%] [G loss: 0.839770]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 71/86 [loss: 0.369783, acc.: 87.99%] [G loss: 0.875043]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 72/86 [loss: 0.322350, acc.: 91.31%] [G loss: 0.884470]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 73/86 [loss: 0.331190, acc.: 90.43%] [G loss: 0.874564]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 74/86 [loss: 0.406208, acc.: 84.57%] [G loss: 0.895067]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 75/86 [loss: 0.393741, acc.: 86.04%] [G loss: 0.872464]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 76/86 [loss: 0.366450, acc.: 89.01%] [G loss: 0.853813]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 77/86 [loss: 0.387300, acc.: 86.72%] [G loss: 0.874762]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 78/86 [loss: 0.330974, acc.: 91.16%] [G loss: 0.875395]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 79/86 [loss: 0.369378, acc.: 87.84%] [G loss: 0.857052]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 80/86 [loss: 0.420105, acc.: 84.03%] [G loss: 0.843285]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 26/200  Batch Size: 81/86 [loss: 0.401446, acc.: 85.45%] [G loss: 0.862840]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 82/86 [loss: 0.401386, acc.: 85.55%] [G loss: 0.829162]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 26/200  Batch Size: 83/86 [loss: 0.397461, acc.: 84.77%] [G loss: 0.885728]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 26/200  Batch Size: 84/86 [loss: 0.326251, acc.: 90.67%] [G loss: 0.884607]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 26/200  Batch Size: 85/86 [loss: 0.401122, acc.: 85.40%] [G loss: 0.840333]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 0/86 [loss: 0.440475, acc.: 81.93%] [G loss: 0.839666]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 1/86 [loss: 0.402762, acc.: 86.18%] [G loss: 0.880626]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 2/86 [loss: 0.305470, acc.: 91.55%] [G loss: 0.845765]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 3/86 [loss: 0.380560, acc.: 86.72%] [G loss: 0.902431]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 4/86 [loss: 0.339298, acc.: 90.28%] [G loss: 0.870175]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 5/86 [loss: 0.340200, acc.: 89.65%] [G loss: 0.861846]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 6/86 [loss: 0.356290, acc.: 89.31%] [G loss: 0.881537]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 7/86 [loss: 0.321938, acc.: 91.60%] [G loss: 0.866900]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 8/86 [loss: 0.394186, acc.: 86.28%] [G loss: 0.888732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 9/86 [loss: 0.448858, acc.: 82.42%] [G loss: 0.851858]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 10/86 [loss: 0.314024, acc.: 92.43%] [G loss: 0.882650]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 11/86 [loss: 0.333896, acc.: 90.28%] [G loss: 0.840825]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 12/86 [loss: 0.363517, acc.: 89.31%] [G loss: 0.877637]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 13/86 [loss: 0.396179, acc.: 85.99%] [G loss: 0.835170]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 14/86 [loss: 0.326177, acc.: 90.87%] [G loss: 0.877040]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 15/86 [loss: 0.390394, acc.: 86.33%] [G loss: 0.847943]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 16/86 [loss: 0.372447, acc.: 87.21%] [G loss: 0.873903]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 17/86 [loss: 0.400946, acc.: 85.84%] [G loss: 0.877510]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 18/86 [loss: 0.346574, acc.: 89.16%] [G loss: 0.855822]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 19/86 [loss: 0.360475, acc.: 88.77%] [G loss: 0.859814]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 20/86 [loss: 0.484853, acc.: 78.76%] [G loss: 0.853619]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 21/86 [loss: 0.360101, acc.: 88.28%] [G loss: 0.808445]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 22/86 [loss: 0.404667, acc.: 85.40%] [G loss: 0.813587]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 23/86 [loss: 0.393842, acc.: 86.33%] [G loss: 0.859146]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 24/86 [loss: 0.409900, acc.: 85.11%] [G loss: 0.906549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 25/86 [loss: 0.438881, acc.: 82.81%] [G loss: 0.888063]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 26/86 [loss: 0.445348, acc.: 82.13%] [G loss: 0.870266]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 27/86 [loss: 0.368095, acc.: 88.28%] [G loss: 0.859298]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 28/86 [loss: 0.418194, acc.: 83.25%] [G loss: 0.869256]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 29/86 [loss: 0.394095, acc.: 85.25%] [G loss: 0.844612]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 30/86 [loss: 0.351730, acc.: 89.65%] [G loss: 0.893962]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 31/86 [loss: 0.343033, acc.: 90.62%] [G loss: 0.851632]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 32/86 [loss: 0.353264, acc.: 88.82%] [G loss: 0.875164]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 33/86 [loss: 0.393751, acc.: 87.16%] [G loss: 0.886289]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 34/86 [loss: 0.394863, acc.: 86.47%] [G loss: 0.911574]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 35/86 [loss: 0.323339, acc.: 90.28%] [G loss: 0.843379]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 36/86 [loss: 0.302819, acc.: 92.43%] [G loss: 0.879408]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 37/86 [loss: 0.332511, acc.: 90.33%] [G loss: 0.872559]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 38/86 [loss: 0.342305, acc.: 89.89%] [G loss: 0.886610]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 39/86 [loss: 0.346579, acc.: 89.50%] [G loss: 0.866793]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 40/86 [loss: 0.381830, acc.: 87.40%] [G loss: 0.860142]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 41/86 [loss: 0.377757, acc.: 88.57%] [G loss: 0.866286]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 42/86 [loss: 0.358002, acc.: 88.82%] [G loss: 0.866079]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 43/86 [loss: 0.349898, acc.: 89.06%] [G loss: 0.831681]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 44/86 [loss: 0.351855, acc.: 89.45%] [G loss: 0.870228]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 45/86 [loss: 0.364069, acc.: 88.57%] [G loss: 0.924891]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 46/86 [loss: 0.379102, acc.: 87.55%] [G loss: 0.884913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 47/86 [loss: 0.338798, acc.: 90.19%] [G loss: 0.902616]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 48/86 [loss: 0.340976, acc.: 89.40%] [G loss: 0.913763]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 49/86 [loss: 0.410903, acc.: 85.01%] [G loss: 0.871107]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 50/86 [loss: 0.377321, acc.: 87.84%] [G loss: 0.851075]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 51/86 [loss: 0.382969, acc.: 87.40%] [G loss: 0.859285]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 52/86 [loss: 0.397328, acc.: 84.96%] [G loss: 0.838298]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 53/86 [loss: 0.388133, acc.: 87.26%] [G loss: 0.820751]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 54/86 [loss: 0.430701, acc.: 83.40%] [G loss: 0.890159]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 55/86 [loss: 0.391347, acc.: 87.06%] [G loss: 0.840373]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 56/86 [loss: 0.438598, acc.: 82.13%] [G loss: 0.875313]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 57/86 [loss: 0.358241, acc.: 88.82%] [G loss: 0.863326]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 58/86 [loss: 0.370410, acc.: 88.18%] [G loss: 0.850999]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 59/86 [loss: 0.411801, acc.: 84.33%] [G loss: 0.880324]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 60/86 [loss: 0.442630, acc.: 81.88%] [G loss: 0.874894]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 61/86 [loss: 0.371052, acc.: 88.04%] [G loss: 0.835079]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 62/86 [loss: 0.393083, acc.: 86.52%] [G loss: 0.824517]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 63/86 [loss: 0.362121, acc.: 88.53%] [G loss: 0.869460]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 64/86 [loss: 0.419895, acc.: 84.42%] [G loss: 0.856686]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 65/86 [loss: 0.428569, acc.: 82.13%] [G loss: 0.895824]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 66/86 [loss: 0.412609, acc.: 84.28%] [G loss: 0.851869]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 67/86 [loss: 0.455734, acc.: 81.15%] [G loss: 0.839855]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 68/86 [loss: 0.294241, acc.: 93.16%] [G loss: 0.879408]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 69/86 [loss: 0.383650, acc.: 86.08%] [G loss: 0.906316]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 70/86 [loss: 0.354276, acc.: 89.36%] [G loss: 0.877241]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 71/86 [loss: 0.391601, acc.: 86.43%] [G loss: 0.850276]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 72/86 [loss: 0.344264, acc.: 88.92%] [G loss: 0.878468]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 73/86 [loss: 0.364883, acc.: 87.89%] [G loss: 0.884070]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 74/86 [loss: 0.384221, acc.: 87.06%] [G loss: 0.854388]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 75/86 [loss: 0.380226, acc.: 87.06%] [G loss: 0.781166]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 76/86 [loss: 0.364435, acc.: 88.04%] [G loss: 0.863440]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 77/86 [loss: 0.352071, acc.: 88.82%] [G loss: 0.860616]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 78/86 [loss: 0.360634, acc.: 88.67%] [G loss: 0.900024]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 79/86 [loss: 0.381526, acc.: 87.11%] [G loss: 0.850435]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 80/86 [loss: 0.337173, acc.: 90.48%] [G loss: 0.887747]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 81/86 [loss: 0.435500, acc.: 82.81%] [G loss: 0.811177]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 27/200  Batch Size: 82/86 [loss: 0.377038, acc.: 87.45%] [G loss: 0.857148]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 27/200  Batch Size: 83/86 [loss: 0.430805, acc.: 82.32%] [G loss: 0.869636]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 27/200  Batch Size: 84/86 [loss: 0.377278, acc.: 87.45%] [G loss: 0.923307]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 27/200  Batch Size: 85/86 [loss: 0.399223, acc.: 86.13%] [G loss: 0.839506]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 0/86 [loss: 0.378007, acc.: 87.11%] [G loss: 0.867903]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 1/86 [loss: 0.396299, acc.: 86.04%] [G loss: 0.838125]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 2/86 [loss: 0.424642, acc.: 82.76%] [G loss: 0.859471]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 3/86 [loss: 0.407797, acc.: 86.04%] [G loss: 0.872118]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 4/86 [loss: 0.383442, acc.: 86.52%] [G loss: 0.868330]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 5/86 [loss: 0.335920, acc.: 90.58%] [G loss: 0.842568]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 6/86 [loss: 0.388167, acc.: 85.89%] [G loss: 0.842558]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 7/86 [loss: 0.371729, acc.: 88.28%] [G loss: 0.884819]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 8/86 [loss: 0.366172, acc.: 87.74%] [G loss: 0.906958]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 9/86 [loss: 0.381695, acc.: 87.89%] [G loss: 0.900216]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 10/86 [loss: 0.316847, acc.: 91.89%] [G loss: 0.876786]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 11/86 [loss: 0.394145, acc.: 86.33%] [G loss: 0.866440]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 12/86 [loss: 0.325661, acc.: 90.62%] [G loss: 0.832153]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 13/86 [loss: 0.409460, acc.: 84.91%] [G loss: 0.849369]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 14/86 [loss: 0.403689, acc.: 83.59%] [G loss: 0.920656]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 15/86 [loss: 0.367177, acc.: 88.72%] [G loss: 0.859849]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 16/86 [loss: 0.476521, acc.: 79.00%] [G loss: 0.884443]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 17/86 [loss: 0.301138, acc.: 92.43%] [G loss: 0.833673]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 18/86 [loss: 0.373032, acc.: 88.04%] [G loss: 0.827866]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 19/86 [loss: 0.357070, acc.: 88.48%] [G loss: 0.855554]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 20/86 [loss: 0.341108, acc.: 90.43%] [G loss: 0.881827]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 21/86 [loss: 0.379251, acc.: 87.60%] [G loss: 0.859211]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 22/86 [loss: 0.428590, acc.: 83.01%] [G loss: 0.866570]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 23/86 [loss: 0.295027, acc.: 92.97%] [G loss: 0.838029]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 24/86 [loss: 0.360848, acc.: 88.53%] [G loss: 0.860004]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 25/86 [loss: 0.378927, acc.: 87.16%] [G loss: 0.842672]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 26/86 [loss: 0.403039, acc.: 85.89%] [G loss: 0.857195]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 27/86 [loss: 0.395880, acc.: 85.60%] [G loss: 0.853707]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 28/86 [loss: 0.340010, acc.: 89.94%] [G loss: 0.815522]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 29/86 [loss: 0.424454, acc.: 83.89%] [G loss: 0.841586]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 30/86 [loss: 0.372813, acc.: 87.65%] [G loss: 0.855895]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 31/86 [loss: 0.356248, acc.: 88.77%] [G loss: 0.836265]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 32/86 [loss: 0.398555, acc.: 85.69%] [G loss: 0.826758]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 33/86 [loss: 0.375695, acc.: 88.33%] [G loss: 0.854447]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 34/86 [loss: 0.370096, acc.: 88.33%] [G loss: 0.861182]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 35/86 [loss: 0.373198, acc.: 87.84%] [G loss: 0.863148]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 36/86 [loss: 0.324098, acc.: 92.09%] [G loss: 0.870901]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 37/86 [loss: 0.343610, acc.: 89.55%] [G loss: 0.891676]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 38/86 [loss: 0.405035, acc.: 86.33%] [G loss: 0.891391]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 39/86 [loss: 0.455172, acc.: 79.79%] [G loss: 0.882658]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 40/86 [loss: 0.336070, acc.: 90.19%] [G loss: 0.851261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 41/86 [loss: 0.374218, acc.: 88.38%] [G loss: 0.862843]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 42/86 [loss: 0.413120, acc.: 84.38%] [G loss: 0.861412]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 43/86 [loss: 0.396505, acc.: 86.43%] [G loss: 0.876773]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 44/86 [loss: 0.381124, acc.: 87.21%] [G loss: 0.866576]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 45/86 [loss: 0.327696, acc.: 91.80%] [G loss: 0.846079]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 46/86 [loss: 0.278204, acc.: 93.31%] [G loss: 0.854278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 47/86 [loss: 0.377193, acc.: 87.06%] [G loss: 0.869407]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 48/86 [loss: 0.238928, acc.: 96.00%] [G loss: 0.929049]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 49/86 [loss: 0.323440, acc.: 91.06%] [G loss: 0.923408]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 50/86 [loss: 0.289736, acc.: 93.36%] [G loss: 0.862319]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 51/86 [loss: 0.331796, acc.: 89.70%] [G loss: 0.892905]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 52/86 [loss: 0.295066, acc.: 92.77%] [G loss: 0.880745]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 53/86 [loss: 0.314082, acc.: 91.65%] [G loss: 0.860316]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 54/86 [loss: 0.414177, acc.: 83.69%] [G loss: 0.878662]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 55/86 [loss: 0.342896, acc.: 90.43%] [G loss: 0.889795]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 56/86 [loss: 0.365100, acc.: 88.33%] [G loss: 0.855737]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 57/86 [loss: 0.306910, acc.: 92.33%] [G loss: 0.861269]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 58/86 [loss: 0.357668, acc.: 88.87%] [G loss: 0.890796]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 59/86 [loss: 0.315741, acc.: 92.14%] [G loss: 0.873908]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 60/86 [loss: 0.331479, acc.: 90.67%] [G loss: 0.892346]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 28/200  Batch Size: 61/86 [loss: 0.300089, acc.: 92.87%] [G loss: 0.863456]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 62/86 [loss: 0.403120, acc.: 85.50%] [G loss: 0.875319]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 63/86 [loss: 0.310883, acc.: 92.82%] [G loss: 0.856049]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 64/86 [loss: 0.374705, acc.: 86.91%] [G loss: 0.885964]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 65/86 [loss: 0.405096, acc.: 84.62%] [G loss: 0.845237]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 66/86 [loss: 0.384237, acc.: 86.82%] [G loss: 0.856824]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 67/86 [loss: 0.359456, acc.: 88.92%] [G loss: 0.875410]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 68/86 [loss: 0.366746, acc.: 87.84%] [G loss: 0.891367]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 69/86 [loss: 0.358985, acc.: 88.72%] [G loss: 0.851362]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 70/86 [loss: 0.363425, acc.: 89.21%] [G loss: 0.839978]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 71/86 [loss: 0.383670, acc.: 87.06%] [G loss: 0.855249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 72/86 [loss: 0.403773, acc.: 85.99%] [G loss: 0.900811]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 73/86 [loss: 0.369743, acc.: 88.57%] [G loss: 0.876928]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 74/86 [loss: 0.379585, acc.: 87.26%] [G loss: 0.875048]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 75/86 [loss: 0.390120, acc.: 86.77%] [G loss: 0.871590]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 76/86 [loss: 0.365917, acc.: 88.48%] [G loss: 0.814646]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 77/86 [loss: 0.376527, acc.: 87.26%] [G loss: 0.866767]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 78/86 [loss: 0.333533, acc.: 89.70%] [G loss: 0.876259]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 79/86 [loss: 0.351974, acc.: 89.36%] [G loss: 0.886325]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 80/86 [loss: 0.406441, acc.: 85.21%] [G loss: 0.856197]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 81/86 [loss: 0.422539, acc.: 84.23%] [G loss: 0.875571]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 82/86 [loss: 0.418581, acc.: 84.42%] [G loss: 0.841648]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 28/200  Batch Size: 83/86 [loss: 0.366829, acc.: 88.48%] [G loss: 0.912871]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 28/200  Batch Size: 84/86 [loss: 0.272783, acc.: 94.63%] [G loss: 0.861541]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 28/200  Batch Size: 85/86 [loss: 0.375708, acc.: 87.79%] [G loss: 0.814147]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 0/86 [loss: 0.311224, acc.: 91.65%] [G loss: 0.910901]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 1/86 [loss: 0.500681, acc.: 76.61%] [G loss: 0.833071]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 2/86 [loss: 0.400317, acc.: 86.13%] [G loss: 0.834859]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 3/86 [loss: 0.311404, acc.: 91.94%] [G loss: 0.880646]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 4/86 [loss: 0.416994, acc.: 84.18%] [G loss: 0.838615]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 5/86 [loss: 0.329532, acc.: 91.21%] [G loss: 0.856558]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 6/86 [loss: 0.377869, acc.: 87.35%] [G loss: 0.880637]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 7/86 [loss: 0.366100, acc.: 88.13%] [G loss: 0.907481]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 8/86 [loss: 0.357156, acc.: 87.70%] [G loss: 0.838909]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 9/86 [loss: 0.351629, acc.: 89.60%] [G loss: 0.855682]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 10/86 [loss: 0.379502, acc.: 87.30%] [G loss: 0.892934]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 11/86 [loss: 0.398405, acc.: 85.21%] [G loss: 0.863794]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 12/86 [loss: 0.444285, acc.: 82.08%] [G loss: 0.846294]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 13/86 [loss: 0.362690, acc.: 89.40%] [G loss: 0.867016]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 14/86 [loss: 0.364906, acc.: 88.72%] [G loss: 0.878637]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 15/86 [loss: 0.313230, acc.: 91.75%] [G loss: 0.901752]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 16/86 [loss: 0.345216, acc.: 89.55%] [G loss: 0.875605]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 17/86 [loss: 0.338420, acc.: 89.84%] [G loss: 0.889804]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 18/86 [loss: 0.292446, acc.: 92.77%] [G loss: 0.863858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 19/86 [loss: 0.398900, acc.: 86.28%] [G loss: 0.847661]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 20/86 [loss: 0.334721, acc.: 89.36%] [G loss: 0.893408]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 21/86 [loss: 0.333946, acc.: 90.58%] [G loss: 0.892226]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 22/86 [loss: 0.330424, acc.: 91.36%] [G loss: 0.865847]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 23/86 [loss: 0.342809, acc.: 89.70%] [G loss: 0.870171]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 24/86 [loss: 0.395444, acc.: 86.18%] [G loss: 0.886351]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 25/86 [loss: 0.352465, acc.: 89.21%] [G loss: 0.847042]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 26/86 [loss: 0.333698, acc.: 90.43%] [G loss: 0.855140]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 27/86 [loss: 0.351551, acc.: 89.89%] [G loss: 0.892490]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 28/86 [loss: 0.370762, acc.: 87.55%] [G loss: 0.853612]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 29/86 [loss: 0.377305, acc.: 87.11%] [G loss: 0.858542]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 30/86 [loss: 0.349584, acc.: 89.31%] [G loss: 0.876964]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 31/86 [loss: 0.357755, acc.: 89.55%] [G loss: 0.881479]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 32/86 [loss: 0.346132, acc.: 89.55%] [G loss: 0.822631]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 33/86 [loss: 0.320802, acc.: 91.41%] [G loss: 0.898499]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 34/86 [loss: 0.476051, acc.: 78.42%] [G loss: 0.863748]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 35/86 [loss: 0.297523, acc.: 93.36%] [G loss: 0.843155]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 36/86 [loss: 0.382390, acc.: 86.43%] [G loss: 0.836911]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 37/86 [loss: 0.336919, acc.: 90.43%] [G loss: 0.884250]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 38/86 [loss: 0.338772, acc.: 90.53%] [G loss: 0.870167]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 39/86 [loss: 0.391396, acc.: 86.33%] [G loss: 0.888372]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 40/86 [loss: 0.309719, acc.: 92.24%] [G loss: 0.899909]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 41/86 [loss: 0.312330, acc.: 91.70%] [G loss: 0.898408]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 42/86 [loss: 0.331427, acc.: 91.26%] [G loss: 0.866122]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 43/86 [loss: 0.457001, acc.: 81.45%] [G loss: 0.823012]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 44/86 [loss: 0.368924, acc.: 88.77%] [G loss: 0.852616]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 45/86 [loss: 0.380966, acc.: 87.21%] [G loss: 0.854750]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 46/86 [loss: 0.396440, acc.: 85.94%] [G loss: 0.846106]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 47/86 [loss: 0.397802, acc.: 85.79%] [G loss: 0.887239]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 48/86 [loss: 0.419115, acc.: 83.98%] [G loss: 0.850045]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 49/86 [loss: 0.419404, acc.: 85.06%] [G loss: 0.889852]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 50/86 [loss: 0.362913, acc.: 88.23%] [G loss: 0.889361]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 51/86 [loss: 0.404447, acc.: 85.25%] [G loss: 0.889676]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 52/86 [loss: 0.341494, acc.: 91.06%] [G loss: 0.855828]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 53/86 [loss: 0.371719, acc.: 88.43%] [G loss: 0.816042]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 54/86 [loss: 0.336638, acc.: 90.92%] [G loss: 0.869909]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 55/86 [loss: 0.357091, acc.: 88.67%] [G loss: 0.864780]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 56/86 [loss: 0.304927, acc.: 92.92%] [G loss: 0.848635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 57/86 [loss: 0.383304, acc.: 87.99%] [G loss: 0.865378]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 58/86 [loss: 0.413856, acc.: 84.42%] [G loss: 0.874915]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 59/86 [loss: 0.347994, acc.: 89.65%] [G loss: 0.864329]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 60/86 [loss: 0.420570, acc.: 83.98%] [G loss: 0.851019]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 61/86 [loss: 0.361008, acc.: 88.92%] [G loss: 0.890844]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 62/86 [loss: 0.421982, acc.: 83.11%] [G loss: 0.830608]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 63/86 [loss: 0.402026, acc.: 85.16%] [G loss: 0.832577]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 64/86 [loss: 0.374013, acc.: 88.13%] [G loss: 0.864150]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 65/86 [loss: 0.301163, acc.: 92.97%] [G loss: 0.855999]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 66/86 [loss: 0.325693, acc.: 91.16%] [G loss: 0.866511]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 67/86 [loss: 0.369329, acc.: 88.96%] [G loss: 0.889913]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 68/86 [loss: 0.329521, acc.: 91.65%] [G loss: 0.832310]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 69/86 [loss: 0.272161, acc.: 94.29%] [G loss: 0.879091]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 70/86 [loss: 0.385804, acc.: 87.89%] [G loss: 0.897695]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 71/86 [loss: 0.292028, acc.: 94.09%] [G loss: 0.869849]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 72/86 [loss: 0.363091, acc.: 89.06%] [G loss: 0.824970]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 73/86 [loss: 0.330399, acc.: 89.79%] [G loss: 0.917771]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 74/86 [loss: 0.344926, acc.: 89.55%] [G loss: 0.906814]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 75/86 [loss: 0.297729, acc.: 92.77%] [G loss: 0.895922]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 76/86 [loss: 0.299473, acc.: 92.68%] [G loss: 0.874262]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 77/86 [loss: 0.323903, acc.: 91.46%] [G loss: 0.854320]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 29/200  Batch Size: 78/86 [loss: 0.293798, acc.: 93.16%] [G loss: 0.876860]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 79/86 [loss: 0.356048, acc.: 88.96%] [G loss: 0.887354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 80/86 [loss: 0.277543, acc.: 94.24%] [G loss: 0.845850]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 81/86 [loss: 0.338036, acc.: 90.62%] [G loss: 0.856325]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 82/86 [loss: 0.338975, acc.: 90.38%] [G loss: 0.850069]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 29/200  Batch Size: 83/86 [loss: 0.296330, acc.: 93.31%] [G loss: 0.891222]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 29/200  Batch Size: 84/86 [loss: 0.386616, acc.: 86.52%] [G loss: 0.872224]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 29/200  Batch Size: 85/86 [loss: 0.327005, acc.: 90.97%] [G loss: 0.868273]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 0/86 [loss: 0.352972, acc.: 89.50%] [G loss: 0.886009]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 1/86 [loss: 0.373171, acc.: 87.79%] [G loss: 0.884928]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 2/86 [loss: 0.298814, acc.: 93.07%] [G loss: 0.904736]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 3/86 [loss: 0.359324, acc.: 88.43%] [G loss: 0.902650]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 4/86 [loss: 0.298350, acc.: 92.72%] [G loss: 0.866760]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 5/86 [loss: 0.318963, acc.: 90.38%] [G loss: 0.822643]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 6/86 [loss: 0.305006, acc.: 93.26%] [G loss: 0.858779]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 7/86 [loss: 0.363090, acc.: 88.96%] [G loss: 0.908054]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 8/86 [loss: 0.313689, acc.: 92.43%] [G loss: 0.888202]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 9/86 [loss: 0.303143, acc.: 92.68%] [G loss: 0.860494]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 10/86 [loss: 0.343153, acc.: 90.33%] [G loss: 0.858514]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 11/86 [loss: 0.366533, acc.: 87.94%] [G loss: 0.898607]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 12/86 [loss: 0.347648, acc.: 90.23%] [G loss: 0.842937]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 13/86 [loss: 0.364473, acc.: 88.33%] [G loss: 0.875699]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 14/86 [loss: 0.296331, acc.: 92.87%] [G loss: 0.919080]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 15/86 [loss: 0.378732, acc.: 86.57%] [G loss: 0.798332]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 16/86 [loss: 0.372029, acc.: 87.60%] [G loss: 0.847436]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 17/86 [loss: 0.369567, acc.: 88.13%] [G loss: 0.867605]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 18/86 [loss: 0.332061, acc.: 90.48%] [G loss: 0.870993]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 19/86 [loss: 0.347932, acc.: 89.79%] [G loss: 0.910745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 20/86 [loss: 0.324173, acc.: 91.26%] [G loss: 0.873695]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 21/86 [loss: 0.394096, acc.: 85.60%] [G loss: 0.861862]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 22/86 [loss: 0.398831, acc.: 84.96%] [G loss: 0.831399]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 23/86 [loss: 0.374618, acc.: 87.84%] [G loss: 0.874631]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 24/86 [loss: 0.393769, acc.: 85.64%] [G loss: 0.878030]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 25/86 [loss: 0.419160, acc.: 84.72%] [G loss: 0.841871]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 26/86 [loss: 0.418020, acc.: 84.62%] [G loss: 0.856387]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 27/86 [loss: 0.336678, acc.: 90.19%] [G loss: 0.934001]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 28/86 [loss: 0.378786, acc.: 87.30%] [G loss: 0.903371]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 29/86 [loss: 0.445249, acc.: 82.28%] [G loss: 0.884666]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 30/86 [loss: 0.395898, acc.: 85.50%] [G loss: 0.856972]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 31/86 [loss: 0.376005, acc.: 88.38%] [G loss: 0.838573]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 32/86 [loss: 0.371832, acc.: 87.45%] [G loss: 0.813158]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 33/86 [loss: 0.428106, acc.: 83.20%] [G loss: 0.844000]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 34/86 [loss: 0.408323, acc.: 85.01%] [G loss: 0.865174]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 35/86 [loss: 0.366060, acc.: 88.48%] [G loss: 0.884495]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 36/86 [loss: 0.417595, acc.: 84.18%] [G loss: 0.863261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 37/86 [loss: 0.433385, acc.: 82.71%] [G loss: 0.887771]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 38/86 [loss: 0.350364, acc.: 89.70%] [G loss: 0.866004]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 39/86 [loss: 0.465839, acc.: 79.59%] [G loss: 0.826443]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 40/86 [loss: 0.373255, acc.: 88.09%] [G loss: 0.867947]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 41/86 [loss: 0.383510, acc.: 86.87%] [G loss: 0.874609]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 42/86 [loss: 0.411105, acc.: 84.23%] [G loss: 0.892617]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 43/86 [loss: 0.311245, acc.: 91.21%] [G loss: 0.902364]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 44/86 [loss: 0.354241, acc.: 89.65%] [G loss: 0.899178]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 45/86 [loss: 0.341172, acc.: 89.06%] [G loss: 0.853837]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 46/86 [loss: 0.336928, acc.: 90.48%] [G loss: 0.884871]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 47/86 [loss: 0.340762, acc.: 89.75%] [G loss: 0.841236]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 48/86 [loss: 0.346294, acc.: 90.43%] [G loss: 0.846226]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 49/86 [loss: 0.414484, acc.: 84.52%] [G loss: 0.860217]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 50/86 [loss: 0.385041, acc.: 87.94%] [G loss: 0.871237]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 51/86 [loss: 0.358461, acc.: 88.67%] [G loss: 0.856001]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 52/86 [loss: 0.336510, acc.: 89.99%] [G loss: 0.874260]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 53/86 [loss: 0.337782, acc.: 90.43%] [G loss: 0.865644]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 54/86 [loss: 0.388415, acc.: 86.77%] [G loss: 0.867643]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 55/86 [loss: 0.368211, acc.: 87.55%] [G loss: 0.901624]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 56/86 [loss: 0.336549, acc.: 90.87%] [G loss: 0.878091]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 57/86 [loss: 0.382737, acc.: 86.62%] [G loss: 0.847817]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 58/86 [loss: 0.363347, acc.: 88.13%] [G loss: 0.899542]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 59/86 [loss: 0.348780, acc.: 89.26%] [G loss: 0.845197]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 60/86 [loss: 0.376863, acc.: 86.62%] [G loss: 0.878722]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 61/86 [loss: 0.370798, acc.: 88.43%] [G loss: 0.889500]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 62/86 [loss: 0.364509, acc.: 88.38%] [G loss: 0.870865]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 63/86 [loss: 0.348804, acc.: 89.65%] [G loss: 0.853192]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 64/86 [loss: 0.374758, acc.: 87.65%] [G loss: 0.892466]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 65/86 [loss: 0.385312, acc.: 86.57%] [G loss: 0.854576]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 66/86 [loss: 0.400786, acc.: 85.01%] [G loss: 0.861654]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 67/86 [loss: 0.393452, acc.: 86.04%] [G loss: 0.858747]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 68/86 [loss: 0.362568, acc.: 88.62%] [G loss: 0.895088]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 69/86 [loss: 0.345538, acc.: 89.84%] [G loss: 0.877452]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 70/86 [loss: 0.371554, acc.: 87.65%] [G loss: 0.858567]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 71/86 [loss: 0.391249, acc.: 86.04%] [G loss: 0.850033]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 72/86 [loss: 0.347010, acc.: 89.65%] [G loss: 0.895320]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 73/86 [loss: 0.375110, acc.: 87.65%] [G loss: 0.847635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 74/86 [loss: 0.369507, acc.: 88.23%] [G loss: 0.847003]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 75/86 [loss: 0.397978, acc.: 86.67%] [G loss: 0.843539]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 30/200  Batch Size: 76/86 [loss: 0.377634, acc.: 87.35%] [G loss: 0.845075]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 77/86 [loss: 0.374566, acc.: 88.62%] [G loss: 0.881385]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 78/86 [loss: 0.364276, acc.: 88.33%] [G loss: 0.859366]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 79/86 [loss: 0.344064, acc.: 89.50%] [G loss: 0.868376]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 80/86 [loss: 0.329935, acc.: 90.87%] [G loss: 0.873962]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 81/86 [loss: 0.393545, acc.: 85.11%] [G loss: 0.865934]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 82/86 [loss: 0.368804, acc.: 87.26%] [G loss: 0.848381]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 30/200  Batch Size: 83/86 [loss: 0.344112, acc.: 89.40%] [G loss: 0.804205]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 30/200  Batch Size: 84/86 [loss: 0.333824, acc.: 90.62%] [G loss: 0.867928]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 30/200  Batch Size: 85/86 [loss: 0.323791, acc.: 90.23%] [G loss: 0.912392]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 0/86 [loss: 0.307315, acc.: 91.85%] [G loss: 0.879001]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 1/86 [loss: 0.360557, acc.: 89.06%] [G loss: 0.838298]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 2/86 [loss: 0.394156, acc.: 86.08%] [G loss: 0.812486]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 31/200  Batch Size: 3/86 [loss: 0.312971, acc.: 92.24%] [G loss: 0.854537]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 4/86 [loss: 0.381736, acc.: 86.87%] [G loss: 0.834129]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 5/86 [loss: 0.375557, acc.: 87.35%] [G loss: 0.901731]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 6/86 [loss: 0.278411, acc.: 93.85%] [G loss: 0.914476]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 7/86 [loss: 0.365181, acc.: 88.38%] [G loss: 0.872323]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 8/86 [loss: 0.366895, acc.: 87.79%] [G loss: 0.873565]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 9/86 [loss: 0.364081, acc.: 88.38%] [G loss: 0.822197]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 10/86 [loss: 0.368653, acc.: 88.04%] [G loss: 0.841345]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 11/86 [loss: 0.325870, acc.: 91.31%] [G loss: 0.852832]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 12/86 [loss: 0.295485, acc.: 92.87%] [G loss: 0.896033]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 13/86 [loss: 0.363510, acc.: 88.82%] [G loss: 0.907720]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 31/200  Batch Size: 14/86 [loss: 0.343332, acc.: 90.14%] [G loss: 0.875819]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 15/86 [loss: 0.314469, acc.: 92.48%] [G loss: 0.873763]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 16/86 [loss: 0.338815, acc.: 90.77%] [G loss: 0.864532]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 17/86 [loss: 0.349724, acc.: 89.26%] [G loss: 0.894007]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 18/86 [loss: 0.470327, acc.: 80.27%] [G loss: 0.816872]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 19/86 [loss: 0.325804, acc.: 91.16%] [G loss: 0.888346]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 20/86 [loss: 0.313064, acc.: 91.75%] [G loss: 0.825612]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 31/200  Batch Size: 21/86 [loss: 0.432371, acc.: 83.06%] [G loss: 0.841948]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 22/86 [loss: 0.370251, acc.: 87.79%] [G loss: 0.876361]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 23/86 [loss: 0.319952, acc.: 92.09%] [G loss: 0.861044]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 24/86 [loss: 0.483162, acc.: 78.56%] [G loss: 0.859656]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 25/86 [loss: 0.338010, acc.: 90.19%] [G loss: 0.869478]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 26/86 [loss: 0.398700, acc.: 86.18%] [G loss: 0.837038]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 27/86 [loss: 0.331708, acc.: 90.97%] [G loss: 0.831680]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 28/86 [loss: 0.376920, acc.: 87.89%] [G loss: 0.840384]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 29/86 [loss: 0.373074, acc.: 88.38%] [G loss: 0.867609]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 30/86 [loss: 0.449548, acc.: 81.40%] [G loss: 0.893974]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 31/86 [loss: 0.436535, acc.: 83.01%] [G loss: 0.896720]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 32/86 [loss: 0.490032, acc.: 78.76%] [G loss: 0.847173]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 33/86 [loss: 0.329078, acc.: 90.48%] [G loss: 0.811271]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 34/86 [loss: 0.323090, acc.: 91.85%] [G loss: 0.852708]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 35/86 [loss: 0.424064, acc.: 82.76%] [G loss: 0.888493]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 31/200  Batch Size: 36/86 [loss: 0.333635, acc.: 90.82%] [G loss: 0.891607]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 37/86 [loss: 0.383953, acc.: 87.40%] [G loss: 0.883099]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 38/86 [loss: 0.327656, acc.: 90.58%] [G loss: 0.860934]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 39/86 [loss: 0.321873, acc.: 91.75%] [G loss: 0.914555]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 40/86 [loss: 0.362598, acc.: 88.09%] [G loss: 0.921667]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 41/86 [loss: 0.380042, acc.: 86.43%] [G loss: 0.919061]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 42/86 [loss: 0.367973, acc.: 86.52%] [G loss: 0.812385]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 43/86 [loss: 0.400817, acc.: 85.94%] [G loss: 0.851998]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 44/86 [loss: 0.311754, acc.: 92.68%] [G loss: 0.829101]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 45/86 [loss: 0.420864, acc.: 84.13%] [G loss: 0.847351]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 31/200  Batch Size: 46/86 [loss: 0.379498, acc.: 86.82%] [G loss: 0.898139]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 47/86 [loss: 0.396444, acc.: 86.33%] [G loss: 0.856031]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 48/86 [loss: 0.361370, acc.: 87.26%] [G loss: 0.813183]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 49/86 [loss: 0.418630, acc.: 84.47%] [G loss: 0.851866]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 50/86 [loss: 0.320800, acc.: 92.29%] [G loss: 0.858382]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 51/86 [loss: 0.387351, acc.: 86.28%] [G loss: 0.876210]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 52/86 [loss: 0.412654, acc.: 84.28%] [G loss: 0.866104]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 53/86 [loss: 0.325428, acc.: 90.33%] [G loss: 0.845330]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 54/86 [loss: 0.327985, acc.: 91.80%] [G loss: 0.883516]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 55/86 [loss: 0.407752, acc.: 84.18%] [G loss: 0.892987]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 56/86 [loss: 0.352645, acc.: 88.77%] [G loss: 0.862236]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 57/86 [loss: 0.423399, acc.: 83.98%] [G loss: 0.875852]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 58/86 [loss: 0.336820, acc.: 91.16%] [G loss: 0.900403]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 59/86 [loss: 0.415903, acc.: 83.64%] [G loss: 0.882365]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 60/86 [loss: 0.490149, acc.: 76.76%] [G loss: 0.831805]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 61/86 [loss: 0.352280, acc.: 88.77%] [G loss: 0.831466]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 62/86 [loss: 0.429113, acc.: 82.57%] [G loss: 0.907336]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 63/86 [loss: 0.349738, acc.: 89.31%] [G loss: 0.849428]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 31/200  Batch Size: 64/86 [loss: 0.282178, acc.: 94.04%] [G loss: 0.858558]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 65/86 [loss: 0.352496, acc.: 89.84%] [G loss: 0.875111]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 66/86 [loss: 0.437273, acc.: 82.37%] [G loss: 0.864374]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 67/86 [loss: 0.380692, acc.: 87.01%] [G loss: 0.881485]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 68/86 [loss: 0.323388, acc.: 91.55%] [G loss: 0.829484]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 69/86 [loss: 0.343585, acc.: 89.75%] [G loss: 0.824729]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 70/86 [loss: 0.388780, acc.: 86.38%] [G loss: 0.894628]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 31/200  Batch Size: 71/86 [loss: 0.307633, acc.: 92.38%] [G loss: 0.858881]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 72/86 [loss: 0.372178, acc.: 87.21%] [G loss: 0.909641]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 73/86 [loss: 0.311553, acc.: 92.33%] [G loss: 0.901948]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 74/86 [loss: 0.331920, acc.: 91.26%] [G loss: 0.855875]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 31/200  Batch Size: 75/86 [loss: 0.365818, acc.: 88.82%] [G loss: 0.856483]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 76/86 [loss: 0.311558, acc.: 92.14%] [G loss: 0.883372]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 77/86 [loss: 0.377494, acc.: 87.26%] [G loss: 0.901793]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 78/86 [loss: 0.299979, acc.: 92.63%] [G loss: 0.879499]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 79/86 [loss: 0.301894, acc.: 91.99%] [G loss: 0.834568]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 80/86 [loss: 0.403638, acc.: 85.50%] [G loss: 0.823463]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 81/86 [loss: 0.382852, acc.: 87.26%] [G loss: 0.843011]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 82/86 [loss: 0.411367, acc.: 84.96%] [G loss: 0.854640]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 83/86 [loss: 0.385297, acc.: 86.77%] [G loss: 0.887550]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 31/200  Batch Size: 84/86 [loss: 0.354493, acc.: 90.53%] [G loss: 0.867248]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 31/200  Batch Size: 85/86 [loss: 0.441050, acc.: 83.35%] [G loss: 0.859963]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 0/86 [loss: 0.332363, acc.: 89.65%] [G loss: 0.819637]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 32/200  Batch Size: 1/86 [loss: 0.460816, acc.: 80.71%] [G loss: 0.838333]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 2/86 [loss: 0.299733, acc.: 93.36%] [G loss: 0.880802]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 3/86 [loss: 0.348059, acc.: 89.75%] [G loss: 0.907864]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 4/86 [loss: 0.321930, acc.: 90.97%] [G loss: 0.851566]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 5/86 [loss: 0.379186, acc.: 86.67%] [G loss: 0.842706]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 6/86 [loss: 0.293351, acc.: 92.97%] [G loss: 0.910152]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 7/86 [loss: 0.337149, acc.: 90.33%] [G loss: 0.911064]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 8/86 [loss: 0.406581, acc.: 84.86%] [G loss: 0.874466]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 9/86 [loss: 0.426369, acc.: 83.11%] [G loss: 0.867876]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 10/86 [loss: 0.409049, acc.: 85.45%] [G loss: 0.892821]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 11/86 [loss: 0.447303, acc.: 80.91%] [G loss: 0.863678]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 12/86 [loss: 0.287540, acc.: 93.41%] [G loss: 0.871218]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 13/86 [loss: 0.397790, acc.: 85.74%] [G loss: 0.892127]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 14/86 [loss: 0.350617, acc.: 88.62%] [G loss: 0.879839]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 15/86 [loss: 0.334882, acc.: 90.38%] [G loss: 0.852553]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 16/86 [loss: 0.355878, acc.: 89.21%] [G loss: 0.800766]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 17/86 [loss: 0.315746, acc.: 91.60%] [G loss: 0.878025]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 18/86 [loss: 0.296300, acc.: 92.33%] [G loss: 0.911306]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 32/200  Batch Size: 19/86 [loss: 0.353357, acc.: 89.40%] [G loss: 0.892798]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 20/86 [loss: 0.298223, acc.: 92.58%] [G loss: 0.863006]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 21/86 [loss: 0.323471, acc.: 91.06%] [G loss: 0.881577]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 22/86 [loss: 0.294483, acc.: 93.36%] [G loss: 0.868318]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 23/86 [loss: 0.277806, acc.: 93.85%] [G loss: 0.905631]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 24/86 [loss: 0.277241, acc.: 94.97%] [G loss: 0.916753]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 25/86 [loss: 0.281686, acc.: 94.43%] [G loss: 0.859843]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 26/86 [loss: 0.354038, acc.: 88.77%] [G loss: 0.880517]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 27/86 [loss: 0.381355, acc.: 86.82%] [G loss: 0.859142]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 28/86 [loss: 0.264606, acc.: 94.92%] [G loss: 0.875861]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 29/86 [loss: 0.299248, acc.: 92.48%] [G loss: 0.885751]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 30/86 [loss: 0.340639, acc.: 90.77%] [G loss: 0.851580]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 31/86 [loss: 0.339011, acc.: 90.53%] [G loss: 0.898706]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 32/86 [loss: 0.348771, acc.: 90.04%] [G loss: 0.896212]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 33/86 [loss: 0.358175, acc.: 89.11%] [G loss: 0.831543]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 32/200  Batch Size: 34/86 [loss: 0.294783, acc.: 93.41%] [G loss: 0.840455]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 35/86 [loss: 0.397287, acc.: 84.81%] [G loss: 0.895998]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 36/86 [loss: 0.347969, acc.: 90.04%] [G loss: 0.842474]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 32/200  Batch Size: 37/86 [loss: 0.363646, acc.: 88.92%] [G loss: 0.860713]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 38/86 [loss: 0.307591, acc.: 92.53%] [G loss: 0.866698]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 39/86 [loss: 0.305098, acc.: 92.82%] [G loss: 0.858868]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 40/86 [loss: 0.303588, acc.: 92.53%] [G loss: 0.875961]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 32/200  Batch Size: 41/86 [loss: 0.340293, acc.: 90.14%] [G loss: 0.861862]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 42/86 [loss: 0.367110, acc.: 88.13%] [G loss: 0.865549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 43/86 [loss: 0.299918, acc.: 92.77%] [G loss: 0.885197]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 32/200  Batch Size: 44/86 [loss: 0.288563, acc.: 93.60%] [G loss: 0.874200]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 45/86 [loss: 0.366401, acc.: 88.96%] [G loss: 0.877122]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 46/86 [loss: 0.359956, acc.: 88.43%] [G loss: 0.831956]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 47/86 [loss: 0.351072, acc.: 90.19%] [G loss: 0.868842]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 48/86 [loss: 0.303025, acc.: 92.77%] [G loss: 0.869193]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 49/86 [loss: 0.353942, acc.: 89.16%] [G loss: 0.877613]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 50/86 [loss: 0.424616, acc.: 83.74%] [G loss: 0.845756]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 51/86 [loss: 0.442883, acc.: 82.52%] [G loss: 0.868886]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 52/86 [loss: 0.370383, acc.: 88.13%] [G loss: 0.849656]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 53/86 [loss: 0.339149, acc.: 90.33%] [G loss: 0.855799]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 54/86 [loss: 0.400524, acc.: 85.79%] [G loss: 0.842349]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 55/86 [loss: 0.370354, acc.: 88.13%] [G loss: 0.871911]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 56/86 [loss: 0.383253, acc.: 85.79%] [G loss: 0.910494]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 57/86 [loss: 0.286888, acc.: 93.80%] [G loss: 0.881263]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 58/86 [loss: 0.330184, acc.: 91.46%] [G loss: 0.850551]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 32/200  Batch Size: 59/86 [loss: 0.313882, acc.: 92.29%] [G loss: 0.846858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 60/86 [loss: 0.334337, acc.: 90.77%] [G loss: 0.881578]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 61/86 [loss: 0.367522, acc.: 88.38%] [G loss: 0.879195]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 62/86 [loss: 0.362258, acc.: 88.09%] [G loss: 0.887711]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 63/86 [loss: 0.344997, acc.: 89.55%] [G loss: 0.873024]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 64/86 [loss: 0.285103, acc.: 93.80%] [G loss: 0.897068]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 65/86 [loss: 0.341554, acc.: 89.50%] [G loss: 0.835415]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 66/86 [loss: 0.408301, acc.: 85.21%] [G loss: 0.832217]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 32/200  Batch Size: 67/86 [loss: 0.338191, acc.: 89.75%] [G loss: 0.847232]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 68/86 [loss: 0.418780, acc.: 84.13%] [G loss: 0.854824]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 69/86 [loss: 0.390591, acc.: 86.57%] [G loss: 0.854524]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 70/86 [loss: 0.328620, acc.: 91.11%] [G loss: 0.820955]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 71/86 [loss: 0.320314, acc.: 91.65%] [G loss: 0.865155]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 72/86 [loss: 0.406870, acc.: 85.35%] [G loss: 0.860265]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 73/86 [loss: 0.406354, acc.: 84.72%] [G loss: 0.847832]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 74/86 [loss: 0.443038, acc.: 82.08%] [G loss: 0.852720]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 75/86 [loss: 0.324616, acc.: 91.46%] [G loss: 0.860186]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 76/86 [loss: 0.433471, acc.: 83.30%] [G loss: 0.853370]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 77/86 [loss: 0.422681, acc.: 83.30%] [G loss: 0.894771]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 78/86 [loss: 0.329854, acc.: 90.28%] [G loss: 0.860985]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 79/86 [loss: 0.387197, acc.: 86.43%] [G loss: 0.854478]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 32/200  Batch Size: 80/86 [loss: 0.386865, acc.: 86.52%] [G loss: 0.895975]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 81/86 [loss: 0.343155, acc.: 89.40%] [G loss: 0.898917]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 82/86 [loss: 0.239772, acc.: 95.41%] [G loss: 0.887521]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 32/200  Batch Size: 83/86 [loss: 0.337477, acc.: 90.67%] [G loss: 0.895248]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 84/86 [loss: 0.292435, acc.: 93.41%] [G loss: 0.883959]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 32/200  Batch Size: 85/86 [loss: 0.359295, acc.: 89.01%] [G loss: 0.866721]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 0/86 [loss: 0.313081, acc.: 92.87%] [G loss: 0.870361]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 1/86 [loss: 0.342601, acc.: 90.04%] [G loss: 0.839132]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 2/86 [loss: 0.360229, acc.: 89.26%] [G loss: 0.907852]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 3/86 [loss: 0.365639, acc.: 88.23%] [G loss: 0.886421]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 4/86 [loss: 0.339859, acc.: 90.28%] [G loss: 0.862207]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 5/86 [loss: 0.322151, acc.: 91.41%] [G loss: 0.893407]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 6/86 [loss: 0.334128, acc.: 90.48%] [G loss: 0.889054]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 7/86 [loss: 0.329883, acc.: 90.04%] [G loss: 0.926151]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 8/86 [loss: 0.345142, acc.: 89.75%] [G loss: 0.860019]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 9/86 [loss: 0.260048, acc.: 94.97%] [G loss: 0.875914]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 10/86 [loss: 0.362013, acc.: 89.60%] [G loss: 0.860564]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 11/86 [loss: 0.292929, acc.: 92.97%] [G loss: 0.847676]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 12/86 [loss: 0.333942, acc.: 90.97%] [G loss: 0.860401]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 13/86 [loss: 0.345475, acc.: 90.48%] [G loss: 0.898033]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 14/86 [loss: 0.371135, acc.: 87.84%] [G loss: 0.854856]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 15/86 [loss: 0.371321, acc.: 87.35%] [G loss: 0.829056]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 16/86 [loss: 0.376635, acc.: 88.09%] [G loss: 0.872047]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 17/86 [loss: 0.429716, acc.: 82.96%] [G loss: 0.862175]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 18/86 [loss: 0.364338, acc.: 88.53%] [G loss: 0.841895]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 19/86 [loss: 0.383477, acc.: 86.96%] [G loss: 0.818751]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 20/86 [loss: 0.368551, acc.: 87.70%] [G loss: 0.874042]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 21/86 [loss: 0.451200, acc.: 81.35%] [G loss: 0.905414]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 22/86 [loss: 0.370070, acc.: 87.35%] [G loss: 0.871934]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 23/86 [loss: 0.383083, acc.: 86.43%] [G loss: 0.869207]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 24/86 [loss: 0.395906, acc.: 86.38%] [G loss: 0.879745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 25/86 [loss: 0.324075, acc.: 91.75%] [G loss: 0.868610]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 26/86 [loss: 0.357650, acc.: 89.21%] [G loss: 0.851999]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 27/86 [loss: 0.301348, acc.: 92.92%] [G loss: 0.881852]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 28/86 [loss: 0.361048, acc.: 88.87%] [G loss: 0.907023]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 29/86 [loss: 0.404104, acc.: 84.62%] [G loss: 0.872581]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 30/86 [loss: 0.339883, acc.: 90.48%] [G loss: 0.845027]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 31/86 [loss: 0.363696, acc.: 87.94%] [G loss: 0.858581]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 32/86 [loss: 0.345099, acc.: 89.79%] [G loss: 0.897578]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 33/86 [loss: 0.392429, acc.: 86.08%] [G loss: 0.854461]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 34/86 [loss: 0.330053, acc.: 90.97%] [G loss: 0.891944]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 35/86 [loss: 0.291638, acc.: 92.87%] [G loss: 0.869646]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 36/86 [loss: 0.293350, acc.: 92.92%] [G loss: 0.886290]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 37/86 [loss: 0.289365, acc.: 93.46%] [G loss: 0.858000]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 38/86 [loss: 0.377055, acc.: 87.79%] [G loss: 0.911999]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 39/86 [loss: 0.330352, acc.: 91.55%] [G loss: 0.867745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 40/86 [loss: 0.248627, acc.: 95.95%] [G loss: 0.948062]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 41/86 [loss: 0.277571, acc.: 94.24%] [G loss: 0.895390]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 42/86 [loss: 0.308173, acc.: 92.29%] [G loss: 0.922414]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 43/86 [loss: 0.410275, acc.: 84.62%] [G loss: 0.837631]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 44/86 [loss: 0.372736, acc.: 87.84%] [G loss: 0.815440]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 45/86 [loss: 0.343795, acc.: 89.50%] [G loss: 0.937225]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 46/86 [loss: 0.322682, acc.: 91.65%] [G loss: 0.912598]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 47/86 [loss: 0.377659, acc.: 86.62%] [G loss: 0.892592]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 48/86 [loss: 0.300994, acc.: 92.38%] [G loss: 0.837773]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 49/86 [loss: 0.320571, acc.: 92.19%] [G loss: 0.871540]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 50/86 [loss: 0.371333, acc.: 87.35%] [G loss: 0.868200]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 51/86 [loss: 0.386512, acc.: 87.70%] [G loss: 0.869540]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 52/86 [loss: 0.451235, acc.: 80.62%] [G loss: 0.833075]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 53/86 [loss: 0.432671, acc.: 82.91%] [G loss: 0.840280]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 54/86 [loss: 0.412189, acc.: 84.52%] [G loss: 0.861134]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 55/86 [loss: 0.383005, acc.: 87.55%] [G loss: 0.894517]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 56/86 [loss: 0.382038, acc.: 87.11%] [G loss: 0.831967]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 57/86 [loss: 0.303160, acc.: 92.68%] [G loss: 0.861295]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 58/86 [loss: 0.360328, acc.: 88.82%] [G loss: 0.865804]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 59/86 [loss: 0.419327, acc.: 83.98%] [G loss: 0.882971]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 60/86 [loss: 0.376670, acc.: 87.84%] [G loss: 0.855866]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 61/86 [loss: 0.334680, acc.: 90.19%] [G loss: 0.864725]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 62/86 [loss: 0.450855, acc.: 80.62%] [G loss: 0.820717]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 63/86 [loss: 0.291406, acc.: 93.60%] [G loss: 0.843667]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 64/86 [loss: 0.313763, acc.: 91.75%] [G loss: 0.846639]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 65/86 [loss: 0.298376, acc.: 93.21%] [G loss: 0.890117]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 66/86 [loss: 0.313747, acc.: 91.41%] [G loss: 0.948515]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 67/86 [loss: 0.389450, acc.: 86.77%] [G loss: 0.849665]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 68/86 [loss: 0.329449, acc.: 91.11%] [G loss: 0.883470]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 69/86 [loss: 0.271676, acc.: 94.48%] [G loss: 0.848072]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 70/86 [loss: 0.326261, acc.: 91.55%] [G loss: 0.850802]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 71/86 [loss: 0.360967, acc.: 88.43%] [G loss: 0.851414]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 72/86 [loss: 0.342663, acc.: 90.72%] [G loss: 0.868661]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 73/86 [loss: 0.336512, acc.: 89.01%] [G loss: 0.808823]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 74/86 [loss: 0.342285, acc.: 90.58%] [G loss: 0.856121]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 75/86 [loss: 0.373742, acc.: 87.94%] [G loss: 0.868496]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 76/86 [loss: 0.371573, acc.: 88.77%] [G loss: 0.868662]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 77/86 [loss: 0.398937, acc.: 86.18%] [G loss: 0.864823]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 78/86 [loss: 0.380842, acc.: 86.28%] [G loss: 0.818864]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 79/86 [loss: 0.379056, acc.: 88.13%] [G loss: 0.827154]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 33/200  Batch Size: 80/86 [loss: 0.398992, acc.: 85.99%] [G loss: 0.860742]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 81/86 [loss: 0.410333, acc.: 84.57%] [G loss: 0.879043]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 33/200  Batch Size: 82/86 [loss: 0.405442, acc.: 85.45%] [G loss: 0.836899]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 33/200  Batch Size: 83/86 [loss: 0.319266, acc.: 91.80%] [G loss: 0.886745]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 84/86 [loss: 0.367537, acc.: 88.28%] [G loss: 0.887566]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 33/200  Batch Size: 85/86 [loss: 0.423518, acc.: 83.40%] [G loss: 0.865927]\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 0/86 [loss: 0.307054, acc.: 92.58%] [G loss: 0.846639]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 1/86 [loss: 0.392852, acc.: 86.82%] [G loss: 0.870343]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 2/86 [loss: 0.416196, acc.: 83.98%] [G loss: 0.852528]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 3/86 [loss: 0.366353, acc.: 88.62%] [G loss: 0.857395]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 4/86 [loss: 0.293859, acc.: 93.12%] [G loss: 0.864291]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 5/86 [loss: 0.368689, acc.: 88.43%] [G loss: 0.874113]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 6/86 [loss: 0.323704, acc.: 90.77%] [G loss: 0.938067]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 7/86 [loss: 0.350803, acc.: 89.21%] [G loss: 0.872141]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 8/86 [loss: 0.302527, acc.: 92.87%] [G loss: 0.852686]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 9/86 [loss: 0.382724, acc.: 86.72%] [G loss: 0.853392]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 10/86 [loss: 0.366785, acc.: 87.45%] [G loss: 0.803831]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 11/86 [loss: 0.382800, acc.: 87.30%] [G loss: 0.845262]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 12/86 [loss: 0.320793, acc.: 91.80%] [G loss: 0.836386]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 13/86 [loss: 0.376106, acc.: 87.65%] [G loss: 0.862638]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 14/86 [loss: 0.416485, acc.: 84.77%] [G loss: 0.848655]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 15/86 [loss: 0.422256, acc.: 84.91%] [G loss: 0.833661]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 16/86 [loss: 0.376682, acc.: 88.33%] [G loss: 0.826323]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 17/86 [loss: 0.410141, acc.: 84.67%] [G loss: 0.889534]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 18/86 [loss: 0.424654, acc.: 83.15%] [G loss: 0.837708]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 19/86 [loss: 0.391991, acc.: 87.30%] [G loss: 0.846673]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 20/86 [loss: 0.359690, acc.: 89.31%] [G loss: 0.855401]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 21/86 [loss: 0.433116, acc.: 82.91%] [G loss: 0.879158]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 22/86 [loss: 0.412211, acc.: 84.47%] [G loss: 0.847279]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 23/86 [loss: 0.481882, acc.: 77.29%] [G loss: 0.756215]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 24/86 [loss: 0.348745, acc.: 90.33%] [G loss: 0.831049]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 25/86 [loss: 0.442013, acc.: 80.13%] [G loss: 0.914794]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 26/86 [loss: 0.435432, acc.: 82.81%] [G loss: 0.823898]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 27/86 [loss: 0.448612, acc.: 81.45%] [G loss: 0.890186]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 28/86 [loss: 0.353863, acc.: 89.11%] [G loss: 0.872912]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 29/86 [loss: 0.402787, acc.: 84.72%] [G loss: 0.912427]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 30/86 [loss: 0.394159, acc.: 85.60%] [G loss: 0.931845]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 31/86 [loss: 0.338000, acc.: 90.43%] [G loss: 0.895785]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 32/86 [loss: 0.345469, acc.: 89.55%] [G loss: 0.897233]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 33/86 [loss: 0.271851, acc.: 94.63%] [G loss: 0.916046]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 34/86 [loss: 0.308448, acc.: 92.92%] [G loss: 0.896363]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 35/86 [loss: 0.398803, acc.: 86.18%] [G loss: 0.876362]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 36/86 [loss: 0.374416, acc.: 88.87%] [G loss: 0.878067]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 37/86 [loss: 0.342342, acc.: 89.70%] [G loss: 0.838057]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 38/86 [loss: 0.267000, acc.: 94.92%] [G loss: 0.898498]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 39/86 [loss: 0.353399, acc.: 89.75%] [G loss: 0.901723]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 40/86 [loss: 0.298109, acc.: 93.55%] [G loss: 0.878298]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 41/86 [loss: 0.353573, acc.: 89.16%] [G loss: 0.940152]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 42/86 [loss: 0.379197, acc.: 88.09%] [G loss: 0.905630]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 43/86 [loss: 0.255919, acc.: 95.26%] [G loss: 0.879548]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 44/86 [loss: 0.290010, acc.: 93.21%] [G loss: 0.867867]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 45/86 [loss: 0.264033, acc.: 95.26%] [G loss: 0.908958]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 46/86 [loss: 0.273291, acc.: 94.19%] [G loss: 0.854719]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 47/86 [loss: 0.293257, acc.: 93.65%] [G loss: 0.877308]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 48/86 [loss: 0.317650, acc.: 91.06%] [G loss: 0.844866]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 49/86 [loss: 0.379914, acc.: 87.55%] [G loss: 0.891039]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 50/86 [loss: 0.408193, acc.: 84.91%] [G loss: 0.890115]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 51/86 [loss: 0.452227, acc.: 81.64%] [G loss: 0.841961]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 52/86 [loss: 0.441589, acc.: 81.54%] [G loss: 0.795558]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 53/86 [loss: 0.428049, acc.: 82.76%] [G loss: 0.828266]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 54/86 [loss: 0.413559, acc.: 84.62%] [G loss: 0.834601]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 55/86 [loss: 0.371274, acc.: 87.84%] [G loss: 0.884504]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 56/86 [loss: 0.413281, acc.: 85.16%] [G loss: 0.846795]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 57/86 [loss: 0.368958, acc.: 87.16%] [G loss: 0.927077]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 58/86 [loss: 0.420389, acc.: 83.89%] [G loss: 0.858045]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 59/86 [loss: 0.404889, acc.: 84.67%] [G loss: 0.830320]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 60/86 [loss: 0.400624, acc.: 86.18%] [G loss: 0.828122]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 61/86 [loss: 0.319787, acc.: 91.65%] [G loss: 0.917310]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 62/86 [loss: 0.375974, acc.: 87.50%] [G loss: 0.869280]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 63/86 [loss: 0.462706, acc.: 80.66%] [G loss: 0.846443]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 64/86 [loss: 0.374040, acc.: 88.04%] [G loss: 0.894232]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 65/86 [loss: 0.346986, acc.: 89.89%] [G loss: 0.911290]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 66/86 [loss: 0.474122, acc.: 79.20%] [G loss: 0.878829]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 67/86 [loss: 0.377938, acc.: 87.65%] [G loss: 0.840412]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 68/86 [loss: 0.436546, acc.: 82.37%] [G loss: 0.786170]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 69/86 [loss: 0.378175, acc.: 87.45%] [G loss: 0.840601]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 70/86 [loss: 0.326547, acc.: 91.80%] [G loss: 0.861912]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 71/86 [loss: 0.382688, acc.: 86.43%] [G loss: 0.880100]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 72/86 [loss: 0.375262, acc.: 87.79%] [G loss: 0.903759]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 73/86 [loss: 0.422355, acc.: 83.35%] [G loss: 0.860069]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 74/86 [loss: 0.348902, acc.: 89.94%] [G loss: 0.853301]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 75/86 [loss: 0.510312, acc.: 76.42%] [G loss: 0.888961]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 76/86 [loss: 0.372128, acc.: 88.04%] [G loss: 0.847959]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 77/86 [loss: 0.328090, acc.: 91.60%] [G loss: 0.886914]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 78/86 [loss: 0.449708, acc.: 82.28%] [G loss: 0.846249]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 79/86 [loss: 0.350647, acc.: 89.11%] [G loss: 0.839289]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 34/200  Batch Size: 80/86 [loss: 0.416241, acc.: 84.57%] [G loss: 0.863768]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 81/86 [loss: 0.341432, acc.: 90.62%] [G loss: 0.859173]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 82/86 [loss: 0.342212, acc.: 89.06%] [G loss: 0.888724]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 34/200  Batch Size: 83/86 [loss: 0.357977, acc.: 89.06%] [G loss: 0.891997]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 34/200  Batch Size: 84/86 [loss: 0.356983, acc.: 89.26%] [G loss: 0.871944]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 34/200  Batch Size: 85/86 [loss: 0.469120, acc.: 79.93%] [G loss: 0.864766]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 0/86 [loss: 0.541919, acc.: 72.95%] [G loss: 0.822343]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 1/86 [loss: 0.330927, acc.: 90.82%] [G loss: 0.829667]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 2/86 [loss: 0.284763, acc.: 93.21%] [G loss: 0.942944]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 3/86 [loss: 0.368248, acc.: 88.43%] [G loss: 0.912619]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 4/86 [loss: 0.210016, acc.: 97.02%] [G loss: 0.910313]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 5/86 [loss: 0.321066, acc.: 91.55%] [G loss: 0.896855]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 6/86 [loss: 0.285326, acc.: 93.99%] [G loss: 0.888974]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 7/86 [loss: 0.341741, acc.: 89.94%] [G loss: 0.882959]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 8/86 [loss: 0.339956, acc.: 89.79%] [G loss: 0.884835]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 9/86 [loss: 0.302978, acc.: 93.12%] [G loss: 0.880374]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 10/86 [loss: 0.305228, acc.: 93.12%] [G loss: 0.902505]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 11/86 [loss: 0.363819, acc.: 87.70%] [G loss: 0.892923]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 12/86 [loss: 0.450259, acc.: 81.93%] [G loss: 0.851998]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 13/86 [loss: 0.334228, acc.: 90.48%] [G loss: 0.865423]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 14/86 [loss: 0.322005, acc.: 91.75%] [G loss: 0.839553]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 15/86 [loss: 0.480529, acc.: 78.71%] [G loss: 0.821718]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 16/86 [loss: 0.363350, acc.: 88.57%] [G loss: 0.863518]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 17/86 [loss: 0.412313, acc.: 83.25%] [G loss: 0.923380]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 18/86 [loss: 0.393707, acc.: 85.45%] [G loss: 0.893086]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 19/86 [loss: 0.356950, acc.: 89.89%] [G loss: 0.898859]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 20/86 [loss: 0.365204, acc.: 89.75%] [G loss: 0.869760]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 21/86 [loss: 0.286179, acc.: 93.60%] [G loss: 0.876164]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 22/86 [loss: 0.330528, acc.: 91.70%] [G loss: 0.857640]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 23/86 [loss: 0.402672, acc.: 85.35%] [G loss: 0.872280]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 24/86 [loss: 0.296377, acc.: 93.31%] [G loss: 0.826209]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 25/86 [loss: 0.325970, acc.: 91.50%] [G loss: 0.839714]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 26/86 [loss: 0.305711, acc.: 92.58%] [G loss: 0.889578]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 27/86 [loss: 0.311902, acc.: 91.99%] [G loss: 0.845477]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 28/86 [loss: 0.261403, acc.: 94.87%] [G loss: 0.870058]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 29/86 [loss: 0.281376, acc.: 93.95%] [G loss: 0.897965]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 30/86 [loss: 0.389337, acc.: 86.13%] [G loss: 0.888915]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 31/86 [loss: 0.307749, acc.: 91.94%] [G loss: 0.863491]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 32/86 [loss: 0.374483, acc.: 88.53%] [G loss: 0.869998]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 33/86 [loss: 0.367078, acc.: 88.38%] [G loss: 0.864241]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 34/86 [loss: 0.362700, acc.: 87.30%] [G loss: 0.807842]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 35/86 [loss: 0.330212, acc.: 91.16%] [G loss: 0.836041]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 36/86 [loss: 0.312074, acc.: 92.87%] [G loss: 0.873670]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 37/86 [loss: 0.475469, acc.: 78.37%] [G loss: 0.893820]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 38/86 [loss: 0.313246, acc.: 91.65%] [G loss: 0.835832]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 39/86 [loss: 0.315649, acc.: 91.85%] [G loss: 0.857996]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 40/86 [loss: 0.345261, acc.: 89.45%] [G loss: 0.893494]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 41/86 [loss: 0.282016, acc.: 94.14%] [G loss: 0.868402]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 42/86 [loss: 0.425474, acc.: 83.98%] [G loss: 0.881353]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 43/86 [loss: 0.325688, acc.: 91.31%] [G loss: 0.876742]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 44/86 [loss: 0.251780, acc.: 95.90%] [G loss: 0.890927]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 45/86 [loss: 0.333831, acc.: 91.26%] [G loss: 0.899294]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 46/86 [loss: 0.281979, acc.: 94.24%] [G loss: 0.900528]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 47/86 [loss: 0.365504, acc.: 89.06%] [G loss: 0.886215]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 48/86 [loss: 0.307445, acc.: 92.43%] [G loss: 0.865558]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 49/86 [loss: 0.330694, acc.: 91.11%] [G loss: 0.911289]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 50/86 [loss: 0.409202, acc.: 84.62%] [G loss: 0.841769]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 51/86 [loss: 0.387647, acc.: 86.57%] [G loss: 0.822646]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 52/86 [loss: 0.280800, acc.: 93.90%] [G loss: 0.864764]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 53/86 [loss: 0.281215, acc.: 93.90%] [G loss: 0.852597]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 54/86 [loss: 0.398748, acc.: 85.74%] [G loss: 0.830038]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 55/86 [loss: 0.287328, acc.: 93.90%] [G loss: 0.856773]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 56/86 [loss: 0.475331, acc.: 79.10%] [G loss: 0.884077]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 57/86 [loss: 0.250432, acc.: 95.65%] [G loss: 0.865472]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 58/86 [loss: 0.295699, acc.: 93.46%] [G loss: 0.857646]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 59/86 [loss: 0.274460, acc.: 94.34%] [G loss: 0.930393]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 60/86 [loss: 0.294536, acc.: 94.09%] [G loss: 0.877466]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 61/86 [loss: 0.294292, acc.: 93.41%] [G loss: 0.859630]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 62/86 [loss: 0.279153, acc.: 94.04%] [G loss: 0.866843]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 63/86 [loss: 0.304761, acc.: 92.82%] [G loss: 0.893053]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 64/86 [loss: 0.322473, acc.: 90.19%] [G loss: 0.868599]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 65/86 [loss: 0.265747, acc.: 94.04%] [G loss: 0.898741]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 66/86 [loss: 0.389296, acc.: 86.23%] [G loss: 0.882567]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 67/86 [loss: 0.251667, acc.: 95.90%] [G loss: 0.908881]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 68/86 [loss: 0.316561, acc.: 91.94%] [G loss: 0.893463]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 69/86 [loss: 0.362561, acc.: 87.74%] [G loss: 0.841407]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 70/86 [loss: 0.272858, acc.: 94.87%] [G loss: 0.871828]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 35/200  Batch Size: 71/86 [loss: 0.335072, acc.: 89.99%] [G loss: 0.876454]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 72/86 [loss: 0.386458, acc.: 86.08%] [G loss: 0.892199]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 73/86 [loss: 0.416962, acc.: 84.33%] [G loss: 0.859306]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 74/86 [loss: 0.361281, acc.: 89.06%] [G loss: 0.815794]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 75/86 [loss: 0.410325, acc.: 85.84%] [G loss: 0.812534]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 76/86 [loss: 0.340997, acc.: 90.58%] [G loss: 0.857443]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 77/86 [loss: 0.374372, acc.: 88.09%] [G loss: 0.830245]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 78/86 [loss: 0.395707, acc.: 85.79%] [G loss: 0.852597]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 79/86 [loss: 0.399052, acc.: 84.91%] [G loss: 0.915828]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 80/86 [loss: 0.476789, acc.: 79.59%] [G loss: 0.839879]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 81/86 [loss: 0.317188, acc.: 92.48%] [G loss: 0.824647]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 82/86 [loss: 0.324172, acc.: 91.02%] [G loss: 0.852165]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 35/200  Batch Size: 83/86 [loss: 0.334308, acc.: 90.62%] [G loss: 0.898222]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 35/200  Batch Size: 84/86 [loss: 0.308881, acc.: 92.29%] [G loss: 0.890334]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 35/200  Batch Size: 85/86 [loss: 0.303833, acc.: 92.77%] [G loss: 0.879316]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 0/86 [loss: 0.383360, acc.: 86.52%] [G loss: 0.901919]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 1/86 [loss: 0.304569, acc.: 93.02%] [G loss: 0.906592]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 2/86 [loss: 0.333263, acc.: 89.65%] [G loss: 0.825178]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 36/200  Batch Size: 3/86 [loss: 0.307853, acc.: 93.21%] [G loss: 0.856506]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 4/86 [loss: 0.333975, acc.: 90.92%] [G loss: 0.853695]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 5/86 [loss: 0.336799, acc.: 90.92%] [G loss: 0.853868]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 6/86 [loss: 0.384706, acc.: 87.74%] [G loss: 0.860574]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 7/86 [loss: 0.396498, acc.: 86.72%] [G loss: 0.812782]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 8/86 [loss: 0.265245, acc.: 94.92%] [G loss: 0.862112]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 9/86 [loss: 0.310269, acc.: 92.87%] [G loss: 0.881715]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 10/86 [loss: 0.376166, acc.: 87.26%] [G loss: 0.871633]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 11/86 [loss: 0.386449, acc.: 87.21%] [G loss: 0.877376]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 12/86 [loss: 0.298988, acc.: 92.87%] [G loss: 0.865480]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 13/86 [loss: 0.291420, acc.: 93.31%] [G loss: 0.884000]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 14/86 [loss: 0.385045, acc.: 87.21%] [G loss: 0.865710]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 36/200  Batch Size: 15/86 [loss: 0.325524, acc.: 91.46%] [G loss: 0.874549]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 16/86 [loss: 0.358969, acc.: 89.26%] [G loss: 0.878884]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 17/86 [loss: 0.460236, acc.: 79.98%] [G loss: 0.851945]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 18/86 [loss: 0.353400, acc.: 89.40%] [G loss: 0.852426]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 36/200  Batch Size: 19/86 [loss: 0.388039, acc.: 86.87%] [G loss: 0.850508]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 20/86 [loss: 0.386698, acc.: 87.35%] [G loss: 0.824650]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 21/86 [loss: 0.389941, acc.: 87.55%] [G loss: 0.828956]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 22/86 [loss: 0.409228, acc.: 84.86%] [G loss: 0.837416]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 36/200  Batch Size: 23/86 [loss: 0.408106, acc.: 85.60%] [G loss: 0.814995]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 24/86 [loss: 0.451438, acc.: 82.52%] [G loss: 0.818783]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 25/86 [loss: 0.418041, acc.: 83.45%] [G loss: 0.786580]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 26/86 [loss: 0.435653, acc.: 82.23%] [G loss: 0.859408]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 27/86 [loss: 0.452537, acc.: 81.54%] [G loss: 0.827656]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 28/86 [loss: 0.363317, acc.: 88.67%] [G loss: 0.894729]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 29/86 [loss: 0.413626, acc.: 84.57%] [G loss: 0.863401]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 30/86 [loss: 0.404325, acc.: 85.30%] [G loss: 0.866519]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 31/86 [loss: 0.355790, acc.: 89.21%] [G loss: 0.859333]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 32/86 [loss: 0.382163, acc.: 87.40%] [G loss: 0.907287]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 36/200  Batch Size: 33/86 [loss: 0.365496, acc.: 88.77%] [G loss: 0.865844]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 34/86 [loss: 0.327796, acc.: 91.41%] [G loss: 0.812946]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 35/86 [loss: 0.304832, acc.: 93.16%] [G loss: 0.872280]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 36/86 [loss: 0.353336, acc.: 89.65%] [G loss: 0.838744]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 37/86 [loss: 0.343457, acc.: 90.77%] [G loss: 0.835021]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 38/86 [loss: 0.341531, acc.: 89.70%] [G loss: 0.889035]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 39/86 [loss: 0.513790, acc.: 75.24%] [G loss: 0.817189]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 36/200  Batch Size: 40/86 [loss: 0.368630, acc.: 87.79%] [G loss: 0.858966]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 41/86 [loss: 0.439812, acc.: 82.47%] [G loss: 0.893550]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 42/86 [loss: 0.371199, acc.: 88.43%] [G loss: 0.873637]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 36/200  Batch Size: 43/86 [loss: 0.377505, acc.: 88.18%] [G loss: 0.825127]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 44/86 [loss: 0.313197, acc.: 92.09%] [G loss: 0.880788]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 45/86 [loss: 0.342102, acc.: 90.87%] [G loss: 0.890922]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 46/86 [loss: 0.384067, acc.: 87.01%] [G loss: 0.862525]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 47/86 [loss: 0.369313, acc.: 88.53%] [G loss: 0.873204]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 48/86 [loss: 0.318416, acc.: 92.04%] [G loss: 0.894701]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 49/86 [loss: 0.408753, acc.: 83.69%] [G loss: 0.810484]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 50/86 [loss: 0.358239, acc.: 88.67%] [G loss: 0.847083]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 51/86 [loss: 0.350368, acc.: 89.75%] [G loss: 0.879001]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 52/86 [loss: 0.388543, acc.: 87.94%] [G loss: 0.853981]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 53/86 [loss: 0.448026, acc.: 82.08%] [G loss: 0.876212]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 54/86 [loss: 0.368895, acc.: 88.53%] [G loss: 0.843501]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 55/86 [loss: 0.357466, acc.: 89.31%] [G loss: 0.849583]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 56/86 [loss: 0.444986, acc.: 81.35%] [G loss: 0.907084]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 57/86 [loss: 0.320355, acc.: 91.46%] [G loss: 0.889535]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 58/86 [loss: 0.443492, acc.: 82.86%] [G loss: 0.865939]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 59/86 [loss: 0.378621, acc.: 87.26%] [G loss: 0.832715]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 60/86 [loss: 0.375510, acc.: 88.23%] [G loss: 0.851584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 61/86 [loss: 0.310943, acc.: 91.99%] [G loss: 0.846532]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 62/86 [loss: 0.391958, acc.: 86.08%] [G loss: 0.855181]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 63/86 [loss: 0.425170, acc.: 82.08%] [G loss: 0.859177]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 64/86 [loss: 0.361413, acc.: 88.92%] [G loss: 0.910445]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 65/86 [loss: 0.355154, acc.: 89.75%] [G loss: 0.870941]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 66/86 [loss: 0.340292, acc.: 90.14%] [G loss: 0.918713]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 36/200  Batch Size: 67/86 [loss: 0.363887, acc.: 88.38%] [G loss: 0.893253]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 68/86 [loss: 0.351306, acc.: 89.11%] [G loss: 0.850158]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 69/86 [loss: 0.334429, acc.: 91.06%] [G loss: 0.850196]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 70/86 [loss: 0.373443, acc.: 87.99%] [G loss: 0.846849]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 36/200  Batch Size: 71/86 [loss: 0.358999, acc.: 88.96%] [G loss: 0.877317]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 72/86 [loss: 0.355614, acc.: 89.36%] [G loss: 0.858560]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 73/86 [loss: 0.372378, acc.: 88.77%] [G loss: 0.870633]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 74/86 [loss: 0.468671, acc.: 80.66%] [G loss: 0.835489]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 75/86 [loss: 0.435724, acc.: 83.54%] [G loss: 0.840573]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 76/86 [loss: 0.460647, acc.: 80.37%] [G loss: 0.861687]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 77/86 [loss: 0.343990, acc.: 90.77%] [G loss: 0.850131]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 78/86 [loss: 0.444142, acc.: 82.81%] [G loss: 0.877839]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 79/86 [loss: 0.481542, acc.: 78.96%] [G loss: 0.876978]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 80/86 [loss: 0.355966, acc.: 88.43%] [G loss: 0.848282]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 81/86 [loss: 0.389978, acc.: 86.82%] [G loss: 0.822874]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 82/86 [loss: 0.322559, acc.: 91.41%] [G loss: 0.833681]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 83/86 [loss: 0.434007, acc.: 82.62%] [G loss: 0.855410]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 36/200  Batch Size: 84/86 [loss: 0.350303, acc.: 89.79%] [G loss: 0.886329]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 36/200  Batch Size: 85/86 [loss: 0.360432, acc.: 88.62%] [G loss: 0.869168]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 0/86 [loss: 0.397276, acc.: 86.52%] [G loss: 0.871197]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 1/86 [loss: 0.396169, acc.: 86.04%] [G loss: 0.906255]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 2/86 [loss: 0.346686, acc.: 89.40%] [G loss: 0.918596]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 3/86 [loss: 0.320109, acc.: 91.46%] [G loss: 0.872176]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 4/86 [loss: 0.355725, acc.: 89.21%] [G loss: 0.880772]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 5/86 [loss: 0.440769, acc.: 82.23%] [G loss: 0.833565]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 6/86 [loss: 0.337089, acc.: 90.97%] [G loss: 0.847321]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 7/86 [loss: 0.334081, acc.: 90.38%] [G loss: 0.839166]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 37/200  Batch Size: 8/86 [loss: 0.400706, acc.: 85.30%] [G loss: 0.896658]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 9/86 [loss: 0.389643, acc.: 87.35%] [G loss: 0.890949]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 10/86 [loss: 0.415765, acc.: 84.38%] [G loss: 0.861097]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 11/86 [loss: 0.358766, acc.: 88.53%] [G loss: 0.854090]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 12/86 [loss: 0.384131, acc.: 86.96%] [G loss: 0.854289]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 13/86 [loss: 0.382014, acc.: 87.70%] [G loss: 0.809130]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 14/86 [loss: 0.413737, acc.: 84.42%] [G loss: 0.858453]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 15/86 [loss: 0.321997, acc.: 92.14%] [G loss: 0.825718]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 16/86 [loss: 0.535594, acc.: 73.14%] [G loss: 0.833848]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 37/200  Batch Size: 17/86 [loss: 0.380105, acc.: 88.57%] [G loss: 0.857837]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 18/86 [loss: 0.393915, acc.: 85.94%] [G loss: 0.888526]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 19/86 [loss: 0.419537, acc.: 83.98%] [G loss: 0.879582]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 20/86 [loss: 0.465064, acc.: 80.22%] [G loss: 0.842233]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 21/86 [loss: 0.326136, acc.: 91.02%] [G loss: 0.868687]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 22/86 [loss: 0.373890, acc.: 87.89%] [G loss: 0.896557]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 23/86 [loss: 0.388376, acc.: 86.91%] [G loss: 0.886650]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 24/86 [loss: 0.443493, acc.: 81.98%] [G loss: 0.809186]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 25/86 [loss: 0.392700, acc.: 86.33%] [G loss: 0.830128]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 26/86 [loss: 0.388497, acc.: 86.96%] [G loss: 0.905379]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 37/200  Batch Size: 27/86 [loss: 0.364947, acc.: 88.13%] [G loss: 0.887573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 28/86 [loss: 0.548687, acc.: 73.39%] [G loss: 0.891758]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 29/86 [loss: 0.354995, acc.: 89.99%] [G loss: 0.891691]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 30/86 [loss: 0.465863, acc.: 79.00%] [G loss: 0.911343]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 31/86 [loss: 0.406523, acc.: 85.16%] [G loss: 0.854454]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 32/86 [loss: 0.318574, acc.: 92.58%] [G loss: 0.874966]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 33/86 [loss: 0.271411, acc.: 94.63%] [G loss: 0.887310]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 34/86 [loss: 0.380571, acc.: 88.18%] [G loss: 0.864828]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 35/86 [loss: 0.407088, acc.: 85.74%] [G loss: 0.875370]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 36/86 [loss: 0.304799, acc.: 92.53%] [G loss: 0.862121]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 37/200  Batch Size: 37/86 [loss: 0.313891, acc.: 91.70%] [G loss: 0.832448]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 38/86 [loss: 0.284204, acc.: 93.90%] [G loss: 0.861959]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 39/86 [loss: 0.356244, acc.: 88.87%] [G loss: 0.864583]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 40/86 [loss: 0.368795, acc.: 88.43%] [G loss: 0.877918]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 41/86 [loss: 0.298232, acc.: 93.36%] [G loss: 0.844968]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 42/86 [loss: 0.356500, acc.: 89.06%] [G loss: 0.846029]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 43/86 [loss: 0.370912, acc.: 88.62%] [G loss: 0.848376]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 44/86 [loss: 0.383595, acc.: 86.82%] [G loss: 0.896686]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 45/86 [loss: 0.313846, acc.: 91.85%] [G loss: 0.839946]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 46/86 [loss: 0.405864, acc.: 85.94%] [G loss: 0.895844]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 47/86 [loss: 0.414742, acc.: 84.67%] [G loss: 0.827027]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 48/86 [loss: 0.365054, acc.: 88.62%] [G loss: 0.877806]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 49/86 [loss: 0.387686, acc.: 87.40%] [G loss: 0.803087]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 50/86 [loss: 0.410796, acc.: 85.89%] [G loss: 0.853623]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 51/86 [loss: 0.425982, acc.: 83.45%] [G loss: 0.867448]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 52/86 [loss: 0.356489, acc.: 89.45%] [G loss: 0.816921]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 53/86 [loss: 0.348517, acc.: 89.60%] [G loss: 0.880518]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 54/86 [loss: 0.395122, acc.: 85.69%] [G loss: 0.810501]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 37/200  Batch Size: 55/86 [loss: 0.374225, acc.: 87.65%] [G loss: 0.906442]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 56/86 [loss: 0.409566, acc.: 85.99%] [G loss: 0.897023]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 57/86 [loss: 0.357386, acc.: 88.33%] [G loss: 0.885262]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 58/86 [loss: 0.268254, acc.: 95.31%] [G loss: 0.846691]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 59/86 [loss: 0.308056, acc.: 92.48%] [G loss: 0.891454]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 60/86 [loss: 0.364892, acc.: 88.28%] [G loss: 0.820590]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 61/86 [loss: 0.323403, acc.: 91.94%] [G loss: 0.866370]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 62/86 [loss: 0.301677, acc.: 92.68%] [G loss: 0.927249]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 63/86 [loss: 0.385952, acc.: 87.30%] [G loss: 0.900085]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 64/86 [loss: 0.422221, acc.: 84.18%] [G loss: 0.875445]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 65/86 [loss: 0.380064, acc.: 87.40%] [G loss: 0.839755]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 66/86 [loss: 0.330698, acc.: 91.16%] [G loss: 0.873138]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 67/86 [loss: 0.390931, acc.: 86.43%] [G loss: 0.847100]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 68/86 [loss: 0.402244, acc.: 85.84%] [G loss: 0.836764]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 69/86 [loss: 0.432715, acc.: 83.11%] [G loss: 0.863691]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 70/86 [loss: 0.436342, acc.: 82.13%] [G loss: 0.824340]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 71/86 [loss: 0.319910, acc.: 91.70%] [G loss: 0.828324]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 72/86 [loss: 0.440776, acc.: 82.23%] [G loss: 0.875256]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 73/86 [loss: 0.386593, acc.: 86.43%] [G loss: 0.866226]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 74/86 [loss: 0.385379, acc.: 88.33%] [G loss: 0.879189]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 37/200  Batch Size: 75/86 [loss: 0.340376, acc.: 91.11%] [G loss: 0.859781]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 76/86 [loss: 0.397222, acc.: 86.23%] [G loss: 0.911146]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 37/200  Batch Size: 77/86 [loss: 0.331467, acc.: 90.87%] [G loss: 0.859985]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 37/200  Batch Size: 78/86 [loss: 0.469797, acc.: 80.13%] [G loss: 0.864236]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 79/86 [loss: 0.374130, acc.: 87.21%] [G loss: 0.855095]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 80/86 [loss: 0.352118, acc.: 88.23%] [G loss: 0.889087]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 37/200  Batch Size: 81/86 [loss: 0.289524, acc.: 93.75%] [G loss: 0.907131]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 37/200  Batch Size: 82/86 [loss: 0.307493, acc.: 92.43%] [G loss: 0.902614]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 83/86 [loss: 0.337416, acc.: 89.99%] [G loss: 0.845211]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 84/86 [loss: 0.300389, acc.: 93.51%] [G loss: 0.849074]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 37/200  Batch Size: 85/86 [loss: 0.293739, acc.: 93.31%] [G loss: 0.835903]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 38/200  Batch Size: 0/86 [loss: 0.423857, acc.: 84.18%] [G loss: 0.870135]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 1/86 [loss: 0.380870, acc.: 87.40%] [G loss: 0.865672]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 2/86 [loss: 0.437968, acc.: 82.67%] [G loss: 0.810882]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 3/86 [loss: 0.356974, acc.: 89.26%] [G loss: 0.884246]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 4/86 [loss: 0.340900, acc.: 90.82%] [G loss: 0.874228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 5/86 [loss: 0.307856, acc.: 92.72%] [G loss: 0.864434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 6/86 [loss: 0.369508, acc.: 87.79%] [G loss: 0.819893]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 38/200  Batch Size: 7/86 [loss: 0.313231, acc.: 92.29%] [G loss: 0.827344]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 8/86 [loss: 0.294061, acc.: 93.70%] [G loss: 0.859337]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 9/86 [loss: 0.374212, acc.: 88.04%] [G loss: 0.870615]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 10/86 [loss: 0.300746, acc.: 92.92%] [G loss: 0.884162]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 11/86 [loss: 0.335739, acc.: 89.75%] [G loss: 0.926827]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 12/86 [loss: 0.296713, acc.: 93.95%] [G loss: 0.907152]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 13/86 [loss: 0.336194, acc.: 90.43%] [G loss: 0.885862]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 14/86 [loss: 0.266599, acc.: 93.95%] [G loss: 0.833465]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 15/86 [loss: 0.317113, acc.: 92.33%] [G loss: 0.865649]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 16/86 [loss: 0.387218, acc.: 87.45%] [G loss: 0.838756]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 17/86 [loss: 0.354702, acc.: 89.60%] [G loss: 0.829005]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 18/86 [loss: 0.329301, acc.: 91.36%] [G loss: 0.867754]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 19/86 [loss: 0.404014, acc.: 85.50%] [G loss: 0.881834]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 20/86 [loss: 0.415952, acc.: 85.21%] [G loss: 0.856624]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 21/86 [loss: 0.356769, acc.: 88.82%] [G loss: 0.822605]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 22/86 [loss: 0.271522, acc.: 95.02%] [G loss: 0.861153]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 23/86 [loss: 0.416300, acc.: 84.33%] [G loss: 0.881745]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 24/86 [loss: 0.302183, acc.: 93.36%] [G loss: 0.890534]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 25/86 [loss: 0.370824, acc.: 88.23%] [G loss: 0.861083]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 26/86 [loss: 0.343489, acc.: 89.55%] [G loss: 0.862801]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 38/200  Batch Size: 27/86 [loss: 0.394129, acc.: 86.82%] [G loss: 0.878968]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 28/86 [loss: 0.338701, acc.: 90.48%] [G loss: 0.882189]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 29/86 [loss: 0.425160, acc.: 83.84%] [G loss: 0.844095]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 30/86 [loss: 0.364188, acc.: 88.82%] [G loss: 0.872286]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 31/86 [loss: 0.295406, acc.: 93.46%] [G loss: 0.880670]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 32/86 [loss: 0.348877, acc.: 89.70%] [G loss: 0.858672]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 33/86 [loss: 0.331774, acc.: 91.21%] [G loss: 0.860207]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 34/86 [loss: 0.275933, acc.: 94.82%] [G loss: 0.863022]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 35/86 [loss: 0.439901, acc.: 83.30%] [G loss: 0.824380]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 36/86 [loss: 0.365056, acc.: 88.13%] [G loss: 0.874771]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 38/200  Batch Size: 37/86 [loss: 0.386308, acc.: 87.21%] [G loss: 0.859442]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 38/86 [loss: 0.377187, acc.: 88.09%] [G loss: 0.849228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 39/86 [loss: 0.303541, acc.: 92.77%] [G loss: 0.864762]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 40/86 [loss: 0.280344, acc.: 94.24%] [G loss: 0.895316]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 38/200  Batch Size: 41/86 [loss: 0.357823, acc.: 89.79%] [G loss: 0.884158]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 42/86 [loss: 0.335349, acc.: 90.48%] [G loss: 0.865787]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 43/86 [loss: 0.340570, acc.: 90.33%] [G loss: 0.851317]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 38/200  Batch Size: 44/86 [loss: 0.369689, acc.: 87.74%] [G loss: 0.890482]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 45/86 [loss: 0.323728, acc.: 91.80%] [G loss: 0.888796]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 46/86 [loss: 0.401204, acc.: 85.69%] [G loss: 0.840690]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 47/86 [loss: 0.338646, acc.: 91.21%] [G loss: 0.842590]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 48/86 [loss: 0.316776, acc.: 91.55%] [G loss: 0.898857]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 49/86 [loss: 0.294949, acc.: 93.21%] [G loss: 0.909618]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 50/86 [loss: 0.394769, acc.: 86.57%] [G loss: 0.878902]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 51/86 [loss: 0.351608, acc.: 89.89%] [G loss: 0.850288]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 52/86 [loss: 0.395347, acc.: 86.23%] [G loss: 0.866502]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 53/86 [loss: 0.358297, acc.: 88.57%] [G loss: 0.853332]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 54/86 [loss: 0.320013, acc.: 91.94%] [G loss: 0.839710]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 55/86 [loss: 0.398499, acc.: 86.82%] [G loss: 0.845910]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 56/86 [loss: 0.388349, acc.: 87.16%] [G loss: 0.831569]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 57/86 [loss: 0.364843, acc.: 88.62%] [G loss: 0.809262]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 58/86 [loss: 0.385041, acc.: 86.96%] [G loss: 0.853446]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 59/86 [loss: 0.366431, acc.: 88.48%] [G loss: 0.870422]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 60/86 [loss: 0.318350, acc.: 91.89%] [G loss: 0.893566]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 61/86 [loss: 0.379801, acc.: 86.87%] [G loss: 0.877852]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 62/86 [loss: 0.369488, acc.: 88.09%] [G loss: 0.870113]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 63/86 [loss: 0.327431, acc.: 91.36%] [G loss: 0.843898]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 64/86 [loss: 0.319021, acc.: 92.24%] [G loss: 0.862695]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 65/86 [loss: 0.435070, acc.: 83.01%] [G loss: 0.864436]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 66/86 [loss: 0.369714, acc.: 88.43%] [G loss: 0.853145]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 67/86 [loss: 0.321326, acc.: 92.29%] [G loss: 0.849869]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 68/86 [loss: 0.387311, acc.: 86.33%] [G loss: 0.896490]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 69/86 [loss: 0.341354, acc.: 90.23%] [G loss: 0.872076]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 70/86 [loss: 0.382351, acc.: 87.55%] [G loss: 0.843595]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 38/200  Batch Size: 71/86 [loss: 0.358757, acc.: 89.36%] [G loss: 0.839482]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 72/86 [loss: 0.390849, acc.: 86.96%] [G loss: 0.847047]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 73/86 [loss: 0.364235, acc.: 88.33%] [G loss: 0.861939]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 74/86 [loss: 0.323279, acc.: 91.36%] [G loss: 0.881084]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 75/86 [loss: 0.381434, acc.: 87.50%] [G loss: 0.873042]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 76/86 [loss: 0.367443, acc.: 88.82%] [G loss: 0.890998]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 77/86 [loss: 0.339202, acc.: 90.04%] [G loss: 0.875789]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 78/86 [loss: 0.338833, acc.: 90.33%] [G loss: 0.851563]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 79/86 [loss: 0.374641, acc.: 86.23%] [G loss: 0.912382]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 80/86 [loss: 0.367814, acc.: 88.92%] [G loss: 0.874643]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 38/200  Batch Size: 81/86 [loss: 0.331119, acc.: 91.89%] [G loss: 0.853123]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 38/200  Batch Size: 82/86 [loss: 0.463147, acc.: 81.45%] [G loss: 0.838043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 83/86 [loss: 0.363627, acc.: 89.70%] [G loss: 0.853663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 84/86 [loss: 0.378266, acc.: 88.57%] [G loss: 0.843349]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 38/200  Batch Size: 85/86 [loss: 0.357574, acc.: 89.26%] [G loss: 0.846238]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 0/86 [loss: 0.389347, acc.: 86.91%] [G loss: 0.876258]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 1/86 [loss: 0.330891, acc.: 91.46%] [G loss: 0.876703]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 2/86 [loss: 0.397566, acc.: 86.43%] [G loss: 0.873158]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 3/86 [loss: 0.350324, acc.: 88.96%] [G loss: 0.846844]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 4/86 [loss: 0.371672, acc.: 88.09%] [G loss: 0.825979]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 5/86 [loss: 0.372977, acc.: 87.60%] [G loss: 0.876110]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 6/86 [loss: 0.244834, acc.: 96.53%] [G loss: 0.861633]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 7/86 [loss: 0.295490, acc.: 92.77%] [G loss: 0.854465]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 8/86 [loss: 0.410530, acc.: 85.16%] [G loss: 0.866513]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 9/86 [loss: 0.341927, acc.: 90.04%] [G loss: 0.886768]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 10/86 [loss: 0.441903, acc.: 81.25%] [G loss: 0.846318]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 11/86 [loss: 0.290455, acc.: 93.99%] [G loss: 0.874422]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 12/86 [loss: 0.408695, acc.: 85.16%] [G loss: 0.899811]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 13/86 [loss: 0.381068, acc.: 87.84%] [G loss: 0.839864]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 14/86 [loss: 0.355565, acc.: 89.36%] [G loss: 0.885616]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 15/86 [loss: 0.321384, acc.: 91.65%] [G loss: 0.827276]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 16/86 [loss: 0.369396, acc.: 87.94%] [G loss: 0.823631]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 17/86 [loss: 0.284421, acc.: 94.73%] [G loss: 0.886831]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 18/86 [loss: 0.356658, acc.: 88.82%] [G loss: 0.884081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 19/86 [loss: 0.365871, acc.: 89.11%] [G loss: 0.836930]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 20/86 [loss: 0.377157, acc.: 87.21%] [G loss: 0.877350]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 21/86 [loss: 0.333025, acc.: 91.41%] [G loss: 0.864752]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 22/86 [loss: 0.327256, acc.: 92.14%] [G loss: 0.867063]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 23/86 [loss: 0.363587, acc.: 89.06%] [G loss: 0.853531]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 24/86 [loss: 0.284649, acc.: 94.43%] [G loss: 0.883421]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 25/86 [loss: 0.351462, acc.: 89.26%] [G loss: 0.874630]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 26/86 [loss: 0.318122, acc.: 91.41%] [G loss: 0.852782]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 27/86 [loss: 0.368686, acc.: 88.48%] [G loss: 0.882836]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 28/86 [loss: 0.338659, acc.: 91.06%] [G loss: 0.892434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 29/86 [loss: 0.483756, acc.: 77.98%] [G loss: 0.846124]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 30/86 [loss: 0.355002, acc.: 89.60%] [G loss: 0.858945]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 31/86 [loss: 0.386284, acc.: 87.21%] [G loss: 0.900791]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 32/86 [loss: 0.332954, acc.: 90.97%] [G loss: 0.885950]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 33/86 [loss: 0.408237, acc.: 85.89%] [G loss: 0.872442]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 34/86 [loss: 0.246525, acc.: 96.34%] [G loss: 0.886960]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 35/86 [loss: 0.321042, acc.: 91.55%] [G loss: 0.884284]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 36/86 [loss: 0.393599, acc.: 86.04%] [G loss: 0.907605]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 37/86 [loss: 0.292999, acc.: 93.36%] [G loss: 0.900714]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 38/86 [loss: 0.352547, acc.: 87.70%] [G loss: 0.781335]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 39/86 [loss: 0.271259, acc.: 95.12%] [G loss: 0.820810]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 40/86 [loss: 0.291169, acc.: 93.75%] [G loss: 0.845008]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 41/86 [loss: 0.304569, acc.: 93.21%] [G loss: 0.884339]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 42/86 [loss: 0.285782, acc.: 93.36%] [G loss: 0.842276]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 43/86 [loss: 0.351175, acc.: 89.40%] [G loss: 0.851564]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 44/86 [loss: 0.364946, acc.: 87.99%] [G loss: 0.814362]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 45/86 [loss: 0.345164, acc.: 89.70%] [G loss: 0.854711]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 46/86 [loss: 0.394493, acc.: 86.13%] [G loss: 0.865674]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 47/86 [loss: 0.421509, acc.: 84.08%] [G loss: 0.848883]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 48/86 [loss: 0.379216, acc.: 87.45%] [G loss: 0.856763]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 49/86 [loss: 0.288408, acc.: 93.99%] [G loss: 0.918812]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 50/86 [loss: 0.361310, acc.: 88.18%] [G loss: 0.911831]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 51/86 [loss: 0.317639, acc.: 92.24%] [G loss: 0.896582]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 52/86 [loss: 0.307805, acc.: 92.97%] [G loss: 0.858807]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 53/86 [loss: 0.355974, acc.: 89.65%] [G loss: 0.868650]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 54/86 [loss: 0.248427, acc.: 96.29%] [G loss: 0.876731]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 55/86 [loss: 0.334552, acc.: 91.26%] [G loss: 0.887569]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 56/86 [loss: 0.362486, acc.: 88.53%] [G loss: 0.862597]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 57/86 [loss: 0.321177, acc.: 91.80%] [G loss: 0.896031]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 58/86 [loss: 0.295058, acc.: 93.46%] [G loss: 0.873173]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 59/86 [loss: 0.396857, acc.: 85.60%] [G loss: 0.842617]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 60/86 [loss: 0.393197, acc.: 86.62%] [G loss: 0.839390]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 61/86 [loss: 0.382215, acc.: 87.55%] [G loss: 0.833407]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 62/86 [loss: 0.307867, acc.: 91.94%] [G loss: 0.856177]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 63/86 [loss: 0.372011, acc.: 87.16%] [G loss: 0.854846]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 64/86 [loss: 0.484321, acc.: 78.03%] [G loss: 0.899147]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 65/86 [loss: 0.378606, acc.: 86.91%] [G loss: 0.826798]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 66/86 [loss: 0.360644, acc.: 89.21%] [G loss: 0.829229]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 67/86 [loss: 0.365810, acc.: 88.72%] [G loss: 0.854456]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 68/86 [loss: 0.347842, acc.: 90.53%] [G loss: 0.853164]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 69/86 [loss: 0.324707, acc.: 91.89%] [G loss: 0.883770]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 70/86 [loss: 0.502273, acc.: 76.90%] [G loss: 0.884758]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 71/86 [loss: 0.356246, acc.: 88.96%] [G loss: 0.874448]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 72/86 [loss: 0.319296, acc.: 91.26%] [G loss: 0.852027]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 73/86 [loss: 0.316838, acc.: 91.65%] [G loss: 0.862416]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 74/86 [loss: 0.398870, acc.: 86.87%] [G loss: 0.830858]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 75/86 [loss: 0.347917, acc.: 89.94%] [G loss: 0.840865]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 76/86 [loss: 0.373990, acc.: 88.38%] [G loss: 0.851974]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 77/86 [loss: 0.361347, acc.: 90.04%] [G loss: 0.887235]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 78/86 [loss: 0.371112, acc.: 88.09%] [G loss: 0.867175]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 79/86 [loss: 0.332865, acc.: 91.70%] [G loss: 0.895811]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 80/86 [loss: 0.384264, acc.: 87.21%] [G loss: 0.850407]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 39/200  Batch Size: 81/86 [loss: 0.361448, acc.: 88.67%] [G loss: 0.862683]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 82/86 [loss: 0.333410, acc.: 91.70%] [G loss: 0.839219]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 39/200  Batch Size: 83/86 [loss: 0.301470, acc.: 93.46%] [G loss: 0.873899]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 39/200  Batch Size: 84/86 [loss: 0.429239, acc.: 81.98%] [G loss: 0.875228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 39/200  Batch Size: 85/86 [loss: 0.290096, acc.: 94.63%] [G loss: 0.878279]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 0/86 [loss: 0.333436, acc.: 91.11%] [G loss: 0.886687]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 1/86 [loss: 0.337174, acc.: 90.62%] [G loss: 0.894745]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 2/86 [loss: 0.281054, acc.: 94.29%] [G loss: 0.842944]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 3/86 [loss: 0.353234, acc.: 88.96%] [G loss: 0.829634]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 4/86 [loss: 0.328131, acc.: 92.58%] [G loss: 0.827088]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 5/86 [loss: 0.313503, acc.: 91.85%] [G loss: 0.845334]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 6/86 [loss: 0.357992, acc.: 89.40%] [G loss: 0.844872]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 7/86 [loss: 0.417820, acc.: 84.18%] [G loss: 0.815306]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 8/86 [loss: 0.411668, acc.: 84.62%] [G loss: 0.859727]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 9/86 [loss: 0.435912, acc.: 82.71%] [G loss: 0.852374]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 10/86 [loss: 0.299178, acc.: 93.41%] [G loss: 0.877533]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 11/86 [loss: 0.379886, acc.: 87.50%] [G loss: 0.882467]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 12/86 [loss: 0.314409, acc.: 93.16%] [G loss: 0.870807]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 13/86 [loss: 0.383965, acc.: 87.21%] [G loss: 0.879732]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 14/86 [loss: 0.298100, acc.: 93.36%] [G loss: 0.880334]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 15/86 [loss: 0.364151, acc.: 87.11%] [G loss: 0.944574]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 16/86 [loss: 0.341989, acc.: 90.67%] [G loss: 0.881961]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 17/86 [loss: 0.314321, acc.: 92.24%] [G loss: 0.831099]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 18/86 [loss: 0.282490, acc.: 94.09%] [G loss: 0.848491]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 19/86 [loss: 0.351093, acc.: 89.99%] [G loss: 0.856400]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 20/86 [loss: 0.169096, acc.: 98.49%] [G loss: 0.857304]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 21/86 [loss: 0.425430, acc.: 83.64%] [G loss: 0.873905]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 22/86 [loss: 0.315821, acc.: 91.26%] [G loss: 0.797004]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 23/86 [loss: 0.364819, acc.: 89.06%] [G loss: 0.829117]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 24/86 [loss: 0.280461, acc.: 94.53%] [G loss: 0.878168]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 25/86 [loss: 0.419973, acc.: 84.03%] [G loss: 0.845454]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 26/86 [loss: 0.320037, acc.: 91.65%] [G loss: 0.852550]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 27/86 [loss: 0.508381, acc.: 76.76%] [G loss: 0.799104]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 28/86 [loss: 0.403218, acc.: 85.40%] [G loss: 0.789732]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 29/86 [loss: 0.376528, acc.: 87.06%] [G loss: 0.865686]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 30/86 [loss: 0.398649, acc.: 85.99%] [G loss: 0.859332]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 31/86 [loss: 0.482216, acc.: 78.03%] [G loss: 0.866008]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 32/86 [loss: 0.319373, acc.: 91.85%] [G loss: 0.882304]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 33/86 [loss: 0.391069, acc.: 86.67%] [G loss: 0.865093]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 34/86 [loss: 0.413415, acc.: 86.23%] [G loss: 0.857081]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 35/86 [loss: 0.491385, acc.: 78.08%] [G loss: 0.897000]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 36/86 [loss: 0.298153, acc.: 93.26%] [G loss: 0.891758]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 37/86 [loss: 0.324639, acc.: 91.65%] [G loss: 0.816310]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 38/86 [loss: 0.488449, acc.: 77.34%] [G loss: 0.834298]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 39/86 [loss: 0.440691, acc.: 82.67%] [G loss: 0.838024]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 40/86 [loss: 0.391340, acc.: 86.96%] [G loss: 0.855162]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 41/86 [loss: 0.479755, acc.: 79.00%] [G loss: 0.845960]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 42/86 [loss: 0.312845, acc.: 92.14%] [G loss: 0.813866]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 43/86 [loss: 0.357891, acc.: 88.87%] [G loss: 0.878493]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 44/86 [loss: 0.325860, acc.: 91.36%] [G loss: 0.879561]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 45/86 [loss: 0.325873, acc.: 91.46%] [G loss: 0.895290]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 46/86 [loss: 0.388524, acc.: 87.89%] [G loss: 0.885210]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 47/86 [loss: 0.332058, acc.: 91.02%] [G loss: 0.843552]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 48/86 [loss: 0.331327, acc.: 90.62%] [G loss: 0.900377]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 49/86 [loss: 0.336757, acc.: 90.97%] [G loss: 0.880907]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 50/86 [loss: 0.300123, acc.: 93.12%] [G loss: 0.862879]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 51/86 [loss: 0.244407, acc.: 95.80%] [G loss: 0.921730]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 52/86 [loss: 0.292775, acc.: 93.31%] [G loss: 0.861445]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 53/86 [loss: 0.358711, acc.: 88.92%] [G loss: 0.885485]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 54/86 [loss: 0.342036, acc.: 90.33%] [G loss: 0.871277]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 55/86 [loss: 0.357442, acc.: 89.26%] [G loss: 0.892109]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 56/86 [loss: 0.280687, acc.: 93.46%] [G loss: 0.838674]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 57/86 [loss: 0.296108, acc.: 93.46%] [G loss: 0.862837]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 58/86 [loss: 0.380701, acc.: 86.72%] [G loss: 0.910685]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 59/86 [loss: 0.323307, acc.: 92.04%] [G loss: 0.866971]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 60/86 [loss: 0.298196, acc.: 93.26%] [G loss: 0.885906]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 61/86 [loss: 0.348517, acc.: 90.04%] [G loss: 0.822288]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 62/86 [loss: 0.280281, acc.: 94.09%] [G loss: 0.860411]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 63/86 [loss: 0.385142, acc.: 86.87%] [G loss: 0.837154]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 64/86 [loss: 0.392157, acc.: 86.91%] [G loss: 0.833304]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 65/86 [loss: 0.380943, acc.: 87.60%] [G loss: 0.861127]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 66/86 [loss: 0.433687, acc.: 82.71%] [G loss: 0.810051]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 67/86 [loss: 0.325167, acc.: 91.85%] [G loss: 0.848260]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 68/86 [loss: 0.243660, acc.: 96.00%] [G loss: 0.879489]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 69/86 [loss: 0.296306, acc.: 92.92%] [G loss: 0.899228]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 70/86 [loss: 0.351572, acc.: 89.55%] [G loss: 0.883817]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 71/86 [loss: 0.416446, acc.: 84.52%] [G loss: 0.794958]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 72/86 [loss: 0.293212, acc.: 93.12%] [G loss: 0.876617]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 73/86 [loss: 0.361659, acc.: 89.79%] [G loss: 0.859538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 74/86 [loss: 0.291766, acc.: 93.21%] [G loss: 0.910418]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 75/86 [loss: 0.449605, acc.: 81.59%] [G loss: 0.878712]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 76/86 [loss: 0.247641, acc.: 96.00%] [G loss: 0.908407]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 77/86 [loss: 0.391532, acc.: 85.45%] [G loss: 0.887847]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 78/86 [loss: 0.387455, acc.: 87.06%] [G loss: 0.846068]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 79/86 [loss: 0.391673, acc.: 86.67%] [G loss: 0.871145]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 40/200  Batch Size: 80/86 [loss: 0.289856, acc.: 93.95%] [G loss: 0.837363]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 40/200  Batch Size: 81/86 [loss: 0.338135, acc.: 90.53%] [G loss: 0.861510]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 82/86 [loss: 0.354275, acc.: 88.77%] [G loss: 0.842151]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 40/200  Batch Size: 83/86 [loss: 0.406489, acc.: 85.11%] [G loss: 0.894446]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 84/86 [loss: 0.293780, acc.: 93.55%] [G loss: 0.869315]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 40/200  Batch Size: 85/86 [loss: 0.238789, acc.: 95.85%] [G loss: 0.883053]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 0/86 [loss: 0.263565, acc.: 95.31%] [G loss: 0.845358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 1/86 [loss: 0.227371, acc.: 95.85%] [G loss: 0.933874]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 2/86 [loss: 0.236389, acc.: 96.19%] [G loss: 0.942157]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 3/86 [loss: 0.242064, acc.: 96.00%] [G loss: 0.933999]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 4/86 [loss: 0.255962, acc.: 95.26%] [G loss: 0.907163]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 5/86 [loss: 0.326135, acc.: 90.92%] [G loss: 0.845468]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 41/200  Batch Size: 6/86 [loss: 0.391741, acc.: 85.69%] [G loss: 0.780820]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 7/86 [loss: 0.405903, acc.: 85.45%] [G loss: 0.777841]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 8/86 [loss: 0.369515, acc.: 87.79%] [G loss: 0.834387]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 9/86 [loss: 0.500499, acc.: 74.76%] [G loss: 0.902804]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 41/200  Batch Size: 10/86 [loss: 0.343925, acc.: 90.58%] [G loss: 0.901516]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 11/86 [loss: 0.300857, acc.: 93.12%] [G loss: 0.846385]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 12/86 [loss: 0.328333, acc.: 90.43%] [G loss: 0.893236]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 13/86 [loss: 0.321701, acc.: 91.99%] [G loss: 0.866730]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 14/86 [loss: 0.325868, acc.: 91.70%] [G loss: 0.872801]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 15/86 [loss: 0.369571, acc.: 86.47%] [G loss: 0.927513]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 16/86 [loss: 0.279154, acc.: 94.63%] [G loss: 0.859670]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 41/200  Batch Size: 17/86 [loss: 0.317366, acc.: 91.41%] [G loss: 0.912224]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 18/86 [loss: 0.221987, acc.: 97.46%] [G loss: 0.895005]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 19/86 [loss: 0.250102, acc.: 96.29%] [G loss: 0.858452]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 20/86 [loss: 0.279932, acc.: 94.19%] [G loss: 0.856051]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 41/200  Batch Size: 21/86 [loss: 0.340380, acc.: 89.60%] [G loss: 0.838603]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 22/86 [loss: 0.293766, acc.: 93.26%] [G loss: 0.809658]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 23/86 [loss: 0.338763, acc.: 90.77%] [G loss: 0.816884]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 24/86 [loss: 0.351734, acc.: 89.36%] [G loss: 0.871123]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 25/86 [loss: 0.385313, acc.: 87.50%] [G loss: 0.886189]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 26/86 [loss: 0.385070, acc.: 86.96%] [G loss: 0.843846]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 27/86 [loss: 0.477904, acc.: 78.96%] [G loss: 0.864311]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 41/200  Batch Size: 28/86 [loss: 0.378296, acc.: 86.47%] [G loss: 0.794535]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 29/86 [loss: 0.403719, acc.: 85.35%] [G loss: 0.819719]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 30/86 [loss: 0.445627, acc.: 81.79%] [G loss: 0.859347]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 31/86 [loss: 0.438748, acc.: 82.71%] [G loss: 0.879637]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 32/86 [loss: 0.387929, acc.: 86.67%] [G loss: 0.873040]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 33/86 [loss: 0.373785, acc.: 88.38%] [G loss: 0.874947]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 34/86 [loss: 0.400751, acc.: 85.25%] [G loss: 0.869989]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 35/86 [loss: 0.313728, acc.: 92.29%] [G loss: 0.868886]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 36/86 [loss: 0.303427, acc.: 92.82%] [G loss: 0.867423]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 37/86 [loss: 0.276542, acc.: 93.80%] [G loss: 0.923411]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 38/86 [loss: 0.255393, acc.: 95.85%] [G loss: 0.934779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 39/86 [loss: 0.349647, acc.: 89.31%] [G loss: 0.866435]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 40/86 [loss: 0.273790, acc.: 94.78%] [G loss: 0.895057]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 41/200  Batch Size: 41/86 [loss: 0.305736, acc.: 92.72%] [G loss: 0.867034]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 42/86 [loss: 0.385204, acc.: 86.38%] [G loss: 0.811002]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 43/86 [loss: 0.335041, acc.: 91.99%] [G loss: 0.859769]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 44/86 [loss: 0.323444, acc.: 91.21%] [G loss: 0.823684]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 45/86 [loss: 0.504403, acc.: 76.95%] [G loss: 0.857319]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 46/86 [loss: 0.318312, acc.: 92.58%] [G loss: 0.872618]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 47/86 [loss: 0.470120, acc.: 79.39%] [G loss: 0.854734]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 48/86 [loss: 0.525428, acc.: 74.51%] [G loss: 0.849454]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 49/86 [loss: 0.340966, acc.: 90.23%] [G loss: 0.865161]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 50/86 [loss: 0.426925, acc.: 83.59%] [G loss: 0.868699]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 51/86 [loss: 0.389786, acc.: 86.72%] [G loss: 0.869253]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 41/200  Batch Size: 52/86 [loss: 0.459672, acc.: 80.91%] [G loss: 0.832351]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 53/86 [loss: 0.368722, acc.: 88.18%] [G loss: 0.882374]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 54/86 [loss: 0.349199, acc.: 89.16%] [G loss: 0.861241]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 55/86 [loss: 0.329592, acc.: 91.36%] [G loss: 0.868041]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 56/86 [loss: 0.432664, acc.: 83.64%] [G loss: 0.836715]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 57/86 [loss: 0.354328, acc.: 88.48%] [G loss: 0.878065]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 58/86 [loss: 0.369568, acc.: 88.72%] [G loss: 0.895326]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 59/86 [loss: 0.299627, acc.: 93.41%] [G loss: 0.874343]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 60/86 [loss: 0.404786, acc.: 86.38%] [G loss: 0.885011]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 61/86 [loss: 0.283605, acc.: 93.07%] [G loss: 0.953657]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 62/86 [loss: 0.316157, acc.: 91.46%] [G loss: 0.895535]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 63/86 [loss: 0.336632, acc.: 90.28%] [G loss: 0.864645]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 64/86 [loss: 0.344355, acc.: 90.67%] [G loss: 0.878542]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 65/86 [loss: 0.421099, acc.: 83.69%] [G loss: 0.815704]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 41/200  Batch Size: 66/86 [loss: 0.566221, acc.: 70.80%] [G loss: 0.804016]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 67/86 [loss: 0.426632, acc.: 83.15%] [G loss: 0.844766]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 68/86 [loss: 0.369391, acc.: 88.13%] [G loss: 0.868885]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 69/86 [loss: 0.282190, acc.: 93.51%] [G loss: 0.910543]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 70/86 [loss: 0.415336, acc.: 84.96%] [G loss: 0.860632]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 71/86 [loss: 0.255836, acc.: 94.58%] [G loss: 0.956041]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 72/86 [loss: 0.328735, acc.: 91.75%] [G loss: 0.927899]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 73/86 [loss: 0.314496, acc.: 92.48%] [G loss: 0.907164]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 74/86 [loss: 0.228046, acc.: 96.58%] [G loss: 0.934523]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 75/86 [loss: 0.319154, acc.: 91.80%] [G loss: 0.857043]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 76/86 [loss: 0.384623, acc.: 87.84%] [G loss: 0.853526]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 41/200  Batch Size: 77/86 [loss: 0.242784, acc.: 96.48%] [G loss: 0.825201]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 78/86 [loss: 0.312611, acc.: 93.16%] [G loss: 0.873223]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 79/86 [loss: 0.359992, acc.: 89.36%] [G loss: 0.866787]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 80/86 [loss: 0.437783, acc.: 83.06%] [G loss: 0.841926]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 81/86 [loss: 0.357467, acc.: 89.55%] [G loss: 0.869810]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 82/86 [loss: 0.305446, acc.: 92.63%] [G loss: 0.857805]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 41/200  Batch Size: 83/86 [loss: 0.367634, acc.: 88.87%] [G loss: 0.883243]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 84/86 [loss: 0.405164, acc.: 85.64%] [G loss: 0.869233]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 41/200  Batch Size: 85/86 [loss: 0.302960, acc.: 93.60%] [G loss: 0.891030]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 0/86 [loss: 0.312875, acc.: 93.07%] [G loss: 0.879010]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 1/86 [loss: 0.389380, acc.: 84.67%] [G loss: 0.899974]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 42/200  Batch Size: 2/86 [loss: 0.284535, acc.: 94.48%] [G loss: 0.914653]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 3/86 [loss: 0.254046, acc.: 95.41%] [G loss: 0.893014]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 4/86 [loss: 0.264345, acc.: 94.29%] [G loss: 0.899834]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 42/200  Batch Size: 5/86 [loss: 0.266066, acc.: 94.73%] [G loss: 0.834421]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 6/86 [loss: 0.285458, acc.: 93.21%] [G loss: 0.864472]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 7/86 [loss: 0.320155, acc.: 92.48%] [G loss: 0.857094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 8/86 [loss: 0.262848, acc.: 95.75%] [G loss: 0.834784]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 9/86 [loss: 0.312593, acc.: 92.19%] [G loss: 0.844772]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 10/86 [loss: 0.376846, acc.: 87.89%] [G loss: 0.823796]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 11/86 [loss: 0.502812, acc.: 78.42%] [G loss: 0.809067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 12/86 [loss: 0.310870, acc.: 92.68%] [G loss: 0.839897]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 13/86 [loss: 0.421698, acc.: 82.67%] [G loss: 0.911588]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 14/86 [loss: 0.405419, acc.: 85.16%] [G loss: 0.848294]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 15/86 [loss: 0.329365, acc.: 91.70%] [G loss: 0.867662]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 16/86 [loss: 0.326130, acc.: 91.26%] [G loss: 0.857258]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 17/86 [loss: 0.244810, acc.: 95.90%] [G loss: 0.848046]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 18/86 [loss: 0.492719, acc.: 78.22%] [G loss: 0.842397]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 19/86 [loss: 0.392798, acc.: 87.21%] [G loss: 0.823338]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 42/200  Batch Size: 20/86 [loss: 0.264408, acc.: 94.92%] [G loss: 0.896021]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 21/86 [loss: 0.345560, acc.: 89.94%] [G loss: 0.860680]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 22/86 [loss: 0.318879, acc.: 91.26%] [G loss: 0.924989]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 23/86 [loss: 0.431754, acc.: 82.67%] [G loss: 0.864339]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 24/86 [loss: 0.329701, acc.: 90.77%] [G loss: 0.838774]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 25/86 [loss: 0.278950, acc.: 94.29%] [G loss: 0.829402]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 26/86 [loss: 0.325688, acc.: 91.26%] [G loss: 0.844996]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 27/86 [loss: 0.368586, acc.: 88.82%] [G loss: 0.847358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 28/86 [loss: 0.421658, acc.: 85.01%] [G loss: 0.835896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 29/86 [loss: 0.399628, acc.: 85.25%] [G loss: 0.808696]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 30/86 [loss: 0.284075, acc.: 95.02%] [G loss: 0.843381]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 31/86 [loss: 0.412253, acc.: 84.81%] [G loss: 0.904856]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 32/86 [loss: 0.314456, acc.: 91.75%] [G loss: 0.844174]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 33/86 [loss: 0.357894, acc.: 89.60%] [G loss: 0.856774]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 34/86 [loss: 0.270896, acc.: 95.12%] [G loss: 0.880767]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 35/86 [loss: 0.377477, acc.: 87.89%] [G loss: 0.907924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 36/86 [loss: 0.319457, acc.: 92.43%] [G loss: 0.947670]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 42/200  Batch Size: 37/86 [loss: 0.323489, acc.: 91.70%] [G loss: 0.888089]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 38/86 [loss: 0.377492, acc.: 86.91%] [G loss: 0.806201]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 39/86 [loss: 0.391872, acc.: 86.23%] [G loss: 0.852290]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 42/200  Batch Size: 40/86 [loss: 0.392264, acc.: 86.43%] [G loss: 0.849457]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 41/86 [loss: 0.312602, acc.: 92.24%] [G loss: 0.873006]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 42/86 [loss: 0.323799, acc.: 92.24%] [G loss: 0.879127]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 43/86 [loss: 0.310233, acc.: 92.97%] [G loss: 0.862837]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 42/200  Batch Size: 44/86 [loss: 0.363853, acc.: 89.06%] [G loss: 0.871117]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 45/86 [loss: 0.319961, acc.: 91.99%] [G loss: 0.904000]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 46/86 [loss: 0.287489, acc.: 94.43%] [G loss: 0.885725]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 47/86 [loss: 0.261153, acc.: 95.21%] [G loss: 0.897722]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 48/86 [loss: 0.273804, acc.: 93.99%] [G loss: 0.923284]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 49/86 [loss: 0.281223, acc.: 93.95%] [G loss: 0.874808]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 50/86 [loss: 0.331207, acc.: 90.82%] [G loss: 0.910926]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 51/86 [loss: 0.350770, acc.: 89.31%] [G loss: 0.832418]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 52/86 [loss: 0.289798, acc.: 93.99%] [G loss: 0.849867]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 53/86 [loss: 0.289915, acc.: 94.14%] [G loss: 0.849081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 54/86 [loss: 0.317674, acc.: 90.67%] [G loss: 0.923772]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 55/86 [loss: 0.292487, acc.: 93.51%] [G loss: 0.881332]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 56/86 [loss: 0.366630, acc.: 87.30%] [G loss: 0.940837]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 57/86 [loss: 0.231320, acc.: 96.73%] [G loss: 0.860031]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 58/86 [loss: 0.296103, acc.: 92.87%] [G loss: 0.831862]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 59/86 [loss: 0.314568, acc.: 92.43%] [G loss: 0.825233]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 60/86 [loss: 0.413050, acc.: 84.03%] [G loss: 0.772995]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 61/86 [loss: 0.284222, acc.: 94.24%] [G loss: 0.802676]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 62/86 [loss: 0.347210, acc.: 90.33%] [G loss: 0.870237]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 63/86 [loss: 0.342352, acc.: 88.96%] [G loss: 0.945629]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 64/86 [loss: 0.297955, acc.: 93.21%] [G loss: 0.914340]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 65/86 [loss: 0.422921, acc.: 84.13%] [G loss: 0.872630]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 66/86 [loss: 0.358075, acc.: 89.01%] [G loss: 0.848622]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 67/86 [loss: 0.291136, acc.: 94.19%] [G loss: 0.822604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 68/86 [loss: 0.324242, acc.: 91.31%] [G loss: 0.856057]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 42/200  Batch Size: 69/86 [loss: 0.341584, acc.: 90.23%] [G loss: 0.863647]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 70/86 [loss: 0.312454, acc.: 92.38%] [G loss: 0.863706]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 71/86 [loss: 0.277878, acc.: 94.09%] [G loss: 0.903361]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 72/86 [loss: 0.244165, acc.: 96.48%] [G loss: 0.889921]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 73/86 [loss: 0.319984, acc.: 92.53%] [G loss: 0.879439]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 74/86 [loss: 0.237509, acc.: 97.02%] [G loss: 0.886104]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 75/86 [loss: 0.225726, acc.: 97.07%] [G loss: 0.920361]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 42/200  Batch Size: 76/86 [loss: 0.417434, acc.: 84.81%] [G loss: 0.874230]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 77/86 [loss: 0.344182, acc.: 89.89%] [G loss: 0.879031]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 78/86 [loss: 0.319357, acc.: 91.85%] [G loss: 0.929857]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 42/200  Batch Size: 79/86 [loss: 0.326913, acc.: 91.50%] [G loss: 0.875693]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 80/86 [loss: 0.265903, acc.: 95.07%] [G loss: 0.873878]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 81/86 [loss: 0.366206, acc.: 88.48%] [G loss: 0.856232]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 82/86 [loss: 0.471686, acc.: 79.20%] [G loss: 0.847083]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 42/200  Batch Size: 83/86 [loss: 0.375505, acc.: 88.77%] [G loss: 0.802255]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 84/86 [loss: 0.352089, acc.: 90.53%] [G loss: 0.819523]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 42/200  Batch Size: 85/86 [loss: 0.408235, acc.: 85.30%] [G loss: 0.858476]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 0/86 [loss: 0.459286, acc.: 80.96%] [G loss: 0.853429]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 1/86 [loss: 0.412820, acc.: 84.62%] [G loss: 0.860877]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 2/86 [loss: 0.380660, acc.: 88.09%] [G loss: 0.870369]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 3/86 [loss: 0.276023, acc.: 94.24%] [G loss: 0.879088]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 4/86 [loss: 0.309242, acc.: 92.97%] [G loss: 0.892862]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 43/200  Batch Size: 5/86 [loss: 0.266294, acc.: 94.48%] [G loss: 0.855812]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 6/86 [loss: 0.297338, acc.: 93.07%] [G loss: 0.851806]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 7/86 [loss: 0.358384, acc.: 89.60%] [G loss: 0.846609]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 8/86 [loss: 0.338000, acc.: 91.02%] [G loss: 0.849871]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 9/86 [loss: 0.416411, acc.: 84.72%] [G loss: 0.843111]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 10/86 [loss: 0.294559, acc.: 93.60%] [G loss: 0.843832]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 11/86 [loss: 0.387145, acc.: 87.21%] [G loss: 0.852595]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 43/200  Batch Size: 12/86 [loss: 0.368462, acc.: 89.36%] [G loss: 0.849288]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 13/86 [loss: 0.345564, acc.: 90.14%] [G loss: 0.870642]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 14/86 [loss: 0.351413, acc.: 89.26%] [G loss: 0.799870]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 15/86 [loss: 0.330918, acc.: 91.36%] [G loss: 0.822687]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 16/86 [loss: 0.288211, acc.: 93.70%] [G loss: 0.866664]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 17/86 [loss: 0.301215, acc.: 92.82%] [G loss: 0.841200]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 18/86 [loss: 0.220567, acc.: 97.31%] [G loss: 0.848113]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 19/86 [loss: 0.379723, acc.: 88.53%] [G loss: 0.885913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 20/86 [loss: 0.396138, acc.: 86.67%] [G loss: 0.886693]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 21/86 [loss: 0.352224, acc.: 89.06%] [G loss: 0.838966]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 22/86 [loss: 0.281787, acc.: 95.07%] [G loss: 0.889790]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 23/86 [loss: 0.342658, acc.: 89.99%] [G loss: 0.902008]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 24/86 [loss: 0.332847, acc.: 91.02%] [G loss: 0.841089]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 25/86 [loss: 0.277442, acc.: 94.34%] [G loss: 0.853818]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 26/86 [loss: 0.278634, acc.: 94.58%] [G loss: 0.821225]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 27/86 [loss: 0.321172, acc.: 90.67%] [G loss: 0.938232]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 28/86 [loss: 0.293116, acc.: 93.65%] [G loss: 0.900164]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 29/86 [loss: 0.198304, acc.: 98.00%] [G loss: 0.853742]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 30/86 [loss: 0.297373, acc.: 93.07%] [G loss: 0.885024]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 43/200  Batch Size: 31/86 [loss: 0.262648, acc.: 95.41%] [G loss: 0.905956]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 32/86 [loss: 0.314800, acc.: 91.70%] [G loss: 0.840062]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 33/86 [loss: 0.392534, acc.: 87.84%] [G loss: 0.855946]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 34/86 [loss: 0.319998, acc.: 91.41%] [G loss: 0.874656]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 35/86 [loss: 0.306189, acc.: 93.07%] [G loss: 0.850725]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 36/86 [loss: 0.411967, acc.: 85.06%] [G loss: 0.818022]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 37/86 [loss: 0.255680, acc.: 96.14%] [G loss: 0.842484]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 38/86 [loss: 0.255861, acc.: 95.61%] [G loss: 0.936446]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 39/86 [loss: 0.217196, acc.: 97.31%] [G loss: 0.937931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 40/86 [loss: 0.336078, acc.: 91.21%] [G loss: 0.918142]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 41/86 [loss: 0.473472, acc.: 80.52%] [G loss: 0.892285]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 42/86 [loss: 0.332659, acc.: 90.82%] [G loss: 0.829361]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 43/86 [loss: 0.297401, acc.: 93.12%] [G loss: 0.837799]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 44/86 [loss: 0.453657, acc.: 81.25%] [G loss: 0.848848]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 45/86 [loss: 0.291052, acc.: 93.60%] [G loss: 0.853490]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 46/86 [loss: 0.405331, acc.: 85.94%] [G loss: 0.876842]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 47/86 [loss: 0.358241, acc.: 89.65%] [G loss: 0.800597]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 48/86 [loss: 0.231180, acc.: 96.00%] [G loss: 0.837453]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 49/86 [loss: 0.308756, acc.: 93.36%] [G loss: 0.879471]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 50/86 [loss: 0.294177, acc.: 93.12%] [G loss: 0.878474]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 51/86 [loss: 0.376546, acc.: 88.18%] [G loss: 0.858791]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 52/86 [loss: 0.311677, acc.: 92.04%] [G loss: 0.868229]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 53/86 [loss: 0.441276, acc.: 82.86%] [G loss: 0.856901]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 43/200  Batch Size: 54/86 [loss: 0.418854, acc.: 84.42%] [G loss: 0.820071]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 55/86 [loss: 0.566577, acc.: 71.58%] [G loss: 0.802068]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 56/86 [loss: 0.331610, acc.: 91.16%] [G loss: 0.852355]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 57/86 [loss: 0.456886, acc.: 80.62%] [G loss: 0.857040]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 58/86 [loss: 0.566029, acc.: 69.92%] [G loss: 0.818683]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 59/86 [loss: 0.425237, acc.: 83.54%] [G loss: 0.840896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 60/86 [loss: 0.417745, acc.: 84.96%] [G loss: 0.850719]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 61/86 [loss: 0.366746, acc.: 88.43%] [G loss: 0.898352]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 62/86 [loss: 0.507346, acc.: 77.00%] [G loss: 0.901803]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 63/86 [loss: 0.478188, acc.: 80.27%] [G loss: 0.800244]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 64/86 [loss: 0.347760, acc.: 89.06%] [G loss: 0.892812]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 65/86 [loss: 0.366420, acc.: 88.96%] [G loss: 0.895228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 66/86 [loss: 0.335206, acc.: 91.60%] [G loss: 0.846842]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 67/86 [loss: 0.378340, acc.: 86.18%] [G loss: 0.934900]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 68/86 [loss: 0.349923, acc.: 89.70%] [G loss: 0.894608]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 69/86 [loss: 0.256358, acc.: 95.85%] [G loss: 0.859244]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 70/86 [loss: 0.402778, acc.: 86.28%] [G loss: 0.875137]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 71/86 [loss: 0.312596, acc.: 93.31%] [G loss: 0.887586]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 72/86 [loss: 0.275250, acc.: 94.97%] [G loss: 0.873473]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 73/86 [loss: 0.295405, acc.: 94.19%] [G loss: 0.862591]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 43/200  Batch Size: 74/86 [loss: 0.294441, acc.: 93.65%] [G loss: 0.883169]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 75/86 [loss: 0.355244, acc.: 90.23%] [G loss: 0.878091]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 76/86 [loss: 0.282665, acc.: 94.34%] [G loss: 0.860880]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 77/86 [loss: 0.280562, acc.: 93.51%] [G loss: 0.927777]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 78/86 [loss: 0.277420, acc.: 95.17%] [G loss: 0.892390]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 79/86 [loss: 0.288007, acc.: 94.24%] [G loss: 0.873516]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 80/86 [loss: 0.350280, acc.: 90.04%] [G loss: 0.886126]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 81/86 [loss: 0.324815, acc.: 90.09%] [G loss: 0.787737]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 43/200  Batch Size: 82/86 [loss: 0.385222, acc.: 87.60%] [G loss: 0.761398]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 83/86 [loss: 0.361501, acc.: 89.79%] [G loss: 0.801501]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 43/200  Batch Size: 84/86 [loss: 0.401989, acc.: 86.67%] [G loss: 0.821625]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 43/200  Batch Size: 85/86 [loss: 0.356187, acc.: 89.94%] [G loss: 0.850045]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 0/86 [loss: 0.504930, acc.: 77.39%] [G loss: 0.813103]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 1/86 [loss: 0.296197, acc.: 93.60%] [G loss: 0.837570]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 2/86 [loss: 0.384247, acc.: 87.45%] [G loss: 0.855972]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 3/86 [loss: 0.324211, acc.: 91.55%] [G loss: 0.820218]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 4/86 [loss: 0.401604, acc.: 85.84%] [G loss: 0.923266]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 5/86 [loss: 0.423021, acc.: 83.89%] [G loss: 0.896473]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 6/86 [loss: 0.302280, acc.: 93.46%] [G loss: 0.888393]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 7/86 [loss: 0.337127, acc.: 90.82%] [G loss: 0.942254]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 8/86 [loss: 0.323893, acc.: 91.11%] [G loss: 0.935542]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 9/86 [loss: 0.277374, acc.: 93.85%] [G loss: 0.849170]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 10/86 [loss: 0.236508, acc.: 96.68%] [G loss: 0.915979]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 11/86 [loss: 0.282166, acc.: 94.48%] [G loss: 0.912354]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 12/86 [loss: 0.332415, acc.: 89.21%] [G loss: 0.799600]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 13/86 [loss: 0.349154, acc.: 90.19%] [G loss: 0.832640]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 14/86 [loss: 0.350365, acc.: 91.02%] [G loss: 0.837027]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 15/86 [loss: 0.330387, acc.: 90.28%] [G loss: 0.860641]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 16/86 [loss: 0.292366, acc.: 93.41%] [G loss: 0.889782]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 17/86 [loss: 0.334300, acc.: 90.92%] [G loss: 0.933847]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 18/86 [loss: 0.253480, acc.: 96.44%] [G loss: 0.890292]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 19/86 [loss: 0.225559, acc.: 96.48%] [G loss: 0.942912]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 44/200  Batch Size: 20/86 [loss: 0.320229, acc.: 92.63%] [G loss: 0.887510]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 21/86 [loss: 0.252614, acc.: 95.70%] [G loss: 0.851245]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 22/86 [loss: 0.371403, acc.: 88.57%] [G loss: 0.807699]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 23/86 [loss: 0.280850, acc.: 94.48%] [G loss: 0.809454]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 24/86 [loss: 0.410923, acc.: 86.28%] [G loss: 0.806460]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 25/86 [loss: 0.461621, acc.: 80.76%] [G loss: 0.788912]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 26/86 [loss: 0.335457, acc.: 91.02%] [G loss: 0.809310]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 27/86 [loss: 0.430107, acc.: 83.40%] [G loss: 0.797410]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 28/86 [loss: 0.358545, acc.: 89.50%] [G loss: 0.816314]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 29/86 [loss: 0.346009, acc.: 90.33%] [G loss: 0.847741]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 30/86 [loss: 0.475923, acc.: 79.35%] [G loss: 0.892980]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 44/200  Batch Size: 31/86 [loss: 0.319569, acc.: 91.80%] [G loss: 0.914041]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 32/86 [loss: 0.300166, acc.: 93.26%] [G loss: 0.871047]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 33/86 [loss: 0.304522, acc.: 93.07%] [G loss: 0.867230]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 34/86 [loss: 0.258518, acc.: 95.56%] [G loss: 0.870222]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 35/86 [loss: 0.338847, acc.: 90.09%] [G loss: 0.868751]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 36/86 [loss: 0.317383, acc.: 91.31%] [G loss: 0.877056]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 37/86 [loss: 0.412469, acc.: 85.55%] [G loss: 0.817114]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 38/86 [loss: 0.321931, acc.: 92.09%] [G loss: 0.827926]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 39/86 [loss: 0.300169, acc.: 92.97%] [G loss: 0.830912]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 40/86 [loss: 0.364781, acc.: 89.01%] [G loss: 0.841372]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 41/86 [loss: 0.282711, acc.: 94.14%] [G loss: 0.855503]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 42/86 [loss: 0.294486, acc.: 93.60%] [G loss: 0.875373]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 43/86 [loss: 0.260692, acc.: 96.00%] [G loss: 0.868600]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 44/86 [loss: 0.363195, acc.: 88.72%] [G loss: 0.884527]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 44/200  Batch Size: 45/86 [loss: 0.224588, acc.: 96.97%] [G loss: 0.849065]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 46/86 [loss: 0.318492, acc.: 92.19%] [G loss: 0.867525]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 47/86 [loss: 0.273460, acc.: 95.36%] [G loss: 0.852553]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 48/86 [loss: 0.277744, acc.: 94.87%] [G loss: 0.892765]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 49/86 [loss: 0.313642, acc.: 92.29%] [G loss: 0.876759]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 50/86 [loss: 0.246038, acc.: 96.58%] [G loss: 0.909526]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 51/86 [loss: 0.288233, acc.: 94.63%] [G loss: 0.938601]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 44/200  Batch Size: 52/86 [loss: 0.262548, acc.: 96.00%] [G loss: 0.903699]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 53/86 [loss: 0.347692, acc.: 89.75%] [G loss: 0.858435]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 54/86 [loss: 0.243700, acc.: 96.44%] [G loss: 0.877628]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 55/86 [loss: 0.211907, acc.: 97.46%] [G loss: 0.903276]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 56/86 [loss: 0.234061, acc.: 96.29%] [G loss: 0.872542]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 57/86 [loss: 0.397979, acc.: 86.87%] [G loss: 0.850613]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 58/86 [loss: 0.313596, acc.: 92.33%] [G loss: 0.914173]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 44/200  Batch Size: 59/86 [loss: 0.316140, acc.: 92.68%] [G loss: 0.846511]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 60/86 [loss: 0.405138, acc.: 85.16%] [G loss: 0.840967]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 61/86 [loss: 0.328971, acc.: 90.62%] [G loss: 0.788764]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 44/200  Batch Size: 62/86 [loss: 0.405212, acc.: 85.55%] [G loss: 0.803367]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 63/86 [loss: 0.401846, acc.: 86.72%] [G loss: 0.816512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 64/86 [loss: 0.390226, acc.: 86.77%] [G loss: 0.901344]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 65/86 [loss: 0.453966, acc.: 82.28%] [G loss: 0.835663]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 66/86 [loss: 0.488084, acc.: 78.42%] [G loss: 0.861854]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 67/86 [loss: 0.430091, acc.: 83.98%] [G loss: 0.811343]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 68/86 [loss: 0.437341, acc.: 83.01%] [G loss: 0.808244]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 44/200  Batch Size: 69/86 [loss: 0.358123, acc.: 88.82%] [G loss: 0.804006]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 70/86 [loss: 0.372880, acc.: 87.26%] [G loss: 0.884244]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 71/86 [loss: 0.384604, acc.: 87.50%] [G loss: 0.866096]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 72/86 [loss: 0.410202, acc.: 84.81%] [G loss: 0.877350]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 73/86 [loss: 0.380757, acc.: 88.28%] [G loss: 0.840280]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 74/86 [loss: 0.304322, acc.: 93.26%] [G loss: 0.880756]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 75/86 [loss: 0.361888, acc.: 88.53%] [G loss: 0.910481]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 76/86 [loss: 0.246473, acc.: 96.04%] [G loss: 0.927256]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 77/86 [loss: 0.344650, acc.: 91.31%] [G loss: 0.920233]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 78/86 [loss: 0.330534, acc.: 89.89%] [G loss: 0.822902]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 79/86 [loss: 0.265322, acc.: 95.17%] [G loss: 0.893207]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 80/86 [loss: 0.357560, acc.: 90.09%] [G loss: 0.859712]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 44/200  Batch Size: 81/86 [loss: 0.217530, acc.: 97.27%] [G loss: 0.940639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 44/200  Batch Size: 82/86 [loss: 0.361676, acc.: 89.60%] [G loss: 0.860611]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 44/200  Batch Size: 83/86 [loss: 0.335784, acc.: 90.87%] [G loss: 0.896981]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 84/86 [loss: 0.272175, acc.: 95.02%] [G loss: 0.805528]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 44/200  Batch Size: 85/86 [loss: 0.307670, acc.: 92.58%] [G loss: 0.864767]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 0/86 [loss: 0.287261, acc.: 93.75%] [G loss: 0.853248]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 1/86 [loss: 0.398833, acc.: 86.52%] [G loss: 0.846038]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 2/86 [loss: 0.286358, acc.: 94.19%] [G loss: 0.866079]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 3/86 [loss: 0.263049, acc.: 94.97%] [G loss: 0.918369]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 4/86 [loss: 0.322642, acc.: 90.82%] [G loss: 0.890295]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 5/86 [loss: 0.262096, acc.: 94.29%] [G loss: 0.920293]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 6/86 [loss: 0.247335, acc.: 96.44%] [G loss: 0.899427]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 7/86 [loss: 0.186044, acc.: 98.24%] [G loss: 0.933150]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 8/86 [loss: 0.237463, acc.: 96.14%] [G loss: 0.874493]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 9/86 [loss: 0.242312, acc.: 95.95%] [G loss: 0.865537]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 10/86 [loss: 0.269635, acc.: 95.75%] [G loss: 0.892963]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 11/86 [loss: 0.475607, acc.: 79.44%] [G loss: 0.804781]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 12/86 [loss: 0.244582, acc.: 96.00%] [G loss: 0.792151]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 13/86 [loss: 0.294595, acc.: 93.12%] [G loss: 0.898162]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 14/86 [loss: 0.307788, acc.: 91.60%] [G loss: 0.903711]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 15/86 [loss: 0.363357, acc.: 87.79%] [G loss: 0.817046]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 16/86 [loss: 0.350826, acc.: 89.94%] [G loss: 0.856813]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 17/86 [loss: 0.425007, acc.: 84.18%] [G loss: 0.823582]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 18/86 [loss: 0.363373, acc.: 88.82%] [G loss: 0.855780]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 19/86 [loss: 0.363258, acc.: 89.01%] [G loss: 0.849182]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 20/86 [loss: 0.420462, acc.: 83.50%] [G loss: 0.878131]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 21/86 [loss: 0.429430, acc.: 83.94%] [G loss: 0.833482]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 22/86 [loss: 0.378458, acc.: 87.70%] [G loss: 0.854843]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 23/86 [loss: 0.308318, acc.: 92.29%] [G loss: 0.912280]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 24/86 [loss: 0.309689, acc.: 92.04%] [G loss: 0.826139]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 25/86 [loss: 0.381766, acc.: 87.65%] [G loss: 0.867713]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 26/86 [loss: 0.274986, acc.: 94.04%] [G loss: 0.923032]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 27/86 [loss: 0.284175, acc.: 93.65%] [G loss: 0.910305]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 28/86 [loss: 0.339840, acc.: 90.92%] [G loss: 0.900436]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 29/86 [loss: 0.391672, acc.: 86.72%] [G loss: 0.811479]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 30/86 [loss: 0.420743, acc.: 84.42%] [G loss: 0.850199]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 31/86 [loss: 0.313670, acc.: 90.48%] [G loss: 0.780687]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 32/86 [loss: 0.457670, acc.: 82.13%] [G loss: 0.813951]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 33/86 [loss: 0.347463, acc.: 89.70%] [G loss: 0.849270]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 34/86 [loss: 0.273290, acc.: 95.36%] [G loss: 0.837411]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 35/86 [loss: 0.221408, acc.: 96.97%] [G loss: 0.924545]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 36/86 [loss: 0.329029, acc.: 90.48%] [G loss: 0.939581]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 37/86 [loss: 0.270506, acc.: 94.43%] [G loss: 0.897407]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 38/86 [loss: 0.272132, acc.: 94.87%] [G loss: 0.834667]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 39/86 [loss: 0.302842, acc.: 92.53%] [G loss: 0.849044]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 40/86 [loss: 0.463933, acc.: 79.69%] [G loss: 0.872479]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 41/86 [loss: 0.268136, acc.: 94.97%] [G loss: 0.843793]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 42/86 [loss: 0.337551, acc.: 90.53%] [G loss: 0.826329]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 43/86 [loss: 0.419356, acc.: 84.33%] [G loss: 0.822735]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 44/86 [loss: 0.412166, acc.: 85.06%] [G loss: 0.873759]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 45/86 [loss: 0.350652, acc.: 90.04%] [G loss: 0.844210]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 46/86 [loss: 0.258725, acc.: 95.31%] [G loss: 0.832260]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 47/86 [loss: 0.446408, acc.: 81.88%] [G loss: 0.804272]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 48/86 [loss: 0.315352, acc.: 92.92%] [G loss: 0.836563]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 49/86 [loss: 0.380140, acc.: 87.79%] [G loss: 0.864797]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 50/86 [loss: 0.328210, acc.: 91.46%] [G loss: 0.870508]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 51/86 [loss: 0.335051, acc.: 91.26%] [G loss: 0.889734]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 52/86 [loss: 0.410316, acc.: 85.69%] [G loss: 0.867433]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 53/86 [loss: 0.315631, acc.: 92.72%] [G loss: 0.871873]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 54/86 [loss: 0.325706, acc.: 91.55%] [G loss: 0.868118]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 55/86 [loss: 0.302295, acc.: 93.21%] [G loss: 0.845063]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 56/86 [loss: 0.441900, acc.: 83.20%] [G loss: 0.818774]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 57/86 [loss: 0.342673, acc.: 90.43%] [G loss: 0.847355]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 58/86 [loss: 0.333266, acc.: 90.09%] [G loss: 0.891537]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 59/86 [loss: 0.369047, acc.: 88.72%] [G loss: 0.848937]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 60/86 [loss: 0.335337, acc.: 90.19%] [G loss: 0.866514]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 61/86 [loss: 0.249013, acc.: 96.19%] [G loss: 0.876542]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 62/86 [loss: 0.339238, acc.: 90.33%] [G loss: 0.901208]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 63/86 [loss: 0.267607, acc.: 94.68%] [G loss: 0.912869]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 64/86 [loss: 0.314287, acc.: 91.99%] [G loss: 0.929577]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 65/86 [loss: 0.309705, acc.: 92.43%] [G loss: 0.854280]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 66/86 [loss: 0.264060, acc.: 94.58%] [G loss: 0.797745]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 67/86 [loss: 0.359372, acc.: 87.94%] [G loss: 0.902353]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 68/86 [loss: 0.364861, acc.: 87.21%] [G loss: 0.810397]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 69/86 [loss: 0.340533, acc.: 90.58%] [G loss: 0.831145]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 70/86 [loss: 0.352800, acc.: 89.55%] [G loss: 0.829753]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 71/86 [loss: 0.469373, acc.: 79.64%] [G loss: 0.829179]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 72/86 [loss: 0.343922, acc.: 90.97%] [G loss: 0.856411]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 73/86 [loss: 0.428743, acc.: 84.18%] [G loss: 0.846873]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 74/86 [loss: 0.345921, acc.: 91.16%] [G loss: 0.869090]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 75/86 [loss: 0.314874, acc.: 92.19%] [G loss: 0.862228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 76/86 [loss: 0.384852, acc.: 87.94%] [G loss: 0.888679]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 77/86 [loss: 0.331519, acc.: 91.85%] [G loss: 0.866608]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 78/86 [loss: 0.436667, acc.: 82.23%] [G loss: 0.802494]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 79/86 [loss: 0.274377, acc.: 94.53%] [G loss: 0.900641]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 45/200  Batch Size: 80/86 [loss: 0.284217, acc.: 93.41%] [G loss: 0.937395]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 81/86 [loss: 0.337022, acc.: 91.31%] [G loss: 0.915915]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 45/200  Batch Size: 82/86 [loss: 0.264598, acc.: 95.31%] [G loss: 0.903951]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 83/86 [loss: 0.281859, acc.: 94.82%] [G loss: 0.909141]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 45/200  Batch Size: 84/86 [loss: 0.349760, acc.: 88.82%] [G loss: 0.897071]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 45/200  Batch Size: 85/86 [loss: 0.240719, acc.: 96.73%] [G loss: 0.878589]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 0/86 [loss: 0.293162, acc.: 93.41%] [G loss: 0.855369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 1/86 [loss: 0.331226, acc.: 91.94%] [G loss: 0.844032]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 2/86 [loss: 0.358569, acc.: 89.89%] [G loss: 0.831889]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 3/86 [loss: 0.291351, acc.: 93.65%] [G loss: 0.894298]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 4/86 [loss: 0.270299, acc.: 94.43%] [G loss: 0.938064]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 5/86 [loss: 0.234019, acc.: 97.07%] [G loss: 0.892994]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 6/86 [loss: 0.264434, acc.: 94.04%] [G loss: 0.918379]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 7/86 [loss: 0.304024, acc.: 93.16%] [G loss: 0.918009]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 8/86 [loss: 0.252008, acc.: 95.80%] [G loss: 0.923555]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 9/86 [loss: 0.337616, acc.: 89.89%] [G loss: 0.832382]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 46/200  Batch Size: 10/86 [loss: 0.287658, acc.: 93.70%] [G loss: 0.924916]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 11/86 [loss: 0.332048, acc.: 91.02%] [G loss: 0.806544]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 12/86 [loss: 0.340548, acc.: 90.28%] [G loss: 0.899145]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 13/86 [loss: 0.442432, acc.: 82.71%] [G loss: 0.864269]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 14/86 [loss: 0.426489, acc.: 83.35%] [G loss: 0.804662]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 15/86 [loss: 0.374787, acc.: 88.33%] [G loss: 0.849184]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 16/86 [loss: 0.389456, acc.: 86.67%] [G loss: 0.888449]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 17/86 [loss: 0.422634, acc.: 84.38%] [G loss: 0.835893]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 18/86 [loss: 0.347076, acc.: 90.38%] [G loss: 0.905002]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 19/86 [loss: 0.411477, acc.: 84.72%] [G loss: 0.824893]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 46/200  Batch Size: 20/86 [loss: 0.325960, acc.: 92.14%] [G loss: 0.847637]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 21/86 [loss: 0.371567, acc.: 88.92%] [G loss: 0.820387]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 22/86 [loss: 0.356087, acc.: 88.48%] [G loss: 0.881281]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 23/86 [loss: 0.434873, acc.: 84.38%] [G loss: 0.847027]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 24/86 [loss: 0.332917, acc.: 91.06%] [G loss: 0.874913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 25/86 [loss: 0.310567, acc.: 92.48%] [G loss: 0.950356]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 46/200  Batch Size: 26/86 [loss: 0.346905, acc.: 90.09%] [G loss: 0.920229]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 27/86 [loss: 0.292568, acc.: 93.90%] [G loss: 0.882703]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 28/86 [loss: 0.240069, acc.: 96.68%] [G loss: 0.861867]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 29/86 [loss: 0.266503, acc.: 94.87%] [G loss: 0.898483]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 30/86 [loss: 0.428621, acc.: 84.42%] [G loss: 0.897938]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 31/86 [loss: 0.234398, acc.: 95.21%] [G loss: 0.833875]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 32/86 [loss: 0.214733, acc.: 97.41%] [G loss: 0.872325]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 46/200  Batch Size: 33/86 [loss: 0.259976, acc.: 95.36%] [G loss: 0.897713]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 34/86 [loss: 0.232551, acc.: 96.97%] [G loss: 0.865226]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 35/86 [loss: 0.371082, acc.: 85.79%] [G loss: 0.774571]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 36/86 [loss: 0.241864, acc.: 96.29%] [G loss: 0.811445]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 37/86 [loss: 0.256735, acc.: 94.73%] [G loss: 0.848701]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 38/86 [loss: 0.373971, acc.: 87.89%] [G loss: 0.823372]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 39/86 [loss: 0.318478, acc.: 92.87%] [G loss: 0.861407]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 40/86 [loss: 0.440416, acc.: 83.15%] [G loss: 0.864077]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 41/86 [loss: 0.483801, acc.: 79.00%] [G loss: 0.893495]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 42/86 [loss: 0.356645, acc.: 89.65%] [G loss: 0.874829]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 43/86 [loss: 0.328278, acc.: 91.16%] [G loss: 0.869702]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 44/86 [loss: 0.471872, acc.: 80.81%] [G loss: 0.826907]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 45/86 [loss: 0.382040, acc.: 87.11%] [G loss: 0.829738]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 46/86 [loss: 0.357481, acc.: 89.45%] [G loss: 0.821993]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 47/86 [loss: 0.314514, acc.: 92.72%] [G loss: 0.833740]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 48/86 [loss: 0.461426, acc.: 81.54%] [G loss: 0.845944]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 49/86 [loss: 0.379948, acc.: 87.55%] [G loss: 0.912002]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 50/86 [loss: 0.346646, acc.: 90.53%] [G loss: 0.926502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 51/86 [loss: 0.211543, acc.: 97.61%] [G loss: 0.908511]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 52/86 [loss: 0.406407, acc.: 85.55%] [G loss: 0.940360]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 53/86 [loss: 0.237883, acc.: 96.19%] [G loss: 0.918778]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 54/86 [loss: 0.351095, acc.: 90.09%] [G loss: 0.866097]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 55/86 [loss: 0.268462, acc.: 95.12%] [G loss: 0.872492]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 56/86 [loss: 0.384434, acc.: 86.23%] [G loss: 0.833101]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 57/86 [loss: 0.233645, acc.: 97.31%] [G loss: 0.821392]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 58/86 [loss: 0.312473, acc.: 92.92%] [G loss: 0.878235]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 59/86 [loss: 0.287561, acc.: 94.58%] [G loss: 0.892768]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 60/86 [loss: 0.285333, acc.: 94.29%] [G loss: 0.901834]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 61/86 [loss: 0.408011, acc.: 86.28%] [G loss: 0.841757]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 62/86 [loss: 0.294229, acc.: 94.04%] [G loss: 0.865081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 63/86 [loss: 0.293060, acc.: 94.24%] [G loss: 0.895338]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 64/86 [loss: 0.300221, acc.: 93.75%] [G loss: 0.878031]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 65/86 [loss: 0.452453, acc.: 81.59%] [G loss: 0.803111]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 66/86 [loss: 0.303454, acc.: 93.60%] [G loss: 0.802608]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 67/86 [loss: 0.331807, acc.: 91.46%] [G loss: 0.862759]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 68/86 [loss: 0.370109, acc.: 88.96%] [G loss: 0.880246]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 46/200  Batch Size: 69/86 [loss: 0.259486, acc.: 95.65%] [G loss: 0.872315]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 70/86 [loss: 0.434870, acc.: 83.11%] [G loss: 0.848859]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 71/86 [loss: 0.370772, acc.: 89.06%] [G loss: 0.858996]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 46/200  Batch Size: 72/86 [loss: 0.356192, acc.: 90.23%] [G loss: 0.832371]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 73/86 [loss: 0.286221, acc.: 94.63%] [G loss: 0.867316]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 74/86 [loss: 0.352477, acc.: 89.55%] [G loss: 0.896040]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 75/86 [loss: 0.328184, acc.: 91.06%] [G loss: 0.864579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 76/86 [loss: 0.325964, acc.: 92.24%] [G loss: 0.856520]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 77/86 [loss: 0.345082, acc.: 91.50%] [G loss: 0.852584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 78/86 [loss: 0.344505, acc.: 90.23%] [G loss: 0.853204]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 46/200  Batch Size: 79/86 [loss: 0.282156, acc.: 93.85%] [G loss: 0.847585]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 80/86 [loss: 0.263536, acc.: 95.07%] [G loss: 0.913142]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 81/86 [loss: 0.291709, acc.: 93.12%] [G loss: 0.934054]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 46/200  Batch Size: 82/86 [loss: 0.257676, acc.: 94.43%] [G loss: 0.968730]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 46/200  Batch Size: 83/86 [loss: 0.214812, acc.: 96.92%] [G loss: 0.921775]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 84/86 [loss: 0.173105, acc.: 98.93%] [G loss: 0.902542]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 46/200  Batch Size: 85/86 [loss: 0.213661, acc.: 97.36%] [G loss: 0.922540]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 0/86 [loss: 0.195485, acc.: 98.14%] [G loss: 0.876759]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 1/86 [loss: 0.192710, acc.: 97.95%] [G loss: 0.911681]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 47/200  Batch Size: 2/86 [loss: 0.183927, acc.: 98.29%] [G loss: 0.940020]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 3/86 [loss: 0.238828, acc.: 96.53%] [G loss: 0.906528]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 4/86 [loss: 0.301373, acc.: 90.58%] [G loss: 0.814326]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 5/86 [loss: 0.270401, acc.: 95.26%] [G loss: 0.850628]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 6/86 [loss: 0.307465, acc.: 93.16%] [G loss: 0.813764]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 7/86 [loss: 0.328976, acc.: 90.77%] [G loss: 0.815838]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 8/86 [loss: 0.330943, acc.: 91.50%] [G loss: 0.837579]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 47/200  Batch Size: 9/86 [loss: 0.356288, acc.: 89.11%] [G loss: 0.817685]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 10/86 [loss: 0.300222, acc.: 93.60%] [G loss: 0.858555]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 11/86 [loss: 0.423097, acc.: 83.59%] [G loss: 0.887387]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 12/86 [loss: 0.388074, acc.: 87.50%] [G loss: 0.820697]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 13/86 [loss: 0.371578, acc.: 88.38%] [G loss: 0.858083]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 14/86 [loss: 0.362970, acc.: 89.21%] [G loss: 0.848783]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 15/86 [loss: 0.253234, acc.: 95.90%] [G loss: 0.874369]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 16/86 [loss: 0.224186, acc.: 96.58%] [G loss: 0.927527]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 17/86 [loss: 0.283187, acc.: 94.43%] [G loss: 0.935552]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 18/86 [loss: 0.252442, acc.: 96.14%] [G loss: 0.925300]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 19/86 [loss: 0.271245, acc.: 95.41%] [G loss: 0.897569]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 20/86 [loss: 0.314861, acc.: 91.99%] [G loss: 0.931160]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 21/86 [loss: 0.245088, acc.: 95.36%] [G loss: 0.829570]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 22/86 [loss: 0.332986, acc.: 90.97%] [G loss: 0.799574]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 23/86 [loss: 0.269023, acc.: 94.58%] [G loss: 0.866354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 24/86 [loss: 0.288441, acc.: 94.73%] [G loss: 0.816669]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 25/86 [loss: 0.218255, acc.: 97.56%] [G loss: 0.834573]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 26/86 [loss: 0.298227, acc.: 93.46%] [G loss: 0.856784]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 27/86 [loss: 0.318736, acc.: 91.06%] [G loss: 0.809210]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 28/86 [loss: 0.278657, acc.: 94.43%] [G loss: 0.840182]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 47/200  Batch Size: 29/86 [loss: 0.328354, acc.: 91.99%] [G loss: 0.885709]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 30/86 [loss: 0.278136, acc.: 94.09%] [G loss: 0.884358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 31/86 [loss: 0.364489, acc.: 88.53%] [G loss: 0.794272]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 32/86 [loss: 0.439042, acc.: 83.01%] [G loss: 0.854875]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 33/86 [loss: 0.395549, acc.: 86.77%] [G loss: 0.856311]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 34/86 [loss: 0.444707, acc.: 82.67%] [G loss: 0.820391]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 35/86 [loss: 0.332255, acc.: 91.36%] [G loss: 0.821137]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 36/86 [loss: 0.322402, acc.: 91.65%] [G loss: 0.786790]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 37/86 [loss: 0.390929, acc.: 85.94%] [G loss: 0.832873]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 38/86 [loss: 0.400587, acc.: 86.43%] [G loss: 0.855395]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 39/86 [loss: 0.377106, acc.: 88.43%] [G loss: 0.826160]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 40/86 [loss: 0.394648, acc.: 86.52%] [G loss: 0.849728]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 41/86 [loss: 0.317882, acc.: 92.24%] [G loss: 0.888990]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 42/86 [loss: 0.322372, acc.: 91.70%] [G loss: 0.892391]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 43/86 [loss: 0.234658, acc.: 96.09%] [G loss: 0.954242]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 44/86 [loss: 0.330931, acc.: 91.99%] [G loss: 0.874574]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 45/86 [loss: 0.344475, acc.: 90.58%] [G loss: 0.861853]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 46/86 [loss: 0.311517, acc.: 93.12%] [G loss: 0.881389]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 47/86 [loss: 0.307442, acc.: 93.21%] [G loss: 0.836691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 48/86 [loss: 0.378566, acc.: 87.74%] [G loss: 0.832198]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 49/86 [loss: 0.378823, acc.: 88.53%] [G loss: 0.818494]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 50/86 [loss: 0.377316, acc.: 88.23%] [G loss: 0.835300]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 51/86 [loss: 0.332915, acc.: 90.33%] [G loss: 0.791138]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 52/86 [loss: 0.494007, acc.: 77.54%] [G loss: 0.794078]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 53/86 [loss: 0.409835, acc.: 84.08%] [G loss: 0.880507]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 54/86 [loss: 0.326519, acc.: 92.24%] [G loss: 0.902343]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 55/86 [loss: 0.380500, acc.: 88.04%] [G loss: 0.860127]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 56/86 [loss: 0.260110, acc.: 95.80%] [G loss: 0.893244]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 57/86 [loss: 0.268038, acc.: 95.56%] [G loss: 0.882977]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 58/86 [loss: 0.271353, acc.: 95.46%] [G loss: 0.853141]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 59/86 [loss: 0.320996, acc.: 91.70%] [G loss: 0.881113]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 60/86 [loss: 0.256849, acc.: 95.51%] [G loss: 0.936557]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 61/86 [loss: 0.232063, acc.: 96.78%] [G loss: 0.920600]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 62/86 [loss: 0.233724, acc.: 96.24%] [G loss: 0.885851]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 63/86 [loss: 0.257676, acc.: 95.41%] [G loss: 0.872952]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 64/86 [loss: 0.194064, acc.: 98.29%] [G loss: 0.925446]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 65/86 [loss: 0.189010, acc.: 97.95%] [G loss: 0.905855]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 66/86 [loss: 0.191555, acc.: 97.46%] [G loss: 0.810151]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 67/86 [loss: 0.307368, acc.: 92.43%] [G loss: 0.834498]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 68/86 [loss: 0.456738, acc.: 82.37%] [G loss: 0.825456]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 69/86 [loss: 0.208934, acc.: 97.46%] [G loss: 0.863708]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 70/86 [loss: 0.327923, acc.: 92.04%] [G loss: 0.845205]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 71/86 [loss: 0.336635, acc.: 92.33%] [G loss: 0.870053]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 72/86 [loss: 0.339414, acc.: 90.67%] [G loss: 0.827812]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 73/86 [loss: 0.192877, acc.: 98.58%] [G loss: 0.819644]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 74/86 [loss: 0.377861, acc.: 86.08%] [G loss: 0.912982]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 75/86 [loss: 0.403630, acc.: 82.81%] [G loss: 0.772674]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 76/86 [loss: 0.345402, acc.: 90.67%] [G loss: 0.824233]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 77/86 [loss: 0.345788, acc.: 90.72%] [G loss: 0.877627]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 78/86 [loss: 0.333637, acc.: 90.77%] [G loss: 0.844251]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 79/86 [loss: 0.283027, acc.: 93.70%] [G loss: 0.924833]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 80/86 [loss: 0.268770, acc.: 95.65%] [G loss: 0.895412]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 47/200  Batch Size: 81/86 [loss: 0.251493, acc.: 95.90%] [G loss: 0.902984]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 82/86 [loss: 0.260019, acc.: 95.95%] [G loss: 0.867198]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 83/86 [loss: 0.287667, acc.: 93.85%] [G loss: 0.895068]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 47/200  Batch Size: 84/86 [loss: 0.241896, acc.: 96.44%] [G loss: 0.919244]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 47/200  Batch Size: 85/86 [loss: 0.260326, acc.: 95.46%] [G loss: 0.876600]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 0/86 [loss: 0.310417, acc.: 92.53%] [G loss: 0.869930]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 1/86 [loss: 0.236705, acc.: 96.53%] [G loss: 0.870548]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 2/86 [loss: 0.313769, acc.: 91.94%] [G loss: 0.905421]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 3/86 [loss: 0.285027, acc.: 94.19%] [G loss: 0.867056]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 4/86 [loss: 0.375835, acc.: 87.74%] [G loss: 0.847662]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 5/86 [loss: 0.289847, acc.: 93.36%] [G loss: 0.868365]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 6/86 [loss: 0.252720, acc.: 96.19%] [G loss: 0.905791]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 7/86 [loss: 0.222461, acc.: 96.83%] [G loss: 0.924002]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 8/86 [loss: 0.353941, acc.: 89.79%] [G loss: 0.909133]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 9/86 [loss: 0.415413, acc.: 86.04%] [G loss: 0.858743]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 10/86 [loss: 0.317308, acc.: 92.24%] [G loss: 0.837938]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 11/86 [loss: 0.266219, acc.: 95.36%] [G loss: 0.866376]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 12/86 [loss: 0.237956, acc.: 95.90%] [G loss: 0.940508]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 13/86 [loss: 0.255659, acc.: 95.65%] [G loss: 0.867404]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 14/86 [loss: 0.197513, acc.: 98.10%] [G loss: 0.873041]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 15/86 [loss: 0.359965, acc.: 89.26%] [G loss: 0.894875]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 16/86 [loss: 0.417350, acc.: 85.25%] [G loss: 0.838700]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 17/86 [loss: 0.305267, acc.: 92.87%] [G loss: 0.820356]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 18/86 [loss: 0.338483, acc.: 90.67%] [G loss: 0.836280]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 19/86 [loss: 0.460321, acc.: 81.64%] [G loss: 0.822156]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 20/86 [loss: 0.413827, acc.: 85.64%] [G loss: 0.794369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 21/86 [loss: 0.383837, acc.: 87.45%] [G loss: 0.806253]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 22/86 [loss: 0.384664, acc.: 88.04%] [G loss: 0.835129]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 23/86 [loss: 0.337866, acc.: 91.41%] [G loss: 0.853591]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 24/86 [loss: 0.323950, acc.: 91.06%] [G loss: 0.862307]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 25/86 [loss: 0.257981, acc.: 95.56%] [G loss: 0.912821]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 26/86 [loss: 0.282934, acc.: 93.51%] [G loss: 0.934391]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 27/86 [loss: 0.428364, acc.: 83.69%] [G loss: 0.863619]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 28/86 [loss: 0.277613, acc.: 94.09%] [G loss: 0.870625]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 29/86 [loss: 0.335057, acc.: 90.72%] [G loss: 0.884607]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 30/86 [loss: 0.270204, acc.: 94.63%] [G loss: 0.905699]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 31/86 [loss: 0.348572, acc.: 90.43%] [G loss: 0.863316]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 32/86 [loss: 0.269582, acc.: 95.02%] [G loss: 0.835766]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 33/86 [loss: 0.395276, acc.: 86.96%] [G loss: 0.825894]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 34/86 [loss: 0.302521, acc.: 92.53%] [G loss: 0.888381]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 35/86 [loss: 0.249350, acc.: 96.53%] [G loss: 0.867530]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 36/86 [loss: 0.254582, acc.: 96.19%] [G loss: 0.899504]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 37/86 [loss: 0.257629, acc.: 95.70%] [G loss: 0.876750]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 38/86 [loss: 0.225160, acc.: 96.29%] [G loss: 0.944775]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 39/86 [loss: 0.214061, acc.: 97.75%] [G loss: 0.860036]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 40/86 [loss: 0.268850, acc.: 94.43%] [G loss: 0.837328]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 41/86 [loss: 0.363808, acc.: 89.21%] [G loss: 0.802255]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 42/86 [loss: 0.249122, acc.: 95.95%] [G loss: 0.887450]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 43/86 [loss: 0.385993, acc.: 87.70%] [G loss: 0.855936]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 44/86 [loss: 0.321966, acc.: 92.53%] [G loss: 0.895164]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 45/86 [loss: 0.383330, acc.: 86.77%] [G loss: 0.933510]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 46/86 [loss: 0.315327, acc.: 92.63%] [G loss: 0.866929]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 47/86 [loss: 0.272486, acc.: 95.41%] [G loss: 0.881783]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 48/86 [loss: 0.356684, acc.: 88.33%] [G loss: 0.800990]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 49/86 [loss: 0.336795, acc.: 90.77%] [G loss: 0.844173]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 50/86 [loss: 0.405529, acc.: 86.87%] [G loss: 0.831194]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 51/86 [loss: 0.334848, acc.: 91.80%] [G loss: 0.861654]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 52/86 [loss: 0.405955, acc.: 85.21%] [G loss: 0.812156]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 53/86 [loss: 0.397036, acc.: 85.84%] [G loss: 0.835789]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 54/86 [loss: 0.308115, acc.: 93.16%] [G loss: 0.863262]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 55/86 [loss: 0.350272, acc.: 88.53%] [G loss: 0.939373]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 56/86 [loss: 0.274225, acc.: 94.19%] [G loss: 0.906883]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 57/86 [loss: 0.325106, acc.: 91.94%] [G loss: 0.849146]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 58/86 [loss: 0.223960, acc.: 97.36%] [G loss: 0.893081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 59/86 [loss: 0.269503, acc.: 93.36%] [G loss: 0.843678]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 60/86 [loss: 0.331502, acc.: 90.67%] [G loss: 0.868049]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 61/86 [loss: 0.332458, acc.: 91.99%] [G loss: 0.882014]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 62/86 [loss: 0.302794, acc.: 93.21%] [G loss: 0.843561]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 63/86 [loss: 0.277760, acc.: 94.24%] [G loss: 0.895151]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 64/86 [loss: 0.331369, acc.: 91.55%] [G loss: 0.876226]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 65/86 [loss: 0.310747, acc.: 91.85%] [G loss: 0.829163]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 66/86 [loss: 0.267036, acc.: 95.56%] [G loss: 0.891366]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 67/86 [loss: 0.255739, acc.: 95.70%] [G loss: 0.871737]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 68/86 [loss: 0.249316, acc.: 96.09%] [G loss: 0.918953]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 69/86 [loss: 0.325374, acc.: 91.80%] [G loss: 0.926866]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 70/86 [loss: 0.378700, acc.: 87.21%] [G loss: 0.821188]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 71/86 [loss: 0.292050, acc.: 93.90%] [G loss: 0.866788]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 72/86 [loss: 0.397022, acc.: 87.30%] [G loss: 0.874955]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 73/86 [loss: 0.288292, acc.: 93.70%] [G loss: 0.853299]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 74/86 [loss: 0.252767, acc.: 95.31%] [G loss: 0.883287]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 75/86 [loss: 0.369445, acc.: 87.35%] [G loss: 0.919077]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 76/86 [loss: 0.249444, acc.: 96.39%] [G loss: 0.882948]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 48/200  Batch Size: 77/86 [loss: 0.274621, acc.: 95.41%] [G loss: 0.843683]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 78/86 [loss: 0.314148, acc.: 92.19%] [G loss: 0.807319]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 79/86 [loss: 0.300963, acc.: 93.31%] [G loss: 0.863514]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 80/86 [loss: 0.506458, acc.: 76.46%] [G loss: 0.820291]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 81/86 [loss: 0.352030, acc.: 90.43%] [G loss: 0.825818]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 82/86 [loss: 0.281894, acc.: 93.90%] [G loss: 0.873189]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 48/200  Batch Size: 83/86 [loss: 0.301420, acc.: 92.19%] [G loss: 0.924081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 48/200  Batch Size: 84/86 [loss: 0.387876, acc.: 87.30%] [G loss: 0.890586]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 48/200  Batch Size: 85/86 [loss: 0.355891, acc.: 88.38%] [G loss: 0.840572]\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 0/86 [loss: 0.434990, acc.: 82.91%] [G loss: 0.799439]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 1/86 [loss: 0.470649, acc.: 79.54%] [G loss: 0.871982]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 2/86 [loss: 0.313894, acc.: 92.68%] [G loss: 0.872718]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 3/86 [loss: 0.401931, acc.: 86.47%] [G loss: 0.862640]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 4/86 [loss: 0.348669, acc.: 90.04%] [G loss: 0.902283]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 5/86 [loss: 0.329831, acc.: 92.33%] [G loss: 0.864704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 6/86 [loss: 0.330911, acc.: 91.41%] [G loss: 0.858198]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 7/86 [loss: 0.320438, acc.: 91.65%] [G loss: 0.827590]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 8/86 [loss: 0.287912, acc.: 92.48%] [G loss: 0.901675]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 9/86 [loss: 0.335281, acc.: 90.82%] [G loss: 0.853571]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 10/86 [loss: 0.301683, acc.: 93.70%] [G loss: 0.876217]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 11/86 [loss: 0.410221, acc.: 85.99%] [G loss: 0.869341]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 12/86 [loss: 0.310346, acc.: 92.58%] [G loss: 0.845624]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 13/86 [loss: 0.351834, acc.: 89.45%] [G loss: 0.809616]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 14/86 [loss: 0.276714, acc.: 94.87%] [G loss: 0.835486]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 15/86 [loss: 0.445021, acc.: 82.28%] [G loss: 0.811352]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 16/86 [loss: 0.415212, acc.: 84.72%] [G loss: 0.849429]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 17/86 [loss: 0.370451, acc.: 88.23%] [G loss: 0.882192]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 18/86 [loss: 0.446678, acc.: 82.71%] [G loss: 0.792134]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 19/86 [loss: 0.463160, acc.: 80.27%] [G loss: 0.826509]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 20/86 [loss: 0.474886, acc.: 79.69%] [G loss: 0.807243]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 21/86 [loss: 0.376509, acc.: 88.38%] [G loss: 0.864656]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 22/86 [loss: 0.427505, acc.: 83.89%] [G loss: 0.846321]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 23/86 [loss: 0.476719, acc.: 79.44%] [G loss: 0.822332]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 24/86 [loss: 0.295187, acc.: 92.14%] [G loss: 0.964271]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 25/86 [loss: 0.320334, acc.: 90.82%] [G loss: 0.952315]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 26/86 [loss: 0.262617, acc.: 96.24%] [G loss: 0.908017]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 27/86 [loss: 0.240257, acc.: 96.24%] [G loss: 0.924195]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 28/86 [loss: 0.194208, acc.: 97.90%] [G loss: 0.940626]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 29/86 [loss: 0.284086, acc.: 94.78%] [G loss: 0.899014]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 30/86 [loss: 0.224652, acc.: 97.46%] [G loss: 0.893238]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 31/86 [loss: 0.274090, acc.: 93.75%] [G loss: 0.889258]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 32/86 [loss: 0.238308, acc.: 96.24%] [G loss: 0.849094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 33/86 [loss: 0.242266, acc.: 96.19%] [G loss: 0.813006]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 34/86 [loss: 0.297343, acc.: 93.60%] [G loss: 0.844275]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 35/86 [loss: 0.270072, acc.: 95.17%] [G loss: 0.818418]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 36/86 [loss: 0.319172, acc.: 92.97%] [G loss: 0.823813]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 37/86 [loss: 0.341322, acc.: 91.94%] [G loss: 0.835763]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 38/86 [loss: 0.317774, acc.: 92.48%] [G loss: 0.881199]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 39/86 [loss: 0.421239, acc.: 84.28%] [G loss: 0.864853]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 40/86 [loss: 0.370662, acc.: 87.84%] [G loss: 0.863034]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 41/86 [loss: 0.368809, acc.: 88.43%] [G loss: 0.823430]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 42/86 [loss: 0.367048, acc.: 88.38%] [G loss: 0.890125]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 43/86 [loss: 0.452428, acc.: 81.93%] [G loss: 0.848962]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 44/86 [loss: 0.428024, acc.: 85.06%] [G loss: 0.848194]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 45/86 [loss: 0.396763, acc.: 85.16%] [G loss: 0.892284]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 46/86 [loss: 0.327668, acc.: 90.58%] [G loss: 0.834870]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 47/86 [loss: 0.370476, acc.: 88.04%] [G loss: 0.867398]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 48/86 [loss: 0.266881, acc.: 95.70%] [G loss: 0.871793]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 49/86 [loss: 0.295280, acc.: 93.80%] [G loss: 0.839103]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 50/86 [loss: 0.401964, acc.: 86.13%] [G loss: 0.853804]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 51/86 [loss: 0.290206, acc.: 93.21%] [G loss: 0.832613]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 52/86 [loss: 0.437792, acc.: 83.06%] [G loss: 0.822079]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 53/86 [loss: 0.432555, acc.: 83.69%] [G loss: 0.856586]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 54/86 [loss: 0.342625, acc.: 90.58%] [G loss: 0.827037]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 55/86 [loss: 0.308886, acc.: 92.82%] [G loss: 0.885413]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 49/200  Batch Size: 56/86 [loss: 0.296943, acc.: 93.31%] [G loss: 0.887933]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 57/86 [loss: 0.310872, acc.: 91.75%] [G loss: 0.932468]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 58/86 [loss: 0.284254, acc.: 94.58%] [G loss: 0.907730]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 59/86 [loss: 0.218621, acc.: 97.66%] [G loss: 0.916080]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 60/86 [loss: 0.295569, acc.: 93.36%] [G loss: 0.874187]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 61/86 [loss: 0.192085, acc.: 98.00%] [G loss: 0.936517]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 62/86 [loss: 0.267984, acc.: 94.53%] [G loss: 0.897712]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 63/86 [loss: 0.247119, acc.: 96.29%] [G loss: 0.894562]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 64/86 [loss: 0.371551, acc.: 88.82%] [G loss: 0.887180]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 65/86 [loss: 0.392681, acc.: 86.23%] [G loss: 0.856630]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 66/86 [loss: 0.375665, acc.: 88.38%] [G loss: 0.825572]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 67/86 [loss: 0.337605, acc.: 90.77%] [G loss: 0.827643]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 68/86 [loss: 0.332880, acc.: 92.38%] [G loss: 0.829620]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 69/86 [loss: 0.276016, acc.: 95.07%] [G loss: 0.858591]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 70/86 [loss: 0.403379, acc.: 85.69%] [G loss: 0.860441]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 71/86 [loss: 0.549081, acc.: 72.27%] [G loss: 0.759129]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 72/86 [loss: 0.478120, acc.: 79.05%] [G loss: 0.811600]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 73/86 [loss: 0.385616, acc.: 88.09%] [G loss: 0.834785]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 74/86 [loss: 0.380311, acc.: 87.60%] [G loss: 0.876769]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 75/86 [loss: 0.337872, acc.: 91.06%] [G loss: 0.883841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 76/86 [loss: 0.237439, acc.: 96.29%] [G loss: 0.938160]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 77/86 [loss: 0.266614, acc.: 95.80%] [G loss: 0.897894]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 78/86 [loss: 0.360136, acc.: 88.96%] [G loss: 0.874545]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 79/86 [loss: 0.326033, acc.: 90.67%] [G loss: 0.789352]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 80/86 [loss: 0.321013, acc.: 91.02%] [G loss: 0.887833]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 81/86 [loss: 0.388400, acc.: 87.70%] [G loss: 0.862149]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 82/86 [loss: 0.327818, acc.: 91.85%] [G loss: 0.853561]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 49/200  Batch Size: 83/86 [loss: 0.352675, acc.: 89.40%] [G loss: 0.795171]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 49/200  Batch Size: 84/86 [loss: 0.407243, acc.: 83.79%] [G loss: 0.930611]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 49/200  Batch Size: 85/86 [loss: 0.266445, acc.: 95.07%] [G loss: 0.945919]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 0/86 [loss: 0.190249, acc.: 98.58%] [G loss: 0.896788]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 1/86 [loss: 0.238700, acc.: 96.53%] [G loss: 0.883215]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 2/86 [loss: 0.256294, acc.: 94.92%] [G loss: 0.939085]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 3/86 [loss: 0.282524, acc.: 94.04%] [G loss: 0.889220]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 4/86 [loss: 0.246891, acc.: 96.48%] [G loss: 0.854371]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 5/86 [loss: 0.322947, acc.: 92.48%] [G loss: 0.838689]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 6/86 [loss: 0.270354, acc.: 94.24%] [G loss: 0.793113]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 7/86 [loss: 0.345902, acc.: 91.11%] [G loss: 0.834228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 8/86 [loss: 0.359788, acc.: 89.11%] [G loss: 0.844913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 9/86 [loss: 0.410807, acc.: 85.45%] [G loss: 0.858131]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 10/86 [loss: 0.416847, acc.: 83.35%] [G loss: 0.769320]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 11/86 [loss: 0.569917, acc.: 70.41%] [G loss: 0.847287]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 12/86 [loss: 0.382503, acc.: 87.26%] [G loss: 0.833023]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 13/86 [loss: 0.381577, acc.: 87.50%] [G loss: 0.831568]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 14/86 [loss: 0.429139, acc.: 85.01%] [G loss: 0.832982]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 15/86 [loss: 0.409402, acc.: 86.04%] [G loss: 0.850969]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 16/86 [loss: 0.364210, acc.: 87.94%] [G loss: 0.874801]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 17/86 [loss: 0.326660, acc.: 91.94%] [G loss: 0.850981]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 18/86 [loss: 0.341916, acc.: 90.67%] [G loss: 0.870607]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 19/86 [loss: 0.309312, acc.: 92.33%] [G loss: 0.929366]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 20/86 [loss: 0.369141, acc.: 89.26%] [G loss: 0.844675]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 21/86 [loss: 0.256266, acc.: 96.00%] [G loss: 0.873206]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 22/86 [loss: 0.300169, acc.: 94.04%] [G loss: 0.911786]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 23/86 [loss: 0.383758, acc.: 87.74%] [G loss: 0.898488]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 24/86 [loss: 0.262577, acc.: 95.36%] [G loss: 0.848747]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 25/86 [loss: 0.225234, acc.: 97.27%] [G loss: 0.857536]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 26/86 [loss: 0.286447, acc.: 93.99%] [G loss: 0.852432]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 27/86 [loss: 0.420769, acc.: 83.35%] [G loss: 0.891732]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 28/86 [loss: 0.380233, acc.: 88.33%] [G loss: 0.844370]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 29/86 [loss: 0.304545, acc.: 92.63%] [G loss: 0.798734]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 30/86 [loss: 0.252251, acc.: 95.26%] [G loss: 0.868365]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 31/86 [loss: 0.336999, acc.: 89.84%] [G loss: 0.913677]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 32/86 [loss: 0.316844, acc.: 92.04%] [G loss: 0.932438]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 33/86 [loss: 0.323562, acc.: 93.02%] [G loss: 0.907245]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 34/86 [loss: 0.191814, acc.: 98.05%] [G loss: 0.858025]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 35/86 [loss: 0.223254, acc.: 96.83%] [G loss: 0.889924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 36/86 [loss: 0.240409, acc.: 96.29%] [G loss: 0.908361]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 37/86 [loss: 0.315303, acc.: 91.50%] [G loss: 0.843641]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 38/86 [loss: 0.373653, acc.: 88.09%] [G loss: 0.878050]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 39/86 [loss: 0.325639, acc.: 89.36%] [G loss: 0.763901]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 40/86 [loss: 0.405703, acc.: 86.18%] [G loss: 0.810490]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 41/86 [loss: 0.315959, acc.: 92.19%] [G loss: 0.796077]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 42/86 [loss: 0.396967, acc.: 86.91%] [G loss: 0.837922]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 43/86 [loss: 0.372843, acc.: 86.13%] [G loss: 0.917561]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 44/86 [loss: 0.257552, acc.: 95.75%] [G loss: 0.919639]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 45/86 [loss: 0.336831, acc.: 91.60%] [G loss: 0.848775]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 46/86 [loss: 0.376463, acc.: 88.28%] [G loss: 0.846474]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 47/86 [loss: 0.257565, acc.: 95.70%] [G loss: 0.887464]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 48/86 [loss: 0.292414, acc.: 94.14%] [G loss: 0.872256]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 49/86 [loss: 0.327403, acc.: 91.41%] [G loss: 0.880866]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 50/86 [loss: 0.352517, acc.: 89.79%] [G loss: 0.860721]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 51/86 [loss: 0.354591, acc.: 89.21%] [G loss: 0.814130]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 52/86 [loss: 0.475348, acc.: 78.96%] [G loss: 0.827689]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 53/86 [loss: 0.347909, acc.: 90.04%] [G loss: 0.785802]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 54/86 [loss: 0.344692, acc.: 90.48%] [G loss: 0.880287]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 55/86 [loss: 0.391807, acc.: 87.11%] [G loss: 0.884621]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 56/86 [loss: 0.417440, acc.: 85.25%] [G loss: 0.843925]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch: 50/200  Batch Size: 57/86 [loss: 0.494248, acc.: 78.56%] [G loss: 0.876135]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 58/86 [loss: 0.297599, acc.: 93.36%] [G loss: 0.879996]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 59/86 [loss: 0.416381, acc.: 84.03%] [G loss: 0.875611]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 60/86 [loss: 0.324164, acc.: 92.04%] [G loss: 0.873896]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 61/86 [loss: 0.393741, acc.: 86.57%] [G loss: 0.821323]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 62/86 [loss: 0.306724, acc.: 92.82%] [G loss: 0.889362]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 63/86 [loss: 0.333949, acc.: 91.65%] [G loss: 0.917015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 64/86 [loss: 0.364233, acc.: 87.55%] [G loss: 0.928018]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 65/86 [loss: 0.405324, acc.: 83.25%] [G loss: 0.840506]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 66/86 [loss: 0.359443, acc.: 89.99%] [G loss: 0.849607]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 67/86 [loss: 0.323560, acc.: 92.48%] [G loss: 0.855579]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 68/86 [loss: 0.179607, acc.: 98.44%] [G loss: 0.882377]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 69/86 [loss: 0.398421, acc.: 87.45%] [G loss: 0.873466]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 70/86 [loss: 0.342481, acc.: 89.50%] [G loss: 0.921840]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 71/86 [loss: 0.274614, acc.: 94.97%] [G loss: 0.921721]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 72/86 [loss: 0.191446, acc.: 98.54%] [G loss: 0.920134]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 73/86 [loss: 0.269842, acc.: 95.51%] [G loss: 0.857251]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 74/86 [loss: 0.379777, acc.: 88.09%] [G loss: 0.804519]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 75/86 [loss: 0.288754, acc.: 93.80%] [G loss: 0.789213]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 76/86 [loss: 0.286859, acc.: 93.90%] [G loss: 0.797041]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 77/86 [loss: 0.386276, acc.: 87.60%] [G loss: 0.820475]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 78/86 [loss: 0.493641, acc.: 78.42%] [G loss: 0.833497]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 79/86 [loss: 0.533343, acc.: 73.34%] [G loss: 0.765601]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 50/200  Batch Size: 80/86 [loss: 0.322106, acc.: 91.31%] [G loss: 0.828167]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 81/86 [loss: 0.341584, acc.: 89.55%] [G loss: 0.930696]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 50/200  Batch Size: 82/86 [loss: 0.301044, acc.: 93.75%] [G loss: 0.873727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 50/200  Batch Size: 83/86 [loss: 0.289503, acc.: 94.09%] [G loss: 0.866705]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 84/86 [loss: 0.234723, acc.: 96.53%] [G loss: 0.900930]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 50/200  Batch Size: 85/86 [loss: 0.299234, acc.: 93.70%] [G loss: 0.876068]\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 51/200  Batch Size: 0/86 [loss: 0.375003, acc.: 87.99%] [G loss: 0.875696]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 1/86 [loss: 0.243274, acc.: 96.39%] [G loss: 0.879534]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 2/86 [loss: 0.263481, acc.: 95.75%] [G loss: 0.847251]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 3/86 [loss: 0.278644, acc.: 94.53%] [G loss: 0.871015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 4/86 [loss: 0.328204, acc.: 91.55%] [G loss: 0.849943]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 5/86 [loss: 0.314842, acc.: 92.77%] [G loss: 0.840307]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 6/86 [loss: 0.371955, acc.: 89.60%] [G loss: 0.816077]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 7/86 [loss: 0.344560, acc.: 90.87%] [G loss: 0.848840]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 8/86 [loss: 0.326798, acc.: 91.70%] [G loss: 0.910834]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 9/86 [loss: 0.245246, acc.: 96.53%] [G loss: 0.852333]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 10/86 [loss: 0.330201, acc.: 91.26%] [G loss: 0.932992]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 11/86 [loss: 0.244031, acc.: 96.39%] [G loss: 0.862709]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 12/86 [loss: 0.243583, acc.: 96.88%] [G loss: 0.839405]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 13/86 [loss: 0.300840, acc.: 93.46%] [G loss: 0.883232]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 14/86 [loss: 0.309878, acc.: 92.04%] [G loss: 0.932541]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 15/86 [loss: 0.216707, acc.: 96.88%] [G loss: 0.850194]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 51/200  Batch Size: 16/86 [loss: 0.295696, acc.: 94.24%] [G loss: 0.847489]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 17/86 [loss: 0.241143, acc.: 97.02%] [G loss: 0.855837]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 18/86 [loss: 0.294731, acc.: 94.14%] [G loss: 0.815742]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 51/200  Batch Size: 19/86 [loss: 0.388762, acc.: 85.30%] [G loss: 0.774081]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 20/86 [loss: 0.490107, acc.: 78.56%] [G loss: 0.811982]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 21/86 [loss: 0.282921, acc.: 94.09%] [G loss: 0.890396]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 22/86 [loss: 0.297506, acc.: 93.85%] [G loss: 0.896546]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 23/86 [loss: 0.371915, acc.: 89.11%] [G loss: 0.869795]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 24/86 [loss: 0.361801, acc.: 89.84%] [G loss: 0.895217]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 25/86 [loss: 0.311936, acc.: 93.46%] [G loss: 0.857219]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 26/86 [loss: 0.341993, acc.: 90.82%] [G loss: 0.838571]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 27/86 [loss: 0.307267, acc.: 92.72%] [G loss: 0.890773]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 28/86 [loss: 0.330093, acc.: 91.46%] [G loss: 0.922985]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 29/86 [loss: 0.207613, acc.: 97.80%] [G loss: 0.936300]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 30/86 [loss: 0.220829, acc.: 97.22%] [G loss: 0.904026]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 31/86 [loss: 0.245766, acc.: 96.39%] [G loss: 0.921732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 32/86 [loss: 0.272674, acc.: 94.48%] [G loss: 0.850790]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 51/200  Batch Size: 33/86 [loss: 0.272433, acc.: 95.75%] [G loss: 0.853471]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 34/86 [loss: 0.352520, acc.: 90.92%] [G loss: 0.854495]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 35/86 [loss: 0.341450, acc.: 90.28%] [G loss: 0.858809]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 36/86 [loss: 0.401289, acc.: 85.74%] [G loss: 0.839788]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 37/86 [loss: 0.298655, acc.: 91.94%] [G loss: 0.774092]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 38/86 [loss: 0.329759, acc.: 91.11%] [G loss: 0.834438]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 39/86 [loss: 0.340407, acc.: 90.19%] [G loss: 0.813316]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 40/86 [loss: 0.469140, acc.: 79.74%] [G loss: 0.827221]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 41/86 [loss: 0.414549, acc.: 85.55%] [G loss: 0.837347]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 42/86 [loss: 0.366871, acc.: 88.57%] [G loss: 0.871511]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 43/86 [loss: 0.406840, acc.: 86.43%] [G loss: 0.806211]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 44/86 [loss: 0.416827, acc.: 85.01%] [G loss: 0.815056]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 45/86 [loss: 0.304145, acc.: 93.65%] [G loss: 0.860704]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 46/86 [loss: 0.346161, acc.: 90.77%] [G loss: 0.842962]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 47/86 [loss: 0.258265, acc.: 95.41%] [G loss: 0.895474]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 48/86 [loss: 0.251833, acc.: 96.24%] [G loss: 0.903345]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 49/86 [loss: 0.251418, acc.: 95.90%] [G loss: 0.866915]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 50/86 [loss: 0.316641, acc.: 93.07%] [G loss: 0.860221]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 51/200  Batch Size: 51/86 [loss: 0.434685, acc.: 82.13%] [G loss: 0.861845]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 52/86 [loss: 0.296283, acc.: 93.75%] [G loss: 0.875879]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 53/86 [loss: 0.281655, acc.: 95.02%] [G loss: 0.899907]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 54/86 [loss: 0.334134, acc.: 91.02%] [G loss: 0.875844]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 55/86 [loss: 0.256848, acc.: 95.41%] [G loss: 0.916560]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 56/86 [loss: 0.258375, acc.: 95.07%] [G loss: 0.859898]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 57/86 [loss: 0.227292, acc.: 96.48%] [G loss: 0.901642]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 58/86 [loss: 0.288109, acc.: 94.43%] [G loss: 0.942094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 59/86 [loss: 0.208575, acc.: 97.80%] [G loss: 0.911624]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 60/86 [loss: 0.277289, acc.: 93.90%] [G loss: 0.934603]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 61/86 [loss: 0.271917, acc.: 94.04%] [G loss: 0.887880]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 62/86 [loss: 0.282785, acc.: 93.99%] [G loss: 0.826530]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 63/86 [loss: 0.380318, acc.: 88.04%] [G loss: 0.854832]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 64/86 [loss: 0.357655, acc.: 89.84%] [G loss: 0.811522]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 65/86 [loss: 0.377264, acc.: 87.01%] [G loss: 0.804647]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 66/86 [loss: 0.385147, acc.: 87.30%] [G loss: 0.817637]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 67/86 [loss: 0.349270, acc.: 90.38%] [G loss: 0.875753]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 68/86 [loss: 0.317275, acc.: 92.19%] [G loss: 0.897631]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 69/86 [loss: 0.497814, acc.: 78.17%] [G loss: 0.858363]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 70/86 [loss: 0.388609, acc.: 86.62%] [G loss: 0.832820]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 51/200  Batch Size: 71/86 [loss: 0.404168, acc.: 87.11%] [G loss: 0.845295]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 72/86 [loss: 0.409798, acc.: 85.45%] [G loss: 0.885062]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 73/86 [loss: 0.309721, acc.: 93.12%] [G loss: 0.868697]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 74/86 [loss: 0.283073, acc.: 93.95%] [G loss: 0.907391]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 75/86 [loss: 0.279114, acc.: 95.17%] [G loss: 0.860656]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 76/86 [loss: 0.340354, acc.: 90.04%] [G loss: 0.937855]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 77/86 [loss: 0.407688, acc.: 85.45%] [G loss: 0.829356]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 78/86 [loss: 0.336087, acc.: 91.50%] [G loss: 0.855880]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 79/86 [loss: 0.324164, acc.: 92.14%] [G loss: 0.851223]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 80/86 [loss: 0.265969, acc.: 95.65%] [G loss: 0.883025]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 51/200  Batch Size: 81/86 [loss: 0.277989, acc.: 94.97%] [G loss: 0.921307]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 82/86 [loss: 0.201931, acc.: 97.56%] [G loss: 0.879542]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 83/86 [loss: 0.291007, acc.: 94.24%] [G loss: 0.864094]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 51/200  Batch Size: 84/86 [loss: 0.278087, acc.: 95.61%] [G loss: 0.862127]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 51/200  Batch Size: 85/86 [loss: 0.264639, acc.: 95.56%] [G loss: 0.857275]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 0/86 [loss: 0.252917, acc.: 96.00%] [G loss: 0.899556]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 1/86 [loss: 0.319509, acc.: 93.51%] [G loss: 0.859727]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 2/86 [loss: 0.339758, acc.: 91.06%] [G loss: 0.867349]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 3/86 [loss: 0.278065, acc.: 93.80%] [G loss: 0.795689]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 4/86 [loss: 0.431352, acc.: 82.37%] [G loss: 0.763250]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 5/86 [loss: 0.459871, acc.: 81.10%] [G loss: 0.788683]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 6/86 [loss: 0.463884, acc.: 80.86%] [G loss: 0.847430]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 7/86 [loss: 0.362792, acc.: 89.79%] [G loss: 0.869081]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 8/86 [loss: 0.494508, acc.: 78.96%] [G loss: 0.845352]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 9/86 [loss: 0.320796, acc.: 92.38%] [G loss: 0.851448]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 10/86 [loss: 0.357453, acc.: 90.92%] [G loss: 0.857197]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 11/86 [loss: 0.410013, acc.: 85.06%] [G loss: 0.872145]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 12/86 [loss: 0.433653, acc.: 79.54%] [G loss: 0.959096]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 13/86 [loss: 0.344696, acc.: 90.38%] [G loss: 0.899831]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 14/86 [loss: 0.215104, acc.: 97.61%] [G loss: 0.879548]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 15/86 [loss: 0.273930, acc.: 95.61%] [G loss: 0.870029]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 16/86 [loss: 0.207575, acc.: 97.95%] [G loss: 0.891547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 17/86 [loss: 0.281043, acc.: 94.58%] [G loss: 0.895323]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 18/86 [loss: 0.348728, acc.: 91.50%] [G loss: 0.859193]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 19/86 [loss: 0.276836, acc.: 94.82%] [G loss: 0.890654]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 20/86 [loss: 0.233240, acc.: 96.73%] [G loss: 0.861422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 21/86 [loss: 0.304652, acc.: 93.95%] [G loss: 0.864107]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 22/86 [loss: 0.286172, acc.: 94.63%] [G loss: 0.845184]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 23/86 [loss: 0.413355, acc.: 85.50%] [G loss: 0.835256]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 24/86 [loss: 0.374028, acc.: 88.43%] [G loss: 0.830350]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 25/86 [loss: 0.337575, acc.: 90.53%] [G loss: 0.862974]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 26/86 [loss: 0.443825, acc.: 82.23%] [G loss: 0.796933]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 27/86 [loss: 0.388319, acc.: 86.96%] [G loss: 0.787705]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 28/86 [loss: 0.327239, acc.: 91.50%] [G loss: 0.843350]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 29/86 [loss: 0.364429, acc.: 89.45%] [G loss: 0.865244]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 30/86 [loss: 0.319391, acc.: 92.24%] [G loss: 0.890396]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 31/86 [loss: 0.341521, acc.: 90.97%] [G loss: 0.886266]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 32/86 [loss: 0.216690, acc.: 96.73%] [G loss: 0.849946]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 33/86 [loss: 0.321859, acc.: 91.85%] [G loss: 0.827532]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 34/86 [loss: 0.343516, acc.: 91.70%] [G loss: 0.890365]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 35/86 [loss: 0.365349, acc.: 89.89%] [G loss: 0.859038]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 36/86 [loss: 0.318767, acc.: 90.38%] [G loss: 0.951700]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 37/86 [loss: 0.280613, acc.: 95.07%] [G loss: 0.900314]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 38/86 [loss: 0.204704, acc.: 98.10%] [G loss: 0.927458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 39/86 [loss: 0.263276, acc.: 95.21%] [G loss: 0.917370]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 40/86 [loss: 0.228046, acc.: 96.83%] [G loss: 0.930791]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 41/86 [loss: 0.182788, acc.: 98.54%] [G loss: 0.924707]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 42/86 [loss: 0.227076, acc.: 96.68%] [G loss: 0.819075]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 43/86 [loss: 0.305194, acc.: 93.75%] [G loss: 0.865950]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 44/86 [loss: 0.250554, acc.: 96.04%] [G loss: 0.865742]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 45/86 [loss: 0.251489, acc.: 96.29%] [G loss: 0.888704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 46/86 [loss: 0.235682, acc.: 96.92%] [G loss: 0.831138]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 47/86 [loss: 0.288205, acc.: 94.48%] [G loss: 0.871222]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 48/86 [loss: 0.302408, acc.: 93.51%] [G loss: 0.846701]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 49/86 [loss: 0.236785, acc.: 96.53%] [G loss: 0.843499]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 50/86 [loss: 0.320436, acc.: 91.41%] [G loss: 0.788058]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 51/86 [loss: 0.316357, acc.: 90.48%] [G loss: 0.936389]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 52/86 [loss: 0.351329, acc.: 90.67%] [G loss: 0.902498]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 53/86 [loss: 0.240252, acc.: 96.92%] [G loss: 0.878600]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 54/86 [loss: 0.288559, acc.: 94.92%] [G loss: 0.902430]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 55/86 [loss: 0.289601, acc.: 93.21%] [G loss: 0.893145]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 56/86 [loss: 0.280905, acc.: 94.19%] [G loss: 0.846342]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 57/86 [loss: 0.311671, acc.: 91.75%] [G loss: 0.811849]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 58/86 [loss: 0.326501, acc.: 91.60%] [G loss: 0.812689]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 59/86 [loss: 0.309605, acc.: 93.36%] [G loss: 0.921326]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 60/86 [loss: 0.340830, acc.: 90.92%] [G loss: 0.881906]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 61/86 [loss: 0.304931, acc.: 93.80%] [G loss: 0.855895]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 62/86 [loss: 0.342843, acc.: 90.72%] [G loss: 0.839929]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 63/86 [loss: 0.243560, acc.: 97.02%] [G loss: 0.851224]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 64/86 [loss: 0.363667, acc.: 89.36%] [G loss: 0.873447]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 65/86 [loss: 0.470161, acc.: 79.88%] [G loss: 0.789971]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 66/86 [loss: 0.299870, acc.: 93.80%] [G loss: 0.845820]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 67/86 [loss: 0.331465, acc.: 92.04%] [G loss: 0.834594]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 68/86 [loss: 0.382428, acc.: 89.01%] [G loss: 0.841234]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 69/86 [loss: 0.600606, acc.: 68.36%] [G loss: 0.858002]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 70/86 [loss: 0.296945, acc.: 93.55%] [G loss: 0.838565]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 52/200  Batch Size: 71/86 [loss: 0.384431, acc.: 87.11%] [G loss: 0.871269]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 72/86 [loss: 0.369640, acc.: 89.31%] [G loss: 0.875359]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 73/86 [loss: 0.318406, acc.: 92.68%] [G loss: 0.866453]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 74/86 [loss: 0.333603, acc.: 91.11%] [G loss: 0.898824]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 75/86 [loss: 0.280650, acc.: 93.21%] [G loss: 0.909093]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 76/86 [loss: 0.192650, acc.: 98.14%] [G loss: 0.921791]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 77/86 [loss: 0.158616, acc.: 99.27%] [G loss: 1.008834]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 78/86 [loss: 0.249911, acc.: 95.90%] [G loss: 0.936976]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 79/86 [loss: 0.241963, acc.: 96.04%] [G loss: 0.902856]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 52/200  Batch Size: 80/86 [loss: 0.228874, acc.: 96.92%] [G loss: 0.892667]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 81/86 [loss: 0.277368, acc.: 94.19%] [G loss: 0.810146]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 82/86 [loss: 0.309239, acc.: 92.38%] [G loss: 0.805958]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 52/200  Batch Size: 83/86 [loss: 0.288148, acc.: 94.58%] [G loss: 0.817387]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 84/86 [loss: 0.226846, acc.: 97.12%] [G loss: 0.879421]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 52/200  Batch Size: 85/86 [loss: 0.340894, acc.: 91.02%] [G loss: 0.910055]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 0/86 [loss: 0.310100, acc.: 92.82%] [G loss: 0.837268]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 1/86 [loss: 0.378499, acc.: 88.18%] [G loss: 0.863833]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 2/86 [loss: 0.366183, acc.: 89.01%] [G loss: 0.814460]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 3/86 [loss: 0.437011, acc.: 82.37%] [G loss: 0.796169]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 4/86 [loss: 0.284651, acc.: 93.80%] [G loss: 0.866019]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 5/86 [loss: 0.333424, acc.: 91.06%] [G loss: 0.877346]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 6/86 [loss: 0.511034, acc.: 76.32%] [G loss: 0.830880]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 7/86 [loss: 0.259279, acc.: 96.34%] [G loss: 0.856522]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 8/86 [loss: 0.349649, acc.: 88.87%] [G loss: 0.922049]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 9/86 [loss: 0.261122, acc.: 95.70%] [G loss: 0.883106]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 10/86 [loss: 0.297937, acc.: 93.16%] [G loss: 0.923424]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 11/86 [loss: 0.296426, acc.: 93.80%] [G loss: 0.917887]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 12/86 [loss: 0.203564, acc.: 97.80%] [G loss: 0.928832]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 13/86 [loss: 0.245649, acc.: 95.65%] [G loss: 0.929304]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 14/86 [loss: 0.214747, acc.: 96.97%] [G loss: 0.890069]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 15/86 [loss: 0.186596, acc.: 98.29%] [G loss: 0.812282]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 16/86 [loss: 0.288289, acc.: 94.14%] [G loss: 0.875206]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 17/86 [loss: 0.276630, acc.: 93.46%] [G loss: 0.952248]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 18/86 [loss: 0.250454, acc.: 95.31%] [G loss: 0.882649]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 19/86 [loss: 0.354150, acc.: 88.38%] [G loss: 0.775338]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 20/86 [loss: 0.385182, acc.: 86.47%] [G loss: 0.760864]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 21/86 [loss: 0.344425, acc.: 89.94%] [G loss: 0.786477]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 22/86 [loss: 0.540236, acc.: 74.27%] [G loss: 0.828815]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 23/86 [loss: 0.425362, acc.: 82.96%] [G loss: 0.778155]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 24/86 [loss: 0.439171, acc.: 82.81%] [G loss: 0.785539]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 25/86 [loss: 0.409178, acc.: 85.64%] [G loss: 0.805017]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 26/86 [loss: 0.353918, acc.: 90.82%] [G loss: 0.827618]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 27/86 [loss: 0.341217, acc.: 91.02%] [G loss: 0.816286]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 28/86 [loss: 0.242343, acc.: 94.43%] [G loss: 0.985354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 29/86 [loss: 0.268670, acc.: 95.36%] [G loss: 0.954695]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 30/86 [loss: 0.269166, acc.: 94.63%] [G loss: 0.966112]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 31/86 [loss: 0.348964, acc.: 89.45%] [G loss: 0.864819]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 32/86 [loss: 0.253719, acc.: 95.75%] [G loss: 0.838600]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 33/86 [loss: 0.250979, acc.: 95.75%] [G loss: 0.886196]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 34/86 [loss: 0.314128, acc.: 91.80%] [G loss: 0.953275]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 35/86 [loss: 0.209185, acc.: 97.90%] [G loss: 0.858880]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 36/86 [loss: 0.288696, acc.: 94.82%] [G loss: 0.871586]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 37/86 [loss: 0.247034, acc.: 96.44%] [G loss: 0.913915]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 38/86 [loss: 0.264859, acc.: 95.56%] [G loss: 0.867016]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 39/86 [loss: 0.229993, acc.: 97.07%] [G loss: 0.861618]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 40/86 [loss: 0.421120, acc.: 83.64%] [G loss: 0.793555]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 41/86 [loss: 0.334084, acc.: 91.60%] [G loss: 0.886258]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 42/86 [loss: 0.351330, acc.: 90.58%] [G loss: 0.866080]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 43/86 [loss: 0.569752, acc.: 69.73%] [G loss: 0.836385]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 44/86 [loss: 0.320533, acc.: 92.14%] [G loss: 0.811534]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 45/86 [loss: 0.397308, acc.: 87.11%] [G loss: 0.799168]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 46/86 [loss: 0.332613, acc.: 90.72%] [G loss: 0.891434]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 47/86 [loss: 0.487104, acc.: 78.81%] [G loss: 0.887014]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 48/86 [loss: 0.222571, acc.: 97.41%] [G loss: 0.847214]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 49/86 [loss: 0.354735, acc.: 90.67%] [G loss: 0.879777]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 50/86 [loss: 0.321875, acc.: 91.99%] [G loss: 0.871431]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 51/86 [loss: 0.293237, acc.: 93.51%] [G loss: 0.879803]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 52/86 [loss: 0.289974, acc.: 94.87%] [G loss: 0.892087]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 53/86 [loss: 0.291241, acc.: 92.87%] [G loss: 0.799771]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 54/86 [loss: 0.301553, acc.: 93.02%] [G loss: 0.797968]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 55/86 [loss: 0.448825, acc.: 82.47%] [G loss: 0.814523]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 56/86 [loss: 0.252668, acc.: 95.56%] [G loss: 0.858235]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 57/86 [loss: 0.287409, acc.: 93.95%] [G loss: 0.838243]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 58/86 [loss: 0.267861, acc.: 95.85%] [G loss: 0.868854]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 59/86 [loss: 0.259164, acc.: 94.87%] [G loss: 0.954513]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 60/86 [loss: 0.261331, acc.: 95.46%] [G loss: 0.932921]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 61/86 [loss: 0.266580, acc.: 95.65%] [G loss: 0.949808]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 62/86 [loss: 0.404132, acc.: 85.21%] [G loss: 0.805977]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 63/86 [loss: 0.307315, acc.: 93.16%] [G loss: 0.823456]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 64/86 [loss: 0.246086, acc.: 96.88%] [G loss: 0.853135]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 65/86 [loss: 0.314623, acc.: 92.87%] [G loss: 0.888035]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 66/86 [loss: 0.426877, acc.: 84.13%] [G loss: 0.869605]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 67/86 [loss: 0.256541, acc.: 96.14%] [G loss: 0.896666]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 68/86 [loss: 0.249342, acc.: 96.00%] [G loss: 0.889626]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 69/86 [loss: 0.264474, acc.: 94.87%] [G loss: 0.891874]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 70/86 [loss: 0.184945, acc.: 97.75%] [G loss: 1.007942]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 71/86 [loss: 0.187425, acc.: 98.10%] [G loss: 1.009475]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 72/86 [loss: 0.201565, acc.: 97.41%] [G loss: 0.860057]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 73/86 [loss: 0.277893, acc.: 94.48%] [G loss: 0.882252]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 74/86 [loss: 0.224675, acc.: 97.27%] [G loss: 0.877596]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 75/86 [loss: 0.295237, acc.: 93.70%] [G loss: 0.881700]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 76/86 [loss: 0.268280, acc.: 96.19%] [G loss: 0.875798]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 77/86 [loss: 0.237221, acc.: 95.02%] [G loss: 0.805434]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 78/86 [loss: 0.366930, acc.: 89.65%] [G loss: 0.873911]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 79/86 [loss: 0.506757, acc.: 76.86%] [G loss: 0.821109]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 53/200  Batch Size: 80/86 [loss: 0.245867, acc.: 96.73%] [G loss: 0.848224]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 53/200  Batch Size: 81/86 [loss: 0.366540, acc.: 89.55%] [G loss: 0.838855]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 82/86 [loss: 0.302100, acc.: 93.21%] [G loss: 0.849993]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 83/86 [loss: 0.488056, acc.: 79.15%] [G loss: 0.800933]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 53/200  Batch Size: 84/86 [loss: 0.437278, acc.: 82.28%] [G loss: 0.773591]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 53/200  Batch Size: 85/86 [loss: 0.346671, acc.: 90.48%] [G loss: 0.832991]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 0/86 [loss: 0.434075, acc.: 83.40%] [G loss: 0.843356]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 1/86 [loss: 0.363118, acc.: 89.40%] [G loss: 0.921920]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 2/86 [loss: 0.554348, acc.: 72.31%] [G loss: 0.893984]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 3/86 [loss: 0.398352, acc.: 87.16%] [G loss: 0.853578]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 4/86 [loss: 0.282982, acc.: 94.38%] [G loss: 0.805602]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 5/86 [loss: 0.345077, acc.: 90.48%] [G loss: 0.889498]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 6/86 [loss: 0.318492, acc.: 91.55%] [G loss: 0.839241]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 7/86 [loss: 0.351632, acc.: 88.23%] [G loss: 0.920189]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 8/86 [loss: 0.329807, acc.: 91.02%] [G loss: 0.923774]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 9/86 [loss: 0.293316, acc.: 93.80%] [G loss: 0.904883]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 10/86 [loss: 0.382969, acc.: 85.79%] [G loss: 0.808210]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 11/86 [loss: 0.185147, acc.: 98.05%] [G loss: 0.922667]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 12/86 [loss: 0.334643, acc.: 92.19%] [G loss: 0.867385]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 13/86 [loss: 0.236716, acc.: 97.07%] [G loss: 0.906298]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 14/86 [loss: 0.242747, acc.: 96.00%] [G loss: 0.955085]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 15/86 [loss: 0.245484, acc.: 96.29%] [G loss: 0.867979]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 16/86 [loss: 0.269842, acc.: 95.07%] [G loss: 0.884132]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 17/86 [loss: 0.295339, acc.: 94.29%] [G loss: 0.871276]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 18/86 [loss: 0.261664, acc.: 95.26%] [G loss: 0.920615]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 19/86 [loss: 0.305600, acc.: 93.55%] [G loss: 0.862869]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 20/86 [loss: 0.301473, acc.: 91.75%] [G loss: 0.825735]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 21/86 [loss: 0.335028, acc.: 91.21%] [G loss: 0.908217]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 22/86 [loss: 0.365882, acc.: 88.96%] [G loss: 0.821754]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 23/86 [loss: 0.238457, acc.: 96.73%] [G loss: 0.886020]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 24/86 [loss: 0.265585, acc.: 95.17%] [G loss: 0.897588]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 25/86 [loss: 0.329356, acc.: 91.94%] [G loss: 0.941370]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 26/86 [loss: 0.195236, acc.: 98.63%] [G loss: 0.894395]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 27/86 [loss: 0.270634, acc.: 95.51%] [G loss: 0.883579]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 28/86 [loss: 0.299339, acc.: 93.75%] [G loss: 0.914171]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 29/86 [loss: 0.262104, acc.: 95.70%] [G loss: 0.861777]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 30/86 [loss: 0.245436, acc.: 96.58%] [G loss: 0.847671]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 31/86 [loss: 0.271452, acc.: 94.73%] [G loss: 0.876481]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 32/86 [loss: 0.278130, acc.: 95.26%] [G loss: 0.833826]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 33/86 [loss: 0.392927, acc.: 85.74%] [G loss: 0.828604]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 34/86 [loss: 0.473664, acc.: 79.93%] [G loss: 0.840488]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 35/86 [loss: 0.248154, acc.: 95.75%] [G loss: 0.866502]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 36/86 [loss: 0.315922, acc.: 92.68%] [G loss: 0.862833]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 37/86 [loss: 0.337786, acc.: 90.23%] [G loss: 0.849102]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 38/86 [loss: 0.397295, acc.: 87.06%] [G loss: 0.837426]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 39/86 [loss: 0.370344, acc.: 88.33%] [G loss: 0.886984]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 40/86 [loss: 0.304515, acc.: 92.77%] [G loss: 0.833049]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 41/86 [loss: 0.364541, acc.: 90.23%] [G loss: 0.903189]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 42/86 [loss: 0.269882, acc.: 94.68%] [G loss: 0.891172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 43/86 [loss: 0.273385, acc.: 94.48%] [G loss: 0.851048]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 44/86 [loss: 0.289742, acc.: 94.92%] [G loss: 0.848090]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 45/86 [loss: 0.304617, acc.: 93.85%] [G loss: 0.871578]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 46/86 [loss: 0.331277, acc.: 92.48%] [G loss: 0.842075]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 47/86 [loss: 0.315902, acc.: 93.16%] [G loss: 0.804189]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 48/86 [loss: 0.288809, acc.: 93.55%] [G loss: 0.820444]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 49/86 [loss: 0.250848, acc.: 96.19%] [G loss: 0.879824]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 50/86 [loss: 0.266442, acc.: 93.95%] [G loss: 0.940099]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 51/86 [loss: 0.290096, acc.: 94.04%] [G loss: 0.939255]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 52/86 [loss: 0.230958, acc.: 96.44%] [G loss: 0.866335]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 53/86 [loss: 0.256098, acc.: 95.80%] [G loss: 0.909992]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 54/86 [loss: 0.265324, acc.: 95.31%] [G loss: 0.924507]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 55/86 [loss: 0.312283, acc.: 92.92%] [G loss: 0.876153]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 56/86 [loss: 0.216775, acc.: 97.71%] [G loss: 0.896818]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 57/86 [loss: 0.285156, acc.: 94.14%] [G loss: 0.920666]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 58/86 [loss: 0.257433, acc.: 96.09%] [G loss: 0.867159]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 59/86 [loss: 0.317950, acc.: 91.65%] [G loss: 0.918366]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 60/86 [loss: 0.447923, acc.: 79.79%] [G loss: 0.795120]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 61/86 [loss: 0.339907, acc.: 89.84%] [G loss: 0.785844]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 62/86 [loss: 0.467559, acc.: 80.18%] [G loss: 0.763118]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 63/86 [loss: 0.407107, acc.: 85.94%] [G loss: 0.816724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 64/86 [loss: 0.365238, acc.: 89.01%] [G loss: 0.832448]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 65/86 [loss: 0.406259, acc.: 83.54%] [G loss: 0.923831]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 66/86 [loss: 0.412441, acc.: 85.64%] [G loss: 0.826757]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 67/86 [loss: 0.336993, acc.: 91.06%] [G loss: 0.831562]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 68/86 [loss: 0.227049, acc.: 97.51%] [G loss: 0.852307]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 69/86 [loss: 0.267537, acc.: 96.04%] [G loss: 0.861744]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 70/86 [loss: 0.418899, acc.: 84.42%] [G loss: 0.825567]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 71/86 [loss: 0.466975, acc.: 80.62%] [G loss: 0.801522]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 72/86 [loss: 0.294908, acc.: 93.70%] [G loss: 0.865597]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 73/86 [loss: 0.273298, acc.: 94.09%] [G loss: 0.908709]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 74/86 [loss: 0.268433, acc.: 94.48%] [G loss: 0.937906]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 75/86 [loss: 0.319160, acc.: 92.09%] [G loss: 0.928813]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 76/86 [loss: 0.272043, acc.: 95.21%] [G loss: 0.881449]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 77/86 [loss: 0.233209, acc.: 96.83%] [G loss: 0.854267]\n",
      "32/32 [==============================] - 0s 7ms/step\n",
      "Epoch: 54/200  Batch Size: 78/86 [loss: 0.325211, acc.: 87.94%] [G loss: 1.002300]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 79/86 [loss: 0.259838, acc.: 94.73%] [G loss: 0.871969]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 80/86 [loss: 0.291176, acc.: 93.99%] [G loss: 0.849966]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 54/200  Batch Size: 81/86 [loss: 0.249353, acc.: 95.70%] [G loss: 0.879346]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 54/200  Batch Size: 82/86 [loss: 0.223925, acc.: 97.17%] [G loss: 0.878924]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 83/86 [loss: 0.252027, acc.: 95.80%] [G loss: 0.797694]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 54/200  Batch Size: 84/86 [loss: 0.355650, acc.: 89.50%] [G loss: 0.790616]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 54/200  Batch Size: 85/86 [loss: 0.395262, acc.: 87.06%] [G loss: 0.841198]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 0/86 [loss: 0.309867, acc.: 92.58%] [G loss: 0.838528]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 1/86 [loss: 0.343746, acc.: 90.04%] [G loss: 0.864755]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 2/86 [loss: 0.261296, acc.: 95.80%] [G loss: 0.849713]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 3/86 [loss: 0.336902, acc.: 91.36%] [G loss: 0.852759]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 4/86 [loss: 0.316276, acc.: 92.63%] [G loss: 0.859544]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 5/86 [loss: 0.297126, acc.: 94.09%] [G loss: 0.861086]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 6/86 [loss: 0.267160, acc.: 94.58%] [G loss: 0.928777]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 7/86 [loss: 0.227214, acc.: 96.78%] [G loss: 0.931594]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 8/86 [loss: 0.208732, acc.: 97.51%] [G loss: 0.863573]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 9/86 [loss: 0.187507, acc.: 98.44%] [G loss: 0.878238]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 10/86 [loss: 0.197159, acc.: 97.90%] [G loss: 0.928428]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 11/86 [loss: 0.270115, acc.: 94.97%] [G loss: 0.906594]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 12/86 [loss: 0.220175, acc.: 97.51%] [G loss: 0.852228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 13/86 [loss: 0.264790, acc.: 95.41%] [G loss: 0.821141]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 14/86 [loss: 0.329587, acc.: 92.04%] [G loss: 0.859898]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 15/86 [loss: 0.318745, acc.: 92.24%] [G loss: 0.902303]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 16/86 [loss: 0.221294, acc.: 96.83%] [G loss: 0.876334]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 17/86 [loss: 0.254572, acc.: 95.21%] [G loss: 0.935898]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 18/86 [loss: 0.233591, acc.: 96.68%] [G loss: 0.871980]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 19/86 [loss: 0.243593, acc.: 96.19%] [G loss: 0.838676]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 20/86 [loss: 0.200995, acc.: 98.05%] [G loss: 0.878664]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 21/86 [loss: 0.264468, acc.: 96.00%] [G loss: 0.872428]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 22/86 [loss: 0.283371, acc.: 94.87%] [G loss: 0.825235]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 23/86 [loss: 0.319673, acc.: 92.72%] [G loss: 0.872827]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 24/86 [loss: 0.208301, acc.: 97.56%] [G loss: 0.895913]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 25/86 [loss: 0.283258, acc.: 93.99%] [G loss: 0.866399]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 26/86 [loss: 0.344107, acc.: 90.28%] [G loss: 0.886931]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 27/86 [loss: 0.419671, acc.: 84.67%] [G loss: 0.848571]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 28/86 [loss: 0.226068, acc.: 97.02%] [G loss: 0.832695]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 29/86 [loss: 0.285202, acc.: 94.24%] [G loss: 0.844270]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 30/86 [loss: 0.333114, acc.: 91.11%] [G loss: 0.824732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 31/86 [loss: 0.326179, acc.: 92.24%] [G loss: 0.871379]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 32/86 [loss: 0.439782, acc.: 82.62%] [G loss: 0.836306]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 33/86 [loss: 0.352118, acc.: 90.43%] [G loss: 0.822447]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 34/86 [loss: 0.310190, acc.: 93.26%] [G loss: 0.852973]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 35/86 [loss: 0.316447, acc.: 92.33%] [G loss: 0.862054]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 36/86 [loss: 0.343709, acc.: 91.06%] [G loss: 0.912920]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 37/86 [loss: 0.307395, acc.: 92.43%] [G loss: 0.803565]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 38/86 [loss: 0.445395, acc.: 83.50%] [G loss: 0.805376]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 39/86 [loss: 0.340591, acc.: 90.58%] [G loss: 0.853232]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 40/86 [loss: 0.285434, acc.: 95.26%] [G loss: 0.862524]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 41/86 [loss: 0.309413, acc.: 91.65%] [G loss: 0.937640]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 42/86 [loss: 0.308938, acc.: 93.02%] [G loss: 0.888622]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 43/86 [loss: 0.242801, acc.: 96.58%] [G loss: 0.890169]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 44/86 [loss: 0.252702, acc.: 95.90%] [G loss: 0.881798]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 45/86 [loss: 0.208457, acc.: 97.80%] [G loss: 0.939410]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 46/86 [loss: 0.226699, acc.: 96.92%] [G loss: 0.852601]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 47/86 [loss: 0.193280, acc.: 98.19%] [G loss: 0.857223]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 48/86 [loss: 0.230086, acc.: 97.07%] [G loss: 0.912539]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 49/86 [loss: 0.307436, acc.: 92.48%] [G loss: 0.862973]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 50/86 [loss: 0.279184, acc.: 95.41%] [G loss: 0.848002]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 51/86 [loss: 0.299247, acc.: 94.09%] [G loss: 0.891284]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 52/86 [loss: 0.338268, acc.: 90.67%] [G loss: 0.912936]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 53/86 [loss: 0.400965, acc.: 85.60%] [G loss: 0.820790]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 54/86 [loss: 0.195800, acc.: 98.00%] [G loss: 0.906993]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 55/86 [loss: 0.293862, acc.: 93.90%] [G loss: 0.867379]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 56/86 [loss: 0.270805, acc.: 94.82%] [G loss: 0.876904]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 57/86 [loss: 0.204551, acc.: 98.05%] [G loss: 0.886956]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 58/86 [loss: 0.301905, acc.: 93.85%] [G loss: 0.896773]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 59/86 [loss: 0.326554, acc.: 91.60%] [G loss: 0.868537]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 60/86 [loss: 0.334606, acc.: 91.65%] [G loss: 0.854448]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 61/86 [loss: 0.322297, acc.: 92.87%] [G loss: 0.831376]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 62/86 [loss: 0.323906, acc.: 92.38%] [G loss: 0.869444]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 63/86 [loss: 0.285111, acc.: 94.48%] [G loss: 0.819881]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 55/200  Batch Size: 64/86 [loss: 0.345587, acc.: 90.09%] [G loss: 0.792572]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 65/86 [loss: 0.487079, acc.: 78.81%] [G loss: 0.848100]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 66/86 [loss: 0.345963, acc.: 89.65%] [G loss: 0.794279]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 67/86 [loss: 0.360770, acc.: 90.14%] [G loss: 0.820913]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 68/86 [loss: 0.394000, acc.: 85.74%] [G loss: 0.926462]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 69/86 [loss: 0.388616, acc.: 84.18%] [G loss: 0.762617]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 70/86 [loss: 0.368148, acc.: 87.84%] [G loss: 0.846842]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 71/86 [loss: 0.436526, acc.: 82.62%] [G loss: 0.846766]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 72/86 [loss: 0.437791, acc.: 81.30%] [G loss: 0.917118]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 73/86 [loss: 0.345293, acc.: 90.92%] [G loss: 0.878441]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 74/86 [loss: 0.287477, acc.: 93.41%] [G loss: 0.924871]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 75/86 [loss: 0.246298, acc.: 95.31%] [G loss: 0.954562]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 76/86 [loss: 0.301456, acc.: 93.60%] [G loss: 0.913027]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 77/86 [loss: 0.344653, acc.: 90.58%] [G loss: 0.812161]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 55/200  Batch Size: 78/86 [loss: 0.340977, acc.: 90.82%] [G loss: 0.803830]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 79/86 [loss: 0.278116, acc.: 94.92%] [G loss: 0.841758]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 80/86 [loss: 0.346045, acc.: 90.72%] [G loss: 0.866767]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 81/86 [loss: 0.379235, acc.: 88.09%] [G loss: 0.866791]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 82/86 [loss: 0.286272, acc.: 94.82%] [G loss: 0.884136]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 83/86 [loss: 0.358502, acc.: 89.55%] [G loss: 0.860086]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 55/200  Batch Size: 84/86 [loss: 0.322426, acc.: 92.43%] [G loss: 0.832025]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 55/200  Batch Size: 85/86 [loss: 0.280128, acc.: 94.14%] [G loss: 0.909354]\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 0/86 [loss: 0.377571, acc.: 88.43%] [G loss: 0.906131]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 1/86 [loss: 0.283454, acc.: 94.97%] [G loss: 0.895151]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 2/86 [loss: 0.270845, acc.: 94.38%] [G loss: 0.837874]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 3/86 [loss: 0.501042, acc.: 77.73%] [G loss: 0.832882]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 4/86 [loss: 0.424286, acc.: 85.11%] [G loss: 0.847622]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 5/86 [loss: 0.471633, acc.: 80.08%] [G loss: 0.795459]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 6/86 [loss: 0.306809, acc.: 93.55%] [G loss: 0.849967]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 7/86 [loss: 0.316387, acc.: 91.55%] [G loss: 0.928163]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 8/86 [loss: 0.269446, acc.: 95.51%] [G loss: 0.899035]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 9/86 [loss: 0.219141, acc.: 96.73%] [G loss: 0.916553]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 10/86 [loss: 0.281610, acc.: 94.04%] [G loss: 0.944699]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 11/86 [loss: 0.202305, acc.: 97.56%] [G loss: 0.883931]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 12/86 [loss: 0.221400, acc.: 96.78%] [G loss: 0.898798]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 13/86 [loss: 0.237929, acc.: 96.58%] [G loss: 0.865054]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 14/86 [loss: 0.308182, acc.: 91.36%] [G loss: 0.946261]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 15/86 [loss: 0.287968, acc.: 94.53%] [G loss: 0.872757]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 16/86 [loss: 0.306399, acc.: 92.82%] [G loss: 0.839137]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 17/86 [loss: 0.202305, acc.: 97.61%] [G loss: 0.894498]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 18/86 [loss: 0.217174, acc.: 97.07%] [G loss: 0.848676]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 19/86 [loss: 0.246697, acc.: 96.19%] [G loss: 0.880327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 20/86 [loss: 0.303935, acc.: 93.36%] [G loss: 0.877180]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 21/86 [loss: 0.387719, acc.: 87.84%] [G loss: 0.859576]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 22/86 [loss: 0.295947, acc.: 92.53%] [G loss: 0.819989]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 23/86 [loss: 0.258610, acc.: 95.75%] [G loss: 0.853635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 24/86 [loss: 0.398725, acc.: 86.96%] [G loss: 0.854691]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 25/86 [loss: 0.488374, acc.: 78.12%] [G loss: 0.849221]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 26/86 [loss: 0.267311, acc.: 95.21%] [G loss: 0.841981]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 27/86 [loss: 0.343852, acc.: 91.11%] [G loss: 0.859354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 28/86 [loss: 0.344463, acc.: 91.02%] [G loss: 0.884314]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 29/86 [loss: 0.310722, acc.: 91.21%] [G loss: 0.787354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 30/86 [loss: 0.253625, acc.: 96.04%] [G loss: 0.860566]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 31/86 [loss: 0.365448, acc.: 85.69%] [G loss: 0.959846]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 32/86 [loss: 0.286266, acc.: 94.48%] [G loss: 0.925840]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 33/86 [loss: 0.249485, acc.: 96.24%] [G loss: 0.879838]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 34/86 [loss: 0.344249, acc.: 91.89%] [G loss: 0.846722]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 35/86 [loss: 0.248020, acc.: 96.19%] [G loss: 0.914662]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 36/86 [loss: 0.337633, acc.: 90.48%] [G loss: 0.836256]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 37/86 [loss: 0.290636, acc.: 94.34%] [G loss: 0.862930]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 38/86 [loss: 0.303131, acc.: 93.65%] [G loss: 0.901351]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 39/86 [loss: 0.254318, acc.: 96.63%] [G loss: 0.862642]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 40/86 [loss: 0.360998, acc.: 88.72%] [G loss: 0.866685]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 41/86 [loss: 0.266888, acc.: 95.41%] [G loss: 0.900043]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 42/86 [loss: 0.282951, acc.: 94.68%] [G loss: 0.853421]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 43/86 [loss: 0.198486, acc.: 97.80%] [G loss: 0.833130]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 44/86 [loss: 0.239569, acc.: 96.48%] [G loss: 0.829891]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 45/86 [loss: 0.342630, acc.: 89.40%] [G loss: 0.875018]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 46/86 [loss: 0.376586, acc.: 86.72%] [G loss: 0.765232]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 47/86 [loss: 0.369359, acc.: 87.94%] [G loss: 0.884513]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 48/86 [loss: 0.356896, acc.: 89.79%] [G loss: 0.860909]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 49/86 [loss: 0.343538, acc.: 90.67%] [G loss: 0.877111]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 50/86 [loss: 0.351984, acc.: 89.99%] [G loss: 0.826495]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 51/86 [loss: 0.240492, acc.: 95.56%] [G loss: 0.787359]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 52/86 [loss: 0.308425, acc.: 92.82%] [G loss: 0.886456]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 53/86 [loss: 0.192500, acc.: 98.39%] [G loss: 0.935034]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 54/86 [loss: 0.225490, acc.: 96.88%] [G loss: 0.918811]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 55/86 [loss: 0.226175, acc.: 97.12%] [G loss: 0.961531]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 56/86 [loss: 0.136789, acc.: 99.12%] [G loss: 1.005279]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 57/86 [loss: 0.164101, acc.: 99.12%] [G loss: 0.955087]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 58/86 [loss: 0.258282, acc.: 96.00%] [G loss: 0.883779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 59/86 [loss: 0.168858, acc.: 99.22%] [G loss: 0.868408]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 60/86 [loss: 0.222232, acc.: 97.07%] [G loss: 0.783503]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 61/86 [loss: 0.335340, acc.: 91.36%] [G loss: 0.817024]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 62/86 [loss: 0.296559, acc.: 93.70%] [G loss: 0.797540]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 63/86 [loss: 0.274589, acc.: 94.97%] [G loss: 0.866339]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 64/86 [loss: 0.332765, acc.: 91.06%] [G loss: 0.855248]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 65/86 [loss: 0.371913, acc.: 88.09%] [G loss: 0.831202]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 66/86 [loss: 0.312099, acc.: 93.90%] [G loss: 0.860694]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 67/86 [loss: 0.282964, acc.: 94.48%] [G loss: 0.873145]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 68/86 [loss: 0.192581, acc.: 98.29%] [G loss: 0.881303]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 69/86 [loss: 0.263723, acc.: 95.17%] [G loss: 0.873033]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 70/86 [loss: 0.263865, acc.: 95.56%] [G loss: 0.859550]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 71/86 [loss: 0.202046, acc.: 97.61%] [G loss: 0.950457]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 72/86 [loss: 0.250690, acc.: 96.58%] [G loss: 0.897755]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 73/86 [loss: 0.340917, acc.: 91.46%] [G loss: 0.880795]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 74/86 [loss: 0.409064, acc.: 84.03%] [G loss: 0.805373]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 75/86 [loss: 0.253089, acc.: 95.46%] [G loss: 0.908667]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 56/200  Batch Size: 76/86 [loss: 0.262365, acc.: 94.58%] [G loss: 0.975928]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 77/86 [loss: 0.235389, acc.: 96.44%] [G loss: 0.969441]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 78/86 [loss: 0.318489, acc.: 92.72%] [G loss: 0.901667]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 79/86 [loss: 0.190454, acc.: 97.85%] [G loss: 0.846839]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 56/200  Batch Size: 80/86 [loss: 0.235442, acc.: 96.14%] [G loss: 0.803983]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 81/86 [loss: 0.352984, acc.: 91.11%] [G loss: 0.824327]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 82/86 [loss: 0.237538, acc.: 96.04%] [G loss: 0.835466]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 83/86 [loss: 0.257319, acc.: 95.41%] [G loss: 0.891277]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 56/200  Batch Size: 84/86 [loss: 0.364077, acc.: 89.75%] [G loss: 0.866419]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 56/200  Batch Size: 85/86 [loss: 0.204806, acc.: 98.14%] [G loss: 0.860580]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 0/86 [loss: 0.266565, acc.: 94.82%] [G loss: 0.944512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 1/86 [loss: 0.336995, acc.: 91.60%] [G loss: 0.880085]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 2/86 [loss: 0.361446, acc.: 89.50%] [G loss: 0.813707]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 3/86 [loss: 0.328007, acc.: 91.94%] [G loss: 0.860631]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 4/86 [loss: 0.312123, acc.: 93.55%] [G loss: 0.875744]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 5/86 [loss: 0.482874, acc.: 80.13%] [G loss: 0.817478]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 6/86 [loss: 0.404368, acc.: 85.74%] [G loss: 0.843557]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 7/86 [loss: 0.412001, acc.: 84.86%] [G loss: 0.782636]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 8/86 [loss: 0.479455, acc.: 79.25%] [G loss: 0.849445]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 9/86 [loss: 0.319320, acc.: 92.63%] [G loss: 0.845284]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 10/86 [loss: 0.290424, acc.: 92.97%] [G loss: 0.946987]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 11/86 [loss: 0.213663, acc.: 96.92%] [G loss: 1.011032]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 12/86 [loss: 0.195971, acc.: 98.10%] [G loss: 1.006837]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 13/86 [loss: 0.207433, acc.: 97.51%] [G loss: 0.967538]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 14/86 [loss: 0.247419, acc.: 96.24%] [G loss: 0.907659]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 15/86 [loss: 0.236542, acc.: 96.34%] [G loss: 0.863165]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 16/86 [loss: 0.291138, acc.: 94.24%] [G loss: 0.821791]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 17/86 [loss: 0.166507, acc.: 98.68%] [G loss: 0.797425]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 18/86 [loss: 0.339111, acc.: 92.09%] [G loss: 0.820039]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 19/86 [loss: 0.416862, acc.: 84.77%] [G loss: 0.836172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 20/86 [loss: 0.195666, acc.: 98.10%] [G loss: 0.901701]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 21/86 [loss: 0.220104, acc.: 97.71%] [G loss: 0.866262]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 22/86 [loss: 0.319273, acc.: 91.99%] [G loss: 0.906483]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 23/86 [loss: 0.260626, acc.: 95.80%] [G loss: 0.903650]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 24/86 [loss: 0.171434, acc.: 98.83%] [G loss: 0.957431]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 25/86 [loss: 0.295429, acc.: 93.85%] [G loss: 0.930656]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 26/86 [loss: 0.249138, acc.: 96.39%] [G loss: 0.873042]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 27/86 [loss: 0.211392, acc.: 97.31%] [G loss: 0.854372]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 28/86 [loss: 0.253121, acc.: 94.97%] [G loss: 0.909241]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 29/86 [loss: 0.335164, acc.: 91.60%] [G loss: 0.875727]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 30/86 [loss: 0.244811, acc.: 96.14%] [G loss: 0.861716]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 31/86 [loss: 0.330484, acc.: 90.62%] [G loss: 0.819197]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 32/86 [loss: 0.342890, acc.: 91.99%] [G loss: 0.854220]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 33/86 [loss: 0.199421, acc.: 97.90%] [G loss: 0.909897]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 34/86 [loss: 0.355935, acc.: 89.84%] [G loss: 0.940870]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 35/86 [loss: 0.259939, acc.: 95.36%] [G loss: 0.927852]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 36/86 [loss: 0.222665, acc.: 96.39%] [G loss: 0.867127]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 37/86 [loss: 0.254399, acc.: 95.95%] [G loss: 0.880126]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 38/86 [loss: 0.282335, acc.: 94.68%] [G loss: 0.843217]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 39/86 [loss: 0.299267, acc.: 94.24%] [G loss: 0.864038]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 40/86 [loss: 0.249844, acc.: 96.44%] [G loss: 0.885922]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 41/86 [loss: 0.213520, acc.: 97.17%] [G loss: 0.826690]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 42/86 [loss: 0.328790, acc.: 91.55%] [G loss: 0.862874]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 43/86 [loss: 0.229078, acc.: 96.97%] [G loss: 0.889819]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 44/86 [loss: 0.305105, acc.: 94.09%] [G loss: 0.859707]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 45/86 [loss: 0.234676, acc.: 97.12%] [G loss: 0.911125]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 46/86 [loss: 0.274122, acc.: 94.82%] [G loss: 0.845769]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 47/86 [loss: 0.321625, acc.: 91.99%] [G loss: 0.803388]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 48/86 [loss: 0.323296, acc.: 92.09%] [G loss: 0.848941]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 49/86 [loss: 0.268512, acc.: 94.68%] [G loss: 0.837755]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 50/86 [loss: 0.202506, acc.: 97.95%] [G loss: 0.857669]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 51/86 [loss: 0.298535, acc.: 92.43%] [G loss: 0.951911]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 52/86 [loss: 0.237111, acc.: 96.58%] [G loss: 0.949563]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 53/86 [loss: 0.204906, acc.: 98.05%] [G loss: 0.906466]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 54/86 [loss: 0.275868, acc.: 92.72%] [G loss: 0.808457]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 55/86 [loss: 0.320640, acc.: 92.68%] [G loss: 0.804350]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 56/86 [loss: 0.367985, acc.: 89.16%] [G loss: 0.807732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 57/86 [loss: 0.328949, acc.: 91.75%] [G loss: 0.785586]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 58/86 [loss: 0.323272, acc.: 92.14%] [G loss: 0.820604]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 59/86 [loss: 0.349244, acc.: 90.14%] [G loss: 0.809117]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 60/86 [loss: 0.274500, acc.: 95.02%] [G loss: 0.847139]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 61/86 [loss: 0.286479, acc.: 94.73%] [G loss: 0.831806]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 62/86 [loss: 0.270808, acc.: 93.75%] [G loss: 0.950204]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 63/86 [loss: 0.331023, acc.: 91.36%] [G loss: 0.926756]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 64/86 [loss: 0.249951, acc.: 96.48%] [G loss: 0.907748]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 65/86 [loss: 0.388627, acc.: 87.99%] [G loss: 0.860182]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 66/86 [loss: 0.351458, acc.: 89.70%] [G loss: 0.832981]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 67/86 [loss: 0.344709, acc.: 91.26%] [G loss: 0.827960]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 68/86 [loss: 0.253696, acc.: 95.90%] [G loss: 0.857015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 69/86 [loss: 0.279859, acc.: 94.92%] [G loss: 0.880718]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 70/86 [loss: 0.268346, acc.: 93.90%] [G loss: 0.948506]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 57/200  Batch Size: 71/86 [loss: 0.434463, acc.: 82.67%] [G loss: 0.849851]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 72/86 [loss: 0.299623, acc.: 92.77%] [G loss: 0.803557]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 73/86 [loss: 0.174206, acc.: 98.63%] [G loss: 0.849760]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 74/86 [loss: 0.197612, acc.: 97.71%] [G loss: 0.935394]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 75/86 [loss: 0.341827, acc.: 90.87%] [G loss: 0.911612]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 76/86 [loss: 0.367432, acc.: 87.74%] [G loss: 0.934992]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 77/86 [loss: 0.274193, acc.: 95.12%] [G loss: 0.936430]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 78/86 [loss: 0.284506, acc.: 94.14%] [G loss: 0.849797]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 79/86 [loss: 0.171317, acc.: 98.73%] [G loss: 0.844963]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 80/86 [loss: 0.317206, acc.: 92.24%] [G loss: 0.896746]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 81/86 [loss: 0.279182, acc.: 95.36%] [G loss: 0.872051]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 82/86 [loss: 0.324086, acc.: 92.53%] [G loss: 0.876995]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 57/200  Batch Size: 83/86 [loss: 0.283531, acc.: 95.02%] [G loss: 0.851362]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 57/200  Batch Size: 84/86 [loss: 0.189697, acc.: 98.10%] [G loss: 0.831358]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 57/200  Batch Size: 85/86 [loss: 0.403266, acc.: 85.21%] [G loss: 0.815651]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 0/86 [loss: 0.302461, acc.: 94.58%] [G loss: 0.877162]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 1/86 [loss: 0.280472, acc.: 94.97%] [G loss: 0.864345]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 2/86 [loss: 0.250784, acc.: 95.95%] [G loss: 0.891079]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 3/86 [loss: 0.272120, acc.: 94.63%] [G loss: 0.888230]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 4/86 [loss: 0.339679, acc.: 91.06%] [G loss: 0.876472]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 5/86 [loss: 0.202898, acc.: 97.51%] [G loss: 0.827589]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 6/86 [loss: 0.293118, acc.: 94.04%] [G loss: 0.861449]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 7/86 [loss: 0.217793, acc.: 97.22%] [G loss: 0.938850]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 8/86 [loss: 0.220408, acc.: 97.12%] [G loss: 0.970360]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 9/86 [loss: 0.201818, acc.: 97.75%] [G loss: 0.978758]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 10/86 [loss: 0.184727, acc.: 98.44%] [G loss: 0.912717]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 11/86 [loss: 0.288782, acc.: 94.38%] [G loss: 0.856025]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 58/200  Batch Size: 12/86 [loss: 0.235788, acc.: 96.48%] [G loss: 0.833115]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 13/86 [loss: 0.310467, acc.: 93.46%] [G loss: 0.811724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 14/86 [loss: 0.444806, acc.: 83.84%] [G loss: 0.806877]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 15/86 [loss: 0.282410, acc.: 94.09%] [G loss: 0.872808]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 16/86 [loss: 0.370723, acc.: 88.67%] [G loss: 0.801110]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 17/86 [loss: 0.452084, acc.: 80.86%] [G loss: 0.801372]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 18/86 [loss: 0.409768, acc.: 84.33%] [G loss: 0.887930]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 19/86 [loss: 0.310395, acc.: 91.94%] [G loss: 0.926570]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 20/86 [loss: 0.192497, acc.: 98.78%] [G loss: 0.926333]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 21/86 [loss: 0.167750, acc.: 98.73%] [G loss: 0.972075]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 22/86 [loss: 0.162688, acc.: 99.02%] [G loss: 0.912120]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 23/86 [loss: 0.169297, acc.: 98.78%] [G loss: 0.920809]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 24/86 [loss: 0.257419, acc.: 96.04%] [G loss: 0.886213]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 25/86 [loss: 0.294946, acc.: 94.24%] [G loss: 0.839979]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 26/86 [loss: 0.208714, acc.: 97.85%] [G loss: 0.863445]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 27/86 [loss: 0.224083, acc.: 97.22%] [G loss: 0.857856]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 28/86 [loss: 0.211794, acc.: 97.66%] [G loss: 0.879052]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 29/86 [loss: 0.297942, acc.: 94.19%] [G loss: 0.889098]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 30/86 [loss: 0.222533, acc.: 96.92%] [G loss: 0.883182]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 31/86 [loss: 0.278026, acc.: 94.92%] [G loss: 0.866084]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 32/86 [loss: 0.298043, acc.: 93.51%] [G loss: 0.832062]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 33/86 [loss: 0.344140, acc.: 91.50%] [G loss: 0.844694]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 34/86 [loss: 0.443401, acc.: 82.91%] [G loss: 0.810270]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 35/86 [loss: 0.368726, acc.: 89.45%] [G loss: 0.790449]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 36/86 [loss: 0.425129, acc.: 84.23%] [G loss: 0.823712]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 37/86 [loss: 0.423248, acc.: 84.42%] [G loss: 0.788606]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 38/86 [loss: 0.380264, acc.: 88.48%] [G loss: 0.877073]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 39/86 [loss: 0.401058, acc.: 85.35%] [G loss: 0.808305]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 40/86 [loss: 0.274610, acc.: 95.36%] [G loss: 0.827357]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 41/86 [loss: 0.356415, acc.: 87.70%] [G loss: 0.935380]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 42/86 [loss: 0.332250, acc.: 90.82%] [G loss: 0.925795]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 43/86 [loss: 0.262219, acc.: 94.53%] [G loss: 0.966424]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 44/86 [loss: 0.291683, acc.: 93.51%] [G loss: 0.906994]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 45/86 [loss: 0.249720, acc.: 96.29%] [G loss: 0.910077]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 46/86 [loss: 0.220913, acc.: 97.22%] [G loss: 0.908510]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 47/86 [loss: 0.289349, acc.: 94.38%] [G loss: 0.894549]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 48/86 [loss: 0.206876, acc.: 97.61%] [G loss: 0.878613]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 49/86 [loss: 0.179474, acc.: 97.41%] [G loss: 1.041621]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 58/200  Batch Size: 50/86 [loss: 0.187175, acc.: 98.10%] [G loss: 0.971341]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 51/86 [loss: 0.179338, acc.: 98.63%] [G loss: 0.944397]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 52/86 [loss: 0.202866, acc.: 98.29%] [G loss: 0.903431]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 53/86 [loss: 0.307419, acc.: 93.51%] [G loss: 0.889616]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 54/86 [loss: 0.275141, acc.: 94.04%] [G loss: 0.859188]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 55/86 [loss: 0.226990, acc.: 96.53%] [G loss: 0.887735]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 56/86 [loss: 0.356950, acc.: 89.55%] [G loss: 0.850135]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 57/86 [loss: 0.201335, acc.: 97.66%] [G loss: 0.828380]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 58/86 [loss: 0.203549, acc.: 97.66%] [G loss: 0.900284]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 59/86 [loss: 0.187181, acc.: 98.10%] [G loss: 0.991507]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 60/86 [loss: 0.270687, acc.: 96.24%] [G loss: 0.934230]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 61/86 [loss: 0.316611, acc.: 92.87%] [G loss: 0.856539]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 62/86 [loss: 0.451966, acc.: 80.81%] [G loss: 0.802207]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 58/200  Batch Size: 63/86 [loss: 0.384163, acc.: 85.16%] [G loss: 0.728027]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 64/86 [loss: 0.413153, acc.: 85.50%] [G loss: 0.827052]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 65/86 [loss: 0.454654, acc.: 82.13%] [G loss: 0.827049]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 66/86 [loss: 0.370069, acc.: 88.77%] [G loss: 0.848100]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 67/86 [loss: 0.461924, acc.: 82.03%] [G loss: 0.827668]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 68/86 [loss: 0.309612, acc.: 92.77%] [G loss: 0.832280]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 69/86 [loss: 0.489060, acc.: 78.42%] [G loss: 0.803374]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 70/86 [loss: 0.387348, acc.: 87.11%] [G loss: 0.850177]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 71/86 [loss: 0.408962, acc.: 84.91%] [G loss: 0.894561]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 72/86 [loss: 0.391126, acc.: 86.62%] [G loss: 0.890810]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 58/200  Batch Size: 73/86 [loss: 0.266386, acc.: 95.36%] [G loss: 0.882548]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 74/86 [loss: 0.289334, acc.: 94.92%] [G loss: 0.860678]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 75/86 [loss: 0.274592, acc.: 96.04%] [G loss: 0.861158]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 76/86 [loss: 0.262111, acc.: 96.00%] [G loss: 0.912806]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 77/86 [loss: 0.367624, acc.: 89.36%] [G loss: 0.867546]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 78/86 [loss: 0.313761, acc.: 93.07%] [G loss: 0.864459]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 79/86 [loss: 0.369562, acc.: 89.60%] [G loss: 0.895528]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 80/86 [loss: 0.300485, acc.: 93.02%] [G loss: 0.824260]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 81/86 [loss: 0.306473, acc.: 93.21%] [G loss: 0.874433]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 82/86 [loss: 0.290857, acc.: 94.04%] [G loss: 0.836495]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 83/86 [loss: 0.344692, acc.: 90.38%] [G loss: 0.881462]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 58/200  Batch Size: 84/86 [loss: 0.396114, acc.: 85.35%] [G loss: 0.802505]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 58/200  Batch Size: 85/86 [loss: 0.315926, acc.: 92.48%] [G loss: 0.806163]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 0/86 [loss: 0.341324, acc.: 91.02%] [G loss: 0.835732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 1/86 [loss: 0.301613, acc.: 93.95%] [G loss: 0.845286]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 2/86 [loss: 0.313421, acc.: 93.21%] [G loss: 0.826962]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 3/86 [loss: 0.750577, acc.: 53.17%] [G loss: 0.787674]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 4/86 [loss: 0.366698, acc.: 87.65%] [G loss: 0.882123]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 5/86 [loss: 0.290211, acc.: 94.24%] [G loss: 0.915054]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 6/86 [loss: 0.429691, acc.: 84.42%] [G loss: 0.868359]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 7/86 [loss: 0.427834, acc.: 83.84%] [G loss: 0.864561]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 8/86 [loss: 0.253711, acc.: 96.53%] [G loss: 0.891247]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 9/86 [loss: 0.417252, acc.: 85.35%] [G loss: 0.879460]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 10/86 [loss: 0.265843, acc.: 94.58%] [G loss: 0.951811]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 11/86 [loss: 0.154087, acc.: 98.63%] [G loss: 0.983565]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 12/86 [loss: 0.134561, acc.: 99.41%] [G loss: 1.003218]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 13/86 [loss: 0.173873, acc.: 98.58%] [G loss: 1.007639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 14/86 [loss: 0.131238, acc.: 99.71%] [G loss: 1.038142]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 15/86 [loss: 0.166673, acc.: 98.29%] [G loss: 0.902732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 16/86 [loss: 0.158248, acc.: 99.27%] [G loss: 0.889407]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 17/86 [loss: 0.179038, acc.: 98.93%] [G loss: 0.862698]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 18/86 [loss: 0.295437, acc.: 92.48%] [G loss: 0.777617]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 19/86 [loss: 0.316869, acc.: 92.87%] [G loss: 0.819807]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 20/86 [loss: 0.327330, acc.: 92.24%] [G loss: 0.825927]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 21/86 [loss: 0.445886, acc.: 83.06%] [G loss: 0.810990]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 22/86 [loss: 0.311098, acc.: 93.31%] [G loss: 0.807473]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 23/86 [loss: 0.409604, acc.: 85.74%] [G loss: 0.853954]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 24/86 [loss: 0.323170, acc.: 91.89%] [G loss: 0.900790]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 25/86 [loss: 0.344251, acc.: 90.62%] [G loss: 0.873165]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 26/86 [loss: 0.305193, acc.: 93.65%] [G loss: 0.922756]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 27/86 [loss: 0.306803, acc.: 93.55%] [G loss: 0.867854]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 28/86 [loss: 0.355254, acc.: 89.89%] [G loss: 0.908784]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 29/86 [loss: 0.223423, acc.: 97.61%] [G loss: 0.890927]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 30/86 [loss: 0.223137, acc.: 97.41%] [G loss: 0.918377]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 31/86 [loss: 0.211441, acc.: 97.75%] [G loss: 0.910354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 32/86 [loss: 0.208913, acc.: 97.66%] [G loss: 0.869352]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 33/86 [loss: 0.198029, acc.: 97.90%] [G loss: 0.921007]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 34/86 [loss: 0.176888, acc.: 98.68%] [G loss: 0.954045]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 35/86 [loss: 0.184105, acc.: 98.73%] [G loss: 0.949048]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 36/86 [loss: 0.178199, acc.: 98.00%] [G loss: 0.994710]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 37/86 [loss: 0.251199, acc.: 96.68%] [G loss: 0.926812]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 38/86 [loss: 0.231410, acc.: 96.58%] [G loss: 0.870503]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 39/86 [loss: 0.261390, acc.: 95.31%] [G loss: 0.821244]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 40/86 [loss: 0.298632, acc.: 93.80%] [G loss: 0.812939]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 41/86 [loss: 0.258691, acc.: 96.48%] [G loss: 0.842533]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 42/86 [loss: 0.346550, acc.: 90.87%] [G loss: 0.804446]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 43/86 [loss: 0.404839, acc.: 83.69%] [G loss: 0.748431]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 44/86 [loss: 0.563516, acc.: 70.85%] [G loss: 0.750294]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 45/86 [loss: 0.337377, acc.: 90.23%] [G loss: 0.846708]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 46/86 [loss: 0.391265, acc.: 82.67%] [G loss: 0.938765]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 47/86 [loss: 0.354575, acc.: 90.72%] [G loss: 0.876945]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 48/86 [loss: 0.238236, acc.: 96.78%] [G loss: 0.890063]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 49/86 [loss: 0.284636, acc.: 94.87%] [G loss: 0.854040]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 50/86 [loss: 0.280588, acc.: 92.97%] [G loss: 0.947303]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 51/86 [loss: 0.251677, acc.: 96.09%] [G loss: 0.900865]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 52/86 [loss: 0.270538, acc.: 95.51%] [G loss: 0.861037]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 53/86 [loss: 0.281823, acc.: 95.17%] [G loss: 0.884292]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 54/86 [loss: 0.264692, acc.: 95.46%] [G loss: 0.892690]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 55/86 [loss: 0.265944, acc.: 95.12%] [G loss: 0.905002]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 56/86 [loss: 0.234272, acc.: 96.58%] [G loss: 0.820104]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 57/86 [loss: 0.180890, acc.: 98.39%] [G loss: 0.837946]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 58/86 [loss: 0.340724, acc.: 92.04%] [G loss: 0.813067]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 59/86 [loss: 0.306181, acc.: 93.80%] [G loss: 0.883224]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 60/86 [loss: 0.271274, acc.: 95.12%] [G loss: 0.875346]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 61/86 [loss: 0.299632, acc.: 93.70%] [G loss: 0.844695]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 62/86 [loss: 0.178775, acc.: 98.68%] [G loss: 0.913018]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 63/86 [loss: 0.362071, acc.: 88.82%] [G loss: 0.873393]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 64/86 [loss: 0.314122, acc.: 93.46%] [G loss: 0.846064]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 65/86 [loss: 0.392445, acc.: 86.33%] [G loss: 0.885500]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 66/86 [loss: 0.345805, acc.: 91.75%] [G loss: 0.840438]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 67/86 [loss: 0.461549, acc.: 79.79%] [G loss: 0.757835]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 68/86 [loss: 0.349913, acc.: 90.04%] [G loss: 0.782402]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 69/86 [loss: 0.433448, acc.: 83.06%] [G loss: 0.887201]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 70/86 [loss: 0.404435, acc.: 86.33%] [G loss: 0.837042]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 71/86 [loss: 0.289360, acc.: 93.36%] [G loss: 0.911740]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 72/86 [loss: 0.394603, acc.: 86.82%] [G loss: 0.891480]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 73/86 [loss: 0.412855, acc.: 84.81%] [G loss: 0.802191]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 74/86 [loss: 0.367380, acc.: 88.38%] [G loss: 0.845696]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 75/86 [loss: 0.300915, acc.: 93.95%] [G loss: 0.835748]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 76/86 [loss: 0.394636, acc.: 85.21%] [G loss: 0.909557]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 59/200  Batch Size: 77/86 [loss: 0.384299, acc.: 87.06%] [G loss: 0.937949]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 78/86 [loss: 0.275255, acc.: 95.31%] [G loss: 0.893287]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch: 59/200  Batch Size: 79/86 [loss: 0.289236, acc.: 93.55%] [G loss: 0.829248]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch: 59/200  Batch Size: 80/86 [loss: 0.248048, acc.: 96.09%] [G loss: 0.866278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 81/86 [loss: 0.344309, acc.: 89.40%] [G loss: 0.900924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 82/86 [loss: 0.256418, acc.: 95.56%] [G loss: 0.931390]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 83/86 [loss: 0.273723, acc.: 94.63%] [G loss: 0.911053]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 84/86 [loss: 0.342039, acc.: 90.67%] [G loss: 0.899553]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 59/200  Batch Size: 85/86 [loss: 0.210662, acc.: 97.66%] [G loss: 0.852258]\n",
      "4/4 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN(28,28,1)\n",
    "dcgan.train(epochs=200, batch_size=1024, save_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = Model([z, label], valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = in_shape[0] * in_shape[1]\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        merge = Concatenate()([in_image, li])\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        fe = Dropout(0.4)(fe)\n",
    "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "        model = Model([in_image, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 128 * 7 * 7\n",
    "        gen = Dense(n_nodes)(in_lat)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Reshape((7, 7, 128))(gen) \n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(merge)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes  # Ensure labels are within valid range\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"CGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('CGAN_mnist', exist_ok=True)\n",
    "        fig.savefig(\"CGAN_mnist/CGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=1024, save_interval=1, gen_steps=3):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, y_train[idx]], labels_real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, gen_labels], labels_fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([noise, gen_labels], valid_y)\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "            if (epoch) % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set image dimensions\n",
    "# img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# # Instantiate and train the DCGAN\n",
    "# cgan = CGAN(img_rows, img_cols, channels)\n",
    "# cgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.00002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = self.define_gan(self.generator,self.discriminator)\n",
    "\n",
    "        \n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        fe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(64, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        out1 = Dense(1, activation='sigmoid')(fe)\n",
    "        out2 = Dense(n_classes, activation='softmax')(fe)\n",
    "        model = Model(in_image, [out1, out2])\n",
    "        opt = Adam(lr=0.00002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 512 * 7 * 7\n",
    "        gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Reshape((7, 7, 512))(gen)\n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "        gen = BatchNormalization()(gen)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "        out_layer = Activation('tanh')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "    \n",
    "        # define the combined generator and discriminator model, for updating the generator\n",
    "    def define_gan(self, g_model, d_model):\n",
    "        # make weights in the discriminator not trainable\n",
    "        for layer in d_model.layers:\n",
    "            if not isinstance(layer, BatchNormalization):\n",
    "                layer.trainable = False\n",
    "        # connect the outputs of the generator to the inputs of the discriminator\n",
    "        gan_output = d_model(g_model.output)\n",
    "        # define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "        model = Model(g_model.input, gan_output)\n",
    "        # compile model\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                # axs[i, j].set_title(chr(sampled_labels[cnt][0] + 65))\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"ACGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('ACGAN_mnist_2', exist_ok=True)\n",
    "        fig.savefig(\"ACGAN_mnist_2/ACGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def generate_latent_points(self, latent_dim, n_samples, n_classes=26):\n",
    "        # generate points in the latent space\n",
    "        x_input = randn(latent_dim * n_samples)\n",
    "        # reshape into a batch of inputs for the network\n",
    "        z_input = x_input.reshape(n_samples, latent_dim)\n",
    "        # generate labels\n",
    "        labels = randint(0, n_classes, n_samples)\n",
    "        return [z_input, labels]\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=2056, save_interval=1, gen_steps=1):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, [labels_real, y_train[idx]])\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [labels_fake, gen_labels])\n",
    "                d_loss_1 = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])\n",
    "                d_loss_2 = 0.5 * np.add(d_loss_real[1], d_loss_fake[1])\n",
    "\n",
    "                for _ in range(gen_steps):\n",
    "                    z_input, z_labels = self.generate_latent_points(self.latent_dim, batch_size)\n",
    "                    y_gan = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss 1: {d_loss_1}, D loss 2: {d_loss_2}, G loss: {g_loss}]\")\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 1/43 [D loss 1: 5.07591700553894, D loss 2: 0.8845160901546478, G loss: [3.950083017349243, 0.6920515894889832, 3.2580313682556152]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 2/43 [D loss 1: 5.070025205612183, D loss 2: 0.9115675687789917, G loss: [3.9487624168395996, 0.6902897357940674, 3.2584726810455322]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 3/43 [D loss 1: 5.0332841873168945, D loss 2: 0.8931187093257904, G loss: [3.9468448162078857, 0.6886608600616455, 3.2581839561462402]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 4/43 [D loss 1: 4.989941120147705, D loss 2: 0.8722847700119019, G loss: [3.9449925422668457, 0.6870461702346802, 3.257946491241455]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 5/43 [D loss 1: 4.971191883087158, D loss 2: 0.8598926067352295, G loss: [3.944643497467041, 0.6860059499740601, 3.2586376667022705]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 6/43 [D loss 1: 4.904329538345337, D loss 2: 0.8344956934452057, G loss: [3.9438390731811523, 0.6852750778198242, 3.258563995361328]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 7/43 [D loss 1: 4.913269758224487, D loss 2: 0.8314395546913147, G loss: [3.9424211978912354, 0.6839473843574524, 3.2584738731384277]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 8/43 [D loss 1: 4.860095739364624, D loss 2: 0.7906611859798431, G loss: [3.942145347595215, 0.682774543762207, 3.259370803833008]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 9/43 [D loss 1: 4.84284234046936, D loss 2: 0.7864927053451538, G loss: [3.9409427642822266, 0.6821892261505127, 3.258753538131714]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 10/43 [D loss 1: 4.830370903015137, D loss 2: 0.7657702565193176, G loss: [3.9419467449188232, 0.6818628907203674, 3.2600839138031006]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 11/43 [D loss 1: 4.75626802444458, D loss 2: 0.7507375776767731, G loss: [3.9409122467041016, 0.6806734204292297, 3.2602388858795166]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 12/43 [D loss 1: 4.819644927978516, D loss 2: 0.7463622093200684, G loss: [3.940704584121704, 0.6791061758995056, 3.2615983486175537]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 13/43 [D loss 1: 4.7850682735443115, D loss 2: 0.7359172403812408, G loss: [3.9358577728271484, 0.6789857149124146, 3.2568719387054443]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 14/43 [D loss 1: 4.818985462188721, D loss 2: 0.7449143528938293, G loss: [3.9369213581085205, 0.6776390075683594, 3.259282350540161]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 15/43 [D loss 1: 4.767596483230591, D loss 2: 0.7248322069644928, G loss: [3.939115524291992, 0.6780299544334412, 3.2610855102539062]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 16/43 [D loss 1: 4.743318796157837, D loss 2: 0.7359158396720886, G loss: [3.937574863433838, 0.6769357919692993, 3.260639190673828]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 17/43 [D loss 1: 4.784788608551025, D loss 2: 0.7724103331565857, G loss: [3.9399352073669434, 0.6767373085021973, 3.263197898864746]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 18/43 [D loss 1: 4.7842116355896, D loss 2: 0.777816891670227, G loss: [3.9400033950805664, 0.6766061782836914, 3.263397216796875]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 19/43 [D loss 1: 4.817892789840698, D loss 2: 0.7937978506088257, G loss: [3.9377219676971436, 0.6756420731544495, 3.262079954147339]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 20/43 [D loss 1: 4.747493743896484, D loss 2: 0.7560080587863922, G loss: [3.939810037612915, 0.6751871705055237, 3.264622926712036]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 21/43 [D loss 1: 4.759363174438477, D loss 2: 0.7320486009120941, G loss: [3.9415171146392822, 0.6764867305755615, 3.2650303840637207]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 22/43 [D loss 1: 4.765618801116943, D loss 2: 0.7428616583347321, G loss: [3.933438777923584, 0.6737281680107117, 3.2597105503082275]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 23/43 [D loss 1: 4.787380933761597, D loss 2: 0.7566502094268799, G loss: [3.9395387172698975, 0.6751462817192078, 3.264392375946045]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 24/43 [D loss 1: 4.8054680824279785, D loss 2: 0.7907784283161163, G loss: [3.938406467437744, 0.6756632924079895, 3.2627432346343994]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 25/43 [D loss 1: 4.807112216949463, D loss 2: 0.7919360399246216, G loss: [3.935399055480957, 0.6732443571090698, 3.2621548175811768]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 26/43 [D loss 1: 4.770796537399292, D loss 2: 0.7652708292007446, G loss: [3.943080186843872, 0.6760971546173096, 3.2669830322265625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 27/43 [D loss 1: 4.686900854110718, D loss 2: 0.7383921444416046, G loss: [3.9395689964294434, 0.6747981309890747, 3.264770984649658]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 28/43 [D loss 1: 4.733527421951294, D loss 2: 0.7249582707881927, G loss: [3.9451186656951904, 0.6737236380577087, 3.271394968032837]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 29/43 [D loss 1: 4.704082250595093, D loss 2: 0.7287471294403076, G loss: [3.9416909217834473, 0.6719011068344116, 3.269789934158325]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 30/43 [D loss 1: 4.699909925460815, D loss 2: 0.7033473551273346, G loss: [3.9370062351226807, 0.6720938086509705, 3.2649123668670654]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 31/43 [D loss 1: 4.665231943130493, D loss 2: 0.6946789622306824, G loss: [3.939164638519287, 0.6712476015090942, 3.2679171562194824]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 32/43 [D loss 1: 4.6573731899261475, D loss 2: 0.7030405402183533, G loss: [3.941307544708252, 0.6716607213020325, 3.2696468830108643]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 33/43 [D loss 1: 4.670935153961182, D loss 2: 0.6853354573249817, G loss: [3.9404187202453613, 0.6711811423301697, 3.269237518310547]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 34/43 [D loss 1: 4.634076833724976, D loss 2: 0.7012009918689728, G loss: [3.9431912899017334, 0.6709563732147217, 3.2722349166870117]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 35/43 [D loss 1: 4.681121110916138, D loss 2: 0.7142938673496246, G loss: [3.937673568725586, 0.6722007989883423, 3.265472650527954]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 36/43 [D loss 1: 4.659386157989502, D loss 2: 0.6957628428936005, G loss: [3.938988208770752, 0.6715925335884094, 3.2673957347869873]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 37/43 [D loss 1: 4.529066324234009, D loss 2: 0.6283451616764069, G loss: [3.943176031112671, 0.6705668568611145, 3.272609233856201]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 38/43 [D loss 1: 4.590491056442261, D loss 2: 0.6540825963020325, G loss: [3.936997890472412, 0.6696285009384155, 3.267369270324707]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 39/43 [D loss 1: 4.646396160125732, D loss 2: 0.6629370748996735, G loss: [3.941500663757324, 0.6688576936721802, 3.2726430892944336]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 40/43 [D loss 1: 4.612391948699951, D loss 2: 0.6805338263511658, G loss: [3.944650411605835, 0.6683120131492615, 3.2763383388519287]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 41/43 [D loss 1: 4.6187756061553955, D loss 2: 0.652235209941864, G loss: [3.9449386596679688, 0.669043242931366, 3.275895357131958]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 42/43 [D loss 1: 4.624312877655029, D loss 2: 0.6892585754394531, G loss: [3.9389195442199707, 0.6682676076889038, 3.2706518173217773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 43/43 [D loss 1: 4.645179510116577, D loss 2: 0.6914825439453125, G loss: [3.9379210472106934, 0.6700860857963562, 3.2678349018096924]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 1/43 [D loss 1: 4.597984075546265, D loss 2: 0.701659619808197, G loss: [3.9453446865081787, 0.6686552166938782, 3.2766895294189453]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 2/43 [D loss 1: 4.579174518585205, D loss 2: 0.6677500903606415, G loss: [3.9432485103607178, 0.6689612865447998, 3.274287223815918]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 3/43 [D loss 1: 4.581939220428467, D loss 2: 0.6974158585071564, G loss: [3.937408924102783, 0.66766357421875, 3.269745349884033]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 4/43 [D loss 1: 4.5834736824035645, D loss 2: 0.6845941841602325, G loss: [3.944072723388672, 0.667485237121582, 3.27658748626709]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 5/43 [D loss 1: 4.645793914794922, D loss 2: 0.7234008312225342, G loss: [3.9534480571746826, 0.6659178137779236, 3.2875301837921143]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 6/43 [D loss 1: 4.584415435791016, D loss 2: 0.7069717943668365, G loss: [3.944124460220337, 0.6671634316444397, 3.276961088180542]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 7/43 [D loss 1: 4.58344030380249, D loss 2: 0.7177419364452362, G loss: [3.9404287338256836, 0.6664404273033142, 3.2739882469177246]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 8/43 [D loss 1: 4.572600364685059, D loss 2: 0.6935344338417053, G loss: [3.94962215423584, 0.667811930179596, 3.2818102836608887]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 9/43 [D loss 1: 4.597766399383545, D loss 2: 0.7343233525753021, G loss: [3.9364285469055176, 0.6660501956939697, 3.270378351211548]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 10/43 [D loss 1: 4.608603477478027, D loss 2: 0.7501964271068573, G loss: [3.952569007873535, 0.6695734262466431, 3.2829957008361816]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 11/43 [D loss 1: 4.626301288604736, D loss 2: 0.7561599612236023, G loss: [3.9526877403259277, 0.6680288314819336, 3.284658908843994]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 12/43 [D loss 1: 4.655248165130615, D loss 2: 0.7565250992774963, G loss: [3.957913637161255, 0.6684708595275879, 3.289442777633667]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 13/43 [D loss 1: 4.609448194503784, D loss 2: 0.7700924277305603, G loss: [3.951084613800049, 0.6672946214675903, 3.283790111541748]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 14/43 [D loss 1: 4.61267876625061, D loss 2: 0.7561737596988678, G loss: [3.9475748538970947, 0.6670463681221008, 3.2805285453796387]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 15/43 [D loss 1: 4.597261428833008, D loss 2: 0.7862518429756165, G loss: [3.9510176181793213, 0.6689128279685974, 3.282104730606079]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 16/43 [D loss 1: 4.588409185409546, D loss 2: 0.7674822807312012, G loss: [3.962750196456909, 0.6708889603614807, 3.2918612957000732]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 17/43 [D loss 1: 4.698210954666138, D loss 2: 0.848526656627655, G loss: [3.955287456512451, 0.6687352061271667, 3.2865521907806396]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 18/43 [D loss 1: 4.660617113113403, D loss 2: 0.8248028755187988, G loss: [3.963026285171509, 0.6669012904167175, 3.2961249351501465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 19/43 [D loss 1: 4.606550693511963, D loss 2: 0.8136124908924103, G loss: [3.962890386581421, 0.6724390387535095, 3.2904512882232666]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 20/43 [D loss 1: 4.601467132568359, D loss 2: 0.8063062727451324, G loss: [3.9595818519592285, 0.6725807189941406, 3.287001132965088]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 21/43 [D loss 1: 4.6366119384765625, D loss 2: 0.8023217618465424, G loss: [3.950254440307617, 0.6695722341537476, 3.28068208694458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 22/43 [D loss 1: 4.628800392150879, D loss 2: 0.8323771357536316, G loss: [3.957864284515381, 0.6721450686454773, 3.285719156265259]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 23/43 [D loss 1: 4.642289876937866, D loss 2: 0.838407427072525, G loss: [3.9555206298828125, 0.6718036532402039, 3.283716917037964]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 24/43 [D loss 1: 4.634688854217529, D loss 2: 0.8098551034927368, G loss: [3.964282989501953, 0.67592853307724, 3.2883543968200684]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 25/43 [D loss 1: 4.621780872344971, D loss 2: 0.8352424502372742, G loss: [3.9588799476623535, 0.6727767586708069, 3.2861032485961914]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 26/43 [D loss 1: 4.6294779777526855, D loss 2: 0.8418627083301544, G loss: [3.960911273956299, 0.6703875064849854, 3.2905237674713135]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 27/43 [D loss 1: 4.624643564224243, D loss 2: 0.8348740637302399, G loss: [3.970545768737793, 0.6684086322784424, 3.3021371364593506]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 28/43 [D loss 1: 4.5855560302734375, D loss 2: 0.817063570022583, G loss: [3.965841770172119, 0.6706946492195129, 3.295147180557251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 29/43 [D loss 1: 4.601551532745361, D loss 2: 0.8117220401763916, G loss: [3.968998908996582, 0.6720642447471619, 3.2969346046447754]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 30/43 [D loss 1: 4.633988857269287, D loss 2: 0.8283922672271729, G loss: [3.962944507598877, 0.6735861897468567, 3.289358377456665]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 31/43 [D loss 1: 4.609807729721069, D loss 2: 0.8334704041481018, G loss: [3.970200538635254, 0.678292453289032, 3.291908025741577]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 32/43 [D loss 1: 4.5454089641571045, D loss 2: 0.8019493520259857, G loss: [3.9727156162261963, 0.6728635430335999, 3.299852132797241]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 33/43 [D loss 1: 4.517362594604492, D loss 2: 0.7709218859672546, G loss: [3.9726603031158447, 0.6790807247161865, 3.293579578399658]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 34/43 [D loss 1: 4.529233932495117, D loss 2: 0.7638784348964691, G loss: [3.9883501529693604, 0.6806706190109253, 3.3076794147491455]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 35/43 [D loss 1: 4.540825366973877, D loss 2: 0.7720483541488647, G loss: [3.977139711380005, 0.6801956295967102, 3.2969441413879395]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 36/43 [D loss 1: 4.467360973358154, D loss 2: 0.7484900057315826, G loss: [3.987091064453125, 0.6801263093948364, 3.306964635848999]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 37/43 [D loss 1: 4.485657215118408, D loss 2: 0.7356580197811127, G loss: [3.9873440265655518, 0.6792259812355042, 3.3081181049346924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 38/43 [D loss 1: 4.476463079452515, D loss 2: 0.7365510165691376, G loss: [3.9699597358703613, 0.679696798324585, 3.2902629375457764]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 39/43 [D loss 1: 4.434771537780762, D loss 2: 0.6938517093658447, G loss: [3.988783359527588, 0.6792235374450684, 3.3095595836639404]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 40/43 [D loss 1: 4.4037206172943115, D loss 2: 0.6993520855903625, G loss: [3.984093189239502, 0.6842818856239319, 3.299811363220215]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 41/43 [D loss 1: 4.43590235710144, D loss 2: 0.7036978304386139, G loss: [3.9820632934570312, 0.6792423129081726, 3.302820920944214]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 42/43 [D loss 1: 4.3978376388549805, D loss 2: 0.6854125261306763, G loss: [3.9773850440979004, 0.6827100515365601, 3.294674873352051]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 43/43 [D loss 1: 4.358840823173523, D loss 2: 0.6855015158653259, G loss: [3.979038715362549, 0.6781919598579407, 3.300846815109253]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 1/43 [D loss 1: 4.373663663864136, D loss 2: 0.6881555914878845, G loss: [3.982062816619873, 0.6766807436943054, 3.305382013320923]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 2/43 [D loss 1: 4.403443455696106, D loss 2: 0.7025763392448425, G loss: [3.9787449836730957, 0.6738671660423279, 3.304877758026123]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 3/43 [D loss 1: 4.419425964355469, D loss 2: 0.7092121839523315, G loss: [3.9917585849761963, 0.6821235418319702, 3.3096354007720947]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 4/43 [D loss 1: 4.352279424667358, D loss 2: 0.694986879825592, G loss: [3.987779140472412, 0.6702260971069336, 3.3175530433654785]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 5/43 [D loss 1: 4.417563199996948, D loss 2: 0.7165413498878479, G loss: [3.991927146911621, 0.6794511079788208, 3.31247615814209]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 6/43 [D loss 1: 4.38386070728302, D loss 2: 0.7518024444580078, G loss: [3.986557960510254, 0.6731730699539185, 3.313385009765625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 7/43 [D loss 1: 4.355570316314697, D loss 2: 0.7370861172676086, G loss: [3.9882564544677734, 0.679893970489502, 3.3083627223968506]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 8/43 [D loss 1: 4.42660927772522, D loss 2: 0.7927375435829163, G loss: [4.013779163360596, 0.6848878264427185, 3.3288915157318115]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 9/43 [D loss 1: 4.507183313369751, D loss 2: 0.8725006580352783, G loss: [4.006386756896973, 0.683926522731781, 3.322460412979126]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 10/43 [D loss 1: 4.49257755279541, D loss 2: 0.8451419770717621, G loss: [4.003791809082031, 0.68621826171875, 3.3175737857818604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 11/43 [D loss 1: 4.548160076141357, D loss 2: 0.9028001427650452, G loss: [4.001100540161133, 0.6878042221069336, 3.3132965564727783]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 12/43 [D loss 1: 4.503247380256653, D loss 2: 0.8942381739616394, G loss: [4.000831604003906, 0.6845834255218506, 3.3162479400634766]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 13/43 [D loss 1: 4.481887102127075, D loss 2: 0.8568393588066101, G loss: [4.0073089599609375, 0.6913287043571472, 3.3159804344177246]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 14/43 [D loss 1: 4.414548397064209, D loss 2: 0.8293868601322174, G loss: [3.9973106384277344, 0.6870320439338684, 3.31027889251709]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 15/43 [D loss 1: 4.449601411819458, D loss 2: 0.8373588919639587, G loss: [4.011338233947754, 0.6900577545166016, 3.3212802410125732]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 16/43 [D loss 1: 4.457760334014893, D loss 2: 0.8531308770179749, G loss: [4.014747619628906, 0.6886811256408691, 3.326066255569458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 17/43 [D loss 1: 4.458022356033325, D loss 2: 0.8590887188911438, G loss: [4.030500411987305, 0.6926407814025879, 3.337859630584717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 18/43 [D loss 1: 4.38360333442688, D loss 2: 0.8350542783737183, G loss: [4.0072221755981445, 0.6914150714874268, 3.3158071041107178]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 19/43 [D loss 1: 4.380320310592651, D loss 2: 0.8196249604225159, G loss: [4.010223865509033, 0.6889923214912415, 3.3212316036224365]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 20/43 [D loss 1: 4.396154880523682, D loss 2: 0.8562540709972382, G loss: [4.022914886474609, 0.6902017593383789, 3.3327128887176514]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 21/43 [D loss 1: 4.395838141441345, D loss 2: 0.8450175523757935, G loss: [4.006133079528809, 0.6833035349845886, 3.3228297233581543]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 22/43 [D loss 1: 4.3665231466293335, D loss 2: 0.8301506638526917, G loss: [4.012240409851074, 0.6902287006378174, 3.322011947631836]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 23/43 [D loss 1: 4.3612518310546875, D loss 2: 0.8410300612449646, G loss: [4.0261406898498535, 0.6928794980049133, 3.333261013031006]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 24/43 [D loss 1: 4.402194857597351, D loss 2: 0.8331001698970795, G loss: [4.020416736602783, 0.6915480494499207, 3.328868865966797]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 25/43 [D loss 1: 4.336475610733032, D loss 2: 0.8240707814693451, G loss: [4.0082573890686035, 0.6954164505004883, 3.3128409385681152]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 26/43 [D loss 1: 4.330551266670227, D loss 2: 0.8231146335601807, G loss: [4.016005516052246, 0.6950288414955139, 3.320976734161377]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 27/43 [D loss 1: 4.385509967803955, D loss 2: 0.8506339490413666, G loss: [4.037339210510254, 0.6960027813911438, 3.341336488723755]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 28/43 [D loss 1: 4.3781352043151855, D loss 2: 0.8672372400760651, G loss: [4.022403717041016, 0.6939507722854614, 3.3284528255462646]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 29/43 [D loss 1: 4.386341333389282, D loss 2: 0.8731017708778381, G loss: [4.022605895996094, 0.6875107288360596, 3.3350954055786133]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 30/43 [D loss 1: 4.352695941925049, D loss 2: 0.8586775064468384, G loss: [4.018411159515381, 0.6912834644317627, 3.327127695083618]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 31/43 [D loss 1: 4.398108720779419, D loss 2: 0.909452885389328, G loss: [4.005258560180664, 0.6878569722175598, 3.31740140914917]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 32/43 [D loss 1: 4.413248538970947, D loss 2: 0.9157793521881104, G loss: [4.003527641296387, 0.6815061569213867, 3.322021245956421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 33/43 [D loss 1: 4.3311299085617065, D loss 2: 0.8800538778305054, G loss: [4.011931419372559, 0.6869534850120544, 3.3249778747558594]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 34/43 [D loss 1: 4.442421197891235, D loss 2: 0.9128846824169159, G loss: [4.014307498931885, 0.6801998019218445, 3.3341076374053955]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 35/43 [D loss 1: 4.366341829299927, D loss 2: 0.8969854116439819, G loss: [3.999386787414551, 0.6763184070587158, 3.323068380355835]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 36/43 [D loss 1: 4.381929397583008, D loss 2: 0.9119778573513031, G loss: [4.0034942626953125, 0.6789329051971436, 3.324561357498169]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 37/43 [D loss 1: 4.364712595939636, D loss 2: 0.9127121269702911, G loss: [4.003266334533691, 0.6680881977081299, 3.3351778984069824]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 38/43 [D loss 1: 4.365118861198425, D loss 2: 0.9498719274997711, G loss: [3.984997272491455, 0.6691372990608215, 3.315859794616699]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 39/43 [D loss 1: 4.39783239364624, D loss 2: 0.9451631307601929, G loss: [3.9955902099609375, 0.6746546030044556, 3.3209354877471924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 40/43 [D loss 1: 4.351737141609192, D loss 2: 0.914695680141449, G loss: [4.000185966491699, 0.674048125743866, 3.3261380195617676]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 41/43 [D loss 1: 4.33185076713562, D loss 2: 0.9114388525485992, G loss: [4.015169143676758, 0.6763516664505005, 3.3388173580169678]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 42/43 [D loss 1: 4.293276309967041, D loss 2: 0.8884410858154297, G loss: [3.9980099201202393, 0.6795349717140198, 3.318474769592285]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 43/43 [D loss 1: 4.231067419052124, D loss 2: 0.8533658981323242, G loss: [4.005630970001221, 0.6788656115531921, 3.326765298843384]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 1/43 [D loss 1: 4.2544344663619995, D loss 2: 0.8423746824264526, G loss: [3.9945125579833984, 0.6736536026000977, 3.320858955383301]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 2/43 [D loss 1: 4.22951340675354, D loss 2: 0.8360477685928345, G loss: [4.008101940155029, 0.681879997253418, 3.3262219429016113]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 3/43 [D loss 1: 4.194126129150391, D loss 2: 0.8338815569877625, G loss: [4.005125045776367, 0.6776587963104248, 3.3274664878845215]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 4/43 [D loss 1: 4.202982664108276, D loss 2: 0.8158175647258759, G loss: [4.001763343811035, 0.6714491844177246, 3.3303139209747314]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 5/43 [D loss 1: 4.123382329940796, D loss 2: 0.7920558750629425, G loss: [3.9997758865356445, 0.6724591255187988, 3.3273165225982666]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 6/43 [D loss 1: 4.135753750801086, D loss 2: 0.8169188797473907, G loss: [3.982945442199707, 0.6693977117538452, 3.3135478496551514]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 7/43 [D loss 1: 4.116321325302124, D loss 2: 0.7963284552097321, G loss: [3.985835552215576, 0.6728723049163818, 3.3129634857177734]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 8/43 [D loss 1: 4.065392374992371, D loss 2: 0.781928539276123, G loss: [3.9861092567443848, 0.6733179092407227, 3.312791109085083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 9/43 [D loss 1: 4.046787977218628, D loss 2: 0.7852844595909119, G loss: [3.976653575897217, 0.6752040982246399, 3.3014495372772217]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 10/43 [D loss 1: 4.047045946121216, D loss 2: 0.8046004772186279, G loss: [3.977041721343994, 0.671019971370697, 3.3060216903686523]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 11/43 [D loss 1: 4.036361575126648, D loss 2: 0.8049847185611725, G loss: [3.961948871612549, 0.6651324033737183, 3.296816349029541]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 12/43 [D loss 1: 3.979491949081421, D loss 2: 0.8283074498176575, G loss: [3.9838504791259766, 0.6675645112991333, 3.316286087036133]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 13/43 [D loss 1: 3.9754912853240967, D loss 2: 0.8024247288703918, G loss: [3.949127674102783, 0.6673444509506226, 3.281783103942871]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 14/43 [D loss 1: 3.914710760116577, D loss 2: 0.7863702178001404, G loss: [3.970275402069092, 0.6744281053543091, 3.2958474159240723]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 15/43 [D loss 1: 3.884441614151001, D loss 2: 0.7769342660903931, G loss: [3.932698965072632, 0.6671295166015625, 3.2655694484710693]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 16/43 [D loss 1: 3.9387084245681763, D loss 2: 0.8057944774627686, G loss: [3.9327564239501953, 0.6572967767715454, 3.2754595279693604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 17/43 [D loss 1: 3.7908931970596313, D loss 2: 0.7412859797477722, G loss: [3.922395706176758, 0.6738914251327515, 3.248504161834717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 18/43 [D loss 1: 3.7801620960235596, D loss 2: 0.7911359369754791, G loss: [3.9238786697387695, 0.6540050506591797, 3.26987361907959]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 19/43 [D loss 1: 3.7879180908203125, D loss 2: 0.8015595078468323, G loss: [3.8688724040985107, 0.6511021256446838, 3.2177703380584717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 20/43 [D loss 1: 3.70202100276947, D loss 2: 0.7759096026420593, G loss: [3.8678340911865234, 0.6456192135810852, 3.222214937210083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 21/43 [D loss 1: 3.663770079612732, D loss 2: 0.7673473954200745, G loss: [3.8712456226348877, 0.6573376655578613, 3.2139079570770264]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 22/43 [D loss 1: 3.7008044719696045, D loss 2: 0.8393977880477905, G loss: [3.836562156677246, 0.6461362838745117, 3.1904258728027344]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 23/43 [D loss 1: 3.59471595287323, D loss 2: 0.7414770424365997, G loss: [3.785770893096924, 0.6547909379005432, 3.1309800148010254]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 24/43 [D loss 1: 3.5222641229629517, D loss 2: 0.8102940618991852, G loss: [3.785726547241211, 0.6424347758293152, 3.143291711807251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 25/43 [D loss 1: 3.508444309234619, D loss 2: 0.7803544998168945, G loss: [3.7502353191375732, 0.6570368409156799, 3.093198537826538]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 26/43 [D loss 1: 3.478851079940796, D loss 2: 0.8280772268772125, G loss: [3.6937661170959473, 0.6353945732116699, 3.0583715438842773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 27/43 [D loss 1: 3.333854556083679, D loss 2: 0.7399417161941528, G loss: [3.6775259971618652, 0.6452388167381287, 3.032287120819092]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 28/43 [D loss 1: 3.3328038454055786, D loss 2: 0.8293476104736328, G loss: [3.6424508094787598, 0.6392431259155273, 3.0032076835632324]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 29/43 [D loss 1: 3.3354848623275757, D loss 2: 0.717313677072525, G loss: [3.635542392730713, 0.6579939723014832, 2.977548360824585]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 30/43 [D loss 1: 3.271485686302185, D loss 2: 0.7788122594356537, G loss: [3.573418140411377, 0.6395107507705688, 2.9339072704315186]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 31/43 [D loss 1: 3.2074555158615112, D loss 2: 0.7717618048191071, G loss: [3.5301544666290283, 0.6399485468864441, 2.8902058601379395]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 32/43 [D loss 1: 3.218937873840332, D loss 2: 0.7458419501781464, G loss: [3.4901304244995117, 0.6412101984024048, 2.8489201068878174]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 33/43 [D loss 1: 3.1236408948898315, D loss 2: 0.7819456458091736, G loss: [3.4287631511688232, 0.6266705989837646, 2.8020925521850586]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 34/43 [D loss 1: 3.0740723609924316, D loss 2: 0.7643399238586426, G loss: [3.4031777381896973, 0.6418179273605347, 2.761359930038452]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 35/43 [D loss 1: 3.055077910423279, D loss 2: 0.6977293789386749, G loss: [3.3267462253570557, 0.6374773979187012, 2.6892688274383545]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 36/43 [D loss 1: 3.0500491857528687, D loss 2: 0.7147528827190399, G loss: [3.2994890213012695, 0.6342195868492126, 2.665269374847412]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 37/43 [D loss 1: 2.989657998085022, D loss 2: 0.7521062195301056, G loss: [3.2366950511932373, 0.631417453289032, 2.6052775382995605]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 38/43 [D loss 1: 2.974358558654785, D loss 2: 0.6981134116649628, G loss: [3.191150188446045, 0.6245728731155396, 2.566577434539795]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 39/43 [D loss 1: 2.8904061317443848, D loss 2: 0.7682191431522369, G loss: [3.1419436931610107, 0.6206738948822021, 2.5212697982788086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 40/43 [D loss 1: 2.891229033470154, D loss 2: 0.7198057174682617, G loss: [3.094831943511963, 0.6165229082107544, 2.478309154510498]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 41/43 [D loss 1: 2.81144917011261, D loss 2: 0.7267067730426788, G loss: [3.0592000484466553, 0.622072160243988, 2.4371278285980225]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 42/43 [D loss 1: 2.7979131937026978, D loss 2: 0.6869189739227295, G loss: [3.032911539077759, 0.631696879863739, 2.401214599609375]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 43/43 [D loss 1: 2.735121726989746, D loss 2: 0.7754694521427155, G loss: [2.992255926132202, 0.6184570789337158, 2.3737988471984863]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 1/43 [D loss 1: 2.755034923553467, D loss 2: 0.7661198079586029, G loss: [2.9822981357574463, 0.6309349536895752, 2.351363182067871]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 2/43 [D loss 1: 2.7023967504501343, D loss 2: 0.7033170163631439, G loss: [2.931335926055908, 0.6256457567214966, 2.305690288543701]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 3/43 [D loss 1: 2.6658767461776733, D loss 2: 0.752815455198288, G loss: [2.8894617557525635, 0.6272035241127014, 2.262258291244507]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 4/43 [D loss 1: 2.6844273805618286, D loss 2: 0.7524745464324951, G loss: [2.8407530784606934, 0.6365647912025452, 2.204188346862793]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 5/43 [D loss 1: 2.663143038749695, D loss 2: 0.8152001202106476, G loss: [2.8338828086853027, 0.628180980682373, 2.2057018280029297]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 6/43 [D loss 1: 2.6321370601654053, D loss 2: 0.7029477059841156, G loss: [2.8074660301208496, 0.6554361581802368, 2.1520297527313232]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 7/43 [D loss 1: 2.6002215147018433, D loss 2: 0.7620640993118286, G loss: [2.790310859680176, 0.638214111328125, 2.152096748352051]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 8/43 [D loss 1: 2.605883479118347, D loss 2: 0.6425932347774506, G loss: [2.7478089332580566, 0.6481643915176392, 2.099644422531128]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 9/43 [D loss 1: 2.6175973415374756, D loss 2: 0.8711031675338745, G loss: [2.7134106159210205, 0.6233005523681641, 2.0901100635528564]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 10/43 [D loss 1: 2.6261366605758667, D loss 2: 0.5791057199239731, G loss: [2.663585901260376, 0.6509396433830261, 2.012646198272705]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 11/43 [D loss 1: 2.6338138580322266, D loss 2: 0.9201488792896271, G loss: [2.658010244369507, 0.6282238364219666, 2.0297863483428955]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 12/43 [D loss 1: 2.4492355585098267, D loss 2: 0.6536107957363129, G loss: [2.622779369354248, 0.6564117670059204, 1.966367483139038]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 13/43 [D loss 1: 2.489737391471863, D loss 2: 0.6646066606044769, G loss: [2.56148624420166, 0.6376429200172424, 1.923843264579773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 14/43 [D loss 1: 2.4673041105270386, D loss 2: 0.8554222881793976, G loss: [2.5178284645080566, 0.6280264854431152, 1.889802098274231]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 15/43 [D loss 1: 2.5013532638549805, D loss 2: 0.6249277293682098, G loss: [2.4848787784576416, 0.6559663414955139, 1.8289124965667725]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 16/43 [D loss 1: 2.439959764480591, D loss 2: 0.8060529232025146, G loss: [2.4653944969177246, 0.6303697824478149, 1.8350247144699097]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 17/43 [D loss 1: 2.422618865966797, D loss 2: 0.6683896481990814, G loss: [2.412339687347412, 0.6435807943344116, 1.768758773803711]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 18/43 [D loss 1: 2.4273314476013184, D loss 2: 0.816843569278717, G loss: [2.4112377166748047, 0.6252626776695251, 1.7859749794006348]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 19/43 [D loss 1: 2.4145073890686035, D loss 2: 0.6386728286743164, G loss: [2.3908884525299072, 0.653020977973938, 1.7378674745559692]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 20/43 [D loss 1: 2.384902060031891, D loss 2: 0.815075010061264, G loss: [2.3499202728271484, 0.6314993500709534, 1.7184208631515503]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 21/43 [D loss 1: 2.3364514112472534, D loss 2: 0.698426365852356, G loss: [2.3258273601531982, 0.6533212661743164, 1.6725060939788818]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 22/43 [D loss 1: 2.314793348312378, D loss 2: 0.7206433415412903, G loss: [2.2730860710144043, 0.6440277695655823, 1.6290583610534668]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 23/43 [D loss 1: 2.378523051738739, D loss 2: 0.7817190885543823, G loss: [2.2679481506347656, 0.636906623840332, 1.6310416460037231]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 24/43 [D loss 1: 2.2996007204055786, D loss 2: 0.6912529170513153, G loss: [2.242372751235962, 0.6431754231452942, 1.5991973876953125]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 25/43 [D loss 1: 2.2903682589530945, D loss 2: 0.832411527633667, G loss: [2.2138047218322754, 0.6429235339164734, 1.5708811283111572]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 26/43 [D loss 1: 2.286483407020569, D loss 2: 0.6985282003879547, G loss: [2.1754159927368164, 0.6598260402679443, 1.5155900716781616]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 27/43 [D loss 1: 2.291093111038208, D loss 2: 0.7494343221187592, G loss: [2.1464526653289795, 0.6490013003349304, 1.4974514245986938]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 28/43 [D loss 1: 2.269552707672119, D loss 2: 0.7771843373775482, G loss: [2.143981695175171, 0.6393894553184509, 1.5045922994613647]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 29/43 [D loss 1: 2.220482110977173, D loss 2: 0.7255133986473083, G loss: [2.0964949131011963, 0.6529226899147034, 1.4435722827911377]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 30/43 [D loss 1: 2.261390447616577, D loss 2: 0.7923220098018646, G loss: [2.0895581245422363, 0.6498252749443054, 1.4397327899932861]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 31/43 [D loss 1: 2.2039144039154053, D loss 2: 0.6810895800590515, G loss: [2.070728302001953, 0.6596585512161255, 1.4110697507858276]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 32/43 [D loss 1: 2.202182948589325, D loss 2: 0.7894690930843353, G loss: [2.0378518104553223, 0.6482738852500916, 1.389577865600586]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 33/43 [D loss 1: 2.196757435798645, D loss 2: 0.6816384792327881, G loss: [2.008333921432495, 0.6652175188064575, 1.3431164026260376]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 34/43 [D loss 1: 2.198372006416321, D loss 2: 0.8023935556411743, G loss: [2.008026599884033, 0.6559169888496399, 1.3521095514297485]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 35/43 [D loss 1: 2.1988377571105957, D loss 2: 0.6034373939037323, G loss: [1.961071491241455, 0.6614496111869812, 1.2996219396591187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 36/43 [D loss 1: 2.2936493158340454, D loss 2: 0.9359251856803894, G loss: [1.9882441759109497, 0.6355146169662476, 1.3527295589447021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 37/43 [D loss 1: 2.471311569213867, D loss 2: 0.4759421795606613, G loss: [1.9480582475662231, 0.6969118118286133, 1.2511464357376099]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 38/43 [D loss 1: 2.3771783113479614, D loss 2: 0.9619829654693604, G loss: [1.9217839241027832, 0.6059336066246033, 1.3158502578735352]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 39/43 [D loss 1: 2.118688225746155, D loss 2: 0.6642458140850067, G loss: [1.871199607849121, 0.6609563827514648, 1.2102432250976562]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 40/43 [D loss 1: 2.1453664302825928, D loss 2: 0.7169196605682373, G loss: [1.8761515617370605, 0.6666943430900574, 1.209457278251648]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 41/43 [D loss 1: 2.142163634300232, D loss 2: 0.8362297117710114, G loss: [1.8077304363250732, 0.6391295194625854, 1.1686009168624878]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 42/43 [D loss 1: 2.0916590690612793, D loss 2: 0.7555460929870605, G loss: [1.8164211511611938, 0.6513164043426514, 1.1651047468185425]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 43/43 [D loss 1: 2.1004804372787476, D loss 2: 0.7250344455242157, G loss: [1.7737102508544922, 0.6537391543388367, 1.1199711561203003]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 1/43 [D loss 1: 2.087941527366638, D loss 2: 0.7722101509571075, G loss: [1.7594428062438965, 0.6448076963424683, 1.1146351099014282]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 2/43 [D loss 1: 2.109066963195801, D loss 2: 0.7346806228160858, G loss: [1.7305200099945068, 0.6673351526260376, 1.0631848573684692]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 3/43 [D loss 1: 2.07627671957016, D loss 2: 0.7909389734268188, G loss: [1.703438401222229, 0.6511873006820679, 1.0522511005401611]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 4/43 [D loss 1: 2.0784939527511597, D loss 2: 0.7516616582870483, G loss: [1.6877415180206299, 0.6683340072631836, 1.0194075107574463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 5/43 [D loss 1: 2.0500649213790894, D loss 2: 0.7677445411682129, G loss: [1.6870489120483398, 0.6667147278785706, 1.020334243774414]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 6/43 [D loss 1: 2.0638004541397095, D loss 2: 0.7694892585277557, G loss: [1.663046956062317, 0.6573113203048706, 1.0057356357574463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 7/43 [D loss 1: 2.0782466530799866, D loss 2: 0.6754185259342194, G loss: [1.6332485675811768, 0.6503310203552246, 0.9829175472259521]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 8/43 [D loss 1: 2.075573980808258, D loss 2: 0.8036783039569855, G loss: [1.606102705001831, 0.6427164077758789, 0.9633862972259521]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 9/43 [D loss 1: 2.0428569316864014, D loss 2: 0.7208528816699982, G loss: [1.617969274520874, 0.6611344814300537, 0.9568347930908203]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 10/43 [D loss 1: 2.011373519897461, D loss 2: 0.7023442685604095, G loss: [1.5642205476760864, 0.6483995914459229, 0.9158209562301636]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 11/43 [D loss 1: 2.074004113674164, D loss 2: 0.8214608728885651, G loss: [1.5454511642456055, 0.6439377069473267, 0.9015134572982788]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 12/43 [D loss 1: 2.001237213611603, D loss 2: 0.7424117922782898, G loss: [1.5381132364273071, 0.6578961610794067, 0.8802170753479004]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 13/43 [D loss 1: 1.9562379121780396, D loss 2: 0.7705677449703217, G loss: [1.534883975982666, 0.6410576105117798, 0.8938263654708862]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 14/43 [D loss 1: 2.015584707260132, D loss 2: 0.669035941362381, G loss: [1.5110775232315063, 0.6452387571334839, 0.8658387660980225]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 15/43 [D loss 1: 2.0010223388671875, D loss 2: 0.7988221049308777, G loss: [1.481903314590454, 0.6333471536636353, 0.8485561013221741]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 16/43 [D loss 1: 2.0103918313980103, D loss 2: 0.7931253015995026, G loss: [1.4678070545196533, 0.6391561031341553, 0.828650951385498]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 17/43 [D loss 1: 2.0332785844802856, D loss 2: 0.6630061864852905, G loss: [1.4517033100128174, 0.6586966514587402, 0.7930067181587219]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 18/43 [D loss 1: 2.07503879070282, D loss 2: 0.8686671257019043, G loss: [1.434328317642212, 0.6251195669174194, 0.8092087507247925]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 19/43 [D loss 1: 1.935235619544983, D loss 2: 0.6791030466556549, G loss: [1.4262704849243164, 0.6522660255432129, 0.7740045189857483]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 20/43 [D loss 1: 1.9700654745101929, D loss 2: 0.840702086687088, G loss: [1.4121043682098389, 0.6336601972579956, 0.7784441709518433]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 21/43 [D loss 1: 1.9416512250900269, D loss 2: 0.7059975862503052, G loss: [1.4203767776489258, 0.6572256088256836, 0.763151228427887]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 22/43 [D loss 1: 2.011933922767639, D loss 2: 0.7477594614028931, G loss: [1.3943777084350586, 0.64838045835495, 0.7459972500801086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 23/43 [D loss 1: 1.9619499444961548, D loss 2: 0.7749089598655701, G loss: [1.3890873193740845, 0.6549704074859619, 0.7341169118881226]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 24/43 [D loss 1: 1.9795133471488953, D loss 2: 0.8039898872375488, G loss: [1.360194444656372, 0.6486713886260986, 0.7115231156349182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 25/43 [D loss 1: 1.9512619972229004, D loss 2: 0.6986000835895538, G loss: [1.3371374607086182, 0.6526902914047241, 0.6844472289085388]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 26/43 [D loss 1: 1.9852479696273804, D loss 2: 0.8284621834754944, G loss: [1.3223621845245361, 0.6227440237998962, 0.6996181607246399]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 27/43 [D loss 1: 1.9358327388763428, D loss 2: 0.6525574922561646, G loss: [1.3196520805358887, 0.6551591157913208, 0.6644929647445679]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 28/43 [D loss 1: 1.9849010705947876, D loss 2: 0.8573966324329376, G loss: [1.2851142883300781, 0.6406645178794861, 0.6444498300552368]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 29/43 [D loss 1: 1.8794505596160889, D loss 2: 0.7131164968013763, G loss: [1.2937943935394287, 0.6560041904449463, 0.6377901434898376]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 30/43 [D loss 1: 1.9513424634933472, D loss 2: 0.7453084588050842, G loss: [1.2566444873809814, 0.6341657042503357, 0.6224787831306458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 31/43 [D loss 1: 1.9387798309326172, D loss 2: 0.7634325325489044, G loss: [1.26856529712677, 0.640305757522583, 0.628259539604187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 32/43 [D loss 1: 1.9528908729553223, D loss 2: 0.8041618764400482, G loss: [1.2508342266082764, 0.6403567790985107, 0.6104775071144104]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 33/43 [D loss 1: 1.9053893089294434, D loss 2: 0.7001895010471344, G loss: [1.2327682971954346, 0.6512759327888489, 0.5814924240112305]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 34/43 [D loss 1: 1.9853414297103882, D loss 2: 0.8561705350875854, G loss: [1.2288211584091187, 0.6195201277732849, 0.6093010306358337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 35/43 [D loss 1: 1.9188839793205261, D loss 2: 0.60556660592556, G loss: [1.2253494262695312, 0.6409163475036621, 0.5844331383705139]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 36/43 [D loss 1: 1.977709174156189, D loss 2: 0.901260495185852, G loss: [1.2137279510498047, 0.6264976859092712, 0.5872302651405334]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 37/43 [D loss 1: 1.8851234316825867, D loss 2: 0.6546306312084198, G loss: [1.1899240016937256, 0.6512103080749512, 0.5387137532234192]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 38/43 [D loss 1: 1.8881497979164124, D loss 2: 0.7921743392944336, G loss: [1.1615557670593262, 0.6192752122879028, 0.5422805547714233]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 39/43 [D loss 1: 1.8847246170043945, D loss 2: 0.7592438459396362, G loss: [1.172060251235962, 0.64235919713974, 0.5297011137008667]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 40/43 [D loss 1: 1.9524157047271729, D loss 2: 0.780724048614502, G loss: [1.135827660560608, 0.6231492161750793, 0.5126784443855286]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 41/43 [D loss 1: 1.9293287992477417, D loss 2: 0.8068923652172089, G loss: [1.1359665393829346, 0.6113203167915344, 0.5246461629867554]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 42/43 [D loss 1: 1.8975951671600342, D loss 2: 0.7949745953083038, G loss: [1.1493972539901733, 0.6309219002723694, 0.518475353717804]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 43/43 [D loss 1: 1.9079289436340332, D loss 2: 0.750560849905014, G loss: [1.1051846742630005, 0.6279304027557373, 0.4772542715072632]]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 1/43 [D loss 1: 1.8669191598892212, D loss 2: 0.8055874407291412, G loss: [1.1135352849960327, 0.6272737383842468, 0.4862615764141083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 2/43 [D loss 1: 1.8670032024383545, D loss 2: 0.7396155297756195, G loss: [1.1182620525360107, 0.6419243216514587, 0.4763377010822296]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 3/43 [D loss 1: 1.8823598623275757, D loss 2: 0.8060621321201324, G loss: [1.097130298614502, 0.6216424703598022, 0.4754878878593445]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 4/43 [D loss 1: 1.8400911688804626, D loss 2: 0.7529380023479462, G loss: [1.0820233821868896, 0.6242490410804749, 0.45777428150177]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 5/43 [D loss 1: 1.87808758020401, D loss 2: 0.7690450251102448, G loss: [1.0818312168121338, 0.634959876537323, 0.44687139987945557]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 6/43 [D loss 1: 1.9230499863624573, D loss 2: 0.7870380580425262, G loss: [1.0580816268920898, 0.6280429363250732, 0.4300386607646942]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 7/43 [D loss 1: 1.8533477187156677, D loss 2: 0.7479755282402039, G loss: [1.058266282081604, 0.6194839477539062, 0.43878233432769775]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 8/43 [D loss 1: 1.8351450562477112, D loss 2: 0.7199176847934723, G loss: [1.0757439136505127, 0.6488706469535828, 0.4268733263015747]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 9/43 [D loss 1: 1.8422815203666687, D loss 2: 0.7131587564945221, G loss: [1.034024953842163, 0.6186196804046631, 0.4154052138328552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 10/43 [D loss 1: 1.9484126567840576, D loss 2: 0.8767369091510773, G loss: [1.0236454010009766, 0.617771327495575, 0.40587401390075684]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 11/43 [D loss 1: 1.835508018732071, D loss 2: 0.7612493634223938, G loss: [1.0206931829452515, 0.6198238134384155, 0.40086936950683594]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 12/43 [D loss 1: 1.8466260433197021, D loss 2: 0.7453805208206177, G loss: [1.034809947013855, 0.6323552131652832, 0.40245476365089417]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 13/43 [D loss 1: 1.8328603506088257, D loss 2: 0.7527886629104614, G loss: [1.0344831943511963, 0.6418571472167969, 0.392626017332077]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 14/43 [D loss 1: 1.8733252882957458, D loss 2: 0.8175574541091919, G loss: [1.0085660219192505, 0.6225773096084595, 0.3859887421131134]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 15/43 [D loss 1: 1.8121511936187744, D loss 2: 0.7358048260211945, G loss: [1.0036147832870483, 0.6262210011482239, 0.3773937523365021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 16/43 [D loss 1: 1.8851070404052734, D loss 2: 0.7920580208301544, G loss: [0.9927977323532104, 0.62924724817276, 0.36355048418045044]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 17/43 [D loss 1: 1.9110975861549377, D loss 2: 0.8938256204128265, G loss: [1.00752592086792, 0.6377182602882385, 0.36980772018432617]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 18/43 [D loss 1: 1.858847737312317, D loss 2: 0.6046361476182938, G loss: [0.9918704032897949, 0.6414027810096741, 0.35046759247779846]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 19/43 [D loss 1: 1.9685584902763367, D loss 2: 0.9413855373859406, G loss: [0.9860180616378784, 0.6094540953636169, 0.37656399607658386]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 20/43 [D loss 1: 1.7964401245117188, D loss 2: 0.6249818652868271, G loss: [0.9875273704528809, 0.6610220670700073, 0.3265053331851959]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 21/43 [D loss 1: 1.8725274205207825, D loss 2: 0.8308491408824921, G loss: [0.9710283875465393, 0.6396497488021851, 0.33137863874435425]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 22/43 [D loss 1: 1.8938166499137878, D loss 2: 0.8123546540737152, G loss: [0.9327107667922974, 0.6139203906059265, 0.31879037618637085]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 23/43 [D loss 1: 1.8655627369880676, D loss 2: 0.7979000508785248, G loss: [0.9467611312866211, 0.6278207898139954, 0.31894031167030334]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 24/43 [D loss 1: 1.8431596159934998, D loss 2: 0.8254736065864563, G loss: [0.9345020651817322, 0.619499921798706, 0.3150021433830261]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 25/43 [D loss 1: 1.8932576179504395, D loss 2: 0.887678474187851, G loss: [0.9563455581665039, 0.6417316198348999, 0.3146139681339264]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 26/43 [D loss 1: 1.837461918592453, D loss 2: 0.7630529701709747, G loss: [0.9448081851005554, 0.6427837014198303, 0.3020244836807251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 27/43 [D loss 1: 1.8428005874156952, D loss 2: 0.7934383451938629, G loss: [0.9237241744995117, 0.6370565891265869, 0.2866675853729248]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 28/43 [D loss 1: 1.8789138793945312, D loss 2: 0.8243609964847565, G loss: [0.9337028861045837, 0.6376146674156189, 0.29608821868896484]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 29/43 [D loss 1: 1.7938556969165802, D loss 2: 0.7588140070438385, G loss: [0.9454946517944336, 0.657065212726593, 0.28842946887016296]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 30/43 [D loss 1: 1.9070464968681335, D loss 2: 0.8739480078220367, G loss: [0.9487965703010559, 0.6509450078010559, 0.2978515625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 31/43 [D loss 1: 1.7879997491836548, D loss 2: 0.7239483594894409, G loss: [0.9332796335220337, 0.6529651880264282, 0.2803144156932831]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 32/43 [D loss 1: 1.8311635851860046, D loss 2: 0.8248659372329712, G loss: [0.9463552832603455, 0.667258083820343, 0.27909719944000244]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 33/43 [D loss 1: 1.7949832677841187, D loss 2: 0.7236174643039703, G loss: [0.9531057476997375, 0.6735645532608032, 0.2795411944389343]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 34/43 [D loss 1: 1.752824455499649, D loss 2: 0.7265817821025848, G loss: [0.9270728230476379, 0.6517965793609619, 0.275276243686676]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 35/43 [D loss 1: 1.9262471795082092, D loss 2: 0.9001048505306244, G loss: [0.9326790571212769, 0.656324565410614, 0.27635452151298523]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 36/43 [D loss 1: 1.7981088161468506, D loss 2: 0.6728216111660004, G loss: [0.9062274694442749, 0.6412028074264526, 0.26502469182014465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 37/43 [D loss 1: 1.936017394065857, D loss 2: 0.9399389624595642, G loss: [0.9202841520309448, 0.6459618210792542, 0.2743223309516907]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 38/43 [D loss 1: 1.7695072889328003, D loss 2: 0.6852979958057404, G loss: [0.9065102338790894, 0.6551553010940552, 0.25135496258735657]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 39/43 [D loss 1: 1.8798733949661255, D loss 2: 0.8411318063735962, G loss: [0.900032639503479, 0.6432381272315979, 0.2567945122718811]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 40/43 [D loss 1: 1.846584975719452, D loss 2: 0.8309854567050934, G loss: [0.8842452168464661, 0.6456192135810852, 0.23862600326538086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 41/43 [D loss 1: 1.8260865807533264, D loss 2: 0.802596390247345, G loss: [0.8873146176338196, 0.6505671739578247, 0.23674745857715607]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 42/43 [D loss 1: 1.7809830904006958, D loss 2: 0.7362532317638397, G loss: [0.8957158327102661, 0.6655123233795166, 0.2302035242319107]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 43/43 [D loss 1: 1.8091145753860474, D loss 2: 0.8274595737457275, G loss: [0.8917636871337891, 0.6483993530273438, 0.24336430430412292]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 1/43 [D loss 1: 1.8402597308158875, D loss 2: 0.8458036482334137, G loss: [0.8914787769317627, 0.6505990028381348, 0.24087980389595032]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 2/43 [D loss 1: 1.7556453049182892, D loss 2: 0.7712204158306122, G loss: [0.8921387791633606, 0.66293865442276, 0.22920013964176178]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 3/43 [D loss 1: 1.8349778056144714, D loss 2: 0.841178834438324, G loss: [0.8898292779922485, 0.6637805104255676, 0.2260487824678421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 4/43 [D loss 1: 1.7897595763206482, D loss 2: 0.7582050263881683, G loss: [0.8883450031280518, 0.6594656109809875, 0.22887936234474182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 5/43 [D loss 1: 1.9084312915802002, D loss 2: 0.8912112414836884, G loss: [0.8727648854255676, 0.6543617844581604, 0.21840311586856842]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 6/43 [D loss 1: 1.7737368941307068, D loss 2: 0.7752673327922821, G loss: [0.8872854709625244, 0.6753373146057129, 0.21194817125797272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 7/43 [D loss 1: 1.7713741958141327, D loss 2: 0.8175169229507446, G loss: [0.8793627619743347, 0.6665592193603516, 0.21280355751514435]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 8/43 [D loss 1: 1.7673170864582062, D loss 2: 0.7780359983444214, G loss: [0.8703840970993042, 0.6614108085632324, 0.20897327363491058]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 9/43 [D loss 1: 1.8105882108211517, D loss 2: 0.8177607655525208, G loss: [0.8741974234580994, 0.6729714274406433, 0.20122598111629486]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 10/43 [D loss 1: 1.7667303681373596, D loss 2: 0.7933732867240906, G loss: [0.8682281970977783, 0.658452570438385, 0.20977561175823212]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 11/43 [D loss 1: 1.7549498081207275, D loss 2: 0.7638983130455017, G loss: [0.8749902248382568, 0.6756875514984131, 0.19930267333984375]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 12/43 [D loss 1: 1.83994922041893, D loss 2: 0.833269476890564, G loss: [0.8647435903549194, 0.666658878326416, 0.19808469712734222]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 13/43 [D loss 1: 1.7205768823623657, D loss 2: 0.7222530245780945, G loss: [0.8619996309280396, 0.6603760123252869, 0.2016235888004303]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 14/43 [D loss 1: 1.8455491065979004, D loss 2: 0.8923244178295135, G loss: [0.8601408004760742, 0.6645958423614502, 0.19554492831230164]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 15/43 [D loss 1: 1.747382402420044, D loss 2: 0.6255773603916168, G loss: [0.8608014583587646, 0.659740686416626, 0.20106074213981628]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 16/43 [D loss 1: 1.8653151988983154, D loss 2: 0.8759492933750153, G loss: [0.8849194049835205, 0.6997594237327576, 0.18515998125076294]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 17/43 [D loss 1: 1.7927760481834412, D loss 2: 0.8257601857185364, G loss: [0.8609423041343689, 0.6717896461486816, 0.18915265798568726]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 18/43 [D loss 1: 1.7393414080142975, D loss 2: 0.6848233342170715, G loss: [0.8595100045204163, 0.6668857932090759, 0.19262421131134033]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 19/43 [D loss 1: 1.9176040291786194, D loss 2: 0.9153762459754944, G loss: [0.8762953877449036, 0.681711733341217, 0.19458366930484772]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 20/43 [D loss 1: 1.7428885102272034, D loss 2: 0.74803626537323, G loss: [0.8415921926498413, 0.6638088822364807, 0.1777832806110382]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 21/43 [D loss 1: 1.6986794769763947, D loss 2: 0.6771332025527954, G loss: [0.850959837436676, 0.6654634475708008, 0.18549638986587524]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 22/43 [D loss 1: 1.852162778377533, D loss 2: 0.823159784078598, G loss: [0.82225102186203, 0.6543596982955933, 0.16789130866527557]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 23/43 [D loss 1: 1.796493798494339, D loss 2: 0.7524102330207825, G loss: [0.8486813306808472, 0.6724742650985718, 0.17620709538459778]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 24/43 [D loss 1: 1.699842095375061, D loss 2: 0.743010550737381, G loss: [0.8451269865036011, 0.6636618375778198, 0.18146516382694244]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 25/43 [D loss 1: 1.7342790365219116, D loss 2: 0.7936651110649109, G loss: [0.8234867453575134, 0.6572531461715698, 0.1662335991859436]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 26/43 [D loss 1: 1.7496678531169891, D loss 2: 0.7473793923854828, G loss: [0.8327677249908447, 0.6697139739990234, 0.1630537509918213]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 27/43 [D loss 1: 1.6816373467445374, D loss 2: 0.6984413862228394, G loss: [0.8367882966995239, 0.6599736213684082, 0.1768147051334381]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 28/43 [D loss 1: 1.837787389755249, D loss 2: 0.8745092451572418, G loss: [0.8432314991950989, 0.6678587794303894, 0.17537270486354828]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 29/43 [D loss 1: 1.725169837474823, D loss 2: 0.6474595367908478, G loss: [0.836327314376831, 0.6687855124473572, 0.16754181683063507]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 30/43 [D loss 1: 1.8240928053855896, D loss 2: 0.8564231991767883, G loss: [0.8185438513755798, 0.6671102046966553, 0.15143363177776337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 31/43 [D loss 1: 1.7392553687095642, D loss 2: 0.808161735534668, G loss: [0.8324352502822876, 0.6788044571876526, 0.1536308228969574]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 32/43 [D loss 1: 1.6745060682296753, D loss 2: 0.6942162811756134, G loss: [0.8295547962188721, 0.6627293825149536, 0.16682541370391846]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 33/43 [D loss 1: 1.8369551301002502, D loss 2: 0.8816452324390411, G loss: [0.8230448961257935, 0.6631187796592712, 0.15992611646652222]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 34/43 [D loss 1: 1.6291632652282715, D loss 2: 0.6683166325092316, G loss: [0.831507682800293, 0.6866226196289062, 0.14488503336906433]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 35/43 [D loss 1: 1.7469860315322876, D loss 2: 0.7692644894123077, G loss: [0.8388233780860901, 0.6884594559669495, 0.15036393702030182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 36/43 [D loss 1: 1.7771152257919312, D loss 2: 0.8002570271492004, G loss: [0.8437581062316895, 0.6877270340919495, 0.1560310572385788]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 37/43 [D loss 1: 1.7767375707626343, D loss 2: 0.8218424618244171, G loss: [0.8244526982307434, 0.6726158857345581, 0.1518368273973465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 38/43 [D loss 1: 1.661433607339859, D loss 2: 0.7367301881313324, G loss: [0.8295122385025024, 0.6862461566925049, 0.14326611161231995]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 39/43 [D loss 1: 1.7571264505386353, D loss 2: 0.8333723247051239, G loss: [0.841681718826294, 0.6936948299407959, 0.14798685908317566]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 40/43 [D loss 1: 1.7115067839622498, D loss 2: 0.7735206186771393, G loss: [0.8599135875701904, 0.7175253629684448, 0.1423882395029068]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 41/43 [D loss 1: 1.7754001319408417, D loss 2: 0.825082927942276, G loss: [0.8409877419471741, 0.7011798620223999, 0.13980787992477417]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 42/43 [D loss 1: 1.6818299889564514, D loss 2: 0.7598327994346619, G loss: [0.8612741231918335, 0.7153556942939758, 0.14591845870018005]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 43/43 [D loss 1: 1.7587588429450989, D loss 2: 0.8003195524215698, G loss: [0.8688074350357056, 0.7211030125617981, 0.14770443737506866]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 1/43 [D loss 1: 1.692039042711258, D loss 2: 0.7549664080142975, G loss: [0.8731814026832581, 0.7147785425186157, 0.15840286016464233]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 2/43 [D loss 1: 1.8552005887031555, D loss 2: 0.8861204087734222, G loss: [0.8624323606491089, 0.724534273147583, 0.13789808750152588]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 3/43 [D loss 1: 1.6352953016757965, D loss 2: 0.6530478596687317, G loss: [0.8427923321723938, 0.7013233304023743, 0.14146901667118073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 4/43 [D loss 1: 1.806024968624115, D loss 2: 0.8808345198631287, G loss: [0.8683792352676392, 0.7362629771232605, 0.13211627304553986]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 5/43 [D loss 1: 1.7171461582183838, D loss 2: 0.765421599149704, G loss: [0.8585875034332275, 0.7116694450378418, 0.14691805839538574]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 6/43 [D loss 1: 1.732184499502182, D loss 2: 0.7996846139431, G loss: [0.8624136447906494, 0.7251202464103699, 0.13729341328144073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 7/43 [D loss 1: 1.66819566488266, D loss 2: 0.7765785753726959, G loss: [0.8667157292366028, 0.7332248091697693, 0.1334909349679947]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 8/43 [D loss 1: 1.6182207465171814, D loss 2: 0.6894863545894623, G loss: [0.8600940704345703, 0.7172197103500366, 0.1428743600845337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 9/43 [D loss 1: 1.8196901082992554, D loss 2: 0.8842717409133911, G loss: [0.8730615377426147, 0.7435332536697388, 0.12952826917171478]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 10/43 [D loss 1: 1.6255329847335815, D loss 2: 0.6592312157154083, G loss: [0.856249988079071, 0.717575192451477, 0.1386748105287552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 11/43 [D loss 1: 1.6834671795368195, D loss 2: 0.7865748107433319, G loss: [0.8381076455116272, 0.7107321619987488, 0.12737548351287842]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 12/43 [D loss 1: 1.6392661333084106, D loss 2: 0.6930260360240936, G loss: [0.8298035860061646, 0.6906940340995789, 0.1391095668077469]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 13/43 [D loss 1: 1.7175404131412506, D loss 2: 0.7513054609298706, G loss: [0.8412503600120544, 0.7147787809371948, 0.1264715939760208]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 14/43 [D loss 1: 1.7461284399032593, D loss 2: 0.8269615769386292, G loss: [0.8604218363761902, 0.7239787578582764, 0.13644307851791382]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 15/43 [D loss 1: 1.6746683418750763, D loss 2: 0.7066468000411987, G loss: [0.8137247562408447, 0.6892964243888855, 0.12442834675312042]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 16/43 [D loss 1: 1.6790272295475006, D loss 2: 0.7353771328926086, G loss: [0.8309539556503296, 0.7037126421928406, 0.127241313457489]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 17/43 [D loss 1: 1.699215829372406, D loss 2: 0.7623102366924286, G loss: [0.8323838114738464, 0.7078232169151306, 0.12456057965755463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 18/43 [D loss 1: 1.6642837226390839, D loss 2: 0.7382721602916718, G loss: [0.8226494789123535, 0.6990364789962769, 0.12361298501491547]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 19/43 [D loss 1: 1.6718375384807587, D loss 2: 0.7411293685436249, G loss: [0.8422046303749084, 0.7137971520423889, 0.12840749323368073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 20/43 [D loss 1: 1.6659037470817566, D loss 2: 0.7459340989589691, G loss: [0.8619253635406494, 0.7273573279380798, 0.1345680058002472]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 21/43 [D loss 1: 1.617406189441681, D loss 2: 0.6642006933689117, G loss: [0.8207270503044128, 0.6959471702575684, 0.12477990239858627]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 22/43 [D loss 1: 1.7538845539093018, D loss 2: 0.8761314153671265, G loss: [0.8617294430732727, 0.7452237010002136, 0.11650574207305908]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 23/43 [D loss 1: 1.581718772649765, D loss 2: 0.6396035254001617, G loss: [0.8302496671676636, 0.6824358701705933, 0.1478138118982315]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 24/43 [D loss 1: 1.946024775505066, D loss 2: 1.005948156118393, G loss: [0.8902836441993713, 0.759996235370636, 0.13028742372989655]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 25/43 [D loss 1: 1.5864135026931763, D loss 2: 0.5929377675056458, G loss: [0.8707258701324463, 0.7540464401245117, 0.11667941510677338]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 26/43 [D loss 1: 1.6785675883293152, D loss 2: 0.7040345072746277, G loss: [0.819218099117279, 0.7125279903411865, 0.10669011622667313]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 27/43 [D loss 1: 1.7086012363433838, D loss 2: 0.7869568467140198, G loss: [0.831816554069519, 0.7036072611808777, 0.12820927798748016]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 28/43 [D loss 1: 1.6156622171401978, D loss 2: 0.7141001224517822, G loss: [0.8471068739891052, 0.7268828749656677, 0.12022397667169571]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 29/43 [D loss 1: 1.579797625541687, D loss 2: 0.6759030520915985, G loss: [0.8311259150505066, 0.7109634876251221, 0.12016244977712631]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 30/43 [D loss 1: 1.7538753747940063, D loss 2: 0.8678408861160278, G loss: [0.8655475378036499, 0.744208574295044, 0.12133897095918655]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 31/43 [D loss 1: 1.5760282576084137, D loss 2: 0.6640399992465973, G loss: [0.8581468462944031, 0.7389408349990845, 0.1192060336470604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 32/43 [D loss 1: 1.6054053902626038, D loss 2: 0.6658749282360077, G loss: [0.8408334255218506, 0.7210169434547424, 0.11981645226478577]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 33/43 [D loss 1: 1.7459412813186646, D loss 2: 0.8576961159706116, G loss: [0.8416340351104736, 0.7175648808479309, 0.12406915426254272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 34/43 [D loss 1: 1.697385162115097, D loss 2: 0.7797947227954865, G loss: [0.8438769578933716, 0.7348222136497498, 0.10905477404594421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 35/43 [D loss 1: 1.5895203351974487, D loss 2: 0.669230580329895, G loss: [0.8662092089653015, 0.7426763772964478, 0.12353280931711197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 36/43 [D loss 1: 1.6292409002780914, D loss 2: 0.7734272181987762, G loss: [0.8336009383201599, 0.7179516553878784, 0.1156492754817009]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 37/43 [D loss 1: 1.5991036891937256, D loss 2: 0.6747349202632904, G loss: [0.8511424660682678, 0.7375535368919373, 0.11358890682458878]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 38/43 [D loss 1: 1.6444230377674103, D loss 2: 0.7132171988487244, G loss: [0.8409485816955566, 0.7194236516952515, 0.12152490019798279]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 39/43 [D loss 1: 1.5733997523784637, D loss 2: 0.6614870429039001, G loss: [0.8357939124107361, 0.7280052900314331, 0.10778862237930298]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 40/43 [D loss 1: 1.7362358570098877, D loss 2: 0.8391448855400085, G loss: [0.8633577227592468, 0.7523561716079712, 0.11100155860185623]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 41/43 [D loss 1: 1.6543315649032593, D loss 2: 0.7473129630088806, G loss: [0.8626539707183838, 0.7523617148399353, 0.11029224842786789]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 42/43 [D loss 1: 1.5990588665008545, D loss 2: 0.6929618418216705, G loss: [0.8526559472084045, 0.7375963926315308, 0.11505956202745438]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 43/43 [D loss 1: 1.7565415501594543, D loss 2: 0.897039920091629, G loss: [0.8577614426612854, 0.7461518049240112, 0.11160963773727417]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 1/43 [D loss 1: 1.6535817384719849, D loss 2: 0.735459178686142, G loss: [0.8872460722923279, 0.7782443165779114, 0.1090017706155777]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 2/43 [D loss 1: 1.5670222640037537, D loss 2: 0.656009703874588, G loss: [0.8845027685165405, 0.7661140561103821, 0.11838869750499725]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 3/43 [D loss 1: 1.7869721055030823, D loss 2: 0.8939661979675293, G loss: [0.8843799233436584, 0.7725469470024109, 0.11183296889066696]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 4/43 [D loss 1: 1.6656945049762726, D loss 2: 0.7479779720306396, G loss: [0.8860855102539062, 0.7707485556602478, 0.11533697694540024]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 5/43 [D loss 1: 1.6965810656547546, D loss 2: 0.8055711090564728, G loss: [0.9083513617515564, 0.8076576590538025, 0.1006937175989151]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 6/43 [D loss 1: 1.7008513808250427, D loss 2: 0.8113875389099121, G loss: [0.9428076148033142, 0.8239535093307495, 0.1188540980219841]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 7/43 [D loss 1: 1.5436462759971619, D loss 2: 0.6274878978729248, G loss: [0.8913621306419373, 0.777927577495575, 0.11343453824520111]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 8/43 [D loss 1: 1.7611858248710632, D loss 2: 0.8728926479816437, G loss: [0.9042130708694458, 0.7940987944602966, 0.11011426150798798]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 9/43 [D loss 1: 1.6422042548656464, D loss 2: 0.7485194504261017, G loss: [0.896252453327179, 0.789033055305481, 0.107219398021698]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 10/43 [D loss 1: 1.6958129107952118, D loss 2: 0.7818354070186615, G loss: [0.9044023752212524, 0.7925863862037659, 0.11181596666574478]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 11/43 [D loss 1: 1.6115809679031372, D loss 2: 0.719307541847229, G loss: [0.9233788251876831, 0.8085364103317261, 0.11484239995479584]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 12/43 [D loss 1: 1.6499479115009308, D loss 2: 0.8039478957653046, G loss: [0.9184897541999817, 0.8199905753135681, 0.09849916398525238]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 13/43 [D loss 1: 1.675185739994049, D loss 2: 0.7703932821750641, G loss: [0.9343026280403137, 0.8114493489265442, 0.12285330146551132]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 14/43 [D loss 1: 1.6987964510917664, D loss 2: 0.8584785163402557, G loss: [0.926916778087616, 0.8169852495193481, 0.10993150621652603]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 15/43 [D loss 1: 1.5544366836547852, D loss 2: 0.6853475570678711, G loss: [0.935566782951355, 0.8085920810699463, 0.1269747018814087]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 16/43 [D loss 1: 1.8801776766777039, D loss 2: 0.9715308248996735, G loss: [0.9540726542472839, 0.8459427356719971, 0.10812992602586746]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 17/43 [D loss 1: 1.6440937221050262, D loss 2: 0.7260192930698395, G loss: [0.933588445186615, 0.8116914629936218, 0.12189696729183197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 18/43 [D loss 1: 1.7093065977096558, D loss 2: 0.8342589437961578, G loss: [0.9690924286842346, 0.8610809445381165, 0.10801150649785995]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 19/43 [D loss 1: 1.6858243644237518, D loss 2: 0.8182710409164429, G loss: [0.9748712778091431, 0.8636594414710999, 0.1112118661403656]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 20/43 [D loss 1: 1.6396313905715942, D loss 2: 0.7466469407081604, G loss: [0.9577327966690063, 0.8416467308998108, 0.11608608812093735]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 21/43 [D loss 1: 1.6863506436347961, D loss 2: 0.8149302899837494, G loss: [0.9461106657981873, 0.8372885584831238, 0.10882212221622467]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 22/43 [D loss 1: 1.6482200026512146, D loss 2: 0.7485990524291992, G loss: [0.9510300159454346, 0.8450471758842468, 0.10598286241292953]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 23/43 [D loss 1: 1.6663945615291595, D loss 2: 0.7587076425552368, G loss: [0.965069055557251, 0.852101743221283, 0.11296732723712921]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 24/43 [D loss 1: 1.7154089212417603, D loss 2: 0.8283106684684753, G loss: [0.9865396618843079, 0.8712478876113892, 0.1152917742729187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 25/43 [D loss 1: 1.6249175071716309, D loss 2: 0.7414049506187439, G loss: [0.9809084534645081, 0.8414627313613892, 0.1394457370042801]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 26/43 [D loss 1: 1.954918384552002, D loss 2: 1.0540171563625336, G loss: [1.0447957515716553, 0.9398841857910156, 0.10491155833005905]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 27/43 [D loss 1: 1.549596130847931, D loss 2: 0.7200197577476501, G loss: [0.9987517595291138, 0.8865690231323242, 0.11218271404504776]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 28/43 [D loss 1: 1.5367779433727264, D loss 2: 0.6682182550430298, G loss: [1.0175163745880127, 0.8893577456474304, 0.1281585991382599]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 29/43 [D loss 1: 1.8687977194786072, D loss 2: 1.0076063573360443, G loss: [1.054382085800171, 0.9618639945983887, 0.09251805394887924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 30/43 [D loss 1: 1.4917097687721252, D loss 2: 0.6251477897167206, G loss: [1.0022635459899902, 0.8817328810691833, 0.12053069472312927]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 31/43 [D loss 1: 1.5870462954044342, D loss 2: 0.724706619977951, G loss: [0.988679826259613, 0.8778430819511414, 0.11083674430847168]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 32/43 [D loss 1: 1.7319979667663574, D loss 2: 0.8524730503559113, G loss: [0.9708737134933472, 0.865973174571991, 0.1049005389213562]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 33/43 [D loss 1: 1.6759561896324158, D loss 2: 0.7946515679359436, G loss: [0.9830236434936523, 0.88136225938797, 0.10166138410568237]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 34/43 [D loss 1: 1.596158117055893, D loss 2: 0.716495007276535, G loss: [0.9648115634918213, 0.8497090339660645, 0.11510251462459564]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 35/43 [D loss 1: 1.63313627243042, D loss 2: 0.7809310257434845, G loss: [0.9800891280174255, 0.875290036201477, 0.10479911416769028]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 36/43 [D loss 1: 1.6055120825767517, D loss 2: 0.7268995046615601, G loss: [0.9871436953544617, 0.884722888469696, 0.10242082178592682]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 37/43 [D loss 1: 1.5381822288036346, D loss 2: 0.6631147563457489, G loss: [0.9978348016738892, 0.8761278390884399, 0.12170694768428802]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 38/43 [D loss 1: 1.8070986866950989, D loss 2: 0.9082598090171814, G loss: [1.0004894733428955, 0.8958582878112793, 0.10463118553161621]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 39/43 [D loss 1: 1.622769445180893, D loss 2: 0.7647057175636292, G loss: [0.976351261138916, 0.8685257434844971, 0.10782553255558014]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 40/43 [D loss 1: 1.5947312414646149, D loss 2: 0.7168582677841187, G loss: [1.0128655433654785, 0.9054387211799622, 0.10742687433958054]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 41/43 [D loss 1: 1.5847695171833038, D loss 2: 0.7457383573055267, G loss: [1.0175068378448486, 0.9036036133766174, 0.11390317976474762]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 42/43 [D loss 1: 1.6375168859958649, D loss 2: 0.783266007900238, G loss: [0.9985141158103943, 0.8871641159057617, 0.11134998500347137]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 43/43 [D loss 1: 1.5352762937545776, D loss 2: 0.7023265361785889, G loss: [0.9944419860839844, 0.8816897869110107, 0.11275222152471542]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 1/43 [D loss 1: 1.5648506581783295, D loss 2: 0.7390713691711426, G loss: [0.9878362417221069, 0.8796943426132202, 0.10814188420772552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 2/43 [D loss 1: 1.635355681180954, D loss 2: 0.7502670288085938, G loss: [0.9985666275024414, 0.9003386497497559, 0.09822793304920197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 3/43 [D loss 1: 1.636286437511444, D loss 2: 0.7577040493488312, G loss: [0.9956918954849243, 0.8936219811439514, 0.1020699292421341]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 4/43 [D loss 1: 1.5538159012794495, D loss 2: 0.7075817584991455, G loss: [1.000744104385376, 0.8920411467552185, 0.1087028980255127]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 5/43 [D loss 1: 1.636974424123764, D loss 2: 0.7556804716587067, G loss: [0.9932881593704224, 0.8836796879768372, 0.10960845649242401]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 6/43 [D loss 1: 1.5882719159126282, D loss 2: 0.7651054859161377, G loss: [0.9753621816635132, 0.8758633136749268, 0.09949889779090881]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 7/43 [D loss 1: 1.613550066947937, D loss 2: 0.7496960759162903, G loss: [1.0001128911972046, 0.8927161693572998, 0.1073966696858406]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 8/43 [D loss 1: 1.5861807465553284, D loss 2: 0.759739339351654, G loss: [0.9960049390792847, 0.8940273523330688, 0.10197758674621582]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 9/43 [D loss 1: 1.724562168121338, D loss 2: 0.852980375289917, G loss: [1.0135101079940796, 0.9201017618179321, 0.09340829402208328]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 10/43 [D loss 1: 1.573344498872757, D loss 2: 0.7316723763942719, G loss: [0.9860031604766846, 0.8880048394203186, 0.09799829125404358]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 11/43 [D loss 1: 1.6891454458236694, D loss 2: 0.8271238207817078, G loss: [1.0191820859909058, 0.9242123365402222, 0.0949697494506836]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 12/43 [D loss 1: 1.5734758377075195, D loss 2: 0.7307780683040619, G loss: [1.0116562843322754, 0.899445652961731, 0.11221059411764145]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 13/43 [D loss 1: 1.6687240302562714, D loss 2: 0.8249054551124573, G loss: [1.055112361907959, 0.9660251140594482, 0.08908727020025253]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 14/43 [D loss 1: 1.6542952358722687, D loss 2: 0.8412657082080841, G loss: [1.0608365535736084, 0.9569755792617798, 0.1038610190153122]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 15/43 [D loss 1: 1.617776334285736, D loss 2: 0.7516493201255798, G loss: [1.055446743965149, 0.935697615146637, 0.11974907666444778]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 16/43 [D loss 1: 1.7683783769607544, D loss 2: 0.8882025182247162, G loss: [1.057342290878296, 0.9709916114807129, 0.08635067194700241]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 17/43 [D loss 1: 1.5735016465187073, D loss 2: 0.7202636897563934, G loss: [1.0714340209960938, 0.9596471190452576, 0.1117868423461914]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 18/43 [D loss 1: 1.6247412860393524, D loss 2: 0.7949364185333252, G loss: [1.0604429244995117, 0.9585257768630981, 0.10191714763641357]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 19/43 [D loss 1: 1.6339849829673767, D loss 2: 0.8156591355800629, G loss: [1.0632928609848022, 0.9620469212532043, 0.10124599188566208]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 20/43 [D loss 1: 1.5389695763587952, D loss 2: 0.7419953048229218, G loss: [1.0710898637771606, 0.946495771408081, 0.12459411472082138]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 21/43 [D loss 1: 1.7501757144927979, D loss 2: 0.8687408268451691, G loss: [1.0450283288955688, 0.9654683470726013, 0.07955998182296753]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 22/43 [D loss 1: 1.5493127703666687, D loss 2: 0.7326042652130127, G loss: [1.0553970336914062, 0.9443761706352234, 0.11102081090211868]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 23/43 [D loss 1: 1.642721563577652, D loss 2: 0.7712947130203247, G loss: [1.060319185256958, 0.9671029448509216, 0.09321625530719757]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 24/43 [D loss 1: 1.6102662086486816, D loss 2: 0.7680872082710266, G loss: [1.0517897605895996, 0.94954514503479, 0.10224461555480957]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 25/43 [D loss 1: 1.5917415618896484, D loss 2: 0.752830445766449, G loss: [1.0493583679199219, 0.93231600522995, 0.11704231053590775]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 26/43 [D loss 1: 1.6680595874786377, D loss 2: 0.8472044765949249, G loss: [1.044530987739563, 0.9514914155006409, 0.09303957968950272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 27/43 [D loss 1: 1.6331333220005035, D loss 2: 0.7613905072212219, G loss: [1.0483859777450562, 0.9486938118934631, 0.09969212114810944]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 28/43 [D loss 1: 1.5650735199451447, D loss 2: 0.7262989580631256, G loss: [1.0399655103683472, 0.933167576789856, 0.1067979708313942]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 29/43 [D loss 1: 1.6625617742538452, D loss 2: 0.8437951505184174, G loss: [1.0577106475830078, 0.9684457182884216, 0.08926496654748917]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 30/43 [D loss 1: 1.5950549840927124, D loss 2: 0.752576619386673, G loss: [1.0405417680740356, 0.9231998324394226, 0.11734197288751602]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 31/43 [D loss 1: 1.743180751800537, D loss 2: 0.8875152468681335, G loss: [1.0272225141525269, 0.9437661170959473, 0.08345640450716019]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 32/43 [D loss 1: 1.596357524394989, D loss 2: 0.7631303668022156, G loss: [1.044336199760437, 0.9369644522666931, 0.1073717251420021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 33/43 [D loss 1: 1.6277619898319244, D loss 2: 0.7964279651641846, G loss: [1.0303055047988892, 0.9360952973365784, 0.09421022981405258]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 34/43 [D loss 1: 1.5785943865776062, D loss 2: 0.7279449701309204, G loss: [1.0623888969421387, 0.9544757008552551, 0.10791320353746414]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 35/43 [D loss 1: 1.6545082032680511, D loss 2: 0.8234165012836456, G loss: [1.030653476715088, 0.9345077872276306, 0.09614570438861847]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 36/43 [D loss 1: 1.6503645777702332, D loss 2: 0.8027017116546631, G loss: [1.0257236957550049, 0.9320623278617859, 0.09366140514612198]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 37/43 [D loss 1: 1.6289423108100891, D loss 2: 0.8057739436626434, G loss: [1.0611428022384644, 0.9619004726409912, 0.09924229979515076]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 38/43 [D loss 1: 1.745169222354889, D loss 2: 0.8956911563873291, G loss: [1.058675765991211, 0.9597910642623901, 0.09888467192649841]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 39/43 [D loss 1: 1.6563555300235748, D loss 2: 0.8380565941333771, G loss: [1.0465563535690308, 0.9166548848152161, 0.1299014836549759]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 40/43 [D loss 1: 1.7655034065246582, D loss 2: 0.9221210181713104, G loss: [1.077305793762207, 0.9896154999732971, 0.08769028633832932]]\n",
      "18/65 [=======>......................] - ETA: 0s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate and train the DCGAN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m acgan \u001b[38;5;241m=\u001b[39m ACGAN(img_rows, img_cols, channels)\n\u001b[1;32m----> 6\u001b[0m \u001b[43macgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 128\u001b[0m, in \u001b[0;36mACGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval, gen_steps)\u001b[0m\n\u001b[0;32m    126\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim))\n\u001b[0;32m    127\u001b[0m gen_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, (batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 128\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m labels_fake \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    131\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(imgs, [labels_real, y_train[idx]])\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2254\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2255\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "acgan = ACGAN(img_rows, img_cols, channels)\n",
    "acgan.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0370418b23d3e6d627974f5b44612aacb169a42c01386bf7ba5dc9099819d8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
