{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, Conv2DTranspose, PReLU\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Concatenate\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, HTML\n",
    "import glob\n",
    "from keras.layers import AveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Add\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import Input\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install scikit-image\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Reshape, UpSampling2D, \\\n",
    "    BatchNormalization, Activation, Input, LeakyReLU, ZeroPadding2D, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "#import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('emnist-letters-train.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {1: 0, \n",
    "           2: 1, \n",
    "           3: 2, \n",
    "           4: 3, \n",
    "           5: 4, \n",
    "           6: 5, \n",
    "           7: 6, \n",
    "           8: 7, \n",
    "           9: 8, \n",
    "           10: 9, \n",
    "           11: 10, \n",
    "           12: 11, \n",
    "           13: 12, \n",
    "           14: 13, \n",
    "           15: 14, \n",
    "           16: 15, \n",
    "           17: 16, \n",
    "           18: 17, \n",
    "           19: 18, \n",
    "           20: 19, \n",
    "           21: 20, \n",
    "           22: 21, \n",
    "           23: 22, \n",
    "           24: 23, \n",
    "           25: 24, \n",
    "           26: 25, \n",
    "           27: 26}\n",
    "\n",
    "        # Map the labels column to its corresponding value\n",
    "df[0] = df[0].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = np.array(df.iloc[:,0].values)\n",
    "y_pre = pd.Categorical(y_pre)\n",
    "X = np.array(df.iloc[:,1:].values)\n",
    "X = X.reshape(-1,28,28,1)\n",
    "preprocessed = []\n",
    "for image in X:\n",
    "    rotated_image = rotate(image, 90, reshape=False)\n",
    "    flipped_image = np.flipud(rotated_image)\n",
    "    preprocessed.append(flipped_image)\n",
    "X_pre = np.array(preprocessed)\n",
    "X = X_pre\n",
    "X = X.astype('float32')\n",
    "X_pre = (X - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pre\n",
      "[22, 6, 15, 14, 16, ..., 19, 8, 5, 11, 0]\n",
      "Length: 26\n",
      "Categories (26, int64): [0, 1, 2, 3, ..., 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f'y_pre\\n{y_pre.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z = 100):\n",
    "        # Input shape\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'])\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "        # For the combined model we will only train the generator\n",
    "        \n",
    "        # The discriminator takes generated images as input and\n",
    "        # determines validity\n",
    "        valid = self.discriminator(img)\n",
    "        # The combined model (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim)) # connect the input to dense layer\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        # upsample from 7*7 to 14*14\n",
    "        model.add(Conv2DTranspose(128, (3,3), strides=(2,2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # upsample to 28x28\n",
    "        model.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\", activation='tanh'))\n",
    "        model.summary()\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        \n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        os.makedirs('Improved DCGAN', exist_ok=True)\n",
    "        fig.savefig(\"Improved DCGAN/DCGAN_{:d}.png\".format(epoch))\n",
    "        plt.close()\n",
    "        \n",
    "    def train(self, epochs, batch_size=1024, save_interval=1, gen_steps=1):\n",
    "        # Load the dataset\n",
    "        X_train = X_pre\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "\n",
    "                # ---------------------\n",
    "                # Train Discriminator\n",
    "                # ---------------------\n",
    "                # Select a random half of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "                # Train the discriminator (real classified as ones\n",
    "                # and generated as zeros)\n",
    "                self.discriminator.trainable = True\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                # ---------------------\n",
    "                # Train Generator\n",
    "                # ---------------------\n",
    "                # Train the generator (wants discriminator to mistake\n",
    "                # images as real)\n",
    "                # Sample noise and generate a batch of new images\n",
    "                self.discriminator.trainable = False\n",
    "                for _ in range(1):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    g_loss = self.combined.train_on_batch(noise, valid)\n",
    "                    \n",
    "                # Plot the progress\n",
    "                print (\"Epoch: %d/%d  Batch Size: %d/%d [loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,epochs,batch,batches_per_epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 14, 14, 128)       1280      \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 12545     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 308,993\n",
      "Trainable params: 308,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 6272)              633472    \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 14, 14, 128)      147584    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 28, 28, 64)       73792     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 855,425\n",
      "Trainable params: 855,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 0/173 [loss: 0.667550, acc.: 45.90%] [G loss: 0.691569]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 1/173 [loss: 0.595176, acc.: 50.00%] [G loss: 0.687152]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 2/173 [loss: 0.543312, acc.: 50.00%] [G loss: 0.675311]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 3/173 [loss: 0.504020, acc.: 50.00%] [G loss: 0.650714]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 4/173 [loss: 0.485531, acc.: 50.00%] [G loss: 0.611177]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 5/173 [loss: 0.493568, acc.: 50.00%] [G loss: 0.561056]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 6/173 [loss: 0.527407, acc.: 50.00%] [G loss: 0.506138]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 7/173 [loss: 0.575187, acc.: 50.00%] [G loss: 0.460357]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 8/173 [loss: 0.618333, acc.: 50.00%] [G loss: 0.432342]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 9/173 [loss: 0.647443, acc.: 50.00%] [G loss: 0.422620]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 10/173 [loss: 0.660910, acc.: 50.00%] [G loss: 0.427537]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 11/173 [loss: 0.660692, acc.: 50.00%] [G loss: 0.445116]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 12/173 [loss: 0.652746, acc.: 50.00%] [G loss: 0.466717]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 13/173 [loss: 0.645240, acc.: 50.00%] [G loss: 0.489574]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 14/173 [loss: 0.627795, acc.: 50.00%] [G loss: 0.511659]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 15/173 [loss: 0.611955, acc.: 50.00%] [G loss: 0.540292]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 16/173 [loss: 0.589585, acc.: 50.00%] [G loss: 0.556708]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 17/173 [loss: 0.576812, acc.: 50.00%] [G loss: 0.570127]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 18/173 [loss: 0.557056, acc.: 50.00%] [G loss: 0.581748]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 19/173 [loss: 0.544083, acc.: 50.00%] [G loss: 0.593635]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 20/173 [loss: 0.530995, acc.: 50.00%] [G loss: 0.603911]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 21/173 [loss: 0.524719, acc.: 50.00%] [G loss: 0.605568]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 22/173 [loss: 0.512010, acc.: 50.00%] [G loss: 0.609219]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 23/173 [loss: 0.503672, acc.: 50.00%] [G loss: 0.612663]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 24/173 [loss: 0.498508, acc.: 50.00%] [G loss: 0.613358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 25/173 [loss: 0.491866, acc.: 50.00%] [G loss: 0.606978]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 26/173 [loss: 0.494432, acc.: 50.00%] [G loss: 0.606692]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 27/173 [loss: 0.495348, acc.: 50.00%] [G loss: 0.598028]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 28/173 [loss: 0.495951, acc.: 50.00%] [G loss: 0.593842]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 29/173 [loss: 0.496076, acc.: 50.00%] [G loss: 0.601539]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 30/173 [loss: 0.499517, acc.: 50.00%] [G loss: 0.611087]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 31/173 [loss: 0.497797, acc.: 50.00%] [G loss: 0.631419]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 32/173 [loss: 0.476548, acc.: 50.00%] [G loss: 0.649266]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 33/173 [loss: 0.466520, acc.: 50.00%] [G loss: 0.665294]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 34/173 [loss: 0.448547, acc.: 50.00%] [G loss: 0.675888]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 35/173 [loss: 0.435236, acc.: 50.10%] [G loss: 0.679621]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 36/173 [loss: 0.423828, acc.: 50.29%] [G loss: 0.682738]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 37/173 [loss: 0.420972, acc.: 51.07%] [G loss: 0.679372]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 38/173 [loss: 0.432880, acc.: 50.00%] [G loss: 0.667076]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 39/173 [loss: 0.449586, acc.: 50.00%] [G loss: 0.653757]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 40/173 [loss: 0.468341, acc.: 50.00%] [G loss: 0.662558]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 41/173 [loss: 0.459495, acc.: 50.10%] [G loss: 0.694388]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 42/173 [loss: 0.442211, acc.: 50.20%] [G loss: 0.709875]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 43/173 [loss: 0.439757, acc.: 50.20%] [G loss: 0.703075]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 44/173 [loss: 0.451174, acc.: 50.59%] [G loss: 0.683584]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 45/173 [loss: 0.485440, acc.: 50.00%] [G loss: 0.645800]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 46/173 [loss: 0.534512, acc.: 50.00%] [G loss: 0.606352]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 47/173 [loss: 0.547578, acc.: 50.00%] [G loss: 0.637906]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 48/173 [loss: 0.545104, acc.: 50.00%] [G loss: 0.641005]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 49/173 [loss: 0.581203, acc.: 50.00%] [G loss: 0.539183]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 50/173 [loss: 0.638023, acc.: 50.00%] [G loss: 0.462931]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 51/173 [loss: 0.692084, acc.: 50.00%] [G loss: 0.419463]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 52/173 [loss: 0.704617, acc.: 50.00%] [G loss: 0.422345]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 53/173 [loss: 0.661855, acc.: 50.00%] [G loss: 0.471517]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 54/173 [loss: 0.611043, acc.: 50.00%] [G loss: 0.513207]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 55/173 [loss: 0.590092, acc.: 50.00%] [G loss: 0.523913]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 56/173 [loss: 0.627517, acc.: 50.00%] [G loss: 0.476198]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 57/173 [loss: 0.849603, acc.: 50.00%] [G loss: 0.340512]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 58/173 [loss: 1.057507, acc.: 50.00%] [G loss: 0.286211]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 59/173 [loss: 0.891727, acc.: 50.00%] [G loss: 0.452468]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 60/173 [loss: 0.823125, acc.: 50.00%] [G loss: 0.540849]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 61/173 [loss: 0.850839, acc.: 50.00%] [G loss: 0.505220]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 62/173 [loss: 0.933143, acc.: 50.00%] [G loss: 0.445014]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 63/173 [loss: 0.984536, acc.: 49.90%] [G loss: 0.422004]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 64/173 [loss: 0.937159, acc.: 49.90%] [G loss: 0.441597]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 65/173 [loss: 0.896440, acc.: 49.12%] [G loss: 0.465762]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 66/173 [loss: 0.864951, acc.: 49.61%] [G loss: 0.468786]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 67/173 [loss: 0.844328, acc.: 50.00%] [G loss: 0.468752]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 68/173 [loss: 0.817175, acc.: 50.00%] [G loss: 0.475993]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 69/173 [loss: 0.796952, acc.: 50.00%] [G loss: 0.488281]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 70/173 [loss: 0.777846, acc.: 50.00%] [G loss: 0.497193]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 71/173 [loss: 0.759197, acc.: 50.00%] [G loss: 0.510018]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 72/173 [loss: 0.744348, acc.: 50.00%] [G loss: 0.524002]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 73/173 [loss: 0.722852, acc.: 50.00%] [G loss: 0.534813]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 74/173 [loss: 0.708234, acc.: 50.00%] [G loss: 0.536812]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 75/173 [loss: 0.692643, acc.: 50.00%] [G loss: 0.549467]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 76/173 [loss: 0.683073, acc.: 50.00%] [G loss: 0.554976]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 77/173 [loss: 0.681247, acc.: 50.00%] [G loss: 0.560445]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 78/173 [loss: 0.670118, acc.: 50.00%] [G loss: 0.554986]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 79/173 [loss: 0.663633, acc.: 50.00%] [G loss: 0.559904]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 80/173 [loss: 0.660551, acc.: 50.00%] [G loss: 0.557542]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 81/173 [loss: 0.659719, acc.: 50.00%] [G loss: 0.535194]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 82/173 [loss: 0.675242, acc.: 50.00%] [G loss: 0.513204]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 83/173 [loss: 0.691738, acc.: 50.00%] [G loss: 0.488367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 84/173 [loss: 0.705039, acc.: 50.00%] [G loss: 0.475465]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 85/173 [loss: 0.705057, acc.: 50.00%] [G loss: 0.476161]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 86/173 [loss: 0.692551, acc.: 50.00%] [G loss: 0.482507]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 87/173 [loss: 0.680360, acc.: 50.00%] [G loss: 0.498029]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 88/173 [loss: 0.667146, acc.: 50.00%] [G loss: 0.503060]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 89/173 [loss: 0.649204, acc.: 50.00%] [G loss: 0.512774]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 90/173 [loss: 0.643731, acc.: 50.00%] [G loss: 0.522998]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 91/173 [loss: 0.634896, acc.: 50.00%] [G loss: 0.527582]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 92/173 [loss: 0.623673, acc.: 50.00%] [G loss: 0.536479]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 93/173 [loss: 0.613684, acc.: 50.00%] [G loss: 0.548084]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 94/173 [loss: 0.603510, acc.: 50.00%] [G loss: 0.552294]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 95/173 [loss: 0.590891, acc.: 50.00%] [G loss: 0.558408]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 96/173 [loss: 0.587105, acc.: 50.00%] [G loss: 0.560340]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 97/173 [loss: 0.579627, acc.: 50.00%] [G loss: 0.564608]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 98/173 [loss: 0.576363, acc.: 50.00%] [G loss: 0.564692]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 99/173 [loss: 0.565700, acc.: 50.00%] [G loss: 0.571157]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 100/173 [loss: 0.566721, acc.: 50.00%] [G loss: 0.569898]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 101/173 [loss: 0.559626, acc.: 50.00%] [G loss: 0.572149]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 102/173 [loss: 0.560660, acc.: 50.00%] [G loss: 0.566282]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 103/173 [loss: 0.565282, acc.: 50.00%] [G loss: 0.556102]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 104/173 [loss: 0.576172, acc.: 50.00%] [G loss: 0.543174]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 105/173 [loss: 0.585740, acc.: 50.00%] [G loss: 0.518421]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 106/173 [loss: 0.624534, acc.: 50.00%] [G loss: 0.478150]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 107/173 [loss: 0.686212, acc.: 50.00%] [G loss: 0.411354]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 108/173 [loss: 0.776969, acc.: 50.00%] [G loss: 0.341031]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 109/173 [loss: 0.881847, acc.: 50.00%] [G loss: 0.283916]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 110/173 [loss: 0.969723, acc.: 50.00%] [G loss: 0.255845]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 111/173 [loss: 0.991953, acc.: 50.00%] [G loss: 0.262149]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 112/173 [loss: 0.979240, acc.: 50.00%] [G loss: 0.302543]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 113/173 [loss: 0.895695, acc.: 50.00%] [G loss: 0.365516]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 114/173 [loss: 0.830960, acc.: 50.00%] [G loss: 0.441527]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 115/173 [loss: 0.781159, acc.: 50.00%] [G loss: 0.498276]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 116/173 [loss: 0.738897, acc.: 50.00%] [G loss: 0.543438]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 117/173 [loss: 0.718770, acc.: 50.00%] [G loss: 0.563970]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 118/173 [loss: 0.706526, acc.: 49.90%] [G loss: 0.577832]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 119/173 [loss: 0.686815, acc.: 50.00%] [G loss: 0.589678]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 120/173 [loss: 0.688277, acc.: 50.00%] [G loss: 0.590538]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 121/173 [loss: 0.680691, acc.: 50.00%] [G loss: 0.583971]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 122/173 [loss: 0.669376, acc.: 50.00%] [G loss: 0.582124]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 123/173 [loss: 0.666261, acc.: 50.00%] [G loss: 0.582740]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 124/173 [loss: 0.661351, acc.: 50.10%] [G loss: 0.584401]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 125/173 [loss: 0.660064, acc.: 50.10%] [G loss: 0.576981]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 126/173 [loss: 0.654010, acc.: 50.29%] [G loss: 0.566642]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 127/173 [loss: 0.663217, acc.: 50.00%] [G loss: 0.567618]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 128/173 [loss: 0.651998, acc.: 50.00%] [G loss: 0.551413]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 129/173 [loss: 0.656805, acc.: 50.10%] [G loss: 0.545646]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 130/173 [loss: 0.662795, acc.: 50.10%] [G loss: 0.527378]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 131/173 [loss: 0.680169, acc.: 50.00%] [G loss: 0.507546]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 132/173 [loss: 0.695912, acc.: 50.10%] [G loss: 0.482968]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 133/173 [loss: 0.714702, acc.: 50.00%] [G loss: 0.451143]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 134/173 [loss: 0.753156, acc.: 50.00%] [G loss: 0.433230]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 135/173 [loss: 0.777849, acc.: 50.00%] [G loss: 0.405649]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 136/173 [loss: 0.803004, acc.: 49.80%] [G loss: 0.391102]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 137/173 [loss: 0.812641, acc.: 50.00%] [G loss: 0.382514]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 138/173 [loss: 0.811614, acc.: 50.00%] [G loss: 0.383640]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 139/173 [loss: 0.802913, acc.: 50.00%] [G loss: 0.392555]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 140/173 [loss: 0.800153, acc.: 49.90%] [G loss: 0.402990]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 141/173 [loss: 0.790867, acc.: 50.00%] [G loss: 0.407556]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 142/173 [loss: 0.777249, acc.: 50.00%] [G loss: 0.418567]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 143/173 [loss: 0.769245, acc.: 50.00%] [G loss: 0.421135]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 144/173 [loss: 0.764020, acc.: 50.00%] [G loss: 0.425869]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 145/173 [loss: 0.752762, acc.: 50.00%] [G loss: 0.430241]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 146/173 [loss: 0.746442, acc.: 50.00%] [G loss: 0.431963]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 147/173 [loss: 0.745968, acc.: 50.00%] [G loss: 0.433686]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 148/173 [loss: 0.743684, acc.: 50.00%] [G loss: 0.435664]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 149/173 [loss: 0.743384, acc.: 50.00%] [G loss: 0.432702]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 150/173 [loss: 0.734273, acc.: 50.00%] [G loss: 0.436370]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 151/173 [loss: 0.731001, acc.: 50.00%] [G loss: 0.440087]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 152/173 [loss: 0.726928, acc.: 50.00%] [G loss: 0.441699]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 153/173 [loss: 0.721696, acc.: 50.00%] [G loss: 0.443664]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 154/173 [loss: 0.722737, acc.: 50.00%] [G loss: 0.446356]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 155/173 [loss: 0.717334, acc.: 50.00%] [G loss: 0.445221]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 156/173 [loss: 0.713920, acc.: 50.00%] [G loss: 0.453082]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 157/173 [loss: 0.713083, acc.: 50.00%] [G loss: 0.455969]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 158/173 [loss: 0.711211, acc.: 50.00%] [G loss: 0.461752]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 159/173 [loss: 0.702063, acc.: 50.00%] [G loss: 0.457305]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 160/173 [loss: 0.701176, acc.: 50.00%] [G loss: 0.463297]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 161/173 [loss: 0.693747, acc.: 50.00%] [G loss: 0.462045]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 162/173 [loss: 0.702988, acc.: 50.00%] [G loss: 0.470116]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 163/173 [loss: 0.693638, acc.: 50.00%] [G loss: 0.463714]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 164/173 [loss: 0.699471, acc.: 50.00%] [G loss: 0.462602]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 165/173 [loss: 0.697781, acc.: 50.00%] [G loss: 0.470905]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 166/173 [loss: 0.701254, acc.: 50.00%] [G loss: 0.467326]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 167/173 [loss: 0.703927, acc.: 50.00%] [G loss: 0.461376]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 168/173 [loss: 0.709372, acc.: 50.00%] [G loss: 0.449400]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 169/173 [loss: 0.713800, acc.: 50.00%] [G loss: 0.442981]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 170/173 [loss: 0.734183, acc.: 50.00%] [G loss: 0.440607]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 171/173 [loss: 0.736448, acc.: 50.00%] [G loss: 0.427602]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 0/200  Batch Size: 172/173 [loss: 0.749565, acc.: 50.00%] [G loss: 0.420964]\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 0/173 [loss: 0.752380, acc.: 50.00%] [G loss: 0.409514]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 1/173 [loss: 0.762517, acc.: 50.00%] [G loss: 0.408880]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 2/173 [loss: 0.771571, acc.: 50.00%] [G loss: 0.403876]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 3/173 [loss: 0.780194, acc.: 50.00%] [G loss: 0.403882]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 4/173 [loss: 0.777221, acc.: 50.00%] [G loss: 0.408358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 5/173 [loss: 0.782376, acc.: 50.00%] [G loss: 0.408932]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 6/173 [loss: 0.780403, acc.: 50.00%] [G loss: 0.402960]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 7/173 [loss: 0.785959, acc.: 50.00%] [G loss: 0.405828]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 8/173 [loss: 0.782425, acc.: 50.00%] [G loss: 0.408972]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 9/173 [loss: 0.789854, acc.: 50.00%] [G loss: 0.408719]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 10/173 [loss: 0.783163, acc.: 50.00%] [G loss: 0.407070]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 11/173 [loss: 0.778695, acc.: 50.00%] [G loss: 0.411252]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 12/173 [loss: 0.783404, acc.: 50.00%] [G loss: 0.412505]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 13/173 [loss: 0.782293, acc.: 50.00%] [G loss: 0.409682]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 14/173 [loss: 0.788226, acc.: 50.00%] [G loss: 0.411530]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 15/173 [loss: 0.787689, acc.: 50.00%] [G loss: 0.411242]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 16/173 [loss: 0.783449, acc.: 50.00%] [G loss: 0.414687]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 17/173 [loss: 0.786170, acc.: 50.00%] [G loss: 0.416989]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 18/173 [loss: 0.785088, acc.: 50.00%] [G loss: 0.421185]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 19/173 [loss: 0.780745, acc.: 50.00%] [G loss: 0.425648]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 20/173 [loss: 0.780202, acc.: 50.00%] [G loss: 0.427710]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 21/173 [loss: 0.778694, acc.: 50.00%] [G loss: 0.432350]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 22/173 [loss: 0.776696, acc.: 50.00%] [G loss: 0.434515]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 23/173 [loss: 0.772984, acc.: 50.00%] [G loss: 0.437564]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 24/173 [loss: 0.772118, acc.: 50.00%] [G loss: 0.439937]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 25/173 [loss: 0.772204, acc.: 50.00%] [G loss: 0.439078]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 26/173 [loss: 0.768492, acc.: 50.00%] [G loss: 0.447120]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 27/173 [loss: 0.763289, acc.: 50.00%] [G loss: 0.448765]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 28/173 [loss: 0.758600, acc.: 50.00%] [G loss: 0.453939]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 29/173 [loss: 0.760092, acc.: 50.00%] [G loss: 0.456829]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 30/173 [loss: 0.760912, acc.: 50.00%] [G loss: 0.454065]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 31/173 [loss: 0.757181, acc.: 50.00%] [G loss: 0.455436]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 32/173 [loss: 0.748006, acc.: 50.00%] [G loss: 0.461103]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 33/173 [loss: 0.753263, acc.: 50.00%] [G loss: 0.457826]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 34/173 [loss: 0.753703, acc.: 50.00%] [G loss: 0.457521]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 35/173 [loss: 0.749227, acc.: 50.00%] [G loss: 0.452742]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 36/173 [loss: 0.753104, acc.: 50.00%] [G loss: 0.448789]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 37/173 [loss: 0.750051, acc.: 50.00%] [G loss: 0.454552]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 38/173 [loss: 0.755234, acc.: 50.00%] [G loss: 0.446486]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 39/173 [loss: 0.753688, acc.: 50.00%] [G loss: 0.451286]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 40/173 [loss: 0.756397, acc.: 50.00%] [G loss: 0.445751]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 41/173 [loss: 0.756152, acc.: 50.00%] [G loss: 0.441223]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 42/173 [loss: 0.753912, acc.: 50.00%] [G loss: 0.439440]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 43/173 [loss: 0.757940, acc.: 50.00%] [G loss: 0.434526]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 44/173 [loss: 0.759416, acc.: 50.00%] [G loss: 0.435332]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 45/173 [loss: 0.762431, acc.: 50.00%] [G loss: 0.432889]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 46/173 [loss: 0.760417, acc.: 50.00%] [G loss: 0.431045]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 47/173 [loss: 0.767938, acc.: 50.00%] [G loss: 0.428114]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 48/173 [loss: 0.763811, acc.: 50.00%] [G loss: 0.427503]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 49/173 [loss: 0.762396, acc.: 50.00%] [G loss: 0.423444]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 50/173 [loss: 0.762991, acc.: 50.00%] [G loss: 0.423074]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 51/173 [loss: 0.765556, acc.: 50.00%] [G loss: 0.422609]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 52/173 [loss: 0.774138, acc.: 50.00%] [G loss: 0.421422]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 53/173 [loss: 0.767508, acc.: 50.00%] [G loss: 0.425541]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 54/173 [loss: 0.770762, acc.: 50.00%] [G loss: 0.420326]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 55/173 [loss: 0.770388, acc.: 50.00%] [G loss: 0.418472]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 56/173 [loss: 0.773772, acc.: 50.00%] [G loss: 0.416118]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 57/173 [loss: 0.771540, acc.: 50.00%] [G loss: 0.419234]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 58/173 [loss: 0.769500, acc.: 50.00%] [G loss: 0.415633]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 59/173 [loss: 0.771616, acc.: 50.00%] [G loss: 0.416949]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 60/173 [loss: 0.769145, acc.: 50.00%] [G loss: 0.417245]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 61/173 [loss: 0.765755, acc.: 50.00%] [G loss: 0.419372]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 62/173 [loss: 0.767311, acc.: 50.00%] [G loss: 0.419511]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 63/173 [loss: 0.770519, acc.: 50.00%] [G loss: 0.419397]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 64/173 [loss: 0.768935, acc.: 50.00%] [G loss: 0.419353]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 65/173 [loss: 0.765599, acc.: 50.00%] [G loss: 0.424502]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 66/173 [loss: 0.764048, acc.: 50.00%] [G loss: 0.424590]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 67/173 [loss: 0.763338, acc.: 50.00%] [G loss: 0.425412]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 68/173 [loss: 0.758856, acc.: 50.00%] [G loss: 0.432021]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 69/173 [loss: 0.761946, acc.: 50.00%] [G loss: 0.425109]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 70/173 [loss: 0.754874, acc.: 50.00%] [G loss: 0.430887]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 71/173 [loss: 0.758615, acc.: 50.00%] [G loss: 0.431105]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 72/173 [loss: 0.756344, acc.: 50.00%] [G loss: 0.430481]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 73/173 [loss: 0.751858, acc.: 50.00%] [G loss: 0.432238]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 74/173 [loss: 0.750142, acc.: 50.00%] [G loss: 0.438521]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 75/173 [loss: 0.748132, acc.: 50.00%] [G loss: 0.438854]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 76/173 [loss: 0.744695, acc.: 50.00%] [G loss: 0.438855]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 77/173 [loss: 0.743315, acc.: 50.00%] [G loss: 0.436572]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 78/173 [loss: 0.742263, acc.: 50.00%] [G loss: 0.439705]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 79/173 [loss: 0.741689, acc.: 50.00%] [G loss: 0.441972]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 80/173 [loss: 0.740071, acc.: 50.00%] [G loss: 0.441653]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 81/173 [loss: 0.739840, acc.: 50.00%] [G loss: 0.442904]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 82/173 [loss: 0.737804, acc.: 50.00%] [G loss: 0.444282]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 83/173 [loss: 0.741485, acc.: 50.00%] [G loss: 0.438466]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 84/173 [loss: 0.736970, acc.: 50.00%] [G loss: 0.444630]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 85/173 [loss: 0.742561, acc.: 50.00%] [G loss: 0.440695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 86/173 [loss: 0.738135, acc.: 50.00%] [G loss: 0.440624]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 87/173 [loss: 0.739968, acc.: 50.00%] [G loss: 0.437725]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 88/173 [loss: 0.742229, acc.: 50.00%] [G loss: 0.439313]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 89/173 [loss: 0.738888, acc.: 50.00%] [G loss: 0.433975]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 90/173 [loss: 0.747177, acc.: 50.00%] [G loss: 0.434191]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 91/173 [loss: 0.742745, acc.: 50.00%] [G loss: 0.434907]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 92/173 [loss: 0.744646, acc.: 50.00%] [G loss: 0.430514]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 93/173 [loss: 0.746172, acc.: 50.00%] [G loss: 0.431731]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 94/173 [loss: 0.751245, acc.: 50.00%] [G loss: 0.432575]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 95/173 [loss: 0.750933, acc.: 50.00%] [G loss: 0.430557]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 96/173 [loss: 0.753423, acc.: 50.00%] [G loss: 0.427988]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 97/173 [loss: 0.754596, acc.: 50.00%] [G loss: 0.426152]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 98/173 [loss: 0.757758, acc.: 50.00%] [G loss: 0.425745]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 99/173 [loss: 0.757315, acc.: 50.00%] [G loss: 0.426845]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 100/173 [loss: 0.758390, acc.: 50.00%] [G loss: 0.423678]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 101/173 [loss: 0.764027, acc.: 50.00%] [G loss: 0.424271]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 102/173 [loss: 0.761191, acc.: 50.00%] [G loss: 0.420058]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 103/173 [loss: 0.764344, acc.: 50.00%] [G loss: 0.422375]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 104/173 [loss: 0.766240, acc.: 50.00%] [G loss: 0.418303]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 105/173 [loss: 0.764679, acc.: 50.00%] [G loss: 0.422357]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 106/173 [loss: 0.766199, acc.: 50.00%] [G loss: 0.418217]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 107/173 [loss: 0.770275, acc.: 50.00%] [G loss: 0.418350]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 108/173 [loss: 0.772781, acc.: 50.00%] [G loss: 0.420011]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 109/173 [loss: 0.766943, acc.: 50.00%] [G loss: 0.422745]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 110/173 [loss: 0.767000, acc.: 50.00%] [G loss: 0.419398]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 111/173 [loss: 0.764540, acc.: 50.00%] [G loss: 0.420722]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 112/173 [loss: 0.767167, acc.: 50.00%] [G loss: 0.422893]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 113/173 [loss: 0.765376, acc.: 50.00%] [G loss: 0.419999]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 114/173 [loss: 0.768451, acc.: 50.00%] [G loss: 0.423251]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 115/173 [loss: 0.763549, acc.: 50.00%] [G loss: 0.425028]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 116/173 [loss: 0.762108, acc.: 50.00%] [G loss: 0.423607]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 117/173 [loss: 0.765073, acc.: 50.00%] [G loss: 0.422029]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 118/173 [loss: 0.762258, acc.: 50.00%] [G loss: 0.428009]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 119/173 [loss: 0.761917, acc.: 50.00%] [G loss: 0.427672]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 120/173 [loss: 0.759074, acc.: 50.00%] [G loss: 0.425686]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 121/173 [loss: 0.760192, acc.: 50.00%] [G loss: 0.430425]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 122/173 [loss: 0.758571, acc.: 50.00%] [G loss: 0.434161]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 123/173 [loss: 0.759363, acc.: 50.00%] [G loss: 0.432456]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 124/173 [loss: 0.752986, acc.: 50.00%] [G loss: 0.430809]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 125/173 [loss: 0.752666, acc.: 50.00%] [G loss: 0.430940]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 126/173 [loss: 0.758729, acc.: 50.00%] [G loss: 0.436613]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 127/173 [loss: 0.752559, acc.: 50.00%] [G loss: 0.433755]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 128/173 [loss: 0.751253, acc.: 50.00%] [G loss: 0.435927]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 129/173 [loss: 0.747613, acc.: 50.00%] [G loss: 0.436154]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 130/173 [loss: 0.753228, acc.: 50.00%] [G loss: 0.438409]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 131/173 [loss: 0.748660, acc.: 50.00%] [G loss: 0.437143]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 132/173 [loss: 0.748017, acc.: 50.00%] [G loss: 0.439475]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 133/173 [loss: 0.745289, acc.: 50.00%] [G loss: 0.438355]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 134/173 [loss: 0.751452, acc.: 50.00%] [G loss: 0.437210]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 135/173 [loss: 0.747342, acc.: 50.00%] [G loss: 0.436527]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 136/173 [loss: 0.745935, acc.: 50.00%] [G loss: 0.439353]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 137/173 [loss: 0.749122, acc.: 50.00%] [G loss: 0.439058]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 138/173 [loss: 0.747910, acc.: 50.00%] [G loss: 0.438539]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 139/173 [loss: 0.747715, acc.: 50.00%] [G loss: 0.436203]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 140/173 [loss: 0.748285, acc.: 50.00%] [G loss: 0.435939]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 141/173 [loss: 0.748561, acc.: 50.00%] [G loss: 0.436293]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 142/173 [loss: 0.749558, acc.: 50.00%] [G loss: 0.436221]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 143/173 [loss: 0.752376, acc.: 50.00%] [G loss: 0.435874]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 144/173 [loss: 0.749255, acc.: 50.00%] [G loss: 0.437392]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 145/173 [loss: 0.750363, acc.: 50.00%] [G loss: 0.438051]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 146/173 [loss: 0.749298, acc.: 50.00%] [G loss: 0.438634]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 147/173 [loss: 0.748250, acc.: 50.00%] [G loss: 0.440568]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 148/173 [loss: 0.748238, acc.: 50.00%] [G loss: 0.437363]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 149/173 [loss: 0.750057, acc.: 50.00%] [G loss: 0.440358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 150/173 [loss: 0.747937, acc.: 50.00%] [G loss: 0.440408]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 151/173 [loss: 0.751395, acc.: 50.00%] [G loss: 0.439155]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 152/173 [loss: 0.751091, acc.: 50.00%] [G loss: 0.440623]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 153/173 [loss: 0.753530, acc.: 50.00%] [G loss: 0.439412]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 154/173 [loss: 0.750196, acc.: 50.00%] [G loss: 0.439628]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 155/173 [loss: 0.753219, acc.: 50.00%] [G loss: 0.437762]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 156/173 [loss: 0.751796, acc.: 50.00%] [G loss: 0.438030]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 157/173 [loss: 0.752534, acc.: 50.00%] [G loss: 0.436757]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 158/173 [loss: 0.754261, acc.: 50.00%] [G loss: 0.435732]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 159/173 [loss: 0.751992, acc.: 50.00%] [G loss: 0.435788]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 160/173 [loss: 0.751198, acc.: 50.00%] [G loss: 0.431040]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 161/173 [loss: 0.756808, acc.: 50.00%] [G loss: 0.437292]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 162/173 [loss: 0.752404, acc.: 50.00%] [G loss: 0.435297]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 163/173 [loss: 0.753133, acc.: 50.00%] [G loss: 0.437382]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 164/173 [loss: 0.758170, acc.: 50.00%] [G loss: 0.436900]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 165/173 [loss: 0.754253, acc.: 50.00%] [G loss: 0.435656]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 166/173 [loss: 0.752838, acc.: 50.00%] [G loss: 0.434049]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 167/173 [loss: 0.753139, acc.: 50.00%] [G loss: 0.433137]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 168/173 [loss: 0.751319, acc.: 50.00%] [G loss: 0.431631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 169/173 [loss: 0.750614, acc.: 50.00%] [G loss: 0.434351]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 170/173 [loss: 0.752634, acc.: 50.00%] [G loss: 0.437065]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 171/173 [loss: 0.750397, acc.: 50.00%] [G loss: 0.438865]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 1/200  Batch Size: 172/173 [loss: 0.742630, acc.: 50.00%] [G loss: 0.438342]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 0/173 [loss: 0.745793, acc.: 50.00%] [G loss: 0.436331]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 1/173 [loss: 0.747923, acc.: 50.00%] [G loss: 0.438013]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 2/173 [loss: 0.747353, acc.: 50.00%] [G loss: 0.441178]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 3/173 [loss: 0.742009, acc.: 50.00%] [G loss: 0.440194]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 4/173 [loss: 0.741518, acc.: 50.00%] [G loss: 0.443459]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 5/173 [loss: 0.741012, acc.: 50.00%] [G loss: 0.445329]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 6/173 [loss: 0.742049, acc.: 50.00%] [G loss: 0.440152]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 7/173 [loss: 0.742566, acc.: 50.00%] [G loss: 0.443166]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 8/173 [loss: 0.746541, acc.: 50.00%] [G loss: 0.437365]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 9/173 [loss: 0.743741, acc.: 50.00%] [G loss: 0.442365]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 10/173 [loss: 0.739956, acc.: 50.00%] [G loss: 0.441675]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 11/173 [loss: 0.740543, acc.: 50.00%] [G loss: 0.440780]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 12/173 [loss: 0.747365, acc.: 50.00%] [G loss: 0.440803]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 13/173 [loss: 0.741096, acc.: 50.00%] [G loss: 0.435611]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 14/173 [loss: 0.747626, acc.: 50.00%] [G loss: 0.439235]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 15/173 [loss: 0.747608, acc.: 50.00%] [G loss: 0.441539]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 16/173 [loss: 0.749945, acc.: 50.00%] [G loss: 0.438147]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 17/173 [loss: 0.748890, acc.: 50.00%] [G loss: 0.446213]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 18/173 [loss: 0.741230, acc.: 50.00%] [G loss: 0.442884]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 19/173 [loss: 0.746450, acc.: 50.00%] [G loss: 0.449098]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 20/173 [loss: 0.737239, acc.: 50.00%] [G loss: 0.454679]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 21/173 [loss: 0.739852, acc.: 50.00%] [G loss: 0.451430]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 22/173 [loss: 0.739463, acc.: 50.00%] [G loss: 0.459346]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 23/173 [loss: 0.734045, acc.: 50.00%] [G loss: 0.456236]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 24/173 [loss: 0.733373, acc.: 50.00%] [G loss: 0.456386]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 25/173 [loss: 0.731326, acc.: 50.00%] [G loss: 0.453762]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 26/173 [loss: 0.735283, acc.: 50.00%] [G loss: 0.457645]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 27/173 [loss: 0.732642, acc.: 50.00%] [G loss: 0.455605]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 28/173 [loss: 0.738922, acc.: 50.00%] [G loss: 0.452843]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 29/173 [loss: 0.738623, acc.: 50.00%] [G loss: 0.443584]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 30/173 [loss: 0.743454, acc.: 50.00%] [G loss: 0.440180]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 31/173 [loss: 0.749528, acc.: 50.00%] [G loss: 0.434100]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 32/173 [loss: 0.751674, acc.: 50.00%] [G loss: 0.429676]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 33/173 [loss: 0.756015, acc.: 50.00%] [G loss: 0.429393]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 34/173 [loss: 0.758835, acc.: 50.00%] [G loss: 0.422253]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 35/173 [loss: 0.762930, acc.: 50.00%] [G loss: 0.417534]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 36/173 [loss: 0.761488, acc.: 50.00%] [G loss: 0.419816]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 37/173 [loss: 0.764541, acc.: 50.00%] [G loss: 0.417018]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 38/173 [loss: 0.764922, acc.: 50.00%] [G loss: 0.415127]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 39/173 [loss: 0.763100, acc.: 50.00%] [G loss: 0.417537]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 40/173 [loss: 0.764187, acc.: 50.00%] [G loss: 0.420725]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 41/173 [loss: 0.764690, acc.: 50.00%] [G loss: 0.419289]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 42/173 [loss: 0.759311, acc.: 50.00%] [G loss: 0.421316]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 43/173 [loss: 0.760223, acc.: 50.00%] [G loss: 0.421809]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 44/173 [loss: 0.755931, acc.: 50.00%] [G loss: 0.423303]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 45/173 [loss: 0.755767, acc.: 50.00%] [G loss: 0.426197]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 46/173 [loss: 0.751773, acc.: 50.00%] [G loss: 0.424594]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 47/173 [loss: 0.753985, acc.: 50.00%] [G loss: 0.425293]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 48/173 [loss: 0.757438, acc.: 50.00%] [G loss: 0.424007]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 49/173 [loss: 0.754623, acc.: 50.00%] [G loss: 0.421815]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 50/173 [loss: 0.752884, acc.: 50.00%] [G loss: 0.421559]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 51/173 [loss: 0.755820, acc.: 50.00%] [G loss: 0.417068]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 52/173 [loss: 0.760743, acc.: 50.00%] [G loss: 0.417380]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 53/173 [loss: 0.760476, acc.: 50.00%] [G loss: 0.418379]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 54/173 [loss: 0.766642, acc.: 50.00%] [G loss: 0.416914]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 55/173 [loss: 0.766587, acc.: 50.00%] [G loss: 0.415182]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 56/173 [loss: 0.771575, acc.: 50.00%] [G loss: 0.412754]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 57/173 [loss: 0.773784, acc.: 50.00%] [G loss: 0.413507]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 58/173 [loss: 0.771718, acc.: 50.00%] [G loss: 0.415976]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 59/173 [loss: 0.771379, acc.: 50.00%] [G loss: 0.414527]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 60/173 [loss: 0.772636, acc.: 50.00%] [G loss: 0.419732]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 61/173 [loss: 0.767855, acc.: 50.00%] [G loss: 0.425490]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 62/173 [loss: 0.766529, acc.: 50.00%] [G loss: 0.427007]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 63/173 [loss: 0.761531, acc.: 50.00%] [G loss: 0.434654]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 64/173 [loss: 0.755115, acc.: 50.00%] [G loss: 0.440000]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 65/173 [loss: 0.753369, acc.: 50.00%] [G loss: 0.443070]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 66/173 [loss: 0.749131, acc.: 50.00%] [G loss: 0.446218]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 67/173 [loss: 0.747447, acc.: 50.00%] [G loss: 0.447591]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 68/173 [loss: 0.742711, acc.: 50.00%] [G loss: 0.453673]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 69/173 [loss: 0.739514, acc.: 50.00%] [G loss: 0.452004]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 70/173 [loss: 0.740266, acc.: 50.00%] [G loss: 0.451716]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 71/173 [loss: 0.744077, acc.: 50.00%] [G loss: 0.453471]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 72/173 [loss: 0.744919, acc.: 50.00%] [G loss: 0.445458]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 73/173 [loss: 0.745929, acc.: 50.00%] [G loss: 0.446631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 74/173 [loss: 0.745574, acc.: 50.00%] [G loss: 0.442750]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 75/173 [loss: 0.748718, acc.: 50.00%] [G loss: 0.439189]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 76/173 [loss: 0.749263, acc.: 50.00%] [G loss: 0.436401]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 77/173 [loss: 0.752302, acc.: 50.00%] [G loss: 0.438403]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 78/173 [loss: 0.751599, acc.: 50.00%] [G loss: 0.434585]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 79/173 [loss: 0.754719, acc.: 50.00%] [G loss: 0.431079]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 80/173 [loss: 0.752958, acc.: 50.00%] [G loss: 0.432807]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 81/173 [loss: 0.752409, acc.: 50.00%] [G loss: 0.427639]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 82/173 [loss: 0.752910, acc.: 50.00%] [G loss: 0.429734]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 83/173 [loss: 0.752097, acc.: 50.00%] [G loss: 0.428626]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 84/173 [loss: 0.754045, acc.: 50.00%] [G loss: 0.432657]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 85/173 [loss: 0.751029, acc.: 50.00%] [G loss: 0.432215]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 86/173 [loss: 0.751031, acc.: 50.00%] [G loss: 0.434585]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 87/173 [loss: 0.742888, acc.: 50.00%] [G loss: 0.432114]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 88/173 [loss: 0.742805, acc.: 50.00%] [G loss: 0.435607]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 89/173 [loss: 0.743318, acc.: 50.00%] [G loss: 0.437284]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 90/173 [loss: 0.737899, acc.: 50.00%] [G loss: 0.433587]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 91/173 [loss: 0.739619, acc.: 50.00%] [G loss: 0.439367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 92/173 [loss: 0.738881, acc.: 50.00%] [G loss: 0.437933]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 93/173 [loss: 0.738421, acc.: 50.00%] [G loss: 0.436150]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 94/173 [loss: 0.737289, acc.: 50.00%] [G loss: 0.438685]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 95/173 [loss: 0.740362, acc.: 50.00%] [G loss: 0.433997]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 96/173 [loss: 0.744575, acc.: 50.00%] [G loss: 0.430676]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 97/173 [loss: 0.739341, acc.: 50.00%] [G loss: 0.429906]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 98/173 [loss: 0.744924, acc.: 50.00%] [G loss: 0.427655]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 99/173 [loss: 0.750406, acc.: 50.00%] [G loss: 0.425783]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 100/173 [loss: 0.753673, acc.: 50.00%] [G loss: 0.424256]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 101/173 [loss: 0.755585, acc.: 50.00%] [G loss: 0.421463]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 102/173 [loss: 0.759134, acc.: 50.00%] [G loss: 0.419207]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 103/173 [loss: 0.757724, acc.: 50.00%] [G loss: 0.417388]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 104/173 [loss: 0.764177, acc.: 50.00%] [G loss: 0.423514]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 105/173 [loss: 0.759969, acc.: 50.00%] [G loss: 0.420021]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 106/173 [loss: 0.759912, acc.: 50.00%] [G loss: 0.424457]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 107/173 [loss: 0.762807, acc.: 50.00%] [G loss: 0.426444]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 108/173 [loss: 0.757436, acc.: 50.00%] [G loss: 0.431883]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 109/173 [loss: 0.753711, acc.: 50.00%] [G loss: 0.432628]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 110/173 [loss: 0.750935, acc.: 50.00%] [G loss: 0.440027]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 111/173 [loss: 0.745997, acc.: 50.00%] [G loss: 0.445067]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 112/173 [loss: 0.745782, acc.: 50.00%] [G loss: 0.448072]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 113/173 [loss: 0.740612, acc.: 50.00%] [G loss: 0.447590]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 114/173 [loss: 0.745391, acc.: 50.00%] [G loss: 0.448065]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 115/173 [loss: 0.740240, acc.: 50.00%] [G loss: 0.448852]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 116/173 [loss: 0.743428, acc.: 50.00%] [G loss: 0.453225]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 117/173 [loss: 0.742713, acc.: 50.00%] [G loss: 0.446447]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 118/173 [loss: 0.746266, acc.: 50.00%] [G loss: 0.442763]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 119/173 [loss: 0.748543, acc.: 50.00%] [G loss: 0.437241]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 120/173 [loss: 0.748909, acc.: 50.00%] [G loss: 0.435317]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 121/173 [loss: 0.752633, acc.: 50.00%] [G loss: 0.432467]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 122/173 [loss: 0.754162, acc.: 50.00%] [G loss: 0.428821]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 123/173 [loss: 0.757008, acc.: 50.00%] [G loss: 0.425181]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 124/173 [loss: 0.760918, acc.: 50.00%] [G loss: 0.422614]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 125/173 [loss: 0.763685, acc.: 50.00%] [G loss: 0.419268]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 126/173 [loss: 0.764488, acc.: 50.00%] [G loss: 0.421954]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 127/173 [loss: 0.763799, acc.: 50.00%] [G loss: 0.419072]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 128/173 [loss: 0.763552, acc.: 50.00%] [G loss: 0.419011]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 129/173 [loss: 0.763279, acc.: 50.00%] [G loss: 0.418416]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 130/173 [loss: 0.758519, acc.: 50.00%] [G loss: 0.420203]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 131/173 [loss: 0.760685, acc.: 50.00%] [G loss: 0.421498]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 132/173 [loss: 0.760187, acc.: 50.00%] [G loss: 0.420995]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 133/173 [loss: 0.756241, acc.: 50.00%] [G loss: 0.422205]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 134/173 [loss: 0.758825, acc.: 50.00%] [G loss: 0.423661]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 135/173 [loss: 0.755017, acc.: 50.00%] [G loss: 0.423504]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 136/173 [loss: 0.755377, acc.: 50.00%] [G loss: 0.428200]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 137/173 [loss: 0.751280, acc.: 50.00%] [G loss: 0.423674]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 138/173 [loss: 0.752005, acc.: 50.00%] [G loss: 0.425150]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 139/173 [loss: 0.753308, acc.: 50.00%] [G loss: 0.421837]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 140/173 [loss: 0.750938, acc.: 50.00%] [G loss: 0.424206]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 141/173 [loss: 0.749768, acc.: 50.00%] [G loss: 0.424712]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 142/173 [loss: 0.752778, acc.: 50.00%] [G loss: 0.423302]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 143/173 [loss: 0.754980, acc.: 50.00%] [G loss: 0.422569]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 144/173 [loss: 0.757381, acc.: 50.00%] [G loss: 0.425113]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 145/173 [loss: 0.754251, acc.: 50.00%] [G loss: 0.422685]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 146/173 [loss: 0.751470, acc.: 50.00%] [G loss: 0.423843]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 147/173 [loss: 0.757257, acc.: 50.00%] [G loss: 0.425638]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 148/173 [loss: 0.755892, acc.: 50.00%] [G loss: 0.423185]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 149/173 [loss: 0.756585, acc.: 50.00%] [G loss: 0.422889]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 150/173 [loss: 0.757838, acc.: 50.00%] [G loss: 0.423052]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 151/173 [loss: 0.751994, acc.: 50.00%] [G loss: 0.427389]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 152/173 [loss: 0.757142, acc.: 50.00%] [G loss: 0.429515]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 153/173 [loss: 0.758397, acc.: 50.00%] [G loss: 0.429298]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 154/173 [loss: 0.752129, acc.: 50.00%] [G loss: 0.431868]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 155/173 [loss: 0.750877, acc.: 50.00%] [G loss: 0.433219]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 156/173 [loss: 0.747405, acc.: 50.00%] [G loss: 0.435795]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 157/173 [loss: 0.746220, acc.: 50.00%] [G loss: 0.439063]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 158/173 [loss: 0.746065, acc.: 50.00%] [G loss: 0.441777]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 159/173 [loss: 0.741180, acc.: 50.00%] [G loss: 0.442080]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 160/173 [loss: 0.745665, acc.: 50.00%] [G loss: 0.446032]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 161/173 [loss: 0.740259, acc.: 50.00%] [G loss: 0.449853]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 162/173 [loss: 0.739715, acc.: 50.00%] [G loss: 0.445528]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 163/173 [loss: 0.742190, acc.: 50.00%] [G loss: 0.446197]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 164/173 [loss: 0.739416, acc.: 50.00%] [G loss: 0.445825]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 165/173 [loss: 0.740960, acc.: 50.00%] [G loss: 0.445555]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 166/173 [loss: 0.741936, acc.: 50.00%] [G loss: 0.444573]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 167/173 [loss: 0.741162, acc.: 50.00%] [G loss: 0.443205]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 168/173 [loss: 0.740501, acc.: 50.00%] [G loss: 0.440828]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 169/173 [loss: 0.742735, acc.: 50.00%] [G loss: 0.437686]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 170/173 [loss: 0.743091, acc.: 50.00%] [G loss: 0.436800]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 171/173 [loss: 0.744266, acc.: 50.00%] [G loss: 0.436449]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 2/200  Batch Size: 172/173 [loss: 0.745175, acc.: 50.00%] [G loss: 0.434121]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 0/173 [loss: 0.744033, acc.: 50.00%] [G loss: 0.432490]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 1/173 [loss: 0.746949, acc.: 50.00%] [G loss: 0.430940]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 2/173 [loss: 0.747688, acc.: 50.00%] [G loss: 0.429304]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 3/173 [loss: 0.750397, acc.: 50.00%] [G loss: 0.430180]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 4/173 [loss: 0.747237, acc.: 50.00%] [G loss: 0.428138]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 5/173 [loss: 0.747654, acc.: 50.00%] [G loss: 0.428879]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 6/173 [loss: 0.748969, acc.: 50.00%] [G loss: 0.428554]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 7/173 [loss: 0.748488, acc.: 50.00%] [G loss: 0.426535]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 8/173 [loss: 0.750733, acc.: 50.00%] [G loss: 0.428287]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 9/173 [loss: 0.749278, acc.: 50.00%] [G loss: 0.427477]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 10/173 [loss: 0.747816, acc.: 50.00%] [G loss: 0.427874]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 11/173 [loss: 0.746388, acc.: 50.00%] [G loss: 0.427493]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 12/173 [loss: 0.751892, acc.: 50.00%] [G loss: 0.426228]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 13/173 [loss: 0.749852, acc.: 50.00%] [G loss: 0.426453]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 14/173 [loss: 0.746493, acc.: 50.00%] [G loss: 0.425604]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 15/173 [loss: 0.751460, acc.: 50.00%] [G loss: 0.425324]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 16/173 [loss: 0.749341, acc.: 50.00%] [G loss: 0.424169]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 17/173 [loss: 0.751164, acc.: 50.00%] [G loss: 0.423743]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 18/173 [loss: 0.749579, acc.: 50.00%] [G loss: 0.427347]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 19/173 [loss: 0.751585, acc.: 50.00%] [G loss: 0.422744]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 20/173 [loss: 0.750613, acc.: 50.00%] [G loss: 0.423832]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 21/173 [loss: 0.751712, acc.: 50.00%] [G loss: 0.422299]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 22/173 [loss: 0.752276, acc.: 50.00%] [G loss: 0.422607]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 23/173 [loss: 0.754810, acc.: 50.00%] [G loss: 0.423324]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 24/173 [loss: 0.757137, acc.: 50.00%] [G loss: 0.422317]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 25/173 [loss: 0.756864, acc.: 50.00%] [G loss: 0.420305]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 26/173 [loss: 0.753885, acc.: 50.00%] [G loss: 0.420564]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 27/173 [loss: 0.756269, acc.: 50.00%] [G loss: 0.422625]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 28/173 [loss: 0.752616, acc.: 50.00%] [G loss: 0.422438]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 29/173 [loss: 0.755325, acc.: 50.00%] [G loss: 0.423939]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 30/173 [loss: 0.757390, acc.: 50.00%] [G loss: 0.426494]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 31/173 [loss: 0.753909, acc.: 50.00%] [G loss: 0.427341]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 32/173 [loss: 0.753054, acc.: 50.00%] [G loss: 0.426514]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 33/173 [loss: 0.753721, acc.: 50.00%] [G loss: 0.430776]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 34/173 [loss: 0.746259, acc.: 50.00%] [G loss: 0.429518]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 35/173 [loss: 0.752980, acc.: 50.00%] [G loss: 0.431217]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 36/173 [loss: 0.749248, acc.: 50.00%] [G loss: 0.429333]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 37/173 [loss: 0.747522, acc.: 50.00%] [G loss: 0.437068]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 38/173 [loss: 0.748437, acc.: 50.00%] [G loss: 0.432638]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 39/173 [loss: 0.746886, acc.: 50.00%] [G loss: 0.434870]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 40/173 [loss: 0.744425, acc.: 50.00%] [G loss: 0.431241]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 41/173 [loss: 0.745993, acc.: 50.00%] [G loss: 0.436254]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 42/173 [loss: 0.746059, acc.: 50.00%] [G loss: 0.435495]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 43/173 [loss: 0.748478, acc.: 50.00%] [G loss: 0.433320]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 44/173 [loss: 0.745877, acc.: 50.00%] [G loss: 0.434451]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 45/173 [loss: 0.745642, acc.: 50.00%] [G loss: 0.432581]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 46/173 [loss: 0.748195, acc.: 50.00%] [G loss: 0.431311]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 47/173 [loss: 0.748819, acc.: 50.00%] [G loss: 0.429287]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 48/173 [loss: 0.745639, acc.: 50.00%] [G loss: 0.431895]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 49/173 [loss: 0.748497, acc.: 50.00%] [G loss: 0.432686]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 50/173 [loss: 0.749012, acc.: 50.00%] [G loss: 0.428751]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 51/173 [loss: 0.749653, acc.: 50.00%] [G loss: 0.428656]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 52/173 [loss: 0.747783, acc.: 50.00%] [G loss: 0.426608]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 53/173 [loss: 0.745904, acc.: 50.00%] [G loss: 0.430366]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 54/173 [loss: 0.746541, acc.: 50.00%] [G loss: 0.427973]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 55/173 [loss: 0.747225, acc.: 50.00%] [G loss: 0.429167]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 56/173 [loss: 0.746164, acc.: 50.00%] [G loss: 0.430347]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 57/173 [loss: 0.744984, acc.: 50.00%] [G loss: 0.430237]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 58/173 [loss: 0.746200, acc.: 50.00%] [G loss: 0.430448]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 59/173 [loss: 0.744792, acc.: 50.00%] [G loss: 0.427919]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 60/173 [loss: 0.742692, acc.: 50.00%] [G loss: 0.429734]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 61/173 [loss: 0.745451, acc.: 50.00%] [G loss: 0.429792]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 62/173 [loss: 0.741372, acc.: 50.00%] [G loss: 0.430973]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 63/173 [loss: 0.744082, acc.: 50.00%] [G loss: 0.428437]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 64/173 [loss: 0.740008, acc.: 50.00%] [G loss: 0.428275]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 65/173 [loss: 0.745510, acc.: 50.00%] [G loss: 0.431571]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 66/173 [loss: 0.742816, acc.: 50.00%] [G loss: 0.427994]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 67/173 [loss: 0.745469, acc.: 50.00%] [G loss: 0.426380]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 68/173 [loss: 0.744779, acc.: 50.00%] [G loss: 0.426220]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 69/173 [loss: 0.743467, acc.: 50.00%] [G loss: 0.428139]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 70/173 [loss: 0.747352, acc.: 50.00%] [G loss: 0.430958]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 71/173 [loss: 0.742400, acc.: 50.00%] [G loss: 0.430714]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 72/173 [loss: 0.744276, acc.: 50.00%] [G loss: 0.431619]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 73/173 [loss: 0.743420, acc.: 50.00%] [G loss: 0.429515]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 74/173 [loss: 0.745048, acc.: 50.00%] [G loss: 0.432388]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 75/173 [loss: 0.742541, acc.: 50.00%] [G loss: 0.432980]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 76/173 [loss: 0.744598, acc.: 50.00%] [G loss: 0.431382]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 77/173 [loss: 0.744540, acc.: 50.00%] [G loss: 0.434428]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 78/173 [loss: 0.742159, acc.: 50.00%] [G loss: 0.431745]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 79/173 [loss: 0.743550, acc.: 50.00%] [G loss: 0.431539]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 80/173 [loss: 0.744307, acc.: 50.00%] [G loss: 0.436801]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 81/173 [loss: 0.745059, acc.: 50.00%] [G loss: 0.433257]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 82/173 [loss: 0.744480, acc.: 50.00%] [G loss: 0.434576]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 83/173 [loss: 0.746595, acc.: 50.00%] [G loss: 0.432705]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 84/173 [loss: 0.744770, acc.: 50.00%] [G loss: 0.429226]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 85/173 [loss: 0.744280, acc.: 50.00%] [G loss: 0.427816]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 86/173 [loss: 0.745482, acc.: 50.00%] [G loss: 0.430375]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 87/173 [loss: 0.743958, acc.: 50.00%] [G loss: 0.429544]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 88/173 [loss: 0.745998, acc.: 50.00%] [G loss: 0.429900]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 89/173 [loss: 0.745819, acc.: 50.00%] [G loss: 0.425291]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 90/173 [loss: 0.746776, acc.: 50.00%] [G loss: 0.425204]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 91/173 [loss: 0.748141, acc.: 50.00%] [G loss: 0.425252]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 92/173 [loss: 0.746727, acc.: 50.00%] [G loss: 0.427211]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 93/173 [loss: 0.747028, acc.: 50.00%] [G loss: 0.425097]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 94/173 [loss: 0.747313, acc.: 50.00%] [G loss: 0.421655]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 95/173 [loss: 0.746276, acc.: 50.00%] [G loss: 0.426456]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 96/173 [loss: 0.746873, acc.: 50.00%] [G loss: 0.424595]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 97/173 [loss: 0.748070, acc.: 50.00%] [G loss: 0.425346]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 98/173 [loss: 0.749091, acc.: 50.00%] [G loss: 0.426475]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 99/173 [loss: 0.748502, acc.: 50.00%] [G loss: 0.426308]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 100/173 [loss: 0.746743, acc.: 50.00%] [G loss: 0.420809]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 101/173 [loss: 0.748429, acc.: 50.00%] [G loss: 0.423555]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 102/173 [loss: 0.748417, acc.: 50.00%] [G loss: 0.424961]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 103/173 [loss: 0.747093, acc.: 50.00%] [G loss: 0.423572]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 104/173 [loss: 0.748058, acc.: 50.00%] [G loss: 0.424828]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 105/173 [loss: 0.748473, acc.: 50.00%] [G loss: 0.423264]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 106/173 [loss: 0.745618, acc.: 50.00%] [G loss: 0.420699]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 107/173 [loss: 0.747846, acc.: 50.00%] [G loss: 0.421682]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 108/173 [loss: 0.747278, acc.: 50.00%] [G loss: 0.423633]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 109/173 [loss: 0.744508, acc.: 50.00%] [G loss: 0.422664]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 110/173 [loss: 0.747812, acc.: 50.00%] [G loss: 0.421790]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 111/173 [loss: 0.751159, acc.: 50.00%] [G loss: 0.423143]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 112/173 [loss: 0.750065, acc.: 50.00%] [G loss: 0.422076]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 113/173 [loss: 0.748476, acc.: 50.00%] [G loss: 0.425737]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 114/173 [loss: 0.751544, acc.: 50.00%] [G loss: 0.423392]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 115/173 [loss: 0.747966, acc.: 50.00%] [G loss: 0.426819]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 116/173 [loss: 0.749535, acc.: 50.00%] [G loss: 0.424863]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 117/173 [loss: 0.750214, acc.: 50.00%] [G loss: 0.425863]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 118/173 [loss: 0.749701, acc.: 50.00%] [G loss: 0.425934]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 119/173 [loss: 0.750069, acc.: 50.00%] [G loss: 0.426294]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 120/173 [loss: 0.745291, acc.: 50.00%] [G loss: 0.425343]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 121/173 [loss: 0.750777, acc.: 50.00%] [G loss: 0.427031]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 122/173 [loss: 0.747542, acc.: 50.00%] [G loss: 0.427652]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 123/173 [loss: 0.751762, acc.: 50.00%] [G loss: 0.428053]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 124/173 [loss: 0.746646, acc.: 50.00%] [G loss: 0.428896]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 125/173 [loss: 0.751778, acc.: 50.00%] [G loss: 0.427046]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 126/173 [loss: 0.748153, acc.: 50.00%] [G loss: 0.428165]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 127/173 [loss: 0.749044, acc.: 50.00%] [G loss: 0.427081]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 128/173 [loss: 0.745028, acc.: 50.00%] [G loss: 0.429438]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 129/173 [loss: 0.747716, acc.: 50.00%] [G loss: 0.426851]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 130/173 [loss: 0.746478, acc.: 50.00%] [G loss: 0.426036]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 131/173 [loss: 0.748058, acc.: 50.00%] [G loss: 0.427215]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 132/173 [loss: 0.748471, acc.: 50.00%] [G loss: 0.427524]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 133/173 [loss: 0.749216, acc.: 50.00%] [G loss: 0.426756]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 134/173 [loss: 0.749201, acc.: 50.00%] [G loss: 0.425529]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 135/173 [loss: 0.747558, acc.: 50.00%] [G loss: 0.427842]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 136/173 [loss: 0.748215, acc.: 50.00%] [G loss: 0.426525]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 137/173 [loss: 0.748094, acc.: 50.00%] [G loss: 0.424221]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 138/173 [loss: 0.748860, acc.: 50.00%] [G loss: 0.426791]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 139/173 [loss: 0.746670, acc.: 50.00%] [G loss: 0.426888]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 140/173 [loss: 0.749180, acc.: 50.00%] [G loss: 0.424411]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 141/173 [loss: 0.746995, acc.: 50.00%] [G loss: 0.425224]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 142/173 [loss: 0.748239, acc.: 50.00%] [G loss: 0.429320]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 143/173 [loss: 0.750944, acc.: 50.00%] [G loss: 0.425150]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 144/173 [loss: 0.747815, acc.: 50.00%] [G loss: 0.426911]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 145/173 [loss: 0.749472, acc.: 50.00%] [G loss: 0.426458]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 146/173 [loss: 0.749539, acc.: 50.00%] [G loss: 0.427680]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 147/173 [loss: 0.748375, acc.: 50.00%] [G loss: 0.426745]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 148/173 [loss: 0.747309, acc.: 50.00%] [G loss: 0.428214]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 149/173 [loss: 0.747564, acc.: 50.00%] [G loss: 0.427643]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 150/173 [loss: 0.746843, acc.: 50.00%] [G loss: 0.431893]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 151/173 [loss: 0.747892, acc.: 50.00%] [G loss: 0.430877]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 152/173 [loss: 0.745463, acc.: 50.00%] [G loss: 0.429259]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 153/173 [loss: 0.747709, acc.: 50.00%] [G loss: 0.430054]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 154/173 [loss: 0.746068, acc.: 50.00%] [G loss: 0.430774]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 155/173 [loss: 0.745808, acc.: 50.00%] [G loss: 0.432285]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 156/173 [loss: 0.744127, acc.: 50.00%] [G loss: 0.431977]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 157/173 [loss: 0.743580, acc.: 50.00%] [G loss: 0.431525]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 158/173 [loss: 0.741733, acc.: 50.00%] [G loss: 0.432148]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 159/173 [loss: 0.744956, acc.: 50.00%] [G loss: 0.433822]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 160/173 [loss: 0.743735, acc.: 50.00%] [G loss: 0.429765]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 161/173 [loss: 0.742807, acc.: 50.00%] [G loss: 0.429073]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 162/173 [loss: 0.744417, acc.: 50.00%] [G loss: 0.430352]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 163/173 [loss: 0.746500, acc.: 50.00%] [G loss: 0.429835]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 164/173 [loss: 0.744400, acc.: 50.00%] [G loss: 0.430919]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 165/173 [loss: 0.744194, acc.: 50.00%] [G loss: 0.430008]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 166/173 [loss: 0.745691, acc.: 50.00%] [G loss: 0.428195]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 167/173 [loss: 0.747819, acc.: 50.00%] [G loss: 0.427429]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 168/173 [loss: 0.746408, acc.: 50.00%] [G loss: 0.424412]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 169/173 [loss: 0.748727, acc.: 50.00%] [G loss: 0.428782]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 170/173 [loss: 0.747696, acc.: 50.00%] [G loss: 0.422659]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 171/173 [loss: 0.745384, acc.: 50.00%] [G loss: 0.423425]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 3/200  Batch Size: 172/173 [loss: 0.749575, acc.: 50.00%] [G loss: 0.423892]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 0/173 [loss: 0.747968, acc.: 50.00%] [G loss: 0.423565]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 1/173 [loss: 0.748576, acc.: 50.00%] [G loss: 0.422346]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 2/173 [loss: 0.750664, acc.: 50.00%] [G loss: 0.422502]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 3/173 [loss: 0.750598, acc.: 50.00%] [G loss: 0.420074]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 4/173 [loss: 0.751439, acc.: 50.00%] [G loss: 0.420843]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 5/173 [loss: 0.752020, acc.: 50.00%] [G loss: 0.420230]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 6/173 [loss: 0.756754, acc.: 50.00%] [G loss: 0.420468]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 7/173 [loss: 0.754857, acc.: 50.00%] [G loss: 0.420475]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 8/173 [loss: 0.754336, acc.: 50.00%] [G loss: 0.419685]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 9/173 [loss: 0.753192, acc.: 50.00%] [G loss: 0.417377]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 10/173 [loss: 0.756368, acc.: 50.00%] [G loss: 0.422306]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 11/173 [loss: 0.754069, acc.: 50.00%] [G loss: 0.422349]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 12/173 [loss: 0.753658, acc.: 50.00%] [G loss: 0.420925]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 13/173 [loss: 0.753032, acc.: 50.00%] [G loss: 0.419884]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 14/173 [loss: 0.751970, acc.: 50.00%] [G loss: 0.419811]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 15/173 [loss: 0.751638, acc.: 50.00%] [G loss: 0.420487]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 16/173 [loss: 0.751944, acc.: 50.00%] [G loss: 0.421787]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 17/173 [loss: 0.748235, acc.: 50.00%] [G loss: 0.422844]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 18/173 [loss: 0.752699, acc.: 50.00%] [G loss: 0.423032]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 19/173 [loss: 0.750555, acc.: 50.00%] [G loss: 0.425401]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 20/173 [loss: 0.751178, acc.: 50.00%] [G loss: 0.423932]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 21/173 [loss: 0.751075, acc.: 50.00%] [G loss: 0.427786]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 22/173 [loss: 0.748277, acc.: 50.00%] [G loss: 0.424853]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 23/173 [loss: 0.748693, acc.: 50.00%] [G loss: 0.429567]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 24/173 [loss: 0.747325, acc.: 50.00%] [G loss: 0.428295]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 25/173 [loss: 0.744063, acc.: 50.00%] [G loss: 0.428388]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 26/173 [loss: 0.747955, acc.: 50.00%] [G loss: 0.429022]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 27/173 [loss: 0.744993, acc.: 50.00%] [G loss: 0.429695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 28/173 [loss: 0.746487, acc.: 50.00%] [G loss: 0.426394]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 29/173 [loss: 0.744920, acc.: 50.00%] [G loss: 0.425384]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 30/173 [loss: 0.744650, acc.: 50.00%] [G loss: 0.426947]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 31/173 [loss: 0.746094, acc.: 50.00%] [G loss: 0.424859]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 32/173 [loss: 0.747946, acc.: 50.00%] [G loss: 0.426910]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 33/173 [loss: 0.747138, acc.: 50.00%] [G loss: 0.426355]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 34/173 [loss: 0.746555, acc.: 50.00%] [G loss: 0.428952]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 35/173 [loss: 0.746414, acc.: 50.00%] [G loss: 0.427579]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 36/173 [loss: 0.746531, acc.: 50.00%] [G loss: 0.424444]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 37/173 [loss: 0.746993, acc.: 50.00%] [G loss: 0.425144]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 38/173 [loss: 0.747075, acc.: 50.00%] [G loss: 0.421818]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 39/173 [loss: 0.749546, acc.: 50.00%] [G loss: 0.423126]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 40/173 [loss: 0.749211, acc.: 50.00%] [G loss: 0.422806]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 41/173 [loss: 0.750548, acc.: 50.00%] [G loss: 0.420467]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 42/173 [loss: 0.751141, acc.: 50.00%] [G loss: 0.420973]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 43/173 [loss: 0.751809, acc.: 50.00%] [G loss: 0.419420]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 44/173 [loss: 0.748106, acc.: 50.00%] [G loss: 0.422381]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 45/173 [loss: 0.751353, acc.: 50.00%] [G loss: 0.419124]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 46/173 [loss: 0.751114, acc.: 50.00%] [G loss: 0.421534]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 47/173 [loss: 0.755230, acc.: 50.00%] [G loss: 0.419658]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 48/173 [loss: 0.753229, acc.: 50.00%] [G loss: 0.420187]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 49/173 [loss: 0.754742, acc.: 50.00%] [G loss: 0.420492]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 50/173 [loss: 0.754995, acc.: 50.00%] [G loss: 0.422094]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 51/173 [loss: 0.754754, acc.: 50.00%] [G loss: 0.424602]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 52/173 [loss: 0.752122, acc.: 50.00%] [G loss: 0.425234]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 53/173 [loss: 0.748618, acc.: 50.00%] [G loss: 0.425598]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 54/173 [loss: 0.753442, acc.: 50.00%] [G loss: 0.424442]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 55/173 [loss: 0.750046, acc.: 50.00%] [G loss: 0.425142]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 56/173 [loss: 0.748672, acc.: 50.00%] [G loss: 0.423940]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 57/173 [loss: 0.750160, acc.: 50.00%] [G loss: 0.423942]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 58/173 [loss: 0.751795, acc.: 50.00%] [G loss: 0.422177]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 59/173 [loss: 0.748515, acc.: 50.00%] [G loss: 0.423375]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 60/173 [loss: 0.750321, acc.: 50.00%] [G loss: 0.423164]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 61/173 [loss: 0.747303, acc.: 50.00%] [G loss: 0.424163]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 62/173 [loss: 0.747841, acc.: 50.00%] [G loss: 0.423603]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 63/173 [loss: 0.749653, acc.: 50.00%] [G loss: 0.424938]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 64/173 [loss: 0.749048, acc.: 50.00%] [G loss: 0.423041]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 65/173 [loss: 0.748178, acc.: 50.00%] [G loss: 0.421853]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 66/173 [loss: 0.747044, acc.: 50.00%] [G loss: 0.426859]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 67/173 [loss: 0.744793, acc.: 50.00%] [G loss: 0.425390]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 68/173 [loss: 0.748505, acc.: 50.00%] [G loss: 0.427385]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 69/173 [loss: 0.745158, acc.: 50.00%] [G loss: 0.425277]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 70/173 [loss: 0.744919, acc.: 50.00%] [G loss: 0.426764]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 71/173 [loss: 0.745672, acc.: 50.00%] [G loss: 0.424691]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 72/173 [loss: 0.744673, acc.: 50.00%] [G loss: 0.425063]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 73/173 [loss: 0.742825, acc.: 50.00%] [G loss: 0.426050]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 74/173 [loss: 0.748468, acc.: 50.00%] [G loss: 0.424468]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 75/173 [loss: 0.747318, acc.: 50.00%] [G loss: 0.424924]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 76/173 [loss: 0.748121, acc.: 50.00%] [G loss: 0.425970]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 77/173 [loss: 0.743835, acc.: 50.00%] [G loss: 0.424730]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 78/173 [loss: 0.748568, acc.: 50.00%] [G loss: 0.422722]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 79/173 [loss: 0.747482, acc.: 50.00%] [G loss: 0.424910]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 80/173 [loss: 0.746524, acc.: 50.00%] [G loss: 0.423269]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 81/173 [loss: 0.747080, acc.: 50.00%] [G loss: 0.427615]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 82/173 [loss: 0.745338, acc.: 50.00%] [G loss: 0.428189]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 83/173 [loss: 0.746098, acc.: 50.00%] [G loss: 0.427846]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 84/173 [loss: 0.749122, acc.: 50.00%] [G loss: 0.427887]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 85/173 [loss: 0.742846, acc.: 50.00%] [G loss: 0.427771]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 86/173 [loss: 0.747286, acc.: 50.00%] [G loss: 0.425627]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 87/173 [loss: 0.750290, acc.: 50.00%] [G loss: 0.424855]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 88/173 [loss: 0.747069, acc.: 50.00%] [G loss: 0.425350]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 89/173 [loss: 0.751473, acc.: 50.00%] [G loss: 0.423017]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 90/173 [loss: 0.751909, acc.: 50.00%] [G loss: 0.420826]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 91/173 [loss: 0.751137, acc.: 50.00%] [G loss: 0.417500]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 92/173 [loss: 0.752267, acc.: 50.00%] [G loss: 0.421248]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 93/173 [loss: 0.754519, acc.: 50.00%] [G loss: 0.418935]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 94/173 [loss: 0.751201, acc.: 50.00%] [G loss: 0.419310]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 95/173 [loss: 0.751175, acc.: 50.00%] [G loss: 0.419265]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 96/173 [loss: 0.750276, acc.: 50.00%] [G loss: 0.419784]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 97/173 [loss: 0.753117, acc.: 50.00%] [G loss: 0.418778]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 98/173 [loss: 0.750339, acc.: 50.00%] [G loss: 0.418931]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 99/173 [loss: 0.752454, acc.: 50.00%] [G loss: 0.418348]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 100/173 [loss: 0.752509, acc.: 50.00%] [G loss: 0.419438]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 101/173 [loss: 0.750128, acc.: 50.00%] [G loss: 0.419127]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 102/173 [loss: 0.747853, acc.: 50.00%] [G loss: 0.418876]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 103/173 [loss: 0.748750, acc.: 50.00%] [G loss: 0.418724]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 104/173 [loss: 0.747438, acc.: 50.00%] [G loss: 0.419696]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 105/173 [loss: 0.746631, acc.: 50.00%] [G loss: 0.422805]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 106/173 [loss: 0.750417, acc.: 50.00%] [G loss: 0.418383]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 107/173 [loss: 0.750395, acc.: 50.00%] [G loss: 0.420624]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 108/173 [loss: 0.751423, acc.: 50.00%] [G loss: 0.423350]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 109/173 [loss: 0.750584, acc.: 50.00%] [G loss: 0.422283]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 110/173 [loss: 0.748978, acc.: 50.00%] [G loss: 0.425798]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 111/173 [loss: 0.746360, acc.: 50.00%] [G loss: 0.423551]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 112/173 [loss: 0.745425, acc.: 50.00%] [G loss: 0.424629]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 113/173 [loss: 0.743604, acc.: 50.00%] [G loss: 0.425156]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 114/173 [loss: 0.743291, acc.: 50.00%] [G loss: 0.428724]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 115/173 [loss: 0.750696, acc.: 50.00%] [G loss: 0.426886]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 116/173 [loss: 0.747229, acc.: 50.00%] [G loss: 0.427174]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 117/173 [loss: 0.748507, acc.: 50.00%] [G loss: 0.424570]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 118/173 [loss: 0.749145, acc.: 50.00%] [G loss: 0.424934]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 119/173 [loss: 0.750046, acc.: 50.00%] [G loss: 0.421448]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 120/173 [loss: 0.750036, acc.: 50.00%] [G loss: 0.422507]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 121/173 [loss: 0.752884, acc.: 50.00%] [G loss: 0.422158]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 122/173 [loss: 0.751962, acc.: 50.00%] [G loss: 0.418690]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 123/173 [loss: 0.754389, acc.: 50.00%] [G loss: 0.418909]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 124/173 [loss: 0.751604, acc.: 50.00%] [G loss: 0.418266]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 125/173 [loss: 0.752657, acc.: 50.00%] [G loss: 0.417536]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 126/173 [loss: 0.751563, acc.: 50.00%] [G loss: 0.417169]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 127/173 [loss: 0.752155, acc.: 50.00%] [G loss: 0.416053]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 128/173 [loss: 0.751992, acc.: 50.00%] [G loss: 0.415218]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 129/173 [loss: 0.751354, acc.: 50.00%] [G loss: 0.415537]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 130/173 [loss: 0.753896, acc.: 50.00%] [G loss: 0.416086]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 131/173 [loss: 0.752682, acc.: 50.00%] [G loss: 0.416468]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 132/173 [loss: 0.751548, acc.: 50.00%] [G loss: 0.416698]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 133/173 [loss: 0.752281, acc.: 50.00%] [G loss: 0.415458]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 134/173 [loss: 0.753738, acc.: 50.00%] [G loss: 0.416220]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 135/173 [loss: 0.755461, acc.: 50.00%] [G loss: 0.416894]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 136/173 [loss: 0.754678, acc.: 50.00%] [G loss: 0.418304]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 137/173 [loss: 0.753619, acc.: 50.00%] [G loss: 0.419559]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 138/173 [loss: 0.752596, acc.: 50.00%] [G loss: 0.420380]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 139/173 [loss: 0.747560, acc.: 50.00%] [G loss: 0.423210]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 140/173 [loss: 0.747479, acc.: 50.00%] [G loss: 0.424479]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 141/173 [loss: 0.747849, acc.: 50.00%] [G loss: 0.426866]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 142/173 [loss: 0.747490, acc.: 50.00%] [G loss: 0.431057]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 143/173 [loss: 0.746653, acc.: 50.00%] [G loss: 0.431698]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 144/173 [loss: 0.742100, acc.: 50.00%] [G loss: 0.435252]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 145/173 [loss: 0.743827, acc.: 50.00%] [G loss: 0.431793]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 146/173 [loss: 0.743693, acc.: 50.00%] [G loss: 0.432344]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 147/173 [loss: 0.742253, acc.: 50.00%] [G loss: 0.431710]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 148/173 [loss: 0.743831, acc.: 50.00%] [G loss: 0.430204]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 149/173 [loss: 0.743381, acc.: 50.00%] [G loss: 0.431000]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 150/173 [loss: 0.745953, acc.: 50.00%] [G loss: 0.426504]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 151/173 [loss: 0.747489, acc.: 50.00%] [G loss: 0.424513]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 152/173 [loss: 0.750997, acc.: 50.00%] [G loss: 0.420564]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 153/173 [loss: 0.750085, acc.: 50.00%] [G loss: 0.419034]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 154/173 [loss: 0.747684, acc.: 50.00%] [G loss: 0.421100]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 155/173 [loss: 0.747539, acc.: 50.00%] [G loss: 0.420375]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 156/173 [loss: 0.747721, acc.: 50.00%] [G loss: 0.419531]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 157/173 [loss: 0.743985, acc.: 50.00%] [G loss: 0.420623]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 158/173 [loss: 0.744708, acc.: 50.00%] [G loss: 0.421795]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 159/173 [loss: 0.747692, acc.: 50.00%] [G loss: 0.420076]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 160/173 [loss: 0.744536, acc.: 50.00%] [G loss: 0.420398]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 161/173 [loss: 0.746729, acc.: 50.00%] [G loss: 0.418188]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 162/173 [loss: 0.749562, acc.: 50.00%] [G loss: 0.415653]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 163/173 [loss: 0.747468, acc.: 50.00%] [G loss: 0.412227]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 164/173 [loss: 0.752065, acc.: 50.00%] [G loss: 0.410730]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 165/173 [loss: 0.756823, acc.: 50.00%] [G loss: 0.409960]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 166/173 [loss: 0.760203, acc.: 50.00%] [G loss: 0.406072]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 167/173 [loss: 0.760821, acc.: 50.00%] [G loss: 0.405857]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 168/173 [loss: 0.760372, acc.: 50.00%] [G loss: 0.410095]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 169/173 [loss: 0.762153, acc.: 50.00%] [G loss: 0.410582]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 170/173 [loss: 0.759235, acc.: 50.00%] [G loss: 0.416197]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 171/173 [loss: 0.755779, acc.: 50.00%] [G loss: 0.422645]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 4/200  Batch Size: 172/173 [loss: 0.747955, acc.: 50.00%] [G loss: 0.430659]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 0/173 [loss: 0.745622, acc.: 50.00%] [G loss: 0.434263]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 1/173 [loss: 0.744610, acc.: 50.00%] [G loss: 0.439052]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 2/173 [loss: 0.741599, acc.: 50.00%] [G loss: 0.438042]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 3/173 [loss: 0.742977, acc.: 50.00%] [G loss: 0.431352]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 4/173 [loss: 0.745092, acc.: 50.00%] [G loss: 0.429845]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 5/173 [loss: 0.748162, acc.: 50.00%] [G loss: 0.424833]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 6/173 [loss: 0.751694, acc.: 50.00%] [G loss: 0.422688]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 7/173 [loss: 0.752619, acc.: 50.00%] [G loss: 0.419393]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 8/173 [loss: 0.753552, acc.: 50.00%] [G loss: 0.416383]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 9/173 [loss: 0.753382, acc.: 50.00%] [G loss: 0.417030]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 10/173 [loss: 0.751441, acc.: 50.00%] [G loss: 0.417694]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 11/173 [loss: 0.752919, acc.: 50.00%] [G loss: 0.416891]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 12/173 [loss: 0.748332, acc.: 50.00%] [G loss: 0.419305]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 13/173 [loss: 0.746381, acc.: 50.00%] [G loss: 0.420956]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 14/173 [loss: 0.743724, acc.: 50.00%] [G loss: 0.421721]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 15/173 [loss: 0.742368, acc.: 50.00%] [G loss: 0.422425]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 16/173 [loss: 0.742289, acc.: 50.00%] [G loss: 0.422243]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 17/173 [loss: 0.742437, acc.: 50.00%] [G loss: 0.420276]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 18/173 [loss: 0.743407, acc.: 50.00%] [G loss: 0.417964]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 19/173 [loss: 0.742479, acc.: 50.00%] [G loss: 0.417990]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 20/173 [loss: 0.749344, acc.: 50.00%] [G loss: 0.411839]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 21/173 [loss: 0.753261, acc.: 50.00%] [G loss: 0.408490]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 22/173 [loss: 0.756626, acc.: 50.00%] [G loss: 0.407679]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 23/173 [loss: 0.757346, acc.: 50.00%] [G loss: 0.410413]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 24/173 [loss: 0.759502, acc.: 50.00%] [G loss: 0.409064]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 25/173 [loss: 0.759742, acc.: 50.00%] [G loss: 0.411370]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 26/173 [loss: 0.756528, acc.: 50.00%] [G loss: 0.418196]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 27/173 [loss: 0.751430, acc.: 50.00%] [G loss: 0.422179]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 28/173 [loss: 0.747342, acc.: 50.00%] [G loss: 0.427536]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 29/173 [loss: 0.748321, acc.: 50.00%] [G loss: 0.431958]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 30/173 [loss: 0.747260, acc.: 50.00%] [G loss: 0.433110]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 31/173 [loss: 0.745719, acc.: 50.00%] [G loss: 0.430203]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 32/173 [loss: 0.743896, acc.: 50.00%] [G loss: 0.433360]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 33/173 [loss: 0.746245, acc.: 50.00%] [G loss: 0.428461]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 34/173 [loss: 0.748665, acc.: 50.00%] [G loss: 0.422725]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 35/173 [loss: 0.748371, acc.: 50.00%] [G loss: 0.421161]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 36/173 [loss: 0.751669, acc.: 50.00%] [G loss: 0.422904]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 37/173 [loss: 0.753719, acc.: 50.00%] [G loss: 0.418358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 38/173 [loss: 0.753771, acc.: 50.00%] [G loss: 0.416249]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 39/173 [loss: 0.752919, acc.: 50.00%] [G loss: 0.418069]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 40/173 [loss: 0.752183, acc.: 50.00%] [G loss: 0.417397]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 41/173 [loss: 0.752373, acc.: 50.00%] [G loss: 0.420200]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 42/173 [loss: 0.748613, acc.: 50.00%] [G loss: 0.418069]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 43/173 [loss: 0.747915, acc.: 50.00%] [G loss: 0.420787]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 44/173 [loss: 0.746382, acc.: 50.00%] [G loss: 0.422062]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 45/173 [loss: 0.744901, acc.: 50.00%] [G loss: 0.422778]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 46/173 [loss: 0.746318, acc.: 50.00%] [G loss: 0.424275]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 47/173 [loss: 0.745135, acc.: 50.00%] [G loss: 0.422594]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 48/173 [loss: 0.744259, acc.: 50.00%] [G loss: 0.421640]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 49/173 [loss: 0.747833, acc.: 50.00%] [G loss: 0.420445]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 50/173 [loss: 0.745481, acc.: 50.00%] [G loss: 0.418938]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 51/173 [loss: 0.750297, acc.: 50.00%] [G loss: 0.420319]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 52/173 [loss: 0.751187, acc.: 50.00%] [G loss: 0.422168]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 53/173 [loss: 0.748711, acc.: 50.00%] [G loss: 0.423699]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 54/173 [loss: 0.749041, acc.: 50.00%] [G loss: 0.423534]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 55/173 [loss: 0.747645, acc.: 50.00%] [G loss: 0.426908]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 56/173 [loss: 0.745286, acc.: 50.00%] [G loss: 0.427969]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 57/173 [loss: 0.745550, acc.: 50.00%] [G loss: 0.429890]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 58/173 [loss: 0.744584, acc.: 50.00%] [G loss: 0.431523]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 59/173 [loss: 0.745215, acc.: 50.00%] [G loss: 0.429570]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 60/173 [loss: 0.746834, acc.: 50.00%] [G loss: 0.427187]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 61/173 [loss: 0.747434, acc.: 50.00%] [G loss: 0.426196]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 62/173 [loss: 0.748529, acc.: 50.00%] [G loss: 0.422046]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 63/173 [loss: 0.752606, acc.: 50.00%] [G loss: 0.419994]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 64/173 [loss: 0.754109, acc.: 50.00%] [G loss: 0.417767]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 65/173 [loss: 0.756036, acc.: 50.00%] [G loss: 0.415664]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 66/173 [loss: 0.756951, acc.: 50.00%] [G loss: 0.412846]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 67/173 [loss: 0.757970, acc.: 50.00%] [G loss: 0.412785]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 68/173 [loss: 0.758834, acc.: 50.00%] [G loss: 0.408686]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 69/173 [loss: 0.759072, acc.: 50.00%] [G loss: 0.411855]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 70/173 [loss: 0.760115, acc.: 50.00%] [G loss: 0.410557]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 71/173 [loss: 0.758145, acc.: 50.00%] [G loss: 0.409470]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 72/173 [loss: 0.758131, acc.: 50.00%] [G loss: 0.411367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 73/173 [loss: 0.757933, acc.: 50.00%] [G loss: 0.412650]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 74/173 [loss: 0.758880, acc.: 50.00%] [G loss: 0.413306]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 75/173 [loss: 0.754863, acc.: 50.00%] [G loss: 0.413236]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 76/173 [loss: 0.755953, acc.: 50.00%] [G loss: 0.415358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 77/173 [loss: 0.754106, acc.: 50.00%] [G loss: 0.416187]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 78/173 [loss: 0.752581, acc.: 50.00%] [G loss: 0.418269]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 79/173 [loss: 0.754331, acc.: 50.00%] [G loss: 0.418802]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 80/173 [loss: 0.751290, acc.: 50.00%] [G loss: 0.422256]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 81/173 [loss: 0.748165, acc.: 50.00%] [G loss: 0.422691]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 82/173 [loss: 0.744214, acc.: 50.00%] [G loss: 0.424046]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 83/173 [loss: 0.744731, acc.: 50.00%] [G loss: 0.424729]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 84/173 [loss: 0.742193, acc.: 50.00%] [G loss: 0.427293]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 85/173 [loss: 0.742053, acc.: 50.00%] [G loss: 0.426571]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 86/173 [loss: 0.745042, acc.: 50.00%] [G loss: 0.426013]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 87/173 [loss: 0.742196, acc.: 50.00%] [G loss: 0.425610]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 88/173 [loss: 0.745213, acc.: 50.00%] [G loss: 0.426448]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 89/173 [loss: 0.745759, acc.: 50.00%] [G loss: 0.423090]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 90/173 [loss: 0.744416, acc.: 50.00%] [G loss: 0.421793]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 91/173 [loss: 0.747152, acc.: 50.00%] [G loss: 0.421567]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 92/173 [loss: 0.748138, acc.: 50.00%] [G loss: 0.419377]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 93/173 [loss: 0.750619, acc.: 50.00%] [G loss: 0.418838]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 94/173 [loss: 0.752613, acc.: 50.00%] [G loss: 0.416107]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 95/173 [loss: 0.753349, acc.: 50.00%] [G loss: 0.415281]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 96/173 [loss: 0.757030, acc.: 50.00%] [G loss: 0.413487]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 97/173 [loss: 0.757066, acc.: 50.00%] [G loss: 0.412093]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 98/173 [loss: 0.757498, acc.: 50.00%] [G loss: 0.411113]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 99/173 [loss: 0.757903, acc.: 50.00%] [G loss: 0.410952]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 100/173 [loss: 0.759709, acc.: 50.00%] [G loss: 0.411274]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 101/173 [loss: 0.760965, acc.: 50.00%] [G loss: 0.409361]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 102/173 [loss: 0.759597, acc.: 50.00%] [G loss: 0.409634]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 103/173 [loss: 0.761077, acc.: 50.00%] [G loss: 0.410878]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 104/173 [loss: 0.761394, acc.: 50.00%] [G loss: 0.409621]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 105/173 [loss: 0.758045, acc.: 50.00%] [G loss: 0.412578]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 106/173 [loss: 0.759506, acc.: 50.00%] [G loss: 0.412813]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 107/173 [loss: 0.757715, acc.: 50.00%] [G loss: 0.414189]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 108/173 [loss: 0.756127, acc.: 50.00%] [G loss: 0.418579]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 109/173 [loss: 0.753605, acc.: 50.00%] [G loss: 0.422313]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 110/173 [loss: 0.751832, acc.: 50.00%] [G loss: 0.423008]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 111/173 [loss: 0.747410, acc.: 50.00%] [G loss: 0.424402]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 112/173 [loss: 0.747468, acc.: 50.00%] [G loss: 0.427452]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 113/173 [loss: 0.743563, acc.: 50.00%] [G loss: 0.431767]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 114/173 [loss: 0.744107, acc.: 50.00%] [G loss: 0.433009]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 115/173 [loss: 0.741477, acc.: 50.00%] [G loss: 0.435902]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 116/173 [loss: 0.738421, acc.: 50.00%] [G loss: 0.435306]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 117/173 [loss: 0.738624, acc.: 50.00%] [G loss: 0.435895]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 118/173 [loss: 0.737015, acc.: 50.00%] [G loss: 0.433456]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 119/173 [loss: 0.735835, acc.: 50.00%] [G loss: 0.433991]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 120/173 [loss: 0.735368, acc.: 50.00%] [G loss: 0.435171]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 121/173 [loss: 0.737844, acc.: 50.00%] [G loss: 0.433065]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 122/173 [loss: 0.737664, acc.: 50.00%] [G loss: 0.431448]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 123/173 [loss: 0.737825, acc.: 50.00%] [G loss: 0.431574]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 124/173 [loss: 0.740811, acc.: 50.00%] [G loss: 0.427776]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 125/173 [loss: 0.741278, acc.: 50.00%] [G loss: 0.425358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 126/173 [loss: 0.742387, acc.: 50.00%] [G loss: 0.424869]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 127/173 [loss: 0.745495, acc.: 50.00%] [G loss: 0.423355]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 128/173 [loss: 0.745639, acc.: 50.00%] [G loss: 0.420006]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 129/173 [loss: 0.752004, acc.: 50.00%] [G loss: 0.418936]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 130/173 [loss: 0.752534, acc.: 50.00%] [G loss: 0.415549]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 131/173 [loss: 0.753822, acc.: 50.00%] [G loss: 0.414168]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 132/173 [loss: 0.754711, acc.: 50.00%] [G loss: 0.413244]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 133/173 [loss: 0.757554, acc.: 50.00%] [G loss: 0.413366]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 134/173 [loss: 0.761115, acc.: 50.00%] [G loss: 0.413565]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 135/173 [loss: 0.759097, acc.: 50.00%] [G loss: 0.409224]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 136/173 [loss: 0.760136, acc.: 50.00%] [G loss: 0.412012]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 137/173 [loss: 0.760466, acc.: 50.00%] [G loss: 0.410336]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 138/173 [loss: 0.760656, acc.: 50.00%] [G loss: 0.410666]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 139/173 [loss: 0.760232, acc.: 50.00%] [G loss: 0.410205]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 140/173 [loss: 0.762843, acc.: 50.00%] [G loss: 0.410553]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 141/173 [loss: 0.762651, acc.: 50.00%] [G loss: 0.413717]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 142/173 [loss: 0.757769, acc.: 50.00%] [G loss: 0.412706]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 143/173 [loss: 0.757342, acc.: 50.00%] [G loss: 0.417481]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 144/173 [loss: 0.755167, acc.: 50.00%] [G loss: 0.418558]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 145/173 [loss: 0.752180, acc.: 50.00%] [G loss: 0.420144]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 146/173 [loss: 0.749177, acc.: 50.00%] [G loss: 0.422607]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 147/173 [loss: 0.747709, acc.: 50.00%] [G loss: 0.425280]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 148/173 [loss: 0.744671, acc.: 50.00%] [G loss: 0.428247]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 149/173 [loss: 0.741836, acc.: 50.00%] [G loss: 0.429997]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 150/173 [loss: 0.737733, acc.: 50.00%] [G loss: 0.432767]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 151/173 [loss: 0.737769, acc.: 50.00%] [G loss: 0.434066]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 152/173 [loss: 0.734612, acc.: 50.00%] [G loss: 0.432619]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 153/173 [loss: 0.735778, acc.: 50.00%] [G loss: 0.433451]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 154/173 [loss: 0.736578, acc.: 50.00%] [G loss: 0.430800]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 155/173 [loss: 0.737800, acc.: 50.00%] [G loss: 0.431991]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 156/173 [loss: 0.737109, acc.: 50.00%] [G loss: 0.428947]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 157/173 [loss: 0.741632, acc.: 50.00%] [G loss: 0.427724]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 158/173 [loss: 0.740983, acc.: 50.00%] [G loss: 0.422830]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 159/173 [loss: 0.745402, acc.: 50.00%] [G loss: 0.418994]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 160/173 [loss: 0.746409, acc.: 50.00%] [G loss: 0.418536]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 161/173 [loss: 0.748510, acc.: 50.00%] [G loss: 0.416357]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 162/173 [loss: 0.751707, acc.: 50.00%] [G loss: 0.412106]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 163/173 [loss: 0.753507, acc.: 50.00%] [G loss: 0.415233]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 164/173 [loss: 0.755575, acc.: 50.00%] [G loss: 0.414760]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 165/173 [loss: 0.756064, acc.: 50.00%] [G loss: 0.416091]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 166/173 [loss: 0.752954, acc.: 50.00%] [G loss: 0.417105]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 167/173 [loss: 0.754499, acc.: 50.00%] [G loss: 0.414380]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 168/173 [loss: 0.757154, acc.: 50.00%] [G loss: 0.414638]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 169/173 [loss: 0.756684, acc.: 50.00%] [G loss: 0.415459]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 170/173 [loss: 0.754710, acc.: 50.00%] [G loss: 0.415627]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 171/173 [loss: 0.757327, acc.: 50.00%] [G loss: 0.414608]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 5/200  Batch Size: 172/173 [loss: 0.757552, acc.: 50.00%] [G loss: 0.414452]\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 0/173 [loss: 0.754856, acc.: 50.00%] [G loss: 0.414631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 1/173 [loss: 0.754534, acc.: 50.00%] [G loss: 0.417240]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 2/173 [loss: 0.757081, acc.: 50.00%] [G loss: 0.417784]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 3/173 [loss: 0.754871, acc.: 50.00%] [G loss: 0.417338]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 4/173 [loss: 0.754370, acc.: 50.00%] [G loss: 0.420712]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 5/173 [loss: 0.750718, acc.: 50.00%] [G loss: 0.419619]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 6/173 [loss: 0.748360, acc.: 50.00%] [G loss: 0.423380]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 7/173 [loss: 0.746999, acc.: 50.00%] [G loss: 0.424500]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 8/173 [loss: 0.746608, acc.: 50.00%] [G loss: 0.429228]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 9/173 [loss: 0.742664, acc.: 50.00%] [G loss: 0.429516]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 10/173 [loss: 0.742056, acc.: 50.00%] [G loss: 0.429395]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 11/173 [loss: 0.739794, acc.: 50.00%] [G loss: 0.428444]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 12/173 [loss: 0.739648, acc.: 50.00%] [G loss: 0.429739]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 13/173 [loss: 0.737793, acc.: 50.00%] [G loss: 0.430852]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 14/173 [loss: 0.737202, acc.: 50.00%] [G loss: 0.431926]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 15/173 [loss: 0.737845, acc.: 50.00%] [G loss: 0.430849]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 16/173 [loss: 0.739721, acc.: 50.00%] [G loss: 0.430535]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 17/173 [loss: 0.739883, acc.: 50.00%] [G loss: 0.430025]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 18/173 [loss: 0.741715, acc.: 50.00%] [G loss: 0.428225]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 19/173 [loss: 0.743117, acc.: 50.00%] [G loss: 0.427874]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 20/173 [loss: 0.741557, acc.: 50.00%] [G loss: 0.428096]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 21/173 [loss: 0.744361, acc.: 50.00%] [G loss: 0.429895]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 22/173 [loss: 0.742836, acc.: 50.00%] [G loss: 0.427192]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 23/173 [loss: 0.742243, acc.: 50.00%] [G loss: 0.427571]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 24/173 [loss: 0.741804, acc.: 50.00%] [G loss: 0.426186]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 25/173 [loss: 0.743096, acc.: 50.00%] [G loss: 0.424133]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 26/173 [loss: 0.745363, acc.: 50.00%] [G loss: 0.424567]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 27/173 [loss: 0.746246, acc.: 50.00%] [G loss: 0.423768]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 28/173 [loss: 0.748191, acc.: 50.00%] [G loss: 0.422373]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 29/173 [loss: 0.749700, acc.: 50.00%] [G loss: 0.417957]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 30/173 [loss: 0.753349, acc.: 50.00%] [G loss: 0.414773]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 31/173 [loss: 0.754645, acc.: 50.00%] [G loss: 0.414796]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 32/173 [loss: 0.757567, acc.: 50.00%] [G loss: 0.413652]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 33/173 [loss: 0.757222, acc.: 50.00%] [G loss: 0.410309]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 34/173 [loss: 0.758758, acc.: 50.00%] [G loss: 0.410498]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 35/173 [loss: 0.758275, acc.: 50.00%] [G loss: 0.411621]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 36/173 [loss: 0.756069, acc.: 50.00%] [G loss: 0.410795]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 37/173 [loss: 0.756082, acc.: 50.00%] [G loss: 0.412547]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 38/173 [loss: 0.755892, acc.: 50.00%] [G loss: 0.413630]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 39/173 [loss: 0.755142, acc.: 50.00%] [G loss: 0.413438]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 40/173 [loss: 0.756038, acc.: 50.00%] [G loss: 0.414594]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 41/173 [loss: 0.749925, acc.: 50.00%] [G loss: 0.415769]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 42/173 [loss: 0.750927, acc.: 50.00%] [G loss: 0.419052]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 43/173 [loss: 0.750210, acc.: 50.00%] [G loss: 0.420308]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 44/173 [loss: 0.748248, acc.: 50.00%] [G loss: 0.421337]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 45/173 [loss: 0.749178, acc.: 50.00%] [G loss: 0.419958]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 46/173 [loss: 0.747996, acc.: 50.00%] [G loss: 0.420209]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 47/173 [loss: 0.748115, acc.: 50.00%] [G loss: 0.424793]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 48/173 [loss: 0.745554, acc.: 50.00%] [G loss: 0.425984]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 49/173 [loss: 0.742629, acc.: 50.00%] [G loss: 0.428422]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 50/173 [loss: 0.741735, acc.: 50.00%] [G loss: 0.429484]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 51/173 [loss: 0.741532, acc.: 50.00%] [G loss: 0.430607]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 52/173 [loss: 0.738431, acc.: 50.00%] [G loss: 0.432792]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 53/173 [loss: 0.736032, acc.: 50.00%] [G loss: 0.431876]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 54/173 [loss: 0.739059, acc.: 50.00%] [G loss: 0.432370]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 55/173 [loss: 0.736365, acc.: 50.00%] [G loss: 0.432358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 56/173 [loss: 0.737952, acc.: 50.00%] [G loss: 0.434378]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 57/173 [loss: 0.742259, acc.: 50.00%] [G loss: 0.429213]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 58/173 [loss: 0.743747, acc.: 50.00%] [G loss: 0.426569]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 59/173 [loss: 0.745212, acc.: 50.00%] [G loss: 0.423716]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 60/173 [loss: 0.746440, acc.: 50.00%] [G loss: 0.420582]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 61/173 [loss: 0.749650, acc.: 50.00%] [G loss: 0.418681]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 62/173 [loss: 0.747387, acc.: 50.00%] [G loss: 0.417308]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 63/173 [loss: 0.751582, acc.: 50.00%] [G loss: 0.418451]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 64/173 [loss: 0.750082, acc.: 50.00%] [G loss: 0.417943]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 65/173 [loss: 0.752653, acc.: 50.00%] [G loss: 0.415731]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 66/173 [loss: 0.752390, acc.: 50.00%] [G loss: 0.415449]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 67/173 [loss: 0.749692, acc.: 50.00%] [G loss: 0.416469]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 68/173 [loss: 0.751760, acc.: 50.00%] [G loss: 0.415974]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 69/173 [loss: 0.751041, acc.: 50.00%] [G loss: 0.414521]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 70/173 [loss: 0.753716, acc.: 50.00%] [G loss: 0.414022]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 71/173 [loss: 0.755222, acc.: 50.00%] [G loss: 0.416583]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 72/173 [loss: 0.753724, acc.: 50.00%] [G loss: 0.417071]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 73/173 [loss: 0.750854, acc.: 50.00%] [G loss: 0.415293]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 74/173 [loss: 0.752684, acc.: 50.00%] [G loss: 0.418476]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 75/173 [loss: 0.750890, acc.: 50.00%] [G loss: 0.418760]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 76/173 [loss: 0.750133, acc.: 50.00%] [G loss: 0.422287]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 77/173 [loss: 0.749600, acc.: 50.00%] [G loss: 0.424387]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 78/173 [loss: 0.749428, acc.: 50.00%] [G loss: 0.425588]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 79/173 [loss: 0.746885, acc.: 50.00%] [G loss: 0.427850]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 80/173 [loss: 0.743612, acc.: 50.00%] [G loss: 0.431250]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 81/173 [loss: 0.743504, acc.: 50.00%] [G loss: 0.430696]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 82/173 [loss: 0.741782, acc.: 50.00%] [G loss: 0.431513]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 83/173 [loss: 0.742067, acc.: 50.00%] [G loss: 0.430749]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 84/173 [loss: 0.742717, acc.: 50.00%] [G loss: 0.428834]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 85/173 [loss: 0.741488, acc.: 50.00%] [G loss: 0.429653]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 86/173 [loss: 0.741407, acc.: 50.00%] [G loss: 0.430816]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 87/173 [loss: 0.739340, acc.: 50.00%] [G loss: 0.429477]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 88/173 [loss: 0.739677, acc.: 50.00%] [G loss: 0.428258]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 89/173 [loss: 0.742132, acc.: 50.00%] [G loss: 0.428790]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 90/173 [loss: 0.738941, acc.: 50.00%] [G loss: 0.429781]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 91/173 [loss: 0.740209, acc.: 50.00%] [G loss: 0.426420]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 92/173 [loss: 0.740380, acc.: 50.00%] [G loss: 0.427013]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 93/173 [loss: 0.742457, acc.: 50.00%] [G loss: 0.425455]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 94/173 [loss: 0.740513, acc.: 50.00%] [G loss: 0.424428]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 95/173 [loss: 0.743460, acc.: 50.00%] [G loss: 0.425424]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 96/173 [loss: 0.742737, acc.: 50.00%] [G loss: 0.422896]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 97/173 [loss: 0.745360, acc.: 50.00%] [G loss: 0.422291]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 98/173 [loss: 0.747663, acc.: 50.00%] [G loss: 0.419380]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 99/173 [loss: 0.746777, acc.: 50.00%] [G loss: 0.422103]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 100/173 [loss: 0.748311, acc.: 50.00%] [G loss: 0.417425]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 101/173 [loss: 0.749224, acc.: 50.00%] [G loss: 0.417745]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 102/173 [loss: 0.750409, acc.: 50.00%] [G loss: 0.418643]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 103/173 [loss: 0.749608, acc.: 50.00%] [G loss: 0.416131]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 104/173 [loss: 0.751882, acc.: 50.00%] [G loss: 0.415885]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 105/173 [loss: 0.753717, acc.: 50.00%] [G loss: 0.417554]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 106/173 [loss: 0.749204, acc.: 50.00%] [G loss: 0.419261]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 107/173 [loss: 0.751752, acc.: 50.00%] [G loss: 0.414342]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 108/173 [loss: 0.752922, acc.: 50.00%] [G loss: 0.418034]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 109/173 [loss: 0.753422, acc.: 50.00%] [G loss: 0.415962]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 110/173 [loss: 0.753096, acc.: 50.00%] [G loss: 0.414085]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 111/173 [loss: 0.754245, acc.: 50.00%] [G loss: 0.414667]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 112/173 [loss: 0.753696, acc.: 50.00%] [G loss: 0.416521]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 113/173 [loss: 0.752430, acc.: 50.00%] [G loss: 0.419323]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 114/173 [loss: 0.751841, acc.: 50.00%] [G loss: 0.417620]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 115/173 [loss: 0.751827, acc.: 50.00%] [G loss: 0.418155]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 116/173 [loss: 0.749817, acc.: 50.00%] [G loss: 0.418263]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 117/173 [loss: 0.747558, acc.: 50.00%] [G loss: 0.419981]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 118/173 [loss: 0.747916, acc.: 50.00%] [G loss: 0.419644]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 119/173 [loss: 0.748008, acc.: 50.00%] [G loss: 0.424016]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 120/173 [loss: 0.744553, acc.: 50.00%] [G loss: 0.421003]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 121/173 [loss: 0.745211, acc.: 50.00%] [G loss: 0.422382]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 122/173 [loss: 0.742796, acc.: 50.00%] [G loss: 0.425026]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 123/173 [loss: 0.742604, acc.: 50.00%] [G loss: 0.424433]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 124/173 [loss: 0.742425, acc.: 50.00%] [G loss: 0.423953]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 125/173 [loss: 0.743479, acc.: 50.00%] [G loss: 0.424304]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 126/173 [loss: 0.742580, acc.: 50.00%] [G loss: 0.425948]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 127/173 [loss: 0.742014, acc.: 50.00%] [G loss: 0.427653]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 128/173 [loss: 0.742544, acc.: 50.00%] [G loss: 0.424708]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 129/173 [loss: 0.742860, acc.: 50.00%] [G loss: 0.424345]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 130/173 [loss: 0.741249, acc.: 50.00%] [G loss: 0.424727]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 131/173 [loss: 0.744555, acc.: 50.00%] [G loss: 0.421633]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 132/173 [loss: 0.745551, acc.: 50.00%] [G loss: 0.422871]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 133/173 [loss: 0.745590, acc.: 50.00%] [G loss: 0.418674]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 134/173 [loss: 0.746058, acc.: 50.00%] [G loss: 0.420939]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 135/173 [loss: 0.747284, acc.: 50.00%] [G loss: 0.418736]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 136/173 [loss: 0.750520, acc.: 50.00%] [G loss: 0.419546]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 137/173 [loss: 0.749679, acc.: 50.00%] [G loss: 0.416396]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 138/173 [loss: 0.752724, acc.: 50.00%] [G loss: 0.415025]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 139/173 [loss: 0.751987, acc.: 50.00%] [G loss: 0.417277]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 140/173 [loss: 0.751635, acc.: 50.00%] [G loss: 0.416564]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 141/173 [loss: 0.753361, acc.: 50.00%] [G loss: 0.415195]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 142/173 [loss: 0.751387, acc.: 50.00%] [G loss: 0.415853]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 143/173 [loss: 0.753158, acc.: 50.00%] [G loss: 0.414517]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 144/173 [loss: 0.753385, acc.: 50.00%] [G loss: 0.416802]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 145/173 [loss: 0.752649, acc.: 50.00%] [G loss: 0.417456]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 146/173 [loss: 0.749051, acc.: 50.00%] [G loss: 0.416688]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 147/173 [loss: 0.751340, acc.: 50.00%] [G loss: 0.417503]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 148/173 [loss: 0.751773, acc.: 50.00%] [G loss: 0.418457]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 149/173 [loss: 0.748366, acc.: 50.00%] [G loss: 0.418515]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 150/173 [loss: 0.751976, acc.: 50.00%] [G loss: 0.421476]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 151/173 [loss: 0.748211, acc.: 50.00%] [G loss: 0.422069]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 152/173 [loss: 0.747346, acc.: 50.00%] [G loss: 0.422126]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 153/173 [loss: 0.745177, acc.: 50.00%] [G loss: 0.424666]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 154/173 [loss: 0.746399, acc.: 50.00%] [G loss: 0.424591]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 155/173 [loss: 0.745819, acc.: 50.00%] [G loss: 0.425484]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 156/173 [loss: 0.741831, acc.: 50.00%] [G loss: 0.426402]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 157/173 [loss: 0.742727, acc.: 50.00%] [G loss: 0.427472]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 158/173 [loss: 0.741414, acc.: 50.00%] [G loss: 0.425842]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 159/173 [loss: 0.740268, acc.: 50.00%] [G loss: 0.427325]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 160/173 [loss: 0.741621, acc.: 50.00%] [G loss: 0.428642]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 161/173 [loss: 0.741949, acc.: 50.00%] [G loss: 0.427631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 162/173 [loss: 0.743336, acc.: 50.00%] [G loss: 0.426817]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 163/173 [loss: 0.738897, acc.: 50.00%] [G loss: 0.429435]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 164/173 [loss: 0.741405, acc.: 50.00%] [G loss: 0.425949]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 165/173 [loss: 0.740397, acc.: 50.00%] [G loss: 0.426555]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 166/173 [loss: 0.743102, acc.: 50.00%] [G loss: 0.425303]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 167/173 [loss: 0.743647, acc.: 50.00%] [G loss: 0.424550]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 168/173 [loss: 0.744689, acc.: 50.00%] [G loss: 0.422939]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 169/173 [loss: 0.743657, acc.: 50.00%] [G loss: 0.420871]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 170/173 [loss: 0.744616, acc.: 50.00%] [G loss: 0.421392]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 171/173 [loss: 0.746113, acc.: 50.00%] [G loss: 0.421184]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 6/200  Batch Size: 172/173 [loss: 0.748370, acc.: 50.00%] [G loss: 0.420376]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 0/173 [loss: 0.747933, acc.: 50.00%] [G loss: 0.419501]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 1/173 [loss: 0.746421, acc.: 50.00%] [G loss: 0.418377]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 2/173 [loss: 0.749488, acc.: 50.00%] [G loss: 0.417389]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 3/173 [loss: 0.749574, acc.: 50.00%] [G loss: 0.420474]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 4/173 [loss: 0.752472, acc.: 50.00%] [G loss: 0.419514]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 5/173 [loss: 0.750450, acc.: 50.00%] [G loss: 0.420449]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 6/173 [loss: 0.750309, acc.: 50.00%] [G loss: 0.420294]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 7/173 [loss: 0.751222, acc.: 50.00%] [G loss: 0.422404]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 8/173 [loss: 0.747221, acc.: 50.00%] [G loss: 0.423688]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 9/173 [loss: 0.745631, acc.: 50.00%] [G loss: 0.421776]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 10/173 [loss: 0.745887, acc.: 50.00%] [G loss: 0.423937]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 11/173 [loss: 0.744972, acc.: 50.00%] [G loss: 0.423163]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 12/173 [loss: 0.747074, acc.: 50.00%] [G loss: 0.423736]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 13/173 [loss: 0.744763, acc.: 50.00%] [G loss: 0.423172]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 14/173 [loss: 0.745331, acc.: 50.00%] [G loss: 0.423579]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 15/173 [loss: 0.747114, acc.: 50.00%] [G loss: 0.424398]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 16/173 [loss: 0.741434, acc.: 50.00%] [G loss: 0.423756]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 17/173 [loss: 0.745103, acc.: 50.00%] [G loss: 0.426878]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 18/173 [loss: 0.742478, acc.: 50.00%] [G loss: 0.424294]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 19/173 [loss: 0.742723, acc.: 50.00%] [G loss: 0.424945]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 20/173 [loss: 0.742637, acc.: 50.00%] [G loss: 0.423458]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 21/173 [loss: 0.743879, acc.: 50.00%] [G loss: 0.424853]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 22/173 [loss: 0.742973, acc.: 50.00%] [G loss: 0.424191]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 23/173 [loss: 0.743238, acc.: 50.00%] [G loss: 0.422358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 24/173 [loss: 0.747040, acc.: 50.00%] [G loss: 0.421353]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 25/173 [loss: 0.744707, acc.: 50.00%] [G loss: 0.423164]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 26/173 [loss: 0.746106, acc.: 50.00%] [G loss: 0.420840]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 27/173 [loss: 0.743724, acc.: 50.00%] [G loss: 0.420673]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 28/173 [loss: 0.745974, acc.: 50.00%] [G loss: 0.419401]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 29/173 [loss: 0.747354, acc.: 50.00%] [G loss: 0.421667]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 30/173 [loss: 0.747182, acc.: 50.00%] [G loss: 0.421587]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 31/173 [loss: 0.745020, acc.: 50.00%] [G loss: 0.422882]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 32/173 [loss: 0.744794, acc.: 50.00%] [G loss: 0.422513]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 33/173 [loss: 0.744974, acc.: 50.00%] [G loss: 0.423405]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 34/173 [loss: 0.743387, acc.: 50.00%] [G loss: 0.423144]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 35/173 [loss: 0.744554, acc.: 50.00%] [G loss: 0.423472]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 36/173 [loss: 0.744252, acc.: 50.00%] [G loss: 0.423168]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 37/173 [loss: 0.745525, acc.: 50.00%] [G loss: 0.420693]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 38/173 [loss: 0.744547, acc.: 50.00%] [G loss: 0.420817]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 39/173 [loss: 0.748641, acc.: 50.00%] [G loss: 0.420582]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 40/173 [loss: 0.749417, acc.: 50.00%] [G loss: 0.420439]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 41/173 [loss: 0.748026, acc.: 50.00%] [G loss: 0.418843]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 42/173 [loss: 0.749435, acc.: 50.00%] [G loss: 0.418856]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 43/173 [loss: 0.749217, acc.: 50.00%] [G loss: 0.418608]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 44/173 [loss: 0.750628, acc.: 50.00%] [G loss: 0.419863]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 45/173 [loss: 0.750993, acc.: 50.00%] [G loss: 0.420381]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 46/173 [loss: 0.751833, acc.: 50.00%] [G loss: 0.419490]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 47/173 [loss: 0.750475, acc.: 50.00%] [G loss: 0.420093]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 48/173 [loss: 0.748863, acc.: 50.00%] [G loss: 0.421883]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 49/173 [loss: 0.748864, acc.: 50.00%] [G loss: 0.421771]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 50/173 [loss: 0.745995, acc.: 50.00%] [G loss: 0.421039]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 51/173 [loss: 0.745387, acc.: 50.00%] [G loss: 0.421836]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 52/173 [loss: 0.746982, acc.: 50.00%] [G loss: 0.426382]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 53/173 [loss: 0.746732, acc.: 50.00%] [G loss: 0.427892]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 54/173 [loss: 0.743791, acc.: 50.00%] [G loss: 0.428874]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 55/173 [loss: 0.743676, acc.: 50.00%] [G loss: 0.428701]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 56/173 [loss: 0.743840, acc.: 50.00%] [G loss: 0.430251]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 57/173 [loss: 0.742928, acc.: 50.00%] [G loss: 0.431988]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 58/173 [loss: 0.738774, acc.: 50.00%] [G loss: 0.431642]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 59/173 [loss: 0.740096, acc.: 50.00%] [G loss: 0.432102]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 60/173 [loss: 0.740107, acc.: 50.00%] [G loss: 0.432409]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 61/173 [loss: 0.739736, acc.: 50.00%] [G loss: 0.430453]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 62/173 [loss: 0.743762, acc.: 50.00%] [G loss: 0.427074]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 63/173 [loss: 0.741295, acc.: 50.00%] [G loss: 0.425487]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 64/173 [loss: 0.744383, acc.: 50.00%] [G loss: 0.422283]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 65/173 [loss: 0.743133, acc.: 50.00%] [G loss: 0.424541]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 66/173 [loss: 0.746973, acc.: 50.00%] [G loss: 0.422652]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 67/173 [loss: 0.748076, acc.: 50.00%] [G loss: 0.419662]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 68/173 [loss: 0.749635, acc.: 50.00%] [G loss: 0.420290]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 69/173 [loss: 0.748494, acc.: 50.00%] [G loss: 0.418156]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 70/173 [loss: 0.750030, acc.: 50.00%] [G loss: 0.416579]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 71/173 [loss: 0.750945, acc.: 50.00%] [G loss: 0.416229]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 72/173 [loss: 0.751235, acc.: 50.00%] [G loss: 0.418905]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 73/173 [loss: 0.751630, acc.: 50.00%] [G loss: 0.416869]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 74/173 [loss: 0.751280, acc.: 50.00%] [G loss: 0.418215]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 75/173 [loss: 0.752446, acc.: 50.00%] [G loss: 0.418001]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 76/173 [loss: 0.749359, acc.: 50.00%] [G loss: 0.419176]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 77/173 [loss: 0.749120, acc.: 50.00%] [G loss: 0.420394]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 78/173 [loss: 0.748360, acc.: 50.00%] [G loss: 0.418817]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 79/173 [loss: 0.746958, acc.: 50.00%] [G loss: 0.421700]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 80/173 [loss: 0.747576, acc.: 50.00%] [G loss: 0.419892]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 81/173 [loss: 0.746170, acc.: 50.00%] [G loss: 0.423935]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 82/173 [loss: 0.744466, acc.: 50.00%] [G loss: 0.426312]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 83/173 [loss: 0.744135, acc.: 50.00%] [G loss: 0.425067]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 84/173 [loss: 0.741227, acc.: 50.00%] [G loss: 0.425096]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 85/173 [loss: 0.743065, acc.: 50.00%] [G loss: 0.426666]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 86/173 [loss: 0.743700, acc.: 50.00%] [G loss: 0.424661]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 87/173 [loss: 0.743425, acc.: 50.00%] [G loss: 0.425379]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 88/173 [loss: 0.746612, acc.: 50.00%] [G loss: 0.425095]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 89/173 [loss: 0.745301, acc.: 50.00%] [G loss: 0.422525]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 90/173 [loss: 0.744334, acc.: 50.00%] [G loss: 0.424127]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 91/173 [loss: 0.748264, acc.: 50.00%] [G loss: 0.419282]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 92/173 [loss: 0.746257, acc.: 50.00%] [G loss: 0.420720]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 93/173 [loss: 0.747846, acc.: 50.00%] [G loss: 0.420270]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 94/173 [loss: 0.749729, acc.: 50.00%] [G loss: 0.418902]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 95/173 [loss: 0.749115, acc.: 50.00%] [G loss: 0.417628]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 96/173 [loss: 0.750764, acc.: 50.00%] [G loss: 0.418431]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 97/173 [loss: 0.749718, acc.: 50.00%] [G loss: 0.417806]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 98/173 [loss: 0.750333, acc.: 50.00%] [G loss: 0.416740]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 99/173 [loss: 0.750340, acc.: 50.00%] [G loss: 0.421505]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 100/173 [loss: 0.751539, acc.: 50.00%] [G loss: 0.420733]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 101/173 [loss: 0.746464, acc.: 50.00%] [G loss: 0.422676]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 102/173 [loss: 0.745407, acc.: 50.00%] [G loss: 0.421328]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 103/173 [loss: 0.744841, acc.: 50.00%] [G loss: 0.422257]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 104/173 [loss: 0.745489, acc.: 50.00%] [G loss: 0.422274]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 105/173 [loss: 0.744638, acc.: 50.00%] [G loss: 0.425240]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 106/173 [loss: 0.743220, acc.: 50.00%] [G loss: 0.426192]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 107/173 [loss: 0.744490, acc.: 50.00%] [G loss: 0.425213]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 108/173 [loss: 0.742118, acc.: 50.00%] [G loss: 0.428979]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 109/173 [loss: 0.743591, acc.: 50.00%] [G loss: 0.427425]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 110/173 [loss: 0.743103, acc.: 50.00%] [G loss: 0.425322]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 111/173 [loss: 0.742598, acc.: 50.00%] [G loss: 0.425861]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 112/173 [loss: 0.743012, acc.: 50.00%] [G loss: 0.423705]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 113/173 [loss: 0.745365, acc.: 50.00%] [G loss: 0.424015]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 114/173 [loss: 0.746856, acc.: 50.00%] [G loss: 0.421846]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 115/173 [loss: 0.747375, acc.: 50.00%] [G loss: 0.421861]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 116/173 [loss: 0.749206, acc.: 50.00%] [G loss: 0.419235]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 117/173 [loss: 0.747725, acc.: 50.00%] [G loss: 0.419119]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 118/173 [loss: 0.748459, acc.: 50.00%] [G loss: 0.421165]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 119/173 [loss: 0.750549, acc.: 50.00%] [G loss: 0.417843]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 120/173 [loss: 0.747547, acc.: 50.00%] [G loss: 0.419188]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 121/173 [loss: 0.747853, acc.: 50.00%] [G loss: 0.419054]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 122/173 [loss: 0.747548, acc.: 50.00%] [G loss: 0.418305]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 123/173 [loss: 0.749989, acc.: 50.00%] [G loss: 0.417473]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 124/173 [loss: 0.749782, acc.: 50.00%] [G loss: 0.419804]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 125/173 [loss: 0.746444, acc.: 50.00%] [G loss: 0.423837]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 126/173 [loss: 0.745916, acc.: 50.00%] [G loss: 0.425045]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 127/173 [loss: 0.746143, acc.: 50.00%] [G loss: 0.423946]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 128/173 [loss: 0.744011, acc.: 50.00%] [G loss: 0.426012]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 129/173 [loss: 0.741241, acc.: 50.00%] [G loss: 0.425545]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 130/173 [loss: 0.741542, acc.: 50.00%] [G loss: 0.427698]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 131/173 [loss: 0.743025, acc.: 50.00%] [G loss: 0.427289]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 132/173 [loss: 0.738327, acc.: 50.00%] [G loss: 0.425387]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 133/173 [loss: 0.741546, acc.: 50.00%] [G loss: 0.424851]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 134/173 [loss: 0.743403, acc.: 50.00%] [G loss: 0.425360]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 135/173 [loss: 0.742742, acc.: 50.00%] [G loss: 0.421576]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 136/173 [loss: 0.749070, acc.: 50.00%] [G loss: 0.418743]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 137/173 [loss: 0.748524, acc.: 50.00%] [G loss: 0.417328]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 138/173 [loss: 0.750075, acc.: 50.00%] [G loss: 0.417993]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 139/173 [loss: 0.752231, acc.: 50.00%] [G loss: 0.417161]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 140/173 [loss: 0.750990, acc.: 50.00%] [G loss: 0.413595]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 141/173 [loss: 0.751100, acc.: 50.00%] [G loss: 0.414448]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 142/173 [loss: 0.755186, acc.: 50.00%] [G loss: 0.412498]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 143/173 [loss: 0.755339, acc.: 50.00%] [G loss: 0.411450]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 144/173 [loss: 0.756479, acc.: 50.00%] [G loss: 0.414060]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 145/173 [loss: 0.755813, acc.: 50.00%] [G loss: 0.416253]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 146/173 [loss: 0.753160, acc.: 50.00%] [G loss: 0.419076]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 147/173 [loss: 0.752353, acc.: 50.00%] [G loss: 0.419668]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 148/173 [loss: 0.748906, acc.: 50.00%] [G loss: 0.422509]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 149/173 [loss: 0.745332, acc.: 50.00%] [G loss: 0.424264]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 150/173 [loss: 0.743656, acc.: 50.00%] [G loss: 0.425662]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 151/173 [loss: 0.741962, acc.: 50.00%] [G loss: 0.431132]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 152/173 [loss: 0.740543, acc.: 50.00%] [G loss: 0.431351]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 153/173 [loss: 0.735853, acc.: 50.00%] [G loss: 0.433551]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 154/173 [loss: 0.737670, acc.: 50.00%] [G loss: 0.434503]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 155/173 [loss: 0.738014, acc.: 50.00%] [G loss: 0.432517]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 156/173 [loss: 0.738173, acc.: 50.00%] [G loss: 0.432311]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 157/173 [loss: 0.735943, acc.: 50.00%] [G loss: 0.429873]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 158/173 [loss: 0.738525, acc.: 50.00%] [G loss: 0.430811]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 159/173 [loss: 0.742613, acc.: 50.00%] [G loss: 0.426039]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 160/173 [loss: 0.742974, acc.: 50.00%] [G loss: 0.422845]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 161/173 [loss: 0.743877, acc.: 50.00%] [G loss: 0.422166]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 162/173 [loss: 0.749365, acc.: 50.00%] [G loss: 0.420852]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 163/173 [loss: 0.750624, acc.: 50.00%] [G loss: 0.418280]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 164/173 [loss: 0.749743, acc.: 50.00%] [G loss: 0.416410]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 165/173 [loss: 0.751411, acc.: 50.00%] [G loss: 0.415030]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 166/173 [loss: 0.755456, acc.: 50.00%] [G loss: 0.415838]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 167/173 [loss: 0.753557, acc.: 50.00%] [G loss: 0.415397]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 168/173 [loss: 0.753240, acc.: 50.00%] [G loss: 0.413657]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 169/173 [loss: 0.753310, acc.: 50.00%] [G loss: 0.416577]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 170/173 [loss: 0.753209, acc.: 50.00%] [G loss: 0.416554]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 171/173 [loss: 0.753019, acc.: 50.00%] [G loss: 0.417655]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 7/200  Batch Size: 172/173 [loss: 0.751143, acc.: 50.00%] [G loss: 0.420113]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 0/173 [loss: 0.748591, acc.: 50.00%] [G loss: 0.422426]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 1/173 [loss: 0.744340, acc.: 50.00%] [G loss: 0.425603]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 2/173 [loss: 0.744023, acc.: 50.00%] [G loss: 0.426517]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 3/173 [loss: 0.741754, acc.: 50.00%] [G loss: 0.429983]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 4/173 [loss: 0.740333, acc.: 50.00%] [G loss: 0.429896]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 5/173 [loss: 0.740339, acc.: 50.00%] [G loss: 0.429203]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 6/173 [loss: 0.740767, acc.: 50.00%] [G loss: 0.430239]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 7/173 [loss: 0.736905, acc.: 50.00%] [G loss: 0.428741]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 8/173 [loss: 0.736522, acc.: 50.00%] [G loss: 0.428623]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 9/173 [loss: 0.738873, acc.: 50.00%] [G loss: 0.429021]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 10/173 [loss: 0.740020, acc.: 50.00%] [G loss: 0.424548]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 11/173 [loss: 0.741268, acc.: 50.00%] [G loss: 0.426021]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 12/173 [loss: 0.743355, acc.: 50.00%] [G loss: 0.424054]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 13/173 [loss: 0.745564, acc.: 50.00%] [G loss: 0.419446]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 14/173 [loss: 0.744737, acc.: 50.00%] [G loss: 0.422726]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 15/173 [loss: 0.746860, acc.: 50.00%] [G loss: 0.421871]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 16/173 [loss: 0.750303, acc.: 50.00%] [G loss: 0.416825]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 17/173 [loss: 0.751097, acc.: 50.00%] [G loss: 0.416653]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 18/173 [loss: 0.750366, acc.: 50.00%] [G loss: 0.414623]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 19/173 [loss: 0.755037, acc.: 50.00%] [G loss: 0.414022]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 20/173 [loss: 0.752415, acc.: 50.00%] [G loss: 0.415484]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 21/173 [loss: 0.753284, acc.: 50.00%] [G loss: 0.416647]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 22/173 [loss: 0.751962, acc.: 50.00%] [G loss: 0.416573]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 23/173 [loss: 0.752199, acc.: 50.00%] [G loss: 0.418651]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 24/173 [loss: 0.749330, acc.: 50.00%] [G loss: 0.420504]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 25/173 [loss: 0.747914, acc.: 50.00%] [G loss: 0.421560]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 26/173 [loss: 0.748209, acc.: 50.00%] [G loss: 0.426560]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 27/173 [loss: 0.744687, acc.: 50.00%] [G loss: 0.425781]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 28/173 [loss: 0.743890, acc.: 50.00%] [G loss: 0.427347]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 29/173 [loss: 0.742064, acc.: 50.00%] [G loss: 0.430504]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 30/173 [loss: 0.738838, acc.: 50.00%] [G loss: 0.431187]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 31/173 [loss: 0.740170, acc.: 50.00%] [G loss: 0.432703]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 32/173 [loss: 0.739383, acc.: 50.00%] [G loss: 0.432120]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 33/173 [loss: 0.737583, acc.: 50.00%] [G loss: 0.430224]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 34/173 [loss: 0.739322, acc.: 50.00%] [G loss: 0.428494]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 35/173 [loss: 0.739490, acc.: 50.00%] [G loss: 0.429849]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 36/173 [loss: 0.739321, acc.: 50.00%] [G loss: 0.427552]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 37/173 [loss: 0.740627, acc.: 50.00%] [G loss: 0.426754]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 38/173 [loss: 0.743599, acc.: 50.00%] [G loss: 0.425922]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 39/173 [loss: 0.747405, acc.: 50.00%] [G loss: 0.424587]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 40/173 [loss: 0.746117, acc.: 50.00%] [G loss: 0.422502]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 41/173 [loss: 0.746710, acc.: 50.00%] [G loss: 0.421617]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 42/173 [loss: 0.748208, acc.: 50.00%] [G loss: 0.418852]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 43/173 [loss: 0.749168, acc.: 50.00%] [G loss: 0.418610]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 44/173 [loss: 0.751648, acc.: 50.00%] [G loss: 0.418990]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 45/173 [loss: 0.749020, acc.: 50.00%] [G loss: 0.415500]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 46/173 [loss: 0.751115, acc.: 50.00%] [G loss: 0.417785]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 47/173 [loss: 0.751767, acc.: 50.00%] [G loss: 0.418297]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 48/173 [loss: 0.748976, acc.: 50.00%] [G loss: 0.420054]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 49/173 [loss: 0.749140, acc.: 50.00%] [G loss: 0.420471]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 50/173 [loss: 0.750510, acc.: 50.00%] [G loss: 0.420727]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 51/173 [loss: 0.746439, acc.: 50.00%] [G loss: 0.421932]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 52/173 [loss: 0.747171, acc.: 50.00%] [G loss: 0.426783]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 53/173 [loss: 0.743206, acc.: 50.00%] [G loss: 0.426242]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 54/173 [loss: 0.743517, acc.: 50.00%] [G loss: 0.427407]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 55/173 [loss: 0.742041, acc.: 50.00%] [G loss: 0.424911]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 56/173 [loss: 0.742112, acc.: 50.00%] [G loss: 0.427292]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 57/173 [loss: 0.741703, acc.: 50.00%] [G loss: 0.426542]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 58/173 [loss: 0.739308, acc.: 50.00%] [G loss: 0.426331]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 59/173 [loss: 0.743220, acc.: 50.00%] [G loss: 0.424497]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 60/173 [loss: 0.741869, acc.: 50.00%] [G loss: 0.424664]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 61/173 [loss: 0.743207, acc.: 50.00%] [G loss: 0.422846]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 62/173 [loss: 0.743974, acc.: 50.00%] [G loss: 0.422377]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 63/173 [loss: 0.747046, acc.: 50.00%] [G loss: 0.420324]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 64/173 [loss: 0.747672, acc.: 50.00%] [G loss: 0.418868]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 65/173 [loss: 0.746336, acc.: 50.00%] [G loss: 0.420206]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 66/173 [loss: 0.748595, acc.: 50.00%] [G loss: 0.420716]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 67/173 [loss: 0.750018, acc.: 50.00%] [G loss: 0.417682]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 68/173 [loss: 0.748552, acc.: 50.00%] [G loss: 0.416686]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 69/173 [loss: 0.748753, acc.: 50.00%] [G loss: 0.418337]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 70/173 [loss: 0.748939, acc.: 50.00%] [G loss: 0.417965]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 71/173 [loss: 0.748699, acc.: 50.00%] [G loss: 0.419368]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 72/173 [loss: 0.752139, acc.: 50.00%] [G loss: 0.421277]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 73/173 [loss: 0.748288, acc.: 50.00%] [G loss: 0.421164]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 74/173 [loss: 0.747840, acc.: 50.00%] [G loss: 0.422608]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 75/173 [loss: 0.749200, acc.: 50.00%] [G loss: 0.424483]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 76/173 [loss: 0.744696, acc.: 50.00%] [G loss: 0.426206]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 77/173 [loss: 0.744684, acc.: 50.00%] [G loss: 0.426528]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 78/173 [loss: 0.744462, acc.: 50.00%] [G loss: 0.427916]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 79/173 [loss: 0.743211, acc.: 50.00%] [G loss: 0.427130]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 80/173 [loss: 0.740467, acc.: 50.00%] [G loss: 0.428449]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 81/173 [loss: 0.740756, acc.: 50.00%] [G loss: 0.427634]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 82/173 [loss: 0.742842, acc.: 50.00%] [G loss: 0.429318]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 83/173 [loss: 0.741676, acc.: 50.00%] [G loss: 0.429567]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 84/173 [loss: 0.742392, acc.: 50.00%] [G loss: 0.428000]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 85/173 [loss: 0.744538, acc.: 50.00%] [G loss: 0.430308]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 86/173 [loss: 0.743915, acc.: 50.00%] [G loss: 0.427166]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 87/173 [loss: 0.741901, acc.: 50.00%] [G loss: 0.426640]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 88/173 [loss: 0.744783, acc.: 50.00%] [G loss: 0.423577]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 89/173 [loss: 0.747901, acc.: 50.00%] [G loss: 0.423542]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 90/173 [loss: 0.747682, acc.: 50.00%] [G loss: 0.421574]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 91/173 [loss: 0.747740, acc.: 50.00%] [G loss: 0.422792]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 92/173 [loss: 0.749926, acc.: 50.00%] [G loss: 0.418755]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 93/173 [loss: 0.748570, acc.: 50.00%] [G loss: 0.421468]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 94/173 [loss: 0.749953, acc.: 50.00%] [G loss: 0.420074]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 95/173 [loss: 0.747858, acc.: 50.00%] [G loss: 0.417980]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 96/173 [loss: 0.749890, acc.: 50.00%] [G loss: 0.420002]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 97/173 [loss: 0.749362, acc.: 50.00%] [G loss: 0.421233]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 98/173 [loss: 0.745175, acc.: 50.00%] [G loss: 0.421497]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 99/173 [loss: 0.745317, acc.: 50.00%] [G loss: 0.421517]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 100/173 [loss: 0.747949, acc.: 50.00%] [G loss: 0.422747]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 101/173 [loss: 0.747053, acc.: 50.00%] [G loss: 0.421790]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 102/173 [loss: 0.743824, acc.: 50.00%] [G loss: 0.421500]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 103/173 [loss: 0.744841, acc.: 50.00%] [G loss: 0.423613]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 104/173 [loss: 0.740775, acc.: 50.00%] [G loss: 0.426791]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 105/173 [loss: 0.743033, acc.: 50.00%] [G loss: 0.422920]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 106/173 [loss: 0.743028, acc.: 50.00%] [G loss: 0.424827]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 107/173 [loss: 0.740581, acc.: 50.00%] [G loss: 0.426253]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 108/173 [loss: 0.741330, acc.: 50.00%] [G loss: 0.425770]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 109/173 [loss: 0.744730, acc.: 50.00%] [G loss: 0.423583]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 110/173 [loss: 0.741456, acc.: 50.00%] [G loss: 0.423577]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 111/173 [loss: 0.741656, acc.: 50.00%] [G loss: 0.425157]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 112/173 [loss: 0.743775, acc.: 50.00%] [G loss: 0.422382]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 113/173 [loss: 0.742883, acc.: 50.00%] [G loss: 0.420298]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 114/173 [loss: 0.747011, acc.: 50.00%] [G loss: 0.419029]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 115/173 [loss: 0.746397, acc.: 50.00%] [G loss: 0.418595]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 116/173 [loss: 0.750198, acc.: 50.00%] [G loss: 0.417716]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 117/173 [loss: 0.748693, acc.: 50.00%] [G loss: 0.419367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 118/173 [loss: 0.750506, acc.: 50.00%] [G loss: 0.417345]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 119/173 [loss: 0.748834, acc.: 50.00%] [G loss: 0.416924]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 120/173 [loss: 0.750660, acc.: 50.00%] [G loss: 0.418083]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 121/173 [loss: 0.749962, acc.: 50.00%] [G loss: 0.416830]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 122/173 [loss: 0.749830, acc.: 50.00%] [G loss: 0.417568]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 123/173 [loss: 0.750316, acc.: 50.00%] [G loss: 0.419388]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 124/173 [loss: 0.750170, acc.: 50.00%] [G loss: 0.421416]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 125/173 [loss: 0.748125, acc.: 50.00%] [G loss: 0.421474]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 126/173 [loss: 0.745630, acc.: 50.00%] [G loss: 0.422772]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 127/173 [loss: 0.745681, acc.: 50.00%] [G loss: 0.422870]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 128/173 [loss: 0.747888, acc.: 50.00%] [G loss: 0.425780]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 129/173 [loss: 0.742653, acc.: 50.00%] [G loss: 0.426372]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 130/173 [loss: 0.744651, acc.: 50.00%] [G loss: 0.428928]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 131/173 [loss: 0.745213, acc.: 50.00%] [G loss: 0.425679]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 132/173 [loss: 0.741856, acc.: 50.00%] [G loss: 0.427439]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 133/173 [loss: 0.742050, acc.: 50.00%] [G loss: 0.427533]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 134/173 [loss: 0.740346, acc.: 50.00%] [G loss: 0.425706]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 135/173 [loss: 0.742046, acc.: 50.00%] [G loss: 0.428727]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 136/173 [loss: 0.738905, acc.: 50.00%] [G loss: 0.424176]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 137/173 [loss: 0.742737, acc.: 50.00%] [G loss: 0.426045]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 138/173 [loss: 0.740251, acc.: 50.00%] [G loss: 0.425895]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 139/173 [loss: 0.743775, acc.: 50.00%] [G loss: 0.426075]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 140/173 [loss: 0.744670, acc.: 50.00%] [G loss: 0.425897]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 141/173 [loss: 0.744424, acc.: 50.00%] [G loss: 0.424250]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 142/173 [loss: 0.746097, acc.: 50.00%] [G loss: 0.421393]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 143/173 [loss: 0.747249, acc.: 50.00%] [G loss: 0.423058]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 144/173 [loss: 0.748074, acc.: 50.00%] [G loss: 0.419756]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 145/173 [loss: 0.750072, acc.: 50.00%] [G loss: 0.416662]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 146/173 [loss: 0.750810, acc.: 50.00%] [G loss: 0.417393]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 147/173 [loss: 0.750376, acc.: 50.00%] [G loss: 0.418138]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 148/173 [loss: 0.748915, acc.: 50.00%] [G loss: 0.419317]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 149/173 [loss: 0.750792, acc.: 50.00%] [G loss: 0.419567]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 150/173 [loss: 0.749375, acc.: 50.00%] [G loss: 0.419265]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 151/173 [loss: 0.748060, acc.: 50.00%] [G loss: 0.418789]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 152/173 [loss: 0.747882, acc.: 50.00%] [G loss: 0.420010]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 153/173 [loss: 0.749308, acc.: 50.00%] [G loss: 0.420185]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 154/173 [loss: 0.747081, acc.: 50.00%] [G loss: 0.421317]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 155/173 [loss: 0.745996, acc.: 50.00%] [G loss: 0.423676]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 156/173 [loss: 0.746108, acc.: 50.00%] [G loss: 0.422536]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 157/173 [loss: 0.745614, acc.: 50.00%] [G loss: 0.424889]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 158/173 [loss: 0.742184, acc.: 50.00%] [G loss: 0.427475]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 159/173 [loss: 0.742923, acc.: 50.00%] [G loss: 0.425790]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 160/173 [loss: 0.739016, acc.: 50.00%] [G loss: 0.424979]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 161/173 [loss: 0.741765, acc.: 50.00%] [G loss: 0.424501]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 162/173 [loss: 0.741113, acc.: 50.00%] [G loss: 0.424958]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 163/173 [loss: 0.740565, acc.: 50.00%] [G loss: 0.427095]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 164/173 [loss: 0.742245, acc.: 50.00%] [G loss: 0.425042]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 165/173 [loss: 0.742424, acc.: 50.00%] [G loss: 0.425310]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 166/173 [loss: 0.743798, acc.: 50.00%] [G loss: 0.424210]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 167/173 [loss: 0.744224, acc.: 50.00%] [G loss: 0.421654]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 168/173 [loss: 0.744523, acc.: 50.00%] [G loss: 0.419851]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 169/173 [loss: 0.745711, acc.: 50.00%] [G loss: 0.421175]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 170/173 [loss: 0.747646, acc.: 50.00%] [G loss: 0.419753]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 171/173 [loss: 0.748295, acc.: 50.00%] [G loss: 0.418732]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 8/200  Batch Size: 172/173 [loss: 0.748801, acc.: 50.00%] [G loss: 0.416707]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 0/173 [loss: 0.749421, acc.: 50.00%] [G loss: 0.416550]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 1/173 [loss: 0.752538, acc.: 50.00%] [G loss: 0.415611]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 2/173 [loss: 0.752229, acc.: 50.00%] [G loss: 0.417586]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 3/173 [loss: 0.751073, acc.: 50.00%] [G loss: 0.417205]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 4/173 [loss: 0.748433, acc.: 50.00%] [G loss: 0.416613]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 5/173 [loss: 0.747834, acc.: 50.00%] [G loss: 0.419352]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 6/173 [loss: 0.752180, acc.: 50.00%] [G loss: 0.419393]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 7/173 [loss: 0.747537, acc.: 50.00%] [G loss: 0.421099]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 8/173 [loss: 0.748679, acc.: 50.00%] [G loss: 0.421856]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 9/173 [loss: 0.745115, acc.: 50.00%] [G loss: 0.424125]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 10/173 [loss: 0.744639, acc.: 50.00%] [G loss: 0.425272]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 11/173 [loss: 0.743256, acc.: 50.00%] [G loss: 0.426106]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 12/173 [loss: 0.742241, acc.: 50.00%] [G loss: 0.427221]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 13/173 [loss: 0.739993, acc.: 50.00%] [G loss: 0.428883]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 14/173 [loss: 0.741669, acc.: 50.00%] [G loss: 0.426666]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 15/173 [loss: 0.741813, acc.: 50.00%] [G loss: 0.428399]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 16/173 [loss: 0.739180, acc.: 50.00%] [G loss: 0.426283]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 17/173 [loss: 0.739220, acc.: 50.00%] [G loss: 0.430977]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 18/173 [loss: 0.740580, acc.: 50.00%] [G loss: 0.424733]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 19/173 [loss: 0.740832, acc.: 50.00%] [G loss: 0.427204]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 20/173 [loss: 0.743749, acc.: 50.00%] [G loss: 0.426831]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 21/173 [loss: 0.742439, acc.: 50.00%] [G loss: 0.420566]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 22/173 [loss: 0.744126, acc.: 50.00%] [G loss: 0.422607]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 23/173 [loss: 0.745665, acc.: 50.00%] [G loss: 0.420127]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 24/173 [loss: 0.748549, acc.: 50.00%] [G loss: 0.421337]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 25/173 [loss: 0.750574, acc.: 50.00%] [G loss: 0.415990]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 26/173 [loss: 0.747982, acc.: 50.00%] [G loss: 0.416914]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 27/173 [loss: 0.751507, acc.: 50.00%] [G loss: 0.415605]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 28/173 [loss: 0.751010, acc.: 50.00%] [G loss: 0.415121]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 29/173 [loss: 0.747966, acc.: 50.00%] [G loss: 0.415729]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 30/173 [loss: 0.751023, acc.: 50.00%] [G loss: 0.414577]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 31/173 [loss: 0.751114, acc.: 50.00%] [G loss: 0.421119]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 32/173 [loss: 0.750775, acc.: 50.00%] [G loss: 0.417220]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 33/173 [loss: 0.748384, acc.: 50.00%] [G loss: 0.419261]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 34/173 [loss: 0.745487, acc.: 50.00%] [G loss: 0.420429]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 35/173 [loss: 0.743316, acc.: 50.00%] [G loss: 0.423634]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 36/173 [loss: 0.742844, acc.: 50.00%] [G loss: 0.423956]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 37/173 [loss: 0.745252, acc.: 50.00%] [G loss: 0.424995]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 38/173 [loss: 0.739563, acc.: 50.00%] [G loss: 0.425309]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 39/173 [loss: 0.739941, acc.: 50.00%] [G loss: 0.424775]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 40/173 [loss: 0.740183, acc.: 50.00%] [G loss: 0.425230]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 41/173 [loss: 0.740260, acc.: 50.00%] [G loss: 0.427702]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 42/173 [loss: 0.742035, acc.: 50.00%] [G loss: 0.424967]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 43/173 [loss: 0.740681, acc.: 50.00%] [G loss: 0.422434]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 44/173 [loss: 0.742818, acc.: 50.00%] [G loss: 0.424716]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 45/173 [loss: 0.744119, acc.: 50.00%] [G loss: 0.422319]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 46/173 [loss: 0.745968, acc.: 50.00%] [G loss: 0.423149]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 47/173 [loss: 0.744354, acc.: 50.00%] [G loss: 0.421397]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 48/173 [loss: 0.744748, acc.: 50.00%] [G loss: 0.422339]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 49/173 [loss: 0.746297, acc.: 50.00%] [G loss: 0.420396]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 50/173 [loss: 0.746928, acc.: 50.00%] [G loss: 0.418454]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 51/173 [loss: 0.747572, acc.: 50.00%] [G loss: 0.416217]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 52/173 [loss: 0.746793, acc.: 50.00%] [G loss: 0.417567]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 53/173 [loss: 0.746142, acc.: 50.00%] [G loss: 0.417851]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 54/173 [loss: 0.750313, acc.: 50.00%] [G loss: 0.414971]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 55/173 [loss: 0.749353, acc.: 50.00%] [G loss: 0.414235]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 56/173 [loss: 0.751475, acc.: 50.00%] [G loss: 0.417051]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 57/173 [loss: 0.750390, acc.: 50.00%] [G loss: 0.416105]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 58/173 [loss: 0.751555, acc.: 50.00%] [G loss: 0.420737]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 59/173 [loss: 0.747950, acc.: 50.00%] [G loss: 0.418988]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 60/173 [loss: 0.748474, acc.: 50.00%] [G loss: 0.421096]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 61/173 [loss: 0.744168, acc.: 50.00%] [G loss: 0.422534]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 62/173 [loss: 0.745494, acc.: 50.00%] [G loss: 0.423749]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 63/173 [loss: 0.742542, acc.: 50.00%] [G loss: 0.424673]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 64/173 [loss: 0.743018, acc.: 50.00%] [G loss: 0.424202]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 65/173 [loss: 0.743790, acc.: 50.00%] [G loss: 0.425700]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 66/173 [loss: 0.742100, acc.: 50.00%] [G loss: 0.425979]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 67/173 [loss: 0.739084, acc.: 50.00%] [G loss: 0.428354]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 68/173 [loss: 0.740627, acc.: 50.00%] [G loss: 0.426595]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 69/173 [loss: 0.741302, acc.: 50.00%] [G loss: 0.430357]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 70/173 [loss: 0.742773, acc.: 50.00%] [G loss: 0.424791]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 71/173 [loss: 0.740777, acc.: 50.00%] [G loss: 0.424839]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 72/173 [loss: 0.740573, acc.: 50.00%] [G loss: 0.424617]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 73/173 [loss: 0.741506, acc.: 50.00%] [G loss: 0.424069]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 74/173 [loss: 0.741830, acc.: 50.00%] [G loss: 0.422263]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 75/173 [loss: 0.746061, acc.: 50.00%] [G loss: 0.421238]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 76/173 [loss: 0.744991, acc.: 50.00%] [G loss: 0.420210]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 77/173 [loss: 0.743761, acc.: 50.00%] [G loss: 0.419544]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 78/173 [loss: 0.745841, acc.: 50.00%] [G loss: 0.420602]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 79/173 [loss: 0.747347, acc.: 50.00%] [G loss: 0.419586]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 80/173 [loss: 0.747490, acc.: 50.00%] [G loss: 0.420665]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 81/173 [loss: 0.746940, acc.: 50.00%] [G loss: 0.419732]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 82/173 [loss: 0.746235, acc.: 50.00%] [G loss: 0.418823]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 83/173 [loss: 0.745744, acc.: 50.00%] [G loss: 0.421871]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 84/173 [loss: 0.748848, acc.: 50.00%] [G loss: 0.419098]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 85/173 [loss: 0.749287, acc.: 50.00%] [G loss: 0.420612]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 86/173 [loss: 0.747207, acc.: 50.00%] [G loss: 0.417798]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 87/173 [loss: 0.748056, acc.: 50.00%] [G loss: 0.419920]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 88/173 [loss: 0.748943, acc.: 50.00%] [G loss: 0.420776]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 89/173 [loss: 0.748807, acc.: 50.00%] [G loss: 0.417849]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 90/173 [loss: 0.747274, acc.: 50.00%] [G loss: 0.419899]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 91/173 [loss: 0.747121, acc.: 50.00%] [G loss: 0.421822]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 92/173 [loss: 0.746063, acc.: 50.00%] [G loss: 0.420559]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 93/173 [loss: 0.747914, acc.: 50.00%] [G loss: 0.422228]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 94/173 [loss: 0.744053, acc.: 50.00%] [G loss: 0.419527]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 95/173 [loss: 0.744719, acc.: 50.00%] [G loss: 0.422138]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 96/173 [loss: 0.743387, acc.: 50.00%] [G loss: 0.423456]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 97/173 [loss: 0.744870, acc.: 50.00%] [G loss: 0.423446]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 98/173 [loss: 0.742464, acc.: 50.00%] [G loss: 0.424923]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 99/173 [loss: 0.744224, acc.: 50.00%] [G loss: 0.422778]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 100/173 [loss: 0.743952, acc.: 50.00%] [G loss: 0.427347]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 101/173 [loss: 0.741773, acc.: 50.00%] [G loss: 0.424728]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 102/173 [loss: 0.742883, acc.: 50.00%] [G loss: 0.423743]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 103/173 [loss: 0.742962, acc.: 50.00%] [G loss: 0.423748]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 104/173 [loss: 0.744932, acc.: 50.00%] [G loss: 0.423280]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 105/173 [loss: 0.742609, acc.: 50.00%] [G loss: 0.423854]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 106/173 [loss: 0.744626, acc.: 50.00%] [G loss: 0.419258]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 107/173 [loss: 0.745005, acc.: 50.00%] [G loss: 0.420774]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 108/173 [loss: 0.748051, acc.: 50.00%] [G loss: 0.418711]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 109/173 [loss: 0.749003, acc.: 50.00%] [G loss: 0.418222]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 110/173 [loss: 0.749531, acc.: 50.00%] [G loss: 0.414942]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 111/173 [loss: 0.748736, acc.: 50.00%] [G loss: 0.416520]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 112/173 [loss: 0.749584, acc.: 50.00%] [G loss: 0.416307]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 113/173 [loss: 0.751459, acc.: 50.00%] [G loss: 0.417093]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 114/173 [loss: 0.748416, acc.: 50.00%] [G loss: 0.415023]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 115/173 [loss: 0.747999, acc.: 50.00%] [G loss: 0.418723]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 116/173 [loss: 0.750623, acc.: 50.00%] [G loss: 0.420146]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 117/173 [loss: 0.748875, acc.: 50.00%] [G loss: 0.420511]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 118/173 [loss: 0.747131, acc.: 50.00%] [G loss: 0.420590]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 119/173 [loss: 0.744963, acc.: 50.00%] [G loss: 0.423905]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 120/173 [loss: 0.743948, acc.: 50.00%] [G loss: 0.424138]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 121/173 [loss: 0.745195, acc.: 50.00%] [G loss: 0.426053]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 122/173 [loss: 0.742318, acc.: 50.00%] [G loss: 0.427320]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 123/173 [loss: 0.742311, acc.: 50.00%] [G loss: 0.429045]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 124/173 [loss: 0.742079, acc.: 50.00%] [G loss: 0.427231]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 125/173 [loss: 0.741698, acc.: 50.00%] [G loss: 0.426880]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 126/173 [loss: 0.740032, acc.: 50.00%] [G loss: 0.428052]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 127/173 [loss: 0.739064, acc.: 50.00%] [G loss: 0.428640]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 128/173 [loss: 0.742998, acc.: 50.00%] [G loss: 0.428401]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 129/173 [loss: 0.741788, acc.: 50.00%] [G loss: 0.426302]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 130/173 [loss: 0.743576, acc.: 50.00%] [G loss: 0.424679]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 131/173 [loss: 0.745255, acc.: 50.00%] [G loss: 0.422972]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 132/173 [loss: 0.747035, acc.: 50.00%] [G loss: 0.421456]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 133/173 [loss: 0.748157, acc.: 50.00%] [G loss: 0.420089]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 134/173 [loss: 0.750228, acc.: 50.00%] [G loss: 0.416853]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 135/173 [loss: 0.748905, acc.: 50.00%] [G loss: 0.418946]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 136/173 [loss: 0.752035, acc.: 50.00%] [G loss: 0.418929]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 137/173 [loss: 0.748483, acc.: 50.00%] [G loss: 0.418447]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 138/173 [loss: 0.747405, acc.: 50.00%] [G loss: 0.419382]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 139/173 [loss: 0.748411, acc.: 50.00%] [G loss: 0.418692]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 140/173 [loss: 0.749787, acc.: 50.00%] [G loss: 0.420306]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 141/173 [loss: 0.747948, acc.: 50.00%] [G loss: 0.420055]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 142/173 [loss: 0.748230, acc.: 50.00%] [G loss: 0.423155]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 143/173 [loss: 0.747141, acc.: 50.00%] [G loss: 0.421637]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 144/173 [loss: 0.745005, acc.: 50.00%] [G loss: 0.423999]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 145/173 [loss: 0.743728, acc.: 50.00%] [G loss: 0.423367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 146/173 [loss: 0.743204, acc.: 50.00%] [G loss: 0.424274]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 147/173 [loss: 0.741733, acc.: 50.00%] [G loss: 0.425584]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 148/173 [loss: 0.742733, acc.: 50.00%] [G loss: 0.426999]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 149/173 [loss: 0.741628, acc.: 50.00%] [G loss: 0.424690]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 150/173 [loss: 0.741015, acc.: 50.00%] [G loss: 0.425356]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 151/173 [loss: 0.743344, acc.: 50.00%] [G loss: 0.426023]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 152/173 [loss: 0.740095, acc.: 50.00%] [G loss: 0.425353]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 153/173 [loss: 0.742270, acc.: 50.00%] [G loss: 0.426243]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 154/173 [loss: 0.742319, acc.: 50.00%] [G loss: 0.424862]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 155/173 [loss: 0.746205, acc.: 50.00%] [G loss: 0.423310]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 156/173 [loss: 0.746966, acc.: 50.00%] [G loss: 0.422085]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 157/173 [loss: 0.743946, acc.: 50.00%] [G loss: 0.419613]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 158/173 [loss: 0.746449, acc.: 50.00%] [G loss: 0.421250]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 159/173 [loss: 0.748643, acc.: 50.00%] [G loss: 0.419234]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 160/173 [loss: 0.748479, acc.: 50.00%] [G loss: 0.420140]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 161/173 [loss: 0.750045, acc.: 50.00%] [G loss: 0.415855]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 162/173 [loss: 0.745717, acc.: 50.00%] [G loss: 0.415097]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 163/173 [loss: 0.750750, acc.: 50.00%] [G loss: 0.415619]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 164/173 [loss: 0.750684, acc.: 50.00%] [G loss: 0.417156]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 165/173 [loss: 0.748743, acc.: 50.00%] [G loss: 0.417109]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 166/173 [loss: 0.749938, acc.: 50.00%] [G loss: 0.418752]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 167/173 [loss: 0.751258, acc.: 50.00%] [G loss: 0.417334]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 168/173 [loss: 0.751808, acc.: 50.00%] [G loss: 0.417323]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 169/173 [loss: 0.747844, acc.: 50.00%] [G loss: 0.418752]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 170/173 [loss: 0.749015, acc.: 50.00%] [G loss: 0.419987]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 171/173 [loss: 0.747740, acc.: 50.00%] [G loss: 0.421534]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 9/200  Batch Size: 172/173 [loss: 0.747016, acc.: 50.00%] [G loss: 0.420489]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 0/173 [loss: 0.746074, acc.: 50.00%] [G loss: 0.422858]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 1/173 [loss: 0.744997, acc.: 50.00%] [G loss: 0.421210]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 2/173 [loss: 0.746168, acc.: 50.00%] [G loss: 0.423889]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 3/173 [loss: 0.744233, acc.: 50.00%] [G loss: 0.423903]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 4/173 [loss: 0.745404, acc.: 50.00%] [G loss: 0.425224]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 5/173 [loss: 0.746605, acc.: 50.00%] [G loss: 0.423001]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 6/173 [loss: 0.742345, acc.: 50.00%] [G loss: 0.425212]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 7/173 [loss: 0.745853, acc.: 50.00%] [G loss: 0.423086]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 8/173 [loss: 0.745530, acc.: 50.00%] [G loss: 0.424863]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 9/173 [loss: 0.744927, acc.: 50.00%] [G loss: 0.425343]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 10/173 [loss: 0.744637, acc.: 50.00%] [G loss: 0.424856]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 11/173 [loss: 0.746320, acc.: 50.00%] [G loss: 0.424062]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 12/173 [loss: 0.743150, acc.: 50.00%] [G loss: 0.422997]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 13/173 [loss: 0.746926, acc.: 50.00%] [G loss: 0.421156]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 14/173 [loss: 0.744754, acc.: 50.00%] [G loss: 0.421892]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 15/173 [loss: 0.748020, acc.: 50.00%] [G loss: 0.419691]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 16/173 [loss: 0.745431, acc.: 50.00%] [G loss: 0.418569]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 17/173 [loss: 0.747577, acc.: 50.00%] [G loss: 0.418033]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 18/173 [loss: 0.748431, acc.: 50.00%] [G loss: 0.417221]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 19/173 [loss: 0.750806, acc.: 50.00%] [G loss: 0.420089]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 20/173 [loss: 0.750216, acc.: 50.00%] [G loss: 0.415696]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 21/173 [loss: 0.747849, acc.: 50.00%] [G loss: 0.418867]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 22/173 [loss: 0.748911, acc.: 50.00%] [G loss: 0.416613]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 23/173 [loss: 0.745864, acc.: 50.00%] [G loss: 0.419119]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 24/173 [loss: 0.748786, acc.: 50.00%] [G loss: 0.419870]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 25/173 [loss: 0.745953, acc.: 50.00%] [G loss: 0.420993]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 26/173 [loss: 0.749802, acc.: 50.00%] [G loss: 0.420567]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 27/173 [loss: 0.747604, acc.: 50.00%] [G loss: 0.421835]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 28/173 [loss: 0.744251, acc.: 50.00%] [G loss: 0.421927]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 29/173 [loss: 0.744208, acc.: 50.00%] [G loss: 0.423896]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 30/173 [loss: 0.743560, acc.: 50.00%] [G loss: 0.424257]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 31/173 [loss: 0.745853, acc.: 50.00%] [G loss: 0.423721]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 32/173 [loss: 0.741990, acc.: 50.00%] [G loss: 0.422832]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 33/173 [loss: 0.742474, acc.: 50.00%] [G loss: 0.424997]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 34/173 [loss: 0.743951, acc.: 50.00%] [G loss: 0.423698]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 35/173 [loss: 0.743768, acc.: 50.00%] [G loss: 0.422062]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 36/173 [loss: 0.745897, acc.: 50.00%] [G loss: 0.419309]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 37/173 [loss: 0.745317, acc.: 50.00%] [G loss: 0.419847]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 38/173 [loss: 0.747132, acc.: 50.00%] [G loss: 0.418608]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 39/173 [loss: 0.745997, acc.: 50.00%] [G loss: 0.419897]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 40/173 [loss: 0.745347, acc.: 50.00%] [G loss: 0.417482]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 41/173 [loss: 0.747256, acc.: 50.00%] [G loss: 0.416464]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 42/173 [loss: 0.750713, acc.: 50.00%] [G loss: 0.415700]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 43/173 [loss: 0.748833, acc.: 50.00%] [G loss: 0.414502]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 44/173 [loss: 0.752084, acc.: 50.00%] [G loss: 0.413257]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 45/173 [loss: 0.752017, acc.: 50.00%] [G loss: 0.414590]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 46/173 [loss: 0.752807, acc.: 50.00%] [G loss: 0.415994]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 47/173 [loss: 0.749731, acc.: 50.00%] [G loss: 0.417443]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 48/173 [loss: 0.750308, acc.: 50.00%] [G loss: 0.419265]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 49/173 [loss: 0.748385, acc.: 50.00%] [G loss: 0.420114]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 50/173 [loss: 0.748115, acc.: 50.00%] [G loss: 0.422310]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 51/173 [loss: 0.745791, acc.: 50.00%] [G loss: 0.424835]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 52/173 [loss: 0.744406, acc.: 50.00%] [G loss: 0.423947]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 53/173 [loss: 0.743389, acc.: 50.00%] [G loss: 0.425158]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 54/173 [loss: 0.745312, acc.: 50.00%] [G loss: 0.427294]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 55/173 [loss: 0.742927, acc.: 50.00%] [G loss: 0.425036]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 56/173 [loss: 0.741872, acc.: 50.00%] [G loss: 0.425904]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 57/173 [loss: 0.740645, acc.: 50.00%] [G loss: 0.423983]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 58/173 [loss: 0.744350, acc.: 50.00%] [G loss: 0.426221]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 59/173 [loss: 0.742509, acc.: 50.00%] [G loss: 0.425355]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 60/173 [loss: 0.744750, acc.: 50.00%] [G loss: 0.423681]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 61/173 [loss: 0.744998, acc.: 50.00%] [G loss: 0.424691]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 62/173 [loss: 0.742402, acc.: 50.00%] [G loss: 0.423742]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 63/173 [loss: 0.745994, acc.: 50.00%] [G loss: 0.420471]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 64/173 [loss: 0.745423, acc.: 50.00%] [G loss: 0.419202]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 65/173 [loss: 0.744905, acc.: 50.00%] [G loss: 0.422231]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 66/173 [loss: 0.747510, acc.: 50.00%] [G loss: 0.421442]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 67/173 [loss: 0.747564, acc.: 50.00%] [G loss: 0.419972]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 68/173 [loss: 0.748206, acc.: 50.00%] [G loss: 0.418957]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 69/173 [loss: 0.747272, acc.: 50.00%] [G loss: 0.422482]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 70/173 [loss: 0.746830, acc.: 50.00%] [G loss: 0.421027]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 71/173 [loss: 0.747123, acc.: 50.00%] [G loss: 0.419534]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 72/173 [loss: 0.747673, acc.: 50.00%] [G loss: 0.421646]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 73/173 [loss: 0.747444, acc.: 50.00%] [G loss: 0.422097]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 74/173 [loss: 0.747111, acc.: 50.00%] [G loss: 0.420996]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 75/173 [loss: 0.749243, acc.: 50.00%] [G loss: 0.423937]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 76/173 [loss: 0.745581, acc.: 50.00%] [G loss: 0.423472]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 77/173 [loss: 0.747287, acc.: 50.00%] [G loss: 0.423288]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 78/173 [loss: 0.745815, acc.: 50.00%] [G loss: 0.421585]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 79/173 [loss: 0.745870, acc.: 50.00%] [G loss: 0.423397]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 80/173 [loss: 0.746614, acc.: 50.00%] [G loss: 0.422695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 81/173 [loss: 0.743982, acc.: 50.00%] [G loss: 0.423055]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 82/173 [loss: 0.744810, acc.: 50.00%] [G loss: 0.421978]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 83/173 [loss: 0.745438, acc.: 50.00%] [G loss: 0.424261]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 84/173 [loss: 0.746167, acc.: 50.00%] [G loss: 0.422570]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 85/173 [loss: 0.746271, acc.: 50.00%] [G loss: 0.422802]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 86/173 [loss: 0.745102, acc.: 50.00%] [G loss: 0.422110]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 87/173 [loss: 0.744942, acc.: 50.00%] [G loss: 0.423284]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 88/173 [loss: 0.743319, acc.: 50.00%] [G loss: 0.422424]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 89/173 [loss: 0.744343, acc.: 50.00%] [G loss: 0.423158]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 90/173 [loss: 0.745301, acc.: 50.00%] [G loss: 0.422861]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 91/173 [loss: 0.743574, acc.: 50.00%] [G loss: 0.420326]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 92/173 [loss: 0.747320, acc.: 50.00%] [G loss: 0.420011]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 93/173 [loss: 0.747899, acc.: 50.00%] [G loss: 0.419701]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 94/173 [loss: 0.747343, acc.: 50.00%] [G loss: 0.417527]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 95/173 [loss: 0.747567, acc.: 50.00%] [G loss: 0.420412]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 96/173 [loss: 0.750249, acc.: 50.00%] [G loss: 0.418490]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 97/173 [loss: 0.749201, acc.: 50.00%] [G loss: 0.418964]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 98/173 [loss: 0.750712, acc.: 50.00%] [G loss: 0.420174]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 99/173 [loss: 0.747881, acc.: 50.00%] [G loss: 0.420516]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 100/173 [loss: 0.747058, acc.: 50.00%] [G loss: 0.422572]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 101/173 [loss: 0.747593, acc.: 50.00%] [G loss: 0.421196]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 102/173 [loss: 0.746288, acc.: 50.00%] [G loss: 0.420764]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 103/173 [loss: 0.744398, acc.: 50.00%] [G loss: 0.423335]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 104/173 [loss: 0.744160, acc.: 50.00%] [G loss: 0.423439]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 105/173 [loss: 0.744211, acc.: 50.00%] [G loss: 0.424890]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 106/173 [loss: 0.742282, acc.: 50.00%] [G loss: 0.423675]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 107/173 [loss: 0.741092, acc.: 50.00%] [G loss: 0.424661]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 108/173 [loss: 0.743040, acc.: 50.00%] [G loss: 0.424458]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 109/173 [loss: 0.742966, acc.: 50.00%] [G loss: 0.424939]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 110/173 [loss: 0.745406, acc.: 50.00%] [G loss: 0.424175]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 111/173 [loss: 0.743284, acc.: 50.00%] [G loss: 0.423980]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 112/173 [loss: 0.745311, acc.: 50.00%] [G loss: 0.420087]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 113/173 [loss: 0.745928, acc.: 50.00%] [G loss: 0.421864]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 114/173 [loss: 0.747562, acc.: 50.00%] [G loss: 0.422120]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 115/173 [loss: 0.745172, acc.: 50.00%] [G loss: 0.420494]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 116/173 [loss: 0.746568, acc.: 50.00%] [G loss: 0.419019]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 117/173 [loss: 0.750119, acc.: 50.00%] [G loss: 0.417679]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 118/173 [loss: 0.748169, acc.: 50.00%] [G loss: 0.418718]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 119/173 [loss: 0.746432, acc.: 50.00%] [G loss: 0.420329]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 120/173 [loss: 0.747693, acc.: 50.00%] [G loss: 0.418896]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 121/173 [loss: 0.750191, acc.: 50.00%] [G loss: 0.418554]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 122/173 [loss: 0.746439, acc.: 50.00%] [G loss: 0.418599]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 123/173 [loss: 0.748981, acc.: 50.00%] [G loss: 0.421373]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 124/173 [loss: 0.749139, acc.: 50.00%] [G loss: 0.421538]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 125/173 [loss: 0.744285, acc.: 50.00%] [G loss: 0.422146]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 126/173 [loss: 0.745645, acc.: 50.00%] [G loss: 0.421886]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 127/173 [loss: 0.744897, acc.: 50.00%] [G loss: 0.422992]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 128/173 [loss: 0.747888, acc.: 50.00%] [G loss: 0.419550]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 129/173 [loss: 0.741913, acc.: 50.00%] [G loss: 0.422885]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 130/173 [loss: 0.744076, acc.: 50.00%] [G loss: 0.422593]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 131/173 [loss: 0.746000, acc.: 50.00%] [G loss: 0.421066]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 132/173 [loss: 0.746978, acc.: 50.00%] [G loss: 0.422472]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 133/173 [loss: 0.748612, acc.: 50.00%] [G loss: 0.422561]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 134/173 [loss: 0.747502, acc.: 50.00%] [G loss: 0.423574]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 135/173 [loss: 0.747184, acc.: 50.00%] [G loss: 0.421085]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 136/173 [loss: 0.744381, acc.: 50.00%] [G loss: 0.422512]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 137/173 [loss: 0.749606, acc.: 50.00%] [G loss: 0.420768]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 138/173 [loss: 0.747783, acc.: 50.00%] [G loss: 0.415995]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 139/173 [loss: 0.747416, acc.: 50.00%] [G loss: 0.419425]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 140/173 [loss: 0.748979, acc.: 50.00%] [G loss: 0.418815]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 141/173 [loss: 0.748266, acc.: 50.00%] [G loss: 0.417812]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 142/173 [loss: 0.747724, acc.: 50.00%] [G loss: 0.418500]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 143/173 [loss: 0.750460, acc.: 50.00%] [G loss: 0.417801]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 144/173 [loss: 0.750112, acc.: 50.00%] [G loss: 0.417281]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 145/173 [loss: 0.748517, acc.: 50.00%] [G loss: 0.418204]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 146/173 [loss: 0.748695, acc.: 50.00%] [G loss: 0.418385]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 147/173 [loss: 0.748262, acc.: 50.00%] [G loss: 0.419102]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 148/173 [loss: 0.750023, acc.: 50.00%] [G loss: 0.419720]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 149/173 [loss: 0.747195, acc.: 50.00%] [G loss: 0.422849]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 150/173 [loss: 0.745509, acc.: 50.00%] [G loss: 0.420904]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 151/173 [loss: 0.746600, acc.: 50.00%] [G loss: 0.421818]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 152/173 [loss: 0.744580, acc.: 50.00%] [G loss: 0.422183]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 153/173 [loss: 0.746863, acc.: 50.00%] [G loss: 0.423585]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 154/173 [loss: 0.746943, acc.: 50.00%] [G loss: 0.423588]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 155/173 [loss: 0.744114, acc.: 50.00%] [G loss: 0.424226]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 156/173 [loss: 0.742956, acc.: 50.00%] [G loss: 0.422779]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 157/173 [loss: 0.743366, acc.: 50.00%] [G loss: 0.423338]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 158/173 [loss: 0.744852, acc.: 50.00%] [G loss: 0.422711]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 159/173 [loss: 0.746660, acc.: 50.00%] [G loss: 0.425913]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 160/173 [loss: 0.744313, acc.: 50.00%] [G loss: 0.423120]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 161/173 [loss: 0.746131, acc.: 50.00%] [G loss: 0.420680]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 162/173 [loss: 0.746492, acc.: 50.00%] [G loss: 0.421520]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 163/173 [loss: 0.748513, acc.: 50.00%] [G loss: 0.419338]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 164/173 [loss: 0.747426, acc.: 50.00%] [G loss: 0.419979]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 165/173 [loss: 0.747084, acc.: 50.00%] [G loss: 0.420566]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 166/173 [loss: 0.748644, acc.: 50.00%] [G loss: 0.418474]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 167/173 [loss: 0.748196, acc.: 50.00%] [G loss: 0.416237]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 168/173 [loss: 0.747935, acc.: 50.00%] [G loss: 0.419695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 169/173 [loss: 0.753715, acc.: 50.00%] [G loss: 0.417055]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 170/173 [loss: 0.750185, acc.: 50.00%] [G loss: 0.418993]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 171/173 [loss: 0.749648, acc.: 50.00%] [G loss: 0.419045]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 10/200  Batch Size: 172/173 [loss: 0.747973, acc.: 50.00%] [G loss: 0.420103]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 0/173 [loss: 0.749341, acc.: 50.00%] [G loss: 0.420433]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 1/173 [loss: 0.745703, acc.: 50.00%] [G loss: 0.419829]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 2/173 [loss: 0.746160, acc.: 50.00%] [G loss: 0.420615]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 3/173 [loss: 0.744933, acc.: 50.00%] [G loss: 0.420070]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 4/173 [loss: 0.747684, acc.: 50.00%] [G loss: 0.421878]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 5/173 [loss: 0.744239, acc.: 50.00%] [G loss: 0.422289]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 6/173 [loss: 0.743999, acc.: 50.00%] [G loss: 0.422269]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 7/173 [loss: 0.745078, acc.: 50.00%] [G loss: 0.419524]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 8/173 [loss: 0.748279, acc.: 50.00%] [G loss: 0.422367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 9/173 [loss: 0.744449, acc.: 50.00%] [G loss: 0.421434]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 10/173 [loss: 0.745274, acc.: 50.00%] [G loss: 0.421754]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 11/173 [loss: 0.746658, acc.: 50.00%] [G loss: 0.420382]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 12/173 [loss: 0.748628, acc.: 50.00%] [G loss: 0.417826]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 13/173 [loss: 0.748090, acc.: 50.00%] [G loss: 0.415868]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 14/173 [loss: 0.749250, acc.: 50.00%] [G loss: 0.418556]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 15/173 [loss: 0.749445, acc.: 50.00%] [G loss: 0.417226]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 16/173 [loss: 0.749444, acc.: 50.00%] [G loss: 0.416932]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 17/173 [loss: 0.750029, acc.: 50.00%] [G loss: 0.418805]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 18/173 [loss: 0.749051, acc.: 50.00%] [G loss: 0.418367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 19/173 [loss: 0.748894, acc.: 50.00%] [G loss: 0.420111]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 20/173 [loss: 0.751022, acc.: 50.00%] [G loss: 0.420628]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 21/173 [loss: 0.745977, acc.: 50.00%] [G loss: 0.420651]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 22/173 [loss: 0.748540, acc.: 50.00%] [G loss: 0.422665]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 23/173 [loss: 0.746357, acc.: 50.00%] [G loss: 0.423013]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 24/173 [loss: 0.746933, acc.: 50.00%] [G loss: 0.424580]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 25/173 [loss: 0.743571, acc.: 50.00%] [G loss: 0.423822]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 26/173 [loss: 0.744805, acc.: 50.00%] [G loss: 0.423432]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 27/173 [loss: 0.742513, acc.: 50.00%] [G loss: 0.423610]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 28/173 [loss: 0.744005, acc.: 50.00%] [G loss: 0.425732]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 29/173 [loss: 0.745618, acc.: 50.00%] [G loss: 0.423190]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 30/173 [loss: 0.743932, acc.: 50.00%] [G loss: 0.422986]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 31/173 [loss: 0.744142, acc.: 50.00%] [G loss: 0.423650]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 32/173 [loss: 0.745881, acc.: 50.00%] [G loss: 0.420899]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 33/173 [loss: 0.747848, acc.: 50.00%] [G loss: 0.419582]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 34/173 [loss: 0.749477, acc.: 50.00%] [G loss: 0.417089]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 35/173 [loss: 0.748921, acc.: 50.00%] [G loss: 0.418625]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 36/173 [loss: 0.747765, acc.: 50.00%] [G loss: 0.421419]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 37/173 [loss: 0.748511, acc.: 50.00%] [G loss: 0.419914]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 38/173 [loss: 0.746010, acc.: 50.00%] [G loss: 0.418822]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 39/173 [loss: 0.749282, acc.: 50.00%] [G loss: 0.418690]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 40/173 [loss: 0.748982, acc.: 50.00%] [G loss: 0.418598]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 41/173 [loss: 0.749471, acc.: 50.00%] [G loss: 0.417784]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 42/173 [loss: 0.750192, acc.: 50.00%] [G loss: 0.419082]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 43/173 [loss: 0.748142, acc.: 50.00%] [G loss: 0.418724]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 44/173 [loss: 0.748535, acc.: 50.00%] [G loss: 0.418930]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 45/173 [loss: 0.747713, acc.: 50.00%] [G loss: 0.421483]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 46/173 [loss: 0.746962, acc.: 50.00%] [G loss: 0.419165]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 47/173 [loss: 0.744880, acc.: 50.00%] [G loss: 0.421630]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 48/173 [loss: 0.747947, acc.: 50.00%] [G loss: 0.421361]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 49/173 [loss: 0.745203, acc.: 50.00%] [G loss: 0.422737]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 50/173 [loss: 0.743257, acc.: 50.00%] [G loss: 0.419426]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 51/173 [loss: 0.746293, acc.: 50.00%] [G loss: 0.423274]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 52/173 [loss: 0.747189, acc.: 50.00%] [G loss: 0.422469]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 53/173 [loss: 0.745506, acc.: 50.00%] [G loss: 0.421336]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 54/173 [loss: 0.746211, acc.: 50.00%] [G loss: 0.420381]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 55/173 [loss: 0.748068, acc.: 50.00%] [G loss: 0.419162]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 56/173 [loss: 0.750033, acc.: 50.00%] [G loss: 0.419480]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 57/173 [loss: 0.746452, acc.: 50.00%] [G loss: 0.419621]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 58/173 [loss: 0.748440, acc.: 50.00%] [G loss: 0.417344]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 59/173 [loss: 0.748358, acc.: 50.00%] [G loss: 0.418783]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 60/173 [loss: 0.750877, acc.: 50.00%] [G loss: 0.416321]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 61/173 [loss: 0.749037, acc.: 50.00%] [G loss: 0.417983]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 62/173 [loss: 0.749931, acc.: 50.00%] [G loss: 0.418316]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 63/173 [loss: 0.749900, acc.: 50.00%] [G loss: 0.417467]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 64/173 [loss: 0.748726, acc.: 50.00%] [G loss: 0.422171]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 65/173 [loss: 0.748633, acc.: 50.00%] [G loss: 0.418253]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 66/173 [loss: 0.748164, acc.: 50.00%] [G loss: 0.420386]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 67/173 [loss: 0.747484, acc.: 50.00%] [G loss: 0.420061]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 68/173 [loss: 0.745018, acc.: 50.00%] [G loss: 0.422268]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 69/173 [loss: 0.747181, acc.: 50.00%] [G loss: 0.419069]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 70/173 [loss: 0.746364, acc.: 50.00%] [G loss: 0.421445]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 71/173 [loss: 0.744290, acc.: 50.00%] [G loss: 0.421718]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 72/173 [loss: 0.744770, acc.: 50.00%] [G loss: 0.421100]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 73/173 [loss: 0.746526, acc.: 50.00%] [G loss: 0.422327]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 74/173 [loss: 0.747095, acc.: 50.00%] [G loss: 0.421819]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 75/173 [loss: 0.745356, acc.: 50.00%] [G loss: 0.421032]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 76/173 [loss: 0.747892, acc.: 50.00%] [G loss: 0.421608]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 77/173 [loss: 0.748089, acc.: 50.00%] [G loss: 0.420165]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 78/173 [loss: 0.748019, acc.: 50.00%] [G loss: 0.419588]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 79/173 [loss: 0.749520, acc.: 50.00%] [G loss: 0.417871]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 80/173 [loss: 0.750288, acc.: 50.00%] [G loss: 0.418135]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 81/173 [loss: 0.747093, acc.: 50.00%] [G loss: 0.417918]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 82/173 [loss: 0.749140, acc.: 50.00%] [G loss: 0.418147]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 83/173 [loss: 0.747787, acc.: 50.00%] [G loss: 0.418073]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 84/173 [loss: 0.748783, acc.: 50.00%] [G loss: 0.418904]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 85/173 [loss: 0.747829, acc.: 50.00%] [G loss: 0.421508]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 86/173 [loss: 0.746617, acc.: 50.00%] [G loss: 0.420685]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 87/173 [loss: 0.746690, acc.: 50.00%] [G loss: 0.421377]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 88/173 [loss: 0.745918, acc.: 50.00%] [G loss: 0.423477]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 89/173 [loss: 0.747989, acc.: 50.00%] [G loss: 0.421255]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 90/173 [loss: 0.747701, acc.: 50.00%] [G loss: 0.422370]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 91/173 [loss: 0.744796, acc.: 50.00%] [G loss: 0.422228]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 92/173 [loss: 0.746415, acc.: 50.00%] [G loss: 0.420682]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 93/173 [loss: 0.746148, acc.: 50.00%] [G loss: 0.420426]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 94/173 [loss: 0.746605, acc.: 50.00%] [G loss: 0.418851]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 95/173 [loss: 0.744625, acc.: 50.00%] [G loss: 0.420798]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 96/173 [loss: 0.746786, acc.: 50.00%] [G loss: 0.422260]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 97/173 [loss: 0.746663, acc.: 50.00%] [G loss: 0.421227]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 98/173 [loss: 0.746894, acc.: 50.00%] [G loss: 0.422629]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 99/173 [loss: 0.747381, acc.: 50.00%] [G loss: 0.421451]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 100/173 [loss: 0.745444, acc.: 50.00%] [G loss: 0.422637]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 101/173 [loss: 0.750114, acc.: 50.00%] [G loss: 0.418993]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 102/173 [loss: 0.750615, acc.: 50.00%] [G loss: 0.419245]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 103/173 [loss: 0.747812, acc.: 50.00%] [G loss: 0.419110]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 104/173 [loss: 0.748511, acc.: 50.00%] [G loss: 0.420108]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 105/173 [loss: 0.749706, acc.: 50.00%] [G loss: 0.418084]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 106/173 [loss: 0.749535, acc.: 50.00%] [G loss: 0.419568]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 107/173 [loss: 0.748366, acc.: 50.00%] [G loss: 0.418789]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 108/173 [loss: 0.749129, acc.: 50.00%] [G loss: 0.420740]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 109/173 [loss: 0.748343, acc.: 50.00%] [G loss: 0.423299]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 110/173 [loss: 0.746274, acc.: 50.00%] [G loss: 0.425073]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 111/173 [loss: 0.744362, acc.: 50.00%] [G loss: 0.424061]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 112/173 [loss: 0.745667, acc.: 50.00%] [G loss: 0.421777]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 113/173 [loss: 0.748473, acc.: 50.00%] [G loss: 0.423071]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 114/173 [loss: 0.745947, acc.: 50.00%] [G loss: 0.421657]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 115/173 [loss: 0.745782, acc.: 50.00%] [G loss: 0.423119]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 116/173 [loss: 0.746800, acc.: 50.00%] [G loss: 0.421865]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 117/173 [loss: 0.744514, acc.: 50.00%] [G loss: 0.420228]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 118/173 [loss: 0.748600, acc.: 50.00%] [G loss: 0.420859]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 119/173 [loss: 0.747875, acc.: 50.00%] [G loss: 0.420739]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 120/173 [loss: 0.747857, acc.: 50.00%] [G loss: 0.419604]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 121/173 [loss: 0.748160, acc.: 50.00%] [G loss: 0.420754]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 122/173 [loss: 0.746534, acc.: 50.00%] [G loss: 0.418329]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 123/173 [loss: 0.747181, acc.: 50.00%] [G loss: 0.419338]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 124/173 [loss: 0.748775, acc.: 50.00%] [G loss: 0.422831]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 125/173 [loss: 0.749075, acc.: 50.00%] [G loss: 0.420842]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 126/173 [loss: 0.746651, acc.: 50.00%] [G loss: 0.420323]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 127/173 [loss: 0.749423, acc.: 50.00%] [G loss: 0.419944]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 128/173 [loss: 0.746304, acc.: 50.00%] [G loss: 0.418031]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 129/173 [loss: 0.747469, acc.: 50.00%] [G loss: 0.422000]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 130/173 [loss: 0.746298, acc.: 50.00%] [G loss: 0.421305]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 131/173 [loss: 0.748120, acc.: 50.00%] [G loss: 0.419012]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 132/173 [loss: 0.748402, acc.: 50.00%] [G loss: 0.419393]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 133/173 [loss: 0.747218, acc.: 50.00%] [G loss: 0.422631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 134/173 [loss: 0.745151, acc.: 50.00%] [G loss: 0.423150]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 135/173 [loss: 0.747665, acc.: 50.00%] [G loss: 0.424476]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 136/173 [loss: 0.745506, acc.: 50.00%] [G loss: 0.421657]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 137/173 [loss: 0.745904, acc.: 50.00%] [G loss: 0.422393]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 138/173 [loss: 0.745943, acc.: 50.00%] [G loss: 0.419720]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 139/173 [loss: 0.748424, acc.: 50.00%] [G loss: 0.421492]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 140/173 [loss: 0.745510, acc.: 50.00%] [G loss: 0.419889]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 141/173 [loss: 0.748081, acc.: 50.00%] [G loss: 0.419645]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 142/173 [loss: 0.749848, acc.: 50.00%] [G loss: 0.418412]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 143/173 [loss: 0.749010, acc.: 50.00%] [G loss: 0.417371]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 144/173 [loss: 0.749617, acc.: 50.00%] [G loss: 0.418476]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 145/173 [loss: 0.749492, acc.: 50.00%] [G loss: 0.419054]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 146/173 [loss: 0.749035, acc.: 50.00%] [G loss: 0.417061]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 147/173 [loss: 0.749147, acc.: 50.00%] [G loss: 0.421081]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 148/173 [loss: 0.750846, acc.: 50.00%] [G loss: 0.424414]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 149/173 [loss: 0.749571, acc.: 50.00%] [G loss: 0.421338]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 150/173 [loss: 0.749264, acc.: 50.00%] [G loss: 0.421892]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 151/173 [loss: 0.746660, acc.: 50.00%] [G loss: 0.421552]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 152/173 [loss: 0.746762, acc.: 50.00%] [G loss: 0.422843]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 153/173 [loss: 0.748540, acc.: 50.00%] [G loss: 0.421189]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 154/173 [loss: 0.745136, acc.: 50.00%] [G loss: 0.423410]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 155/173 [loss: 0.749086, acc.: 50.00%] [G loss: 0.420597]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 156/173 [loss: 0.749087, acc.: 50.00%] [G loss: 0.420141]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 157/173 [loss: 0.748637, acc.: 50.00%] [G loss: 0.421809]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 158/173 [loss: 0.748580, acc.: 50.00%] [G loss: 0.418065]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 159/173 [loss: 0.747987, acc.: 50.00%] [G loss: 0.421085]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 160/173 [loss: 0.747565, acc.: 50.00%] [G loss: 0.419304]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 161/173 [loss: 0.749746, acc.: 50.00%] [G loss: 0.420841]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 162/173 [loss: 0.746766, acc.: 50.00%] [G loss: 0.419963]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 163/173 [loss: 0.749844, acc.: 50.00%] [G loss: 0.417850]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 164/173 [loss: 0.748987, acc.: 50.00%] [G loss: 0.418599]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 165/173 [loss: 0.749629, acc.: 50.00%] [G loss: 0.420858]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 166/173 [loss: 0.749092, acc.: 50.00%] [G loss: 0.420291]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 167/173 [loss: 0.748013, acc.: 50.00%] [G loss: 0.419728]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 168/173 [loss: 0.747601, acc.: 50.00%] [G loss: 0.421481]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 169/173 [loss: 0.749012, acc.: 50.00%] [G loss: 0.419839]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 170/173 [loss: 0.749380, acc.: 50.00%] [G loss: 0.419970]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 171/173 [loss: 0.748292, acc.: 50.00%] [G loss: 0.419961]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 11/200  Batch Size: 172/173 [loss: 0.746373, acc.: 50.00%] [G loss: 0.419917]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 0/173 [loss: 0.749231, acc.: 50.00%] [G loss: 0.417877]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 1/173 [loss: 0.747765, acc.: 50.00%] [G loss: 0.419134]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 2/173 [loss: 0.746860, acc.: 50.00%] [G loss: 0.420073]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 3/173 [loss: 0.746783, acc.: 50.00%] [G loss: 0.420781]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 4/173 [loss: 0.747238, acc.: 50.00%] [G loss: 0.420166]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 5/173 [loss: 0.746257, acc.: 50.00%] [G loss: 0.417682]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 6/173 [loss: 0.748947, acc.: 50.00%] [G loss: 0.419463]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 7/173 [loss: 0.748973, acc.: 50.00%] [G loss: 0.418842]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 8/173 [loss: 0.750610, acc.: 50.00%] [G loss: 0.417695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 9/173 [loss: 0.747843, acc.: 50.00%] [G loss: 0.419182]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 10/173 [loss: 0.750622, acc.: 50.00%] [G loss: 0.417875]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 11/173 [loss: 0.748016, acc.: 50.00%] [G loss: 0.419471]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 12/173 [loss: 0.751853, acc.: 50.00%] [G loss: 0.420094]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 13/173 [loss: 0.748565, acc.: 50.00%] [G loss: 0.418591]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 14/173 [loss: 0.748113, acc.: 50.00%] [G loss: 0.419154]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 15/173 [loss: 0.748489, acc.: 50.00%] [G loss: 0.419551]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 16/173 [loss: 0.748633, acc.: 50.00%] [G loss: 0.420805]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 17/173 [loss: 0.747701, acc.: 50.00%] [G loss: 0.419115]\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Epoch: 12/200  Batch Size: 18/173 [loss: 0.747505, acc.: 50.00%] [G loss: 0.420737]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 19/173 [loss: 0.746911, acc.: 50.00%] [G loss: 0.420087]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 20/173 [loss: 0.746965, acc.: 50.00%] [G loss: 0.420873]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 21/173 [loss: 0.748291, acc.: 50.00%] [G loss: 0.420203]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 22/173 [loss: 0.747054, acc.: 50.00%] [G loss: 0.421232]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 23/173 [loss: 0.748858, acc.: 50.00%] [G loss: 0.421619]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 24/173 [loss: 0.747609, acc.: 50.00%] [G loss: 0.422641]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 25/173 [loss: 0.747241, acc.: 50.00%] [G loss: 0.419933]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 26/173 [loss: 0.746836, acc.: 50.00%] [G loss: 0.419420]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 27/173 [loss: 0.747283, acc.: 50.00%] [G loss: 0.420336]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 28/173 [loss: 0.748851, acc.: 50.00%] [G loss: 0.417736]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 29/173 [loss: 0.749187, acc.: 50.00%] [G loss: 0.418313]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 30/173 [loss: 0.751047, acc.: 50.00%] [G loss: 0.419490]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 31/173 [loss: 0.751804, acc.: 50.00%] [G loss: 0.419900]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 32/173 [loss: 0.749878, acc.: 50.00%] [G loss: 0.417810]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 33/173 [loss: 0.751257, acc.: 50.00%] [G loss: 0.418726]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 34/173 [loss: 0.749238, acc.: 50.00%] [G loss: 0.420045]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 35/173 [loss: 0.749224, acc.: 50.00%] [G loss: 0.419087]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 36/173 [loss: 0.746890, acc.: 50.00%] [G loss: 0.419778]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 37/173 [loss: 0.747296, acc.: 50.00%] [G loss: 0.418918]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 38/173 [loss: 0.747147, acc.: 50.00%] [G loss: 0.419016]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 39/173 [loss: 0.748291, acc.: 50.00%] [G loss: 0.420456]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 40/173 [loss: 0.749278, acc.: 50.00%] [G loss: 0.419230]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 41/173 [loss: 0.748653, acc.: 50.00%] [G loss: 0.419209]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 42/173 [loss: 0.749139, acc.: 50.00%] [G loss: 0.420535]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 43/173 [loss: 0.749147, acc.: 50.00%] [G loss: 0.418244]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 44/173 [loss: 0.749076, acc.: 50.00%] [G loss: 0.419497]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 45/173 [loss: 0.747377, acc.: 50.00%] [G loss: 0.422165]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 46/173 [loss: 0.746761, acc.: 50.00%] [G loss: 0.421934]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 47/173 [loss: 0.748200, acc.: 50.00%] [G loss: 0.418062]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 48/173 [loss: 0.748259, acc.: 50.00%] [G loss: 0.421936]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 49/173 [loss: 0.747674, acc.: 50.00%] [G loss: 0.421013]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 50/173 [loss: 0.748451, acc.: 50.00%] [G loss: 0.418945]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 51/173 [loss: 0.747937, acc.: 50.00%] [G loss: 0.419686]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 52/173 [loss: 0.748822, acc.: 50.00%] [G loss: 0.421549]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 53/173 [loss: 0.749240, acc.: 50.00%] [G loss: 0.421933]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 54/173 [loss: 0.748009, acc.: 50.00%] [G loss: 0.420601]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 55/173 [loss: 0.747646, acc.: 50.00%] [G loss: 0.420874]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 56/173 [loss: 0.747340, acc.: 50.00%] [G loss: 0.422213]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 57/173 [loss: 0.747546, acc.: 50.00%] [G loss: 0.421225]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 58/173 [loss: 0.747865, acc.: 50.00%] [G loss: 0.422508]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 59/173 [loss: 0.748268, acc.: 50.00%] [G loss: 0.422035]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 60/173 [loss: 0.746402, acc.: 50.00%] [G loss: 0.423714]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 61/173 [loss: 0.746391, acc.: 50.00%] [G loss: 0.420747]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 62/173 [loss: 0.748804, acc.: 50.00%] [G loss: 0.419680]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 63/173 [loss: 0.747382, acc.: 50.00%] [G loss: 0.420739]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 64/173 [loss: 0.746956, acc.: 50.00%] [G loss: 0.421129]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 65/173 [loss: 0.747767, acc.: 50.00%] [G loss: 0.420912]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 66/173 [loss: 0.747429, acc.: 50.00%] [G loss: 0.421502]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 67/173 [loss: 0.747742, acc.: 50.00%] [G loss: 0.421389]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 68/173 [loss: 0.746920, acc.: 50.00%] [G loss: 0.421162]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 69/173 [loss: 0.746897, acc.: 50.00%] [G loss: 0.421506]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 70/173 [loss: 0.747863, acc.: 50.00%] [G loss: 0.422304]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 71/173 [loss: 0.745723, acc.: 50.00%] [G loss: 0.420535]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 72/173 [loss: 0.747553, acc.: 50.00%] [G loss: 0.418718]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 73/173 [loss: 0.750138, acc.: 50.00%] [G loss: 0.421250]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 74/173 [loss: 0.747471, acc.: 50.00%] [G loss: 0.420935]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 75/173 [loss: 0.749045, acc.: 50.00%] [G loss: 0.419425]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 76/173 [loss: 0.749567, acc.: 50.00%] [G loss: 0.419684]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 77/173 [loss: 0.749301, acc.: 50.00%] [G loss: 0.420882]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 78/173 [loss: 0.748990, acc.: 50.00%] [G loss: 0.418631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 79/173 [loss: 0.747431, acc.: 50.00%] [G loss: 0.421386]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 80/173 [loss: 0.747813, acc.: 50.00%] [G loss: 0.421508]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 81/173 [loss: 0.748615, acc.: 50.00%] [G loss: 0.419432]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 82/173 [loss: 0.749833, acc.: 50.00%] [G loss: 0.418730]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 83/173 [loss: 0.746423, acc.: 50.00%] [G loss: 0.420884]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 84/173 [loss: 0.745579, acc.: 50.00%] [G loss: 0.418867]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 85/173 [loss: 0.749457, acc.: 50.00%] [G loss: 0.418039]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 86/173 [loss: 0.748222, acc.: 50.00%] [G loss: 0.420028]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 87/173 [loss: 0.751477, acc.: 50.00%] [G loss: 0.421146]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 88/173 [loss: 0.746421, acc.: 50.00%] [G loss: 0.420907]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 89/173 [loss: 0.747226, acc.: 50.00%] [G loss: 0.422126]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 90/173 [loss: 0.747983, acc.: 50.00%] [G loss: 0.419171]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 91/173 [loss: 0.746888, acc.: 50.00%] [G loss: 0.419042]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 92/173 [loss: 0.747818, acc.: 50.00%] [G loss: 0.418503]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 93/173 [loss: 0.748130, acc.: 50.00%] [G loss: 0.417906]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 94/173 [loss: 0.747626, acc.: 50.00%] [G loss: 0.420179]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 95/173 [loss: 0.745462, acc.: 50.00%] [G loss: 0.421260]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 96/173 [loss: 0.748148, acc.: 50.00%] [G loss: 0.421822]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 97/173 [loss: 0.748067, acc.: 50.00%] [G loss: 0.419322]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 98/173 [loss: 0.746518, acc.: 50.00%] [G loss: 0.420841]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 99/173 [loss: 0.746855, acc.: 50.00%] [G loss: 0.420356]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 100/173 [loss: 0.748224, acc.: 50.00%] [G loss: 0.418836]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 101/173 [loss: 0.750516, acc.: 50.00%] [G loss: 0.417855]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 102/173 [loss: 0.748160, acc.: 50.00%] [G loss: 0.417460]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 103/173 [loss: 0.747618, acc.: 50.00%] [G loss: 0.419568]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 104/173 [loss: 0.748228, acc.: 50.00%] [G loss: 0.419112]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 105/173 [loss: 0.749347, acc.: 50.00%] [G loss: 0.418925]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 106/173 [loss: 0.748179, acc.: 50.00%] [G loss: 0.420144]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 107/173 [loss: 0.748597, acc.: 50.00%] [G loss: 0.419637]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 108/173 [loss: 0.749125, acc.: 50.00%] [G loss: 0.419278]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 109/173 [loss: 0.747362, acc.: 50.00%] [G loss: 0.419121]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 110/173 [loss: 0.747853, acc.: 50.00%] [G loss: 0.419403]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 111/173 [loss: 0.749644, acc.: 50.00%] [G loss: 0.419312]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 112/173 [loss: 0.749148, acc.: 50.00%] [G loss: 0.419804]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 113/173 [loss: 0.748788, acc.: 50.00%] [G loss: 0.420061]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 114/173 [loss: 0.749848, acc.: 50.00%] [G loss: 0.421731]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 115/173 [loss: 0.747477, acc.: 50.00%] [G loss: 0.419796]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 116/173 [loss: 0.748077, acc.: 50.00%] [G loss: 0.421796]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 117/173 [loss: 0.748890, acc.: 50.00%] [G loss: 0.420326]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 118/173 [loss: 0.749334, acc.: 50.00%] [G loss: 0.420022]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 119/173 [loss: 0.748935, acc.: 50.00%] [G loss: 0.419392]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 120/173 [loss: 0.747795, acc.: 50.00%] [G loss: 0.421822]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 121/173 [loss: 0.748780, acc.: 50.00%] [G loss: 0.420379]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 122/173 [loss: 0.748946, acc.: 50.00%] [G loss: 0.419859]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 123/173 [loss: 0.746757, acc.: 50.00%] [G loss: 0.420693]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 124/173 [loss: 0.747710, acc.: 50.00%] [G loss: 0.421054]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 125/173 [loss: 0.746523, acc.: 50.00%] [G loss: 0.419948]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 126/173 [loss: 0.747095, acc.: 50.00%] [G loss: 0.421365]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 127/173 [loss: 0.747328, acc.: 50.00%] [G loss: 0.420345]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 128/173 [loss: 0.747139, acc.: 50.00%] [G loss: 0.419306]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 129/173 [loss: 0.747570, acc.: 50.00%] [G loss: 0.419751]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 130/173 [loss: 0.748583, acc.: 50.00%] [G loss: 0.419527]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 131/173 [loss: 0.747370, acc.: 50.00%] [G loss: 0.421236]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 132/173 [loss: 0.747355, acc.: 50.00%] [G loss: 0.420176]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 133/173 [loss: 0.745996, acc.: 50.00%] [G loss: 0.420239]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 134/173 [loss: 0.746768, acc.: 50.00%] [G loss: 0.419688]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 135/173 [loss: 0.750285, acc.: 50.00%] [G loss: 0.418989]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 136/173 [loss: 0.748159, acc.: 50.00%] [G loss: 0.419229]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 137/173 [loss: 0.746666, acc.: 50.00%] [G loss: 0.418563]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 138/173 [loss: 0.749192, acc.: 50.00%] [G loss: 0.419738]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 139/173 [loss: 0.749166, acc.: 50.00%] [G loss: 0.420557]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 140/173 [loss: 0.748656, acc.: 50.00%] [G loss: 0.419438]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 141/173 [loss: 0.750506, acc.: 50.00%] [G loss: 0.418921]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 142/173 [loss: 0.747967, acc.: 50.00%] [G loss: 0.420603]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 143/173 [loss: 0.748467, acc.: 50.00%] [G loss: 0.418517]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 144/173 [loss: 0.746620, acc.: 50.00%] [G loss: 0.420125]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 145/173 [loss: 0.750477, acc.: 50.00%] [G loss: 0.420471]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 146/173 [loss: 0.749141, acc.: 50.00%] [G loss: 0.417091]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 147/173 [loss: 0.750388, acc.: 50.00%] [G loss: 0.421057]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 148/173 [loss: 0.748517, acc.: 50.00%] [G loss: 0.418320]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 149/173 [loss: 0.749939, acc.: 50.00%] [G loss: 0.417919]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 150/173 [loss: 0.745794, acc.: 50.00%] [G loss: 0.420726]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 151/173 [loss: 0.746965, acc.: 50.00%] [G loss: 0.418948]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 152/173 [loss: 0.750237, acc.: 50.00%] [G loss: 0.420219]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 153/173 [loss: 0.748751, acc.: 50.00%] [G loss: 0.419691]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 154/173 [loss: 0.747083, acc.: 50.00%] [G loss: 0.421329]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 155/173 [loss: 0.748150, acc.: 50.00%] [G loss: 0.419713]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 156/173 [loss: 0.747792, acc.: 50.00%] [G loss: 0.420249]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 157/173 [loss: 0.748890, acc.: 50.00%] [G loss: 0.418429]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 158/173 [loss: 0.746929, acc.: 50.00%] [G loss: 0.419576]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 159/173 [loss: 0.748121, acc.: 50.00%] [G loss: 0.421058]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 160/173 [loss: 0.746745, acc.: 50.00%] [G loss: 0.420358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 161/173 [loss: 0.748318, acc.: 50.00%] [G loss: 0.420119]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 162/173 [loss: 0.747579, acc.: 50.00%] [G loss: 0.422648]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 163/173 [loss: 0.746188, acc.: 50.00%] [G loss: 0.421319]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 164/173 [loss: 0.748099, acc.: 50.00%] [G loss: 0.421005]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 165/173 [loss: 0.748790, acc.: 50.00%] [G loss: 0.419571]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 166/173 [loss: 0.748872, acc.: 50.00%] [G loss: 0.421036]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 167/173 [loss: 0.750480, acc.: 50.00%] [G loss: 0.420385]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 168/173 [loss: 0.747958, acc.: 50.00%] [G loss: 0.420546]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 169/173 [loss: 0.748628, acc.: 50.00%] [G loss: 0.417480]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 170/173 [loss: 0.748531, acc.: 50.00%] [G loss: 0.418459]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 171/173 [loss: 0.750415, acc.: 50.00%] [G loss: 0.420360]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 12/200  Batch Size: 172/173 [loss: 0.745301, acc.: 50.00%] [G loss: 0.419734]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 0/173 [loss: 0.749654, acc.: 50.00%] [G loss: 0.421286]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 1/173 [loss: 0.748257, acc.: 50.00%] [G loss: 0.419556]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 2/173 [loss: 0.747686, acc.: 50.00%] [G loss: 0.419131]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 3/173 [loss: 0.750336, acc.: 50.00%] [G loss: 0.422178]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 4/173 [loss: 0.748690, acc.: 50.00%] [G loss: 0.418633]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 5/173 [loss: 0.747266, acc.: 50.00%] [G loss: 0.420007]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 6/173 [loss: 0.749303, acc.: 50.00%] [G loss: 0.419020]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 7/173 [loss: 0.749162, acc.: 50.00%] [G loss: 0.417333]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 8/173 [loss: 0.749823, acc.: 50.00%] [G loss: 0.418975]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 9/173 [loss: 0.750769, acc.: 50.00%] [G loss: 0.419731]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 10/173 [loss: 0.750077, acc.: 50.00%] [G loss: 0.419919]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 11/173 [loss: 0.748543, acc.: 50.00%] [G loss: 0.419983]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 12/173 [loss: 0.750268, acc.: 50.00%] [G loss: 0.421920]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 13/173 [loss: 0.747232, acc.: 50.00%] [G loss: 0.417986]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 14/173 [loss: 0.747733, acc.: 50.00%] [G loss: 0.419408]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 15/173 [loss: 0.747868, acc.: 50.00%] [G loss: 0.417973]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 16/173 [loss: 0.747354, acc.: 50.00%] [G loss: 0.419261]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 17/173 [loss: 0.747712, acc.: 50.00%] [G loss: 0.419151]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 18/173 [loss: 0.748740, acc.: 50.00%] [G loss: 0.418955]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 19/173 [loss: 0.750555, acc.: 50.00%] [G loss: 0.420479]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 20/173 [loss: 0.748763, acc.: 50.00%] [G loss: 0.420251]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 21/173 [loss: 0.747594, acc.: 50.00%] [G loss: 0.419431]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 22/173 [loss: 0.751122, acc.: 50.00%] [G loss: 0.417586]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 23/173 [loss: 0.749142, acc.: 50.00%] [G loss: 0.420033]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 24/173 [loss: 0.750527, acc.: 50.00%] [G loss: 0.420099]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 25/173 [loss: 0.748654, acc.: 50.00%] [G loss: 0.418433]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 26/173 [loss: 0.747963, acc.: 50.00%] [G loss: 0.419293]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 27/173 [loss: 0.750808, acc.: 50.00%] [G loss: 0.420002]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 28/173 [loss: 0.747687, acc.: 50.00%] [G loss: 0.420690]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 29/173 [loss: 0.749138, acc.: 50.00%] [G loss: 0.420009]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 30/173 [loss: 0.747077, acc.: 50.00%] [G loss: 0.421265]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 31/173 [loss: 0.749469, acc.: 50.00%] [G loss: 0.420125]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 32/173 [loss: 0.747999, acc.: 50.00%] [G loss: 0.419153]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 33/173 [loss: 0.747374, acc.: 50.00%] [G loss: 0.420135]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 34/173 [loss: 0.750547, acc.: 50.00%] [G loss: 0.421455]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 35/173 [loss: 0.748150, acc.: 50.00%] [G loss: 0.418235]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 36/173 [loss: 0.748572, acc.: 50.00%] [G loss: 0.419896]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 37/173 [loss: 0.749610, acc.: 50.00%] [G loss: 0.421497]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 38/173 [loss: 0.747381, acc.: 50.00%] [G loss: 0.420012]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 39/173 [loss: 0.750468, acc.: 50.00%] [G loss: 0.421520]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 40/173 [loss: 0.748053, acc.: 50.00%] [G loss: 0.419620]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 41/173 [loss: 0.748711, acc.: 50.00%] [G loss: 0.419661]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 42/173 [loss: 0.749674, acc.: 50.00%] [G loss: 0.420731]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 43/173 [loss: 0.748142, acc.: 50.00%] [G loss: 0.421092]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 44/173 [loss: 0.749448, acc.: 50.00%] [G loss: 0.421209]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 45/173 [loss: 0.747581, acc.: 50.00%] [G loss: 0.420135]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 46/173 [loss: 0.748111, acc.: 50.00%] [G loss: 0.417063]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 47/173 [loss: 0.748473, acc.: 50.00%] [G loss: 0.419935]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 48/173 [loss: 0.749433, acc.: 50.00%] [G loss: 0.420051]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 49/173 [loss: 0.748813, acc.: 50.00%] [G loss: 0.420583]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 50/173 [loss: 0.749252, acc.: 50.00%] [G loss: 0.420216]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 51/173 [loss: 0.747207, acc.: 50.00%] [G loss: 0.419580]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 52/173 [loss: 0.748822, acc.: 50.00%] [G loss: 0.421590]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 53/173 [loss: 0.749066, acc.: 50.00%] [G loss: 0.417472]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 54/173 [loss: 0.751161, acc.: 50.00%] [G loss: 0.418948]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 55/173 [loss: 0.751231, acc.: 50.00%] [G loss: 0.418011]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 56/173 [loss: 0.748749, acc.: 50.00%] [G loss: 0.418001]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 57/173 [loss: 0.751786, acc.: 50.00%] [G loss: 0.418358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 58/173 [loss: 0.749045, acc.: 50.00%] [G loss: 0.417957]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 59/173 [loss: 0.747258, acc.: 50.00%] [G loss: 0.418985]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 60/173 [loss: 0.750746, acc.: 50.00%] [G loss: 0.416605]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 61/173 [loss: 0.751049, acc.: 50.00%] [G loss: 0.418998]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 62/173 [loss: 0.749555, acc.: 50.00%] [G loss: 0.418684]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 63/173 [loss: 0.749783, acc.: 50.00%] [G loss: 0.417723]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 64/173 [loss: 0.748555, acc.: 50.00%] [G loss: 0.418649]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 65/173 [loss: 0.748871, acc.: 50.00%] [G loss: 0.418704]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 66/173 [loss: 0.748931, acc.: 50.00%] [G loss: 0.419956]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 67/173 [loss: 0.751422, acc.: 50.00%] [G loss: 0.420729]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 68/173 [loss: 0.750884, acc.: 50.00%] [G loss: 0.417480]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 69/173 [loss: 0.747107, acc.: 50.00%] [G loss: 0.420825]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 70/173 [loss: 0.750970, acc.: 50.00%] [G loss: 0.417507]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 71/173 [loss: 0.750831, acc.: 50.00%] [G loss: 0.419505]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 72/173 [loss: 0.748897, acc.: 50.00%] [G loss: 0.418926]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 73/173 [loss: 0.748800, acc.: 50.00%] [G loss: 0.418941]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 74/173 [loss: 0.747804, acc.: 50.00%] [G loss: 0.421695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 75/173 [loss: 0.748902, acc.: 50.00%] [G loss: 0.421536]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 76/173 [loss: 0.747080, acc.: 50.00%] [G loss: 0.421510]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 77/173 [loss: 0.748595, acc.: 50.00%] [G loss: 0.421732]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 78/173 [loss: 0.749116, acc.: 50.00%] [G loss: 0.419850]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 79/173 [loss: 0.748456, acc.: 50.00%] [G loss: 0.419806]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 80/173 [loss: 0.749235, acc.: 50.00%] [G loss: 0.421240]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 81/173 [loss: 0.747902, acc.: 50.00%] [G loss: 0.420703]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 82/173 [loss: 0.748757, acc.: 50.00%] [G loss: 0.420245]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 83/173 [loss: 0.748362, acc.: 50.00%] [G loss: 0.421159]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 84/173 [loss: 0.747287, acc.: 50.00%] [G loss: 0.421543]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 85/173 [loss: 0.748176, acc.: 50.00%] [G loss: 0.421287]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 86/173 [loss: 0.749086, acc.: 50.00%] [G loss: 0.419791]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 87/173 [loss: 0.747376, acc.: 50.00%] [G loss: 0.420883]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 88/173 [loss: 0.747737, acc.: 50.00%] [G loss: 0.421517]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 89/173 [loss: 0.746478, acc.: 50.00%] [G loss: 0.421245]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 90/173 [loss: 0.746227, acc.: 50.00%] [G loss: 0.420166]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 91/173 [loss: 0.747689, acc.: 50.00%] [G loss: 0.418271]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 92/173 [loss: 0.745062, acc.: 50.00%] [G loss: 0.418719]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 93/173 [loss: 0.749040, acc.: 50.00%] [G loss: 0.419027]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 94/173 [loss: 0.747579, acc.: 50.00%] [G loss: 0.418615]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 95/173 [loss: 0.749030, acc.: 50.00%] [G loss: 0.419120]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 96/173 [loss: 0.747417, acc.: 50.00%] [G loss: 0.419092]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 97/173 [loss: 0.751391, acc.: 50.00%] [G loss: 0.418819]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 98/173 [loss: 0.749423, acc.: 50.00%] [G loss: 0.419864]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 99/173 [loss: 0.748899, acc.: 50.00%] [G loss: 0.419889]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 100/173 [loss: 0.750679, acc.: 50.00%] [G loss: 0.417844]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 101/173 [loss: 0.750490, acc.: 50.00%] [G loss: 0.418786]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 102/173 [loss: 0.748735, acc.: 50.00%] [G loss: 0.420372]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 103/173 [loss: 0.750350, acc.: 50.00%] [G loss: 0.420351]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 104/173 [loss: 0.749374, acc.: 50.00%] [G loss: 0.420477]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 105/173 [loss: 0.748597, acc.: 50.00%] [G loss: 0.418766]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 106/173 [loss: 0.747870, acc.: 50.00%] [G loss: 0.419829]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 107/173 [loss: 0.747639, acc.: 50.00%] [G loss: 0.419601]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 108/173 [loss: 0.748292, acc.: 50.00%] [G loss: 0.422926]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 109/173 [loss: 0.746396, acc.: 50.00%] [G loss: 0.419592]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 110/173 [loss: 0.747626, acc.: 50.00%] [G loss: 0.419978]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 111/173 [loss: 0.750433, acc.: 50.00%] [G loss: 0.420459]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 112/173 [loss: 0.749747, acc.: 50.00%] [G loss: 0.419649]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 113/173 [loss: 0.748565, acc.: 50.00%] [G loss: 0.421344]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 114/173 [loss: 0.747811, acc.: 50.00%] [G loss: 0.417511]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 115/173 [loss: 0.748691, acc.: 50.00%] [G loss: 0.421561]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 116/173 [loss: 0.749036, acc.: 50.00%] [G loss: 0.421569]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 117/173 [loss: 0.748994, acc.: 50.00%] [G loss: 0.421717]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 118/173 [loss: 0.748669, acc.: 50.00%] [G loss: 0.419838]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 119/173 [loss: 0.748244, acc.: 50.00%] [G loss: 0.422137]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 120/173 [loss: 0.748448, acc.: 50.00%] [G loss: 0.419724]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 121/173 [loss: 0.747777, acc.: 50.00%] [G loss: 0.418097]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 122/173 [loss: 0.746403, acc.: 50.00%] [G loss: 0.417633]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 123/173 [loss: 0.749010, acc.: 50.00%] [G loss: 0.420674]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 124/173 [loss: 0.747058, acc.: 50.00%] [G loss: 0.419311]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 125/173 [loss: 0.747445, acc.: 50.00%] [G loss: 0.417877]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 126/173 [loss: 0.748382, acc.: 50.00%] [G loss: 0.417938]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 127/173 [loss: 0.749593, acc.: 50.00%] [G loss: 0.419325]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 128/173 [loss: 0.748954, acc.: 50.00%] [G loss: 0.421844]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 129/173 [loss: 0.749889, acc.: 50.00%] [G loss: 0.418875]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 130/173 [loss: 0.749885, acc.: 50.00%] [G loss: 0.419061]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 131/173 [loss: 0.749483, acc.: 50.00%] [G loss: 0.419600]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 132/173 [loss: 0.748149, acc.: 50.00%] [G loss: 0.417448]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 133/173 [loss: 0.747488, acc.: 50.00%] [G loss: 0.417714]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 134/173 [loss: 0.746711, acc.: 50.00%] [G loss: 0.418544]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 135/173 [loss: 0.750962, acc.: 50.00%] [G loss: 0.421509]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 136/173 [loss: 0.748482, acc.: 50.00%] [G loss: 0.420284]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 137/173 [loss: 0.749730, acc.: 50.00%] [G loss: 0.419785]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 138/173 [loss: 0.749095, acc.: 50.00%] [G loss: 0.419142]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 139/173 [loss: 0.749240, acc.: 50.00%] [G loss: 0.418643]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 140/173 [loss: 0.750195, acc.: 50.00%] [G loss: 0.418900]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 141/173 [loss: 0.749095, acc.: 50.00%] [G loss: 0.419042]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 142/173 [loss: 0.749346, acc.: 50.00%] [G loss: 0.418605]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 143/173 [loss: 0.750748, acc.: 50.00%] [G loss: 0.418108]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 144/173 [loss: 0.749132, acc.: 50.00%] [G loss: 0.418566]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 145/173 [loss: 0.748293, acc.: 50.00%] [G loss: 0.421599]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 146/173 [loss: 0.748551, acc.: 50.00%] [G loss: 0.419458]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 147/173 [loss: 0.749874, acc.: 50.00%] [G loss: 0.419137]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 148/173 [loss: 0.749104, acc.: 50.00%] [G loss: 0.418624]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 149/173 [loss: 0.751526, acc.: 50.00%] [G loss: 0.419111]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 150/173 [loss: 0.748990, acc.: 50.00%] [G loss: 0.419276]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 151/173 [loss: 0.747363, acc.: 50.00%] [G loss: 0.420594]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 152/173 [loss: 0.748380, acc.: 50.00%] [G loss: 0.420873]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 153/173 [loss: 0.746206, acc.: 50.00%] [G loss: 0.420653]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 154/173 [loss: 0.748305, acc.: 50.00%] [G loss: 0.418035]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 155/173 [loss: 0.749208, acc.: 50.00%] [G loss: 0.419434]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 156/173 [loss: 0.748538, acc.: 50.00%] [G loss: 0.419681]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 157/173 [loss: 0.747659, acc.: 50.00%] [G loss: 0.419694]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 158/173 [loss: 0.746813, acc.: 50.00%] [G loss: 0.418560]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 159/173 [loss: 0.747328, acc.: 50.00%] [G loss: 0.420277]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 160/173 [loss: 0.749562, acc.: 50.00%] [G loss: 0.419261]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 161/173 [loss: 0.747314, acc.: 50.00%] [G loss: 0.418959]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 162/173 [loss: 0.749285, acc.: 50.00%] [G loss: 0.418488]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 163/173 [loss: 0.747822, acc.: 50.00%] [G loss: 0.419831]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 164/173 [loss: 0.749017, acc.: 50.00%] [G loss: 0.421136]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 165/173 [loss: 0.749847, acc.: 50.00%] [G loss: 0.417779]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 166/173 [loss: 0.749194, acc.: 50.00%] [G loss: 0.419341]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 167/173 [loss: 0.748969, acc.: 50.00%] [G loss: 0.421240]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 168/173 [loss: 0.747809, acc.: 50.00%] [G loss: 0.420530]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 169/173 [loss: 0.749258, acc.: 50.00%] [G loss: 0.420774]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 170/173 [loss: 0.748771, acc.: 50.00%] [G loss: 0.419797]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 171/173 [loss: 0.747379, acc.: 50.00%] [G loss: 0.420419]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 13/200  Batch Size: 172/173 [loss: 0.749518, acc.: 50.00%] [G loss: 0.420138]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 0/173 [loss: 0.747100, acc.: 50.00%] [G loss: 0.418374]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 1/173 [loss: 0.747131, acc.: 50.00%] [G loss: 0.418910]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 2/173 [loss: 0.748975, acc.: 50.00%] [G loss: 0.420451]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 3/173 [loss: 0.747727, acc.: 50.00%] [G loss: 0.420470]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 4/173 [loss: 0.748202, acc.: 50.00%] [G loss: 0.420104]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 5/173 [loss: 0.747418, acc.: 50.00%] [G loss: 0.420832]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 6/173 [loss: 0.746567, acc.: 50.00%] [G loss: 0.420146]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 7/173 [loss: 0.748839, acc.: 50.00%] [G loss: 0.418155]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 8/173 [loss: 0.750249, acc.: 50.00%] [G loss: 0.417205]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 9/173 [loss: 0.747079, acc.: 50.00%] [G loss: 0.420438]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 10/173 [loss: 0.748530, acc.: 50.00%] [G loss: 0.419667]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 11/173 [loss: 0.750005, acc.: 50.00%] [G loss: 0.419159]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 12/173 [loss: 0.750234, acc.: 50.00%] [G loss: 0.417904]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 13/173 [loss: 0.749796, acc.: 50.00%] [G loss: 0.417857]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 14/173 [loss: 0.749524, acc.: 50.00%] [G loss: 0.418872]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 15/173 [loss: 0.750005, acc.: 50.00%] [G loss: 0.420927]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 16/173 [loss: 0.747000, acc.: 50.00%] [G loss: 0.419681]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 17/173 [loss: 0.747814, acc.: 50.00%] [G loss: 0.418427]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 18/173 [loss: 0.750421, acc.: 50.00%] [G loss: 0.418814]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 19/173 [loss: 0.748593, acc.: 50.00%] [G loss: 0.416358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 20/173 [loss: 0.747730, acc.: 50.00%] [G loss: 0.417664]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 21/173 [loss: 0.751351, acc.: 50.00%] [G loss: 0.419133]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 22/173 [loss: 0.748056, acc.: 50.00%] [G loss: 0.418764]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 23/173 [loss: 0.748519, acc.: 50.00%] [G loss: 0.419372]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 24/173 [loss: 0.748962, acc.: 50.00%] [G loss: 0.418595]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 25/173 [loss: 0.749194, acc.: 50.00%] [G loss: 0.419910]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 26/173 [loss: 0.749831, acc.: 50.00%] [G loss: 0.419759]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 27/173 [loss: 0.749658, acc.: 50.00%] [G loss: 0.418531]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 28/173 [loss: 0.749393, acc.: 50.00%] [G loss: 0.417696]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 29/173 [loss: 0.747143, acc.: 50.00%] [G loss: 0.419584]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 30/173 [loss: 0.747860, acc.: 50.00%] [G loss: 0.419070]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 31/173 [loss: 0.748539, acc.: 50.00%] [G loss: 0.418668]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 32/173 [loss: 0.748833, acc.: 50.00%] [G loss: 0.418777]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 33/173 [loss: 0.750307, acc.: 50.00%] [G loss: 0.419981]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 34/173 [loss: 0.746649, acc.: 50.00%] [G loss: 0.422499]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 35/173 [loss: 0.748882, acc.: 50.00%] [G loss: 0.420638]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 36/173 [loss: 0.746920, acc.: 50.00%] [G loss: 0.421122]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 37/173 [loss: 0.750732, acc.: 50.00%] [G loss: 0.419175]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 38/173 [loss: 0.749903, acc.: 50.00%] [G loss: 0.417430]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 39/173 [loss: 0.749050, acc.: 50.00%] [G loss: 0.420910]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 40/173 [loss: 0.749223, acc.: 50.00%] [G loss: 0.419738]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 41/173 [loss: 0.747494, acc.: 50.00%] [G loss: 0.418184]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 42/173 [loss: 0.748316, acc.: 50.00%] [G loss: 0.419039]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 43/173 [loss: 0.748716, acc.: 50.00%] [G loss: 0.417677]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 44/173 [loss: 0.747430, acc.: 50.00%] [G loss: 0.417065]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 45/173 [loss: 0.749961, acc.: 50.00%] [G loss: 0.419412]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 46/173 [loss: 0.748565, acc.: 50.00%] [G loss: 0.418236]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 47/173 [loss: 0.750973, acc.: 50.00%] [G loss: 0.418156]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 48/173 [loss: 0.750962, acc.: 50.00%] [G loss: 0.418293]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 49/173 [loss: 0.751148, acc.: 50.00%] [G loss: 0.418108]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 50/173 [loss: 0.749259, acc.: 50.00%] [G loss: 0.418871]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 51/173 [loss: 0.747254, acc.: 50.00%] [G loss: 0.417069]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 52/173 [loss: 0.748766, acc.: 50.00%] [G loss: 0.417695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 53/173 [loss: 0.749365, acc.: 50.00%] [G loss: 0.419469]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 54/173 [loss: 0.748420, acc.: 50.00%] [G loss: 0.418124]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 55/173 [loss: 0.748730, acc.: 50.00%] [G loss: 0.418952]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 56/173 [loss: 0.747781, acc.: 50.00%] [G loss: 0.418555]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 57/173 [loss: 0.748853, acc.: 50.00%] [G loss: 0.417363]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 58/173 [loss: 0.750725, acc.: 50.00%] [G loss: 0.415077]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 59/173 [loss: 0.750024, acc.: 50.00%] [G loss: 0.416751]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 60/173 [loss: 0.748549, acc.: 50.00%] [G loss: 0.418605]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 61/173 [loss: 0.749759, acc.: 50.00%] [G loss: 0.420924]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 62/173 [loss: 0.749050, acc.: 50.00%] [G loss: 0.419802]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 63/173 [loss: 0.749098, acc.: 50.00%] [G loss: 0.417672]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 64/173 [loss: 0.748635, acc.: 50.00%] [G loss: 0.420270]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 65/173 [loss: 0.750413, acc.: 50.00%] [G loss: 0.421255]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 66/173 [loss: 0.747973, acc.: 50.00%] [G loss: 0.421209]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 67/173 [loss: 0.746219, acc.: 50.00%] [G loss: 0.418170]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 68/173 [loss: 0.748663, acc.: 50.00%] [G loss: 0.419042]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 69/173 [loss: 0.749396, acc.: 50.00%] [G loss: 0.419337]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 70/173 [loss: 0.750313, acc.: 50.00%] [G loss: 0.425234]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 71/173 [loss: 0.747652, acc.: 50.00%] [G loss: 0.422894]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 72/173 [loss: 0.748362, acc.: 50.00%] [G loss: 0.420081]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 73/173 [loss: 0.748426, acc.: 50.00%] [G loss: 0.420148]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 74/173 [loss: 0.748124, acc.: 50.00%] [G loss: 0.420465]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 75/173 [loss: 0.749885, acc.: 50.00%] [G loss: 0.418701]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 76/173 [loss: 0.750862, acc.: 50.00%] [G loss: 0.419013]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 77/173 [loss: 0.748875, acc.: 50.00%] [G loss: 0.418002]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 78/173 [loss: 0.749254, acc.: 50.00%] [G loss: 0.419103]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 79/173 [loss: 0.746431, acc.: 50.00%] [G loss: 0.421559]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 80/173 [loss: 0.747627, acc.: 50.00%] [G loss: 0.421127]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 81/173 [loss: 0.747512, acc.: 50.00%] [G loss: 0.419203]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 82/173 [loss: 0.748321, acc.: 50.00%] [G loss: 0.418425]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 83/173 [loss: 0.747989, acc.: 50.00%] [G loss: 0.419403]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 84/173 [loss: 0.749120, acc.: 50.00%] [G loss: 0.418691]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 85/173 [loss: 0.750414, acc.: 50.00%] [G loss: 0.416922]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 86/173 [loss: 0.751102, acc.: 50.00%] [G loss: 0.415792]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 87/173 [loss: 0.749705, acc.: 50.00%] [G loss: 0.416629]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 88/173 [loss: 0.752218, acc.: 50.00%] [G loss: 0.418710]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 89/173 [loss: 0.748649, acc.: 50.00%] [G loss: 0.419701]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 90/173 [loss: 0.750009, acc.: 50.00%] [G loss: 0.421332]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 91/173 [loss: 0.747149, acc.: 50.00%] [G loss: 0.421243]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 92/173 [loss: 0.751245, acc.: 50.00%] [G loss: 0.418830]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 93/173 [loss: 0.749181, acc.: 50.00%] [G loss: 0.418183]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 94/173 [loss: 0.749460, acc.: 50.00%] [G loss: 0.417700]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 95/173 [loss: 0.749798, acc.: 50.00%] [G loss: 0.418096]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 96/173 [loss: 0.751168, acc.: 50.00%] [G loss: 0.418049]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 97/173 [loss: 0.749348, acc.: 50.00%] [G loss: 0.418318]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 98/173 [loss: 0.749513, acc.: 50.00%] [G loss: 0.419247]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 99/173 [loss: 0.748272, acc.: 50.00%] [G loss: 0.418915]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 100/173 [loss: 0.747307, acc.: 50.00%] [G loss: 0.420069]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 101/173 [loss: 0.751362, acc.: 50.00%] [G loss: 0.420043]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 102/173 [loss: 0.748562, acc.: 50.00%] [G loss: 0.420921]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 103/173 [loss: 0.749880, acc.: 50.00%] [G loss: 0.420380]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 104/173 [loss: 0.747391, acc.: 50.00%] [G loss: 0.420180]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 105/173 [loss: 0.747879, acc.: 50.00%] [G loss: 0.421064]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 106/173 [loss: 0.748439, acc.: 50.00%] [G loss: 0.420800]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 107/173 [loss: 0.750146, acc.: 50.00%] [G loss: 0.422233]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 108/173 [loss: 0.749139, acc.: 50.00%] [G loss: 0.419113]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 109/173 [loss: 0.748478, acc.: 50.00%] [G loss: 0.419832]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 110/173 [loss: 0.748545, acc.: 50.00%] [G loss: 0.418261]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 111/173 [loss: 0.748273, acc.: 50.00%] [G loss: 0.422328]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 112/173 [loss: 0.747767, acc.: 50.00%] [G loss: 0.419884]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 113/173 [loss: 0.744562, acc.: 50.00%] [G loss: 0.418658]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 114/173 [loss: 0.748558, acc.: 50.00%] [G loss: 0.418992]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 115/173 [loss: 0.747565, acc.: 50.00%] [G loss: 0.420791]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 116/173 [loss: 0.748938, acc.: 50.00%] [G loss: 0.420597]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 117/173 [loss: 0.746521, acc.: 50.00%] [G loss: 0.419746]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 118/173 [loss: 0.746994, acc.: 50.00%] [G loss: 0.420633]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 119/173 [loss: 0.749489, acc.: 50.00%] [G loss: 0.420260]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 120/173 [loss: 0.748529, acc.: 50.00%] [G loss: 0.418883]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 121/173 [loss: 0.748060, acc.: 50.00%] [G loss: 0.418741]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 122/173 [loss: 0.749383, acc.: 50.00%] [G loss: 0.420534]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 123/173 [loss: 0.751451, acc.: 50.00%] [G loss: 0.418829]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 124/173 [loss: 0.749510, acc.: 50.00%] [G loss: 0.419195]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 125/173 [loss: 0.749925, acc.: 50.00%] [G loss: 0.416975]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 126/173 [loss: 0.750343, acc.: 50.00%] [G loss: 0.417602]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 127/173 [loss: 0.750036, acc.: 50.00%] [G loss: 0.419483]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 128/173 [loss: 0.750767, acc.: 50.00%] [G loss: 0.420673]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 129/173 [loss: 0.749374, acc.: 50.00%] [G loss: 0.420183]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 130/173 [loss: 0.747480, acc.: 50.00%] [G loss: 0.419257]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 131/173 [loss: 0.749759, acc.: 50.00%] [G loss: 0.418507]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 132/173 [loss: 0.751464, acc.: 50.00%] [G loss: 0.419275]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 133/173 [loss: 0.748872, acc.: 50.00%] [G loss: 0.419032]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 134/173 [loss: 0.750097, acc.: 50.00%] [G loss: 0.419165]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 135/173 [loss: 0.749040, acc.: 50.00%] [G loss: 0.417823]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 136/173 [loss: 0.750300, acc.: 50.00%] [G loss: 0.419973]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 137/173 [loss: 0.747695, acc.: 50.00%] [G loss: 0.417991]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 138/173 [loss: 0.747539, acc.: 50.00%] [G loss: 0.421452]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 139/173 [loss: 0.746860, acc.: 50.00%] [G loss: 0.422013]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 140/173 [loss: 0.748119, acc.: 50.00%] [G loss: 0.419715]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 141/173 [loss: 0.747980, acc.: 50.00%] [G loss: 0.419980]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 142/173 [loss: 0.748474, acc.: 50.00%] [G loss: 0.416992]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 143/173 [loss: 0.750563, acc.: 50.00%] [G loss: 0.418878]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 144/173 [loss: 0.748210, acc.: 50.00%] [G loss: 0.418554]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 145/173 [loss: 0.750128, acc.: 50.00%] [G loss: 0.419061]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 146/173 [loss: 0.750085, acc.: 50.00%] [G loss: 0.419882]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 147/173 [loss: 0.750303, acc.: 50.00%] [G loss: 0.418139]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 148/173 [loss: 0.750792, acc.: 50.00%] [G loss: 0.419453]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 149/173 [loss: 0.748674, acc.: 50.00%] [G loss: 0.419420]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 150/173 [loss: 0.748321, acc.: 50.00%] [G loss: 0.420130]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 151/173 [loss: 0.749308, acc.: 50.00%] [G loss: 0.418857]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 152/173 [loss: 0.746970, acc.: 50.00%] [G loss: 0.418121]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 153/173 [loss: 0.748327, acc.: 50.00%] [G loss: 0.418194]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 154/173 [loss: 0.748306, acc.: 50.00%] [G loss: 0.417749]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 155/173 [loss: 0.747760, acc.: 50.00%] [G loss: 0.417786]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 156/173 [loss: 0.748967, acc.: 50.00%] [G loss: 0.418691]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 157/173 [loss: 0.749649, acc.: 50.00%] [G loss: 0.418911]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 158/173 [loss: 0.751225, acc.: 50.00%] [G loss: 0.419319]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 159/173 [loss: 0.749136, acc.: 50.00%] [G loss: 0.418667]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 160/173 [loss: 0.748841, acc.: 50.00%] [G loss: 0.417042]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 161/173 [loss: 0.748727, acc.: 50.00%] [G loss: 0.418892]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 162/173 [loss: 0.749066, acc.: 50.00%] [G loss: 0.419761]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 163/173 [loss: 0.748996, acc.: 50.00%] [G loss: 0.418027]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 164/173 [loss: 0.748397, acc.: 50.00%] [G loss: 0.418335]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 165/173 [loss: 0.748518, acc.: 50.00%] [G loss: 0.418640]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 166/173 [loss: 0.750142, acc.: 50.00%] [G loss: 0.417547]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 167/173 [loss: 0.748174, acc.: 50.00%] [G loss: 0.420329]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 168/173 [loss: 0.750358, acc.: 50.00%] [G loss: 0.419326]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 169/173 [loss: 0.751598, acc.: 50.00%] [G loss: 0.419209]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 170/173 [loss: 0.749351, acc.: 50.00%] [G loss: 0.420027]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 171/173 [loss: 0.746681, acc.: 50.00%] [G loss: 0.419645]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 14/200  Batch Size: 172/173 [loss: 0.747717, acc.: 50.00%] [G loss: 0.418377]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 0/173 [loss: 0.750301, acc.: 50.00%] [G loss: 0.420420]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 1/173 [loss: 0.747952, acc.: 50.00%] [G loss: 0.420966]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 2/173 [loss: 0.748812, acc.: 50.00%] [G loss: 0.421110]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 3/173 [loss: 0.750877, acc.: 50.00%] [G loss: 0.419836]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 4/173 [loss: 0.749907, acc.: 50.00%] [G loss: 0.419973]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 5/173 [loss: 0.749370, acc.: 50.00%] [G loss: 0.419206]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 6/173 [loss: 0.749507, acc.: 50.00%] [G loss: 0.418747]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 7/173 [loss: 0.750098, acc.: 50.00%] [G loss: 0.417874]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 8/173 [loss: 0.748931, acc.: 50.00%] [G loss: 0.420529]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 9/173 [loss: 0.750332, acc.: 50.00%] [G loss: 0.417748]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 10/173 [loss: 0.749980, acc.: 50.00%] [G loss: 0.419037]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 11/173 [loss: 0.749754, acc.: 50.00%] [G loss: 0.419015]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 12/173 [loss: 0.748296, acc.: 50.00%] [G loss: 0.421915]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 13/173 [loss: 0.748815, acc.: 50.00%] [G loss: 0.417816]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 14/173 [loss: 0.749613, acc.: 50.00%] [G loss: 0.420002]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 15/173 [loss: 0.746964, acc.: 50.00%] [G loss: 0.418397]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 16/173 [loss: 0.749989, acc.: 50.00%] [G loss: 0.418639]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 17/173 [loss: 0.749680, acc.: 50.00%] [G loss: 0.416515]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 18/173 [loss: 0.747825, acc.: 50.00%] [G loss: 0.419314]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 19/173 [loss: 0.749181, acc.: 50.00%] [G loss: 0.420517]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 20/173 [loss: 0.747819, acc.: 50.00%] [G loss: 0.419580]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 21/173 [loss: 0.747533, acc.: 50.00%] [G loss: 0.420110]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 22/173 [loss: 0.749404, acc.: 50.00%] [G loss: 0.416666]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 23/173 [loss: 0.746110, acc.: 50.00%] [G loss: 0.419376]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 24/173 [loss: 0.747445, acc.: 50.00%] [G loss: 0.418650]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 25/173 [loss: 0.749727, acc.: 50.00%] [G loss: 0.420850]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 26/173 [loss: 0.750049, acc.: 50.00%] [G loss: 0.419944]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 27/173 [loss: 0.749826, acc.: 50.00%] [G loss: 0.418492]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 28/173 [loss: 0.750665, acc.: 50.00%] [G loss: 0.417076]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 29/173 [loss: 0.750452, acc.: 50.00%] [G loss: 0.420235]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 30/173 [loss: 0.749656, acc.: 50.00%] [G loss: 0.420095]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 31/173 [loss: 0.749125, acc.: 50.00%] [G loss: 0.421322]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 32/173 [loss: 0.750124, acc.: 50.00%] [G loss: 0.418934]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 33/173 [loss: 0.746516, acc.: 50.00%] [G loss: 0.419356]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 34/173 [loss: 0.748091, acc.: 50.00%] [G loss: 0.420162]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 35/173 [loss: 0.750218, acc.: 50.00%] [G loss: 0.419788]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 36/173 [loss: 0.750430, acc.: 50.00%] [G loss: 0.419643]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 37/173 [loss: 0.749022, acc.: 50.00%] [G loss: 0.417861]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 38/173 [loss: 0.749087, acc.: 50.00%] [G loss: 0.420263]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 39/173 [loss: 0.748615, acc.: 50.00%] [G loss: 0.419884]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 40/173 [loss: 0.748690, acc.: 50.00%] [G loss: 0.419229]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 41/173 [loss: 0.748296, acc.: 50.00%] [G loss: 0.418542]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 42/173 [loss: 0.749639, acc.: 50.00%] [G loss: 0.418893]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 43/173 [loss: 0.750242, acc.: 50.00%] [G loss: 0.419378]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 44/173 [loss: 0.749128, acc.: 50.00%] [G loss: 0.418744]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 45/173 [loss: 0.748057, acc.: 50.00%] [G loss: 0.419252]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 46/173 [loss: 0.748219, acc.: 50.00%] [G loss: 0.419540]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 47/173 [loss: 0.749905, acc.: 50.00%] [G loss: 0.419689]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 48/173 [loss: 0.750423, acc.: 50.00%] [G loss: 0.419463]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 49/173 [loss: 0.749048, acc.: 50.00%] [G loss: 0.419631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 50/173 [loss: 0.746833, acc.: 50.00%] [G loss: 0.418705]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 51/173 [loss: 0.747589, acc.: 50.00%] [G loss: 0.418490]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 52/173 [loss: 0.748897, acc.: 50.00%] [G loss: 0.418454]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 53/173 [loss: 0.748089, acc.: 50.00%] [G loss: 0.419758]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 54/173 [loss: 0.748414, acc.: 50.00%] [G loss: 0.420681]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 55/173 [loss: 0.748971, acc.: 50.00%] [G loss: 0.417339]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 56/173 [loss: 0.748418, acc.: 50.00%] [G loss: 0.419394]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 57/173 [loss: 0.748070, acc.: 50.00%] [G loss: 0.419367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 58/173 [loss: 0.749067, acc.: 50.00%] [G loss: 0.419499]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 59/173 [loss: 0.749331, acc.: 50.00%] [G loss: 0.419081]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 60/173 [loss: 0.748581, acc.: 50.00%] [G loss: 0.419894]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 61/173 [loss: 0.748934, acc.: 50.00%] [G loss: 0.417564]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 62/173 [loss: 0.749216, acc.: 50.00%] [G loss: 0.420170]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 63/173 [loss: 0.748630, acc.: 50.00%] [G loss: 0.419591]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 64/173 [loss: 0.749057, acc.: 50.00%] [G loss: 0.418700]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 65/173 [loss: 0.749380, acc.: 50.00%] [G loss: 0.417959]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 66/173 [loss: 0.749779, acc.: 50.00%] [G loss: 0.419697]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 67/173 [loss: 0.747932, acc.: 50.00%] [G loss: 0.420994]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 68/173 [loss: 0.746238, acc.: 50.00%] [G loss: 0.417137]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 69/173 [loss: 0.748707, acc.: 50.00%] [G loss: 0.418321]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 70/173 [loss: 0.747382, acc.: 50.00%] [G loss: 0.416789]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 71/173 [loss: 0.748009, acc.: 50.00%] [G loss: 0.418376]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 72/173 [loss: 0.750336, acc.: 50.00%] [G loss: 0.419858]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 73/173 [loss: 0.747991, acc.: 50.00%] [G loss: 0.417692]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 74/173 [loss: 0.746856, acc.: 50.00%] [G loss: 0.419370]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 75/173 [loss: 0.749728, acc.: 50.00%] [G loss: 0.416463]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 76/173 [loss: 0.747354, acc.: 50.00%] [G loss: 0.417233]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 77/173 [loss: 0.750407, acc.: 50.00%] [G loss: 0.418251]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 78/173 [loss: 0.749668, acc.: 50.00%] [G loss: 0.418737]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 79/173 [loss: 0.746473, acc.: 50.00%] [G loss: 0.418792]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 80/173 [loss: 0.748904, acc.: 50.00%] [G loss: 0.417363]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 81/173 [loss: 0.747888, acc.: 50.00%] [G loss: 0.416791]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 82/173 [loss: 0.749525, acc.: 50.00%] [G loss: 0.418079]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 83/173 [loss: 0.749379, acc.: 50.00%] [G loss: 0.417122]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 84/173 [loss: 0.749773, acc.: 50.00%] [G loss: 0.417808]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 85/173 [loss: 0.748003, acc.: 50.00%] [G loss: 0.421223]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 86/173 [loss: 0.748216, acc.: 50.00%] [G loss: 0.417886]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 87/173 [loss: 0.749708, acc.: 50.00%] [G loss: 0.420446]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 88/173 [loss: 0.748821, acc.: 50.00%] [G loss: 0.420829]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 89/173 [loss: 0.746914, acc.: 50.00%] [G loss: 0.417199]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 90/173 [loss: 0.751197, acc.: 50.00%] [G loss: 0.418442]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 91/173 [loss: 0.750793, acc.: 50.00%] [G loss: 0.419815]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 92/173 [loss: 0.751106, acc.: 50.00%] [G loss: 0.419198]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 93/173 [loss: 0.748188, acc.: 50.00%] [G loss: 0.420238]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 94/173 [loss: 0.749010, acc.: 50.00%] [G loss: 0.418907]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 95/173 [loss: 0.747658, acc.: 50.00%] [G loss: 0.421896]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 96/173 [loss: 0.748545, acc.: 50.00%] [G loss: 0.420262]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 97/173 [loss: 0.748142, acc.: 50.00%] [G loss: 0.419953]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 98/173 [loss: 0.748351, acc.: 50.00%] [G loss: 0.417358]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 99/173 [loss: 0.747847, acc.: 50.00%] [G loss: 0.419640]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 100/173 [loss: 0.749222, acc.: 50.00%] [G loss: 0.418474]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 101/173 [loss: 0.750764, acc.: 50.00%] [G loss: 0.419552]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 102/173 [loss: 0.749282, acc.: 50.00%] [G loss: 0.418670]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 103/173 [loss: 0.746893, acc.: 50.00%] [G loss: 0.418375]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 104/173 [loss: 0.749015, acc.: 50.00%] [G loss: 0.418728]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 105/173 [loss: 0.749834, acc.: 50.00%] [G loss: 0.418877]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 106/173 [loss: 0.746811, acc.: 50.00%] [G loss: 0.419779]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 107/173 [loss: 0.748196, acc.: 50.00%] [G loss: 0.421030]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 108/173 [loss: 0.748309, acc.: 50.00%] [G loss: 0.418674]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 109/173 [loss: 0.751217, acc.: 50.00%] [G loss: 0.418765]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 110/173 [loss: 0.748216, acc.: 50.00%] [G loss: 0.418424]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 111/173 [loss: 0.750910, acc.: 50.00%] [G loss: 0.419783]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 112/173 [loss: 0.748613, acc.: 50.00%] [G loss: 0.419631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 113/173 [loss: 0.749210, acc.: 50.00%] [G loss: 0.420382]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 114/173 [loss: 0.747344, acc.: 50.00%] [G loss: 0.418357]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 115/173 [loss: 0.749385, acc.: 50.00%] [G loss: 0.418056]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 116/173 [loss: 0.750260, acc.: 50.00%] [G loss: 0.418120]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 117/173 [loss: 0.750388, acc.: 50.00%] [G loss: 0.420400]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 118/173 [loss: 0.748252, acc.: 50.00%] [G loss: 0.418571]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 119/173 [loss: 0.747236, acc.: 50.00%] [G loss: 0.419337]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 120/173 [loss: 0.751150, acc.: 50.00%] [G loss: 0.418822]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 121/173 [loss: 0.751609, acc.: 50.00%] [G loss: 0.417622]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 122/173 [loss: 0.748281, acc.: 50.00%] [G loss: 0.418925]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 123/173 [loss: 0.749571, acc.: 50.00%] [G loss: 0.420474]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 124/173 [loss: 0.750653, acc.: 50.00%] [G loss: 0.419608]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 125/173 [loss: 0.748871, acc.: 50.00%] [G loss: 0.418749]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 126/173 [loss: 0.748543, acc.: 50.00%] [G loss: 0.419208]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 127/173 [loss: 0.747315, acc.: 50.00%] [G loss: 0.419563]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 128/173 [loss: 0.749262, acc.: 50.00%] [G loss: 0.417821]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 129/173 [loss: 0.748210, acc.: 50.00%] [G loss: 0.418579]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 130/173 [loss: 0.750704, acc.: 50.00%] [G loss: 0.418695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 131/173 [loss: 0.749672, acc.: 50.00%] [G loss: 0.419669]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 132/173 [loss: 0.749346, acc.: 50.00%] [G loss: 0.419152]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 133/173 [loss: 0.747802, acc.: 50.00%] [G loss: 0.420403]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 134/173 [loss: 0.748636, acc.: 50.00%] [G loss: 0.422301]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 135/173 [loss: 0.748715, acc.: 50.00%] [G loss: 0.418780]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 136/173 [loss: 0.747549, acc.: 50.00%] [G loss: 0.418902]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 137/173 [loss: 0.749902, acc.: 50.00%] [G loss: 0.419520]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 138/173 [loss: 0.748982, acc.: 50.00%] [G loss: 0.421082]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 139/173 [loss: 0.748670, acc.: 50.00%] [G loss: 0.418095]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 140/173 [loss: 0.746181, acc.: 50.00%] [G loss: 0.419060]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 141/173 [loss: 0.747490, acc.: 50.00%] [G loss: 0.420372]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 142/173 [loss: 0.747342, acc.: 50.00%] [G loss: 0.420557]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 143/173 [loss: 0.748273, acc.: 50.00%] [G loss: 0.421053]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 144/173 [loss: 0.746630, acc.: 50.00%] [G loss: 0.418560]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 145/173 [loss: 0.749825, acc.: 50.00%] [G loss: 0.418214]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 146/173 [loss: 0.748118, acc.: 50.00%] [G loss: 0.419021]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 147/173 [loss: 0.747646, acc.: 50.00%] [G loss: 0.418473]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 148/173 [loss: 0.748037, acc.: 50.00%] [G loss: 0.418775]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 149/173 [loss: 0.749238, acc.: 50.00%] [G loss: 0.417256]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 150/173 [loss: 0.747770, acc.: 50.00%] [G loss: 0.418307]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 151/173 [loss: 0.748904, acc.: 50.00%] [G loss: 0.416067]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 152/173 [loss: 0.749987, acc.: 50.00%] [G loss: 0.416147]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 153/173 [loss: 0.748143, acc.: 50.00%] [G loss: 0.418708]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 154/173 [loss: 0.748412, acc.: 50.00%] [G loss: 0.417570]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 155/173 [loss: 0.747928, acc.: 50.00%] [G loss: 0.419576]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 156/173 [loss: 0.748902, acc.: 50.00%] [G loss: 0.418337]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 157/173 [loss: 0.749991, acc.: 50.00%] [G loss: 0.418230]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 158/173 [loss: 0.750026, acc.: 50.00%] [G loss: 0.417537]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 159/173 [loss: 0.750420, acc.: 50.00%] [G loss: 0.417700]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 160/173 [loss: 0.750518, acc.: 50.00%] [G loss: 0.417296]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 161/173 [loss: 0.749782, acc.: 50.00%] [G loss: 0.419094]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 162/173 [loss: 0.748213, acc.: 50.00%] [G loss: 0.418383]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 163/173 [loss: 0.748688, acc.: 50.00%] [G loss: 0.417618]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 164/173 [loss: 0.748785, acc.: 50.00%] [G loss: 0.417957]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 165/173 [loss: 0.749931, acc.: 50.00%] [G loss: 0.419169]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 166/173 [loss: 0.747817, acc.: 50.00%] [G loss: 0.419228]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 167/173 [loss: 0.750420, acc.: 50.00%] [G loss: 0.418601]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 168/173 [loss: 0.749271, acc.: 50.00%] [G loss: 0.418209]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 169/173 [loss: 0.749790, acc.: 50.00%] [G loss: 0.417615]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 170/173 [loss: 0.748238, acc.: 50.00%] [G loss: 0.419706]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 171/173 [loss: 0.748210, acc.: 50.00%] [G loss: 0.419779]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 15/200  Batch Size: 172/173 [loss: 0.749471, acc.: 50.00%] [G loss: 0.419852]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 0/173 [loss: 0.748010, acc.: 50.00%] [G loss: 0.418655]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 1/173 [loss: 0.750858, acc.: 50.00%] [G loss: 0.417773]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 2/173 [loss: 0.748715, acc.: 50.00%] [G loss: 0.418902]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 3/173 [loss: 0.749721, acc.: 50.00%] [G loss: 0.419177]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 4/173 [loss: 0.748336, acc.: 50.00%] [G loss: 0.419926]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 5/173 [loss: 0.750058, acc.: 50.00%] [G loss: 0.418239]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 6/173 [loss: 0.748970, acc.: 50.00%] [G loss: 0.417982]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 7/173 [loss: 0.749276, acc.: 50.00%] [G loss: 0.419273]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 8/173 [loss: 0.748510, acc.: 50.00%] [G loss: 0.417272]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 9/173 [loss: 0.748146, acc.: 50.00%] [G loss: 0.419790]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 10/173 [loss: 0.748855, acc.: 50.00%] [G loss: 0.420157]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 11/173 [loss: 0.749056, acc.: 50.00%] [G loss: 0.418615]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 12/173 [loss: 0.749063, acc.: 50.00%] [G loss: 0.418460]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 13/173 [loss: 0.747652, acc.: 50.00%] [G loss: 0.418386]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 14/173 [loss: 0.746316, acc.: 50.00%] [G loss: 0.419644]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 15/173 [loss: 0.747713, acc.: 50.00%] [G loss: 0.418498]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 16/173 [loss: 0.747489, acc.: 50.00%] [G loss: 0.418769]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 17/173 [loss: 0.746826, acc.: 50.00%] [G loss: 0.419945]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 18/173 [loss: 0.747247, acc.: 50.00%] [G loss: 0.417891]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 19/173 [loss: 0.747718, acc.: 50.00%] [G loss: 0.418843]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 20/173 [loss: 0.749765, acc.: 50.00%] [G loss: 0.417891]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 21/173 [loss: 0.748959, acc.: 50.00%] [G loss: 0.419084]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 22/173 [loss: 0.749022, acc.: 50.00%] [G loss: 0.420276]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 23/173 [loss: 0.748570, acc.: 50.00%] [G loss: 0.420562]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 24/173 [loss: 0.747902, acc.: 50.00%] [G loss: 0.417715]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 25/173 [loss: 0.749491, acc.: 50.00%] [G loss: 0.418009]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 26/173 [loss: 0.751175, acc.: 50.00%] [G loss: 0.420367]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 27/173 [loss: 0.747778, acc.: 50.00%] [G loss: 0.419999]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 28/173 [loss: 0.748593, acc.: 50.00%] [G loss: 0.418383]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 29/173 [loss: 0.747920, acc.: 50.00%] [G loss: 0.417186]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 30/173 [loss: 0.750293, acc.: 50.00%] [G loss: 0.418072]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 31/173 [loss: 0.748821, acc.: 50.00%] [G loss: 0.420288]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 32/173 [loss: 0.746742, acc.: 50.00%] [G loss: 0.420999]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 33/173 [loss: 0.747550, acc.: 50.00%] [G loss: 0.418797]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 34/173 [loss: 0.748617, acc.: 50.00%] [G loss: 0.419815]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 35/173 [loss: 0.748395, acc.: 50.00%] [G loss: 0.420086]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 36/173 [loss: 0.748540, acc.: 50.00%] [G loss: 0.419715]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 37/173 [loss: 0.747731, acc.: 50.00%] [G loss: 0.418913]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 38/173 [loss: 0.747610, acc.: 50.00%] [G loss: 0.420302]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 39/173 [loss: 0.748173, acc.: 50.00%] [G loss: 0.418597]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 40/173 [loss: 0.751051, acc.: 50.00%] [G loss: 0.420315]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 41/173 [loss: 0.748502, acc.: 50.00%] [G loss: 0.421659]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 42/173 [loss: 0.747553, acc.: 50.00%] [G loss: 0.420383]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 43/173 [loss: 0.749761, acc.: 50.00%] [G loss: 0.419986]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 44/173 [loss: 0.748612, acc.: 50.00%] [G loss: 0.417502]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 45/173 [loss: 0.749468, acc.: 50.00%] [G loss: 0.419828]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 46/173 [loss: 0.748704, acc.: 50.00%] [G loss: 0.419522]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 47/173 [loss: 0.749255, acc.: 50.00%] [G loss: 0.420909]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 48/173 [loss: 0.749970, acc.: 50.00%] [G loss: 0.417468]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 49/173 [loss: 0.748449, acc.: 50.00%] [G loss: 0.417788]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 50/173 [loss: 0.749256, acc.: 50.00%] [G loss: 0.419159]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 51/173 [loss: 0.749262, acc.: 50.00%] [G loss: 0.419296]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 52/173 [loss: 0.748960, acc.: 50.00%] [G loss: 0.420308]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 53/173 [loss: 0.747529, acc.: 50.00%] [G loss: 0.418154]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 54/173 [loss: 0.750588, acc.: 50.00%] [G loss: 0.417718]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 55/173 [loss: 0.749987, acc.: 50.00%] [G loss: 0.420849]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 56/173 [loss: 0.747555, acc.: 50.00%] [G loss: 0.418564]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 57/173 [loss: 0.747943, acc.: 50.00%] [G loss: 0.418361]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 58/173 [loss: 0.749134, acc.: 50.00%] [G loss: 0.419077]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 59/173 [loss: 0.749155, acc.: 50.00%] [G loss: 0.417632]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 60/173 [loss: 0.750629, acc.: 50.00%] [G loss: 0.415436]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 61/173 [loss: 0.747683, acc.: 50.00%] [G loss: 0.417751]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 62/173 [loss: 0.747667, acc.: 50.00%] [G loss: 0.419392]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 63/173 [loss: 0.748324, acc.: 50.00%] [G loss: 0.417421]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 64/173 [loss: 0.749991, acc.: 50.00%] [G loss: 0.418364]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 65/173 [loss: 0.749144, acc.: 50.00%] [G loss: 0.417713]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 66/173 [loss: 0.750089, acc.: 50.00%] [G loss: 0.417880]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 67/173 [loss: 0.752213, acc.: 50.00%] [G loss: 0.417699]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 68/173 [loss: 0.748885, acc.: 50.00%] [G loss: 0.418005]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 69/173 [loss: 0.749340, acc.: 50.00%] [G loss: 0.416877]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 70/173 [loss: 0.748642, acc.: 50.00%] [G loss: 0.418068]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 71/173 [loss: 0.747873, acc.: 50.00%] [G loss: 0.419497]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 72/173 [loss: 0.748776, acc.: 50.00%] [G loss: 0.419355]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 73/173 [loss: 0.748946, acc.: 50.00%] [G loss: 0.419368]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 74/173 [loss: 0.747322, acc.: 50.00%] [G loss: 0.419544]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 75/173 [loss: 0.749904, acc.: 50.00%] [G loss: 0.419066]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 76/173 [loss: 0.747262, acc.: 50.00%] [G loss: 0.417409]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 77/173 [loss: 0.748092, acc.: 50.00%] [G loss: 0.417217]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 78/173 [loss: 0.749593, acc.: 50.00%] [G loss: 0.417613]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 79/173 [loss: 0.749231, acc.: 50.00%] [G loss: 0.417251]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 80/173 [loss: 0.749355, acc.: 50.00%] [G loss: 0.418162]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 81/173 [loss: 0.749516, acc.: 50.00%] [G loss: 0.416537]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 82/173 [loss: 0.749805, acc.: 50.00%] [G loss: 0.418344]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 83/173 [loss: 0.749019, acc.: 50.00%] [G loss: 0.418510]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 84/173 [loss: 0.749278, acc.: 50.00%] [G loss: 0.419346]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 85/173 [loss: 0.748767, acc.: 50.00%] [G loss: 0.419541]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 86/173 [loss: 0.749464, acc.: 50.00%] [G loss: 0.417625]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 87/173 [loss: 0.748143, acc.: 50.00%] [G loss: 0.419065]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 88/173 [loss: 0.748760, acc.: 50.00%] [G loss: 0.419146]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 89/173 [loss: 0.749743, acc.: 50.00%] [G loss: 0.418206]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 90/173 [loss: 0.749392, acc.: 50.00%] [G loss: 0.418374]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 91/173 [loss: 0.748459, acc.: 50.00%] [G loss: 0.422398]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 92/173 [loss: 0.749078, acc.: 50.00%] [G loss: 0.420097]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 93/173 [loss: 0.748942, acc.: 50.00%] [G loss: 0.419421]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 94/173 [loss: 0.746936, acc.: 50.00%] [G loss: 0.420332]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 95/173 [loss: 0.747388, acc.: 50.00%] [G loss: 0.419033]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 96/173 [loss: 0.748466, acc.: 50.00%] [G loss: 0.416483]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 97/173 [loss: 0.750707, acc.: 50.00%] [G loss: 0.419194]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 98/173 [loss: 0.747001, acc.: 50.00%] [G loss: 0.418583]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 99/173 [loss: 0.749697, acc.: 50.00%] [G loss: 0.419889]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 100/173 [loss: 0.748863, acc.: 50.00%] [G loss: 0.417961]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 101/173 [loss: 0.749567, acc.: 50.00%] [G loss: 0.413882]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 102/173 [loss: 0.750679, acc.: 50.00%] [G loss: 0.418545]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 103/173 [loss: 0.749419, acc.: 50.00%] [G loss: 0.420854]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 104/173 [loss: 0.747349, acc.: 50.00%] [G loss: 0.422834]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 105/173 [loss: 0.747899, acc.: 50.00%] [G loss: 0.420424]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 106/173 [loss: 0.750268, acc.: 50.00%] [G loss: 0.417806]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 107/173 [loss: 0.749762, acc.: 50.00%] [G loss: 0.418516]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 108/173 [loss: 0.748575, acc.: 50.00%] [G loss: 0.419913]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 109/173 [loss: 0.748639, acc.: 50.00%] [G loss: 0.418980]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 110/173 [loss: 0.749867, acc.: 50.00%] [G loss: 0.418986]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 111/173 [loss: 0.747463, acc.: 50.00%] [G loss: 0.419403]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 112/173 [loss: 0.751033, acc.: 50.00%] [G loss: 0.417225]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 113/173 [loss: 0.747409, acc.: 50.00%] [G loss: 0.418777]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 114/173 [loss: 0.747304, acc.: 50.00%] [G loss: 0.420634]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 115/173 [loss: 0.750976, acc.: 50.00%] [G loss: 0.420717]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 116/173 [loss: 0.749446, acc.: 50.00%] [G loss: 0.418363]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 117/173 [loss: 0.750690, acc.: 50.00%] [G loss: 0.418254]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 118/173 [loss: 0.749390, acc.: 50.00%] [G loss: 0.418750]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 119/173 [loss: 0.748148, acc.: 50.00%] [G loss: 0.418983]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 120/173 [loss: 0.748025, acc.: 50.00%] [G loss: 0.417813]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 121/173 [loss: 0.748153, acc.: 50.00%] [G loss: 0.418940]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 122/173 [loss: 0.750001, acc.: 50.00%] [G loss: 0.417809]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 123/173 [loss: 0.750412, acc.: 50.00%] [G loss: 0.418050]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 124/173 [loss: 0.748976, acc.: 50.00%] [G loss: 0.419140]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 125/173 [loss: 0.749706, acc.: 50.00%] [G loss: 0.416844]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 126/173 [loss: 0.750371, acc.: 50.00%] [G loss: 0.417418]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 127/173 [loss: 0.748615, acc.: 50.00%] [G loss: 0.417625]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 128/173 [loss: 0.750095, acc.: 50.00%] [G loss: 0.419394]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 129/173 [loss: 0.749788, acc.: 50.00%] [G loss: 0.417280]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 130/173 [loss: 0.749472, acc.: 50.00%] [G loss: 0.419163]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 131/173 [loss: 0.749028, acc.: 50.00%] [G loss: 0.417682]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 132/173 [loss: 0.750189, acc.: 50.00%] [G loss: 0.417465]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 133/173 [loss: 0.749475, acc.: 50.00%] [G loss: 0.418959]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 134/173 [loss: 0.749147, acc.: 50.00%] [G loss: 0.419928]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 135/173 [loss: 0.750489, acc.: 50.00%] [G loss: 0.419829]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 136/173 [loss: 0.748648, acc.: 50.00%] [G loss: 0.418326]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 137/173 [loss: 0.749409, acc.: 50.00%] [G loss: 0.419143]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 138/173 [loss: 0.747492, acc.: 50.00%] [G loss: 0.420398]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 139/173 [loss: 0.747810, acc.: 50.00%] [G loss: 0.418927]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 140/173 [loss: 0.746683, acc.: 50.00%] [G loss: 0.418637]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 141/173 [loss: 0.749494, acc.: 50.00%] [G loss: 0.420178]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 142/173 [loss: 0.749007, acc.: 50.00%] [G loss: 0.419636]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 143/173 [loss: 0.747734, acc.: 50.00%] [G loss: 0.420299]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 144/173 [loss: 0.749199, acc.: 50.00%] [G loss: 0.421127]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 145/173 [loss: 0.750127, acc.: 50.00%] [G loss: 0.418894]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 146/173 [loss: 0.749989, acc.: 50.00%] [G loss: 0.417420]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 147/173 [loss: 0.748051, acc.: 50.00%] [G loss: 0.421243]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 148/173 [loss: 0.748046, acc.: 50.00%] [G loss: 0.419954]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 149/173 [loss: 0.748339, acc.: 50.00%] [G loss: 0.420044]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 150/173 [loss: 0.751318, acc.: 50.00%] [G loss: 0.417866]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 151/173 [loss: 0.750931, acc.: 50.00%] [G loss: 0.419640]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 152/173 [loss: 0.747422, acc.: 50.00%] [G loss: 0.421510]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 153/173 [loss: 0.747187, acc.: 50.00%] [G loss: 0.422819]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 154/173 [loss: 0.748223, acc.: 50.00%] [G loss: 0.420131]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 155/173 [loss: 0.747715, acc.: 50.00%] [G loss: 0.420275]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 156/173 [loss: 0.748354, acc.: 50.00%] [G loss: 0.419542]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 157/173 [loss: 0.748247, acc.: 50.00%] [G loss: 0.419470]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 158/173 [loss: 0.745687, acc.: 50.00%] [G loss: 0.418790]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 159/173 [loss: 0.751362, acc.: 50.00%] [G loss: 0.418020]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 160/173 [loss: 0.749605, acc.: 50.00%] [G loss: 0.416779]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 161/173 [loss: 0.748791, acc.: 50.00%] [G loss: 0.417457]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 162/173 [loss: 0.749950, acc.: 50.00%] [G loss: 0.420862]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 163/173 [loss: 0.747793, acc.: 50.00%] [G loss: 0.421955]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 164/173 [loss: 0.748979, acc.: 50.00%] [G loss: 0.420071]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 165/173 [loss: 0.748690, acc.: 50.00%] [G loss: 0.418597]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 166/173 [loss: 0.748602, acc.: 50.00%] [G loss: 0.419594]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 167/173 [loss: 0.750381, acc.: 50.00%] [G loss: 0.417524]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 168/173 [loss: 0.749331, acc.: 50.00%] [G loss: 0.417626]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 169/173 [loss: 0.748842, acc.: 50.00%] [G loss: 0.417824]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 170/173 [loss: 0.747780, acc.: 50.00%] [G loss: 0.418002]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 171/173 [loss: 0.746480, acc.: 50.00%] [G loss: 0.418655]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 16/200  Batch Size: 172/173 [loss: 0.749227, acc.: 50.00%] [G loss: 0.418430]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 0/173 [loss: 0.745605, acc.: 50.00%] [G loss: 0.419213]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 1/173 [loss: 0.747422, acc.: 50.00%] [G loss: 0.416153]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 2/173 [loss: 0.749451, acc.: 50.00%] [G loss: 0.416300]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 3/173 [loss: 0.749067, acc.: 50.00%] [G loss: 0.417350]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 4/173 [loss: 0.748623, acc.: 50.00%] [G loss: 0.419384]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 5/173 [loss: 0.748020, acc.: 50.00%] [G loss: 0.417443]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 6/173 [loss: 0.749513, acc.: 50.00%] [G loss: 0.416747]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 7/173 [loss: 0.750341, acc.: 50.00%] [G loss: 0.418283]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 8/173 [loss: 0.749065, acc.: 50.00%] [G loss: 0.417595]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 9/173 [loss: 0.749271, acc.: 50.00%] [G loss: 0.417656]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 10/173 [loss: 0.748019, acc.: 50.00%] [G loss: 0.417185]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 11/173 [loss: 0.750722, acc.: 50.00%] [G loss: 0.417527]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 12/173 [loss: 0.750765, acc.: 50.00%] [G loss: 0.417569]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 13/173 [loss: 0.748071, acc.: 50.00%] [G loss: 0.417669]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 14/173 [loss: 0.746275, acc.: 50.00%] [G loss: 0.418844]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 15/173 [loss: 0.748357, acc.: 50.00%] [G loss: 0.418133]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 16/173 [loss: 0.751248, acc.: 50.00%] [G loss: 0.418314]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 17/173 [loss: 0.749091, acc.: 50.00%] [G loss: 0.420967]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 18/173 [loss: 0.750386, acc.: 50.00%] [G loss: 0.420573]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 19/173 [loss: 0.746870, acc.: 50.00%] [G loss: 0.419058]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 20/173 [loss: 0.747223, acc.: 50.00%] [G loss: 0.418784]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 21/173 [loss: 0.747291, acc.: 50.00%] [G loss: 0.418873]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 22/173 [loss: 0.749547, acc.: 50.00%] [G loss: 0.419487]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 23/173 [loss: 0.746501, acc.: 50.00%] [G loss: 0.417820]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 24/173 [loss: 0.746197, acc.: 50.00%] [G loss: 0.419624]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 25/173 [loss: 0.747799, acc.: 50.00%] [G loss: 0.418454]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 26/173 [loss: 0.749037, acc.: 50.00%] [G loss: 0.417563]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 27/173 [loss: 0.747290, acc.: 50.00%] [G loss: 0.419760]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 28/173 [loss: 0.746263, acc.: 50.00%] [G loss: 0.421623]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 29/173 [loss: 0.748717, acc.: 50.00%] [G loss: 0.419997]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 30/173 [loss: 0.747751, acc.: 50.00%] [G loss: 0.420453]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 31/173 [loss: 0.749123, acc.: 50.00%] [G loss: 0.417483]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 32/173 [loss: 0.748668, acc.: 50.00%] [G loss: 0.419657]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 33/173 [loss: 0.748387, acc.: 50.00%] [G loss: 0.416586]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 34/173 [loss: 0.750089, acc.: 50.00%] [G loss: 0.418428]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 35/173 [loss: 0.749423, acc.: 50.00%] [G loss: 0.418348]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 36/173 [loss: 0.749606, acc.: 50.00%] [G loss: 0.419374]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 37/173 [loss: 0.751803, acc.: 50.00%] [G loss: 0.417786]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 38/173 [loss: 0.749324, acc.: 50.00%] [G loss: 0.418822]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 39/173 [loss: 0.747807, acc.: 50.00%] [G loss: 0.422820]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 40/173 [loss: 0.749389, acc.: 50.00%] [G loss: 0.418922]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 41/173 [loss: 0.750149, acc.: 50.00%] [G loss: 0.417631]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 42/173 [loss: 0.749636, acc.: 50.00%] [G loss: 0.417446]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 43/173 [loss: 0.748876, acc.: 50.00%] [G loss: 0.418035]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 44/173 [loss: 0.749500, acc.: 50.00%] [G loss: 0.419858]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 45/173 [loss: 0.751125, acc.: 50.00%] [G loss: 0.417381]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 46/173 [loss: 0.747950, acc.: 50.00%] [G loss: 0.416704]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 47/173 [loss: 0.749170, acc.: 50.00%] [G loss: 0.417314]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 48/173 [loss: 0.749023, acc.: 50.00%] [G loss: 0.420301]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 49/173 [loss: 0.749243, acc.: 50.00%] [G loss: 0.418536]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 50/173 [loss: 0.750607, acc.: 50.00%] [G loss: 0.418271]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 51/173 [loss: 0.749188, acc.: 50.00%] [G loss: 0.415203]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 52/173 [loss: 0.750076, acc.: 50.00%] [G loss: 0.418796]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 53/173 [loss: 0.747219, acc.: 50.00%] [G loss: 0.418793]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 54/173 [loss: 0.748818, acc.: 50.00%] [G loss: 0.418796]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 55/173 [loss: 0.749806, acc.: 50.00%] [G loss: 0.419462]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 56/173 [loss: 0.747395, acc.: 50.00%] [G loss: 0.417937]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 57/173 [loss: 0.751086, acc.: 50.00%] [G loss: 0.419250]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 58/173 [loss: 0.749244, acc.: 50.00%] [G loss: 0.420157]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 59/173 [loss: 0.745559, acc.: 50.00%] [G loss: 0.418118]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 60/173 [loss: 0.746816, acc.: 50.00%] [G loss: 0.420262]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 61/173 [loss: 0.749217, acc.: 50.00%] [G loss: 0.419767]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 62/173 [loss: 0.748068, acc.: 50.00%] [G loss: 0.419281]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 63/173 [loss: 0.746420, acc.: 50.00%] [G loss: 0.419277]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 64/173 [loss: 0.748254, acc.: 50.00%] [G loss: 0.418949]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 65/173 [loss: 0.750251, acc.: 50.00%] [G loss: 0.418727]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 66/173 [loss: 0.750402, acc.: 50.00%] [G loss: 0.417543]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 67/173 [loss: 0.750222, acc.: 50.00%] [G loss: 0.417902]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 68/173 [loss: 0.748489, acc.: 50.00%] [G loss: 0.418099]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 69/173 [loss: 0.747461, acc.: 50.00%] [G loss: 0.419290]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 70/173 [loss: 0.748514, acc.: 50.00%] [G loss: 0.418792]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 71/173 [loss: 0.748804, acc.: 50.00%] [G loss: 0.416229]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 72/173 [loss: 0.748798, acc.: 50.00%] [G loss: 0.418761]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 73/173 [loss: 0.747744, acc.: 50.00%] [G loss: 0.418909]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 74/173 [loss: 0.749555, acc.: 50.00%] [G loss: 0.417888]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 75/173 [loss: 0.749203, acc.: 50.00%] [G loss: 0.418261]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 76/173 [loss: 0.749260, acc.: 50.00%] [G loss: 0.420335]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 77/173 [loss: 0.750046, acc.: 50.00%] [G loss: 0.420611]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 78/173 [loss: 0.749776, acc.: 50.00%] [G loss: 0.418244]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 79/173 [loss: 0.748904, acc.: 50.00%] [G loss: 0.419025]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 80/173 [loss: 0.747336, acc.: 50.00%] [G loss: 0.420668]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 81/173 [loss: 0.746705, acc.: 50.00%] [G loss: 0.420185]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 82/173 [loss: 0.748598, acc.: 50.00%] [G loss: 0.416627]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 83/173 [loss: 0.749736, acc.: 50.00%] [G loss: 0.418034]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 84/173 [loss: 0.748114, acc.: 50.00%] [G loss: 0.417339]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 85/173 [loss: 0.749497, acc.: 50.00%] [G loss: 0.419804]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 86/173 [loss: 0.749096, acc.: 50.00%] [G loss: 0.419532]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 87/173 [loss: 0.749734, acc.: 50.00%] [G loss: 0.420465]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 88/173 [loss: 0.748443, acc.: 50.00%] [G loss: 0.419610]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 89/173 [loss: 0.747790, acc.: 50.00%] [G loss: 0.419500]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 90/173 [loss: 0.751329, acc.: 50.00%] [G loss: 0.419454]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 91/173 [loss: 0.748073, acc.: 50.00%] [G loss: 0.420588]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 92/173 [loss: 0.748231, acc.: 50.00%] [G loss: 0.418402]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 93/173 [loss: 0.751730, acc.: 50.00%] [G loss: 0.418381]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 94/173 [loss: 0.749503, acc.: 50.00%] [G loss: 0.419695]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 95/173 [loss: 0.749218, acc.: 50.00%] [G loss: 0.419457]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 96/173 [loss: 0.748089, acc.: 50.00%] [G loss: 0.420984]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 97/173 [loss: 0.747230, acc.: 50.00%] [G loss: 0.418264]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 98/173 [loss: 0.749611, acc.: 50.00%] [G loss: 0.420984]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 99/173 [loss: 0.748313, acc.: 50.00%] [G loss: 0.417860]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 100/173 [loss: 0.746153, acc.: 50.00%] [G loss: 0.418740]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 101/173 [loss: 0.748969, acc.: 50.00%] [G loss: 0.418178]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 102/173 [loss: 0.748873, acc.: 50.00%] [G loss: 0.419257]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 103/173 [loss: 0.747999, acc.: 50.00%] [G loss: 0.419448]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 104/173 [loss: 0.748349, acc.: 50.00%] [G loss: 0.418159]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 105/173 [loss: 0.747698, acc.: 50.00%] [G loss: 0.417765]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 106/173 [loss: 0.749136, acc.: 50.00%] [G loss: 0.419600]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 107/173 [loss: 0.748682, acc.: 50.00%] [G loss: 0.420332]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 108/173 [loss: 0.747446, acc.: 50.00%] [G loss: 0.418016]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 109/173 [loss: 0.746200, acc.: 50.00%] [G loss: 0.420076]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 110/173 [loss: 0.747444, acc.: 50.00%] [G loss: 0.418743]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 111/173 [loss: 0.748132, acc.: 50.00%] [G loss: 0.416218]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 112/173 [loss: 0.749884, acc.: 50.00%] [G loss: 0.419124]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 113/173 [loss: 0.749935, acc.: 50.00%] [G loss: 0.419418]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 114/173 [loss: 0.748451, acc.: 50.00%] [G loss: 0.420914]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 115/173 [loss: 0.749649, acc.: 50.00%] [G loss: 0.419619]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 116/173 [loss: 0.749442, acc.: 50.00%] [G loss: 0.420253]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 117/173 [loss: 0.749365, acc.: 50.00%] [G loss: 0.418577]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 118/173 [loss: 0.749135, acc.: 50.00%] [G loss: 0.420933]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 119/173 [loss: 0.749523, acc.: 50.00%] [G loss: 0.418410]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 120/173 [loss: 0.748455, acc.: 50.00%] [G loss: 0.418165]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 121/173 [loss: 0.750128, acc.: 50.00%] [G loss: 0.418450]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 122/173 [loss: 0.750252, acc.: 50.00%] [G loss: 0.417907]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 123/173 [loss: 0.747436, acc.: 50.00%] [G loss: 0.418550]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 124/173 [loss: 0.747152, acc.: 50.00%] [G loss: 0.420731]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 125/173 [loss: 0.747086, acc.: 50.00%] [G loss: 0.419488]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 126/173 [loss: 0.748537, acc.: 50.00%] [G loss: 0.418881]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 127/173 [loss: 0.747761, acc.: 50.00%] [G loss: 0.421799]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 128/173 [loss: 0.747318, acc.: 50.00%] [G loss: 0.420834]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 129/173 [loss: 0.746075, acc.: 50.00%] [G loss: 0.417795]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 130/173 [loss: 0.748102, acc.: 50.00%] [G loss: 0.419257]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 131/173 [loss: 0.747944, acc.: 50.00%] [G loss: 0.416652]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 132/173 [loss: 0.749108, acc.: 50.00%] [G loss: 0.418155]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 133/173 [loss: 0.747616, acc.: 50.00%] [G loss: 0.420412]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 134/173 [loss: 0.748253, acc.: 50.00%] [G loss: 0.417848]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 135/173 [loss: 0.748604, acc.: 50.00%] [G loss: 0.418460]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 136/173 [loss: 0.749639, acc.: 50.00%] [G loss: 0.417666]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 137/173 [loss: 0.746942, acc.: 50.00%] [G loss: 0.416830]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 138/173 [loss: 0.747528, acc.: 50.00%] [G loss: 0.419095]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 139/173 [loss: 0.751398, acc.: 50.00%] [G loss: 0.416775]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 140/173 [loss: 0.750729, acc.: 50.00%] [G loss: 0.418863]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 141/173 [loss: 0.750517, acc.: 50.00%] [G loss: 0.419533]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 142/173 [loss: 0.747721, acc.: 50.00%] [G loss: 0.419260]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 143/173 [loss: 0.747915, acc.: 50.00%] [G loss: 0.417852]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 144/173 [loss: 0.749725, acc.: 50.00%] [G loss: 0.419453]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 145/173 [loss: 0.750132, acc.: 50.00%] [G loss: 0.417366]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 146/173 [loss: 0.749695, acc.: 50.00%] [G loss: 0.419644]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 147/173 [loss: 0.750073, acc.: 50.00%] [G loss: 0.420529]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 148/173 [loss: 0.747552, acc.: 50.00%] [G loss: 0.419767]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 149/173 [loss: 0.750996, acc.: 50.00%] [G loss: 0.417796]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 150/173 [loss: 0.747815, acc.: 50.00%] [G loss: 0.421523]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 151/173 [loss: 0.747500, acc.: 50.00%] [G loss: 0.418501]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 152/173 [loss: 0.750414, acc.: 50.00%] [G loss: 0.418697]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 153/173 [loss: 0.751102, acc.: 50.00%] [G loss: 0.418016]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 154/173 [loss: 0.748304, acc.: 50.00%] [G loss: 0.418294]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 155/173 [loss: 0.747240, acc.: 50.00%] [G loss: 0.417865]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 156/173 [loss: 0.747834, acc.: 50.00%] [G loss: 0.418716]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 157/173 [loss: 0.748969, acc.: 50.00%] [G loss: 0.416208]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 158/173 [loss: 0.751866, acc.: 50.00%] [G loss: 0.420140]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 159/173 [loss: 0.748119, acc.: 50.00%] [G loss: 0.419470]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 160/173 [loss: 0.749887, acc.: 50.00%] [G loss: 0.419434]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 161/173 [loss: 0.749149, acc.: 50.00%] [G loss: 0.418740]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 162/173 [loss: 0.747907, acc.: 50.00%] [G loss: 0.419360]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 163/173 [loss: 0.748238, acc.: 50.00%] [G loss: 0.419178]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 164/173 [loss: 0.747138, acc.: 50.00%] [G loss: 0.419930]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 165/173 [loss: 0.747141, acc.: 50.00%] [G loss: 0.418790]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 166/173 [loss: 0.749716, acc.: 50.00%] [G loss: 0.416995]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 167/173 [loss: 0.748968, acc.: 50.00%] [G loss: 0.420515]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 168/173 [loss: 0.747225, acc.: 50.00%] [G loss: 0.420968]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 169/173 [loss: 0.747800, acc.: 50.00%] [G loss: 0.421107]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 170/173 [loss: 0.748832, acc.: 50.00%] [G loss: 0.419022]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 171/173 [loss: 0.748045, acc.: 50.00%] [G loss: 0.418184]\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Epoch: 17/200  Batch Size: 172/173 [loss: 0.745886, acc.: 50.00%] [G loss: 0.418664]\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "dcgan4 = DCGAN(28,28,1)\n",
    "\n",
    "\n",
    "dcgan4.train(epochs=200, batch_size=512, save_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = Model([z, label], valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = in_shape[0] * in_shape[1]\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        merge = Concatenate()([in_image, li])\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        fe = Dropout(0.4)(fe)\n",
    "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "        model = Model([in_image, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 128 * 7 * 7\n",
    "        gen = Dense(n_nodes)(in_lat)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Reshape((7, 7, 128))(gen) \n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(merge)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes  # Ensure labels are within valid range\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"CGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('CGAN_mnist', exist_ok=True)\n",
    "        fig.savefig(\"CGAN_mnist/CGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=1024, save_interval=1, gen_steps=3):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, y_train[idx]], labels_real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, gen_labels], labels_fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([noise, gen_labels], valid_y)\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "            if (epoch) % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set image dimensions\n",
    "# img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# # Instantiate and train the DCGAN\n",
    "# cgan = CGAN(img_rows, img_cols, channels)\n",
    "# cgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.00002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = self.define_gan(self.generator,self.discriminator)\n",
    "\n",
    "        \n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        fe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(64, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        out1 = Dense(1, activation='sigmoid')(fe)\n",
    "        out2 = Dense(n_classes, activation='softmax')(fe)\n",
    "        model = Model(in_image, [out1, out2])\n",
    "        opt = Adam(lr=0.00002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 512 * 7 * 7\n",
    "        gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Reshape((7, 7, 512))(gen)\n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "        gen = BatchNormalization()(gen)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "        out_layer = Activation('tanh')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "    \n",
    "        # define the combined generator and discriminator model, for updating the generator\n",
    "    def define_gan(self, g_model, d_model):\n",
    "        # make weights in the discriminator not trainable\n",
    "        for layer in d_model.layers:\n",
    "            if not isinstance(layer, BatchNormalization):\n",
    "                layer.trainable = False\n",
    "        # connect the outputs of the generator to the inputs of the discriminator\n",
    "        gan_output = d_model(g_model.output)\n",
    "        # define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "        model = Model(g_model.input, gan_output)\n",
    "        # compile model\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                # axs[i, j].set_title(chr(sampled_labels[cnt][0] + 65))\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"ACGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('ACGAN_mnist_2', exist_ok=True)\n",
    "        fig.savefig(\"ACGAN_mnist_2/ACGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def generate_latent_points(self, latent_dim, n_samples, n_classes=26):\n",
    "        # generate points in the latent space\n",
    "        x_input = randn(latent_dim * n_samples)\n",
    "        # reshape into a batch of inputs for the network\n",
    "        z_input = x_input.reshape(n_samples, latent_dim)\n",
    "        # generate labels\n",
    "        labels = randint(0, n_classes, n_samples)\n",
    "        return [z_input, labels]\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=2056, save_interval=1, gen_steps=1):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, [labels_real, y_train[idx]])\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [labels_fake, gen_labels])\n",
    "                d_loss_1 = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])\n",
    "                d_loss_2 = 0.5 * np.add(d_loss_real[1], d_loss_fake[1])\n",
    "\n",
    "                for _ in range(gen_steps):\n",
    "                    z_input, z_labels = self.generate_latent_points(self.latent_dim, batch_size)\n",
    "                    y_gan = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss 1: {d_loss_1}, D loss 2: {d_loss_2}, G loss: {g_loss}]\")\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 1/43 [D loss 1: 5.07591700553894, D loss 2: 0.8845160901546478, G loss: [3.950083017349243, 0.6920515894889832, 3.2580313682556152]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 2/43 [D loss 1: 5.070025205612183, D loss 2: 0.9115675687789917, G loss: [3.9487624168395996, 0.6902897357940674, 3.2584726810455322]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 3/43 [D loss 1: 5.0332841873168945, D loss 2: 0.8931187093257904, G loss: [3.9468448162078857, 0.6886608600616455, 3.2581839561462402]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 4/43 [D loss 1: 4.989941120147705, D loss 2: 0.8722847700119019, G loss: [3.9449925422668457, 0.6870461702346802, 3.257946491241455]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 5/43 [D loss 1: 4.971191883087158, D loss 2: 0.8598926067352295, G loss: [3.944643497467041, 0.6860059499740601, 3.2586376667022705]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 6/43 [D loss 1: 4.904329538345337, D loss 2: 0.8344956934452057, G loss: [3.9438390731811523, 0.6852750778198242, 3.258563995361328]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 7/43 [D loss 1: 4.913269758224487, D loss 2: 0.8314395546913147, G loss: [3.9424211978912354, 0.6839473843574524, 3.2584738731384277]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 8/43 [D loss 1: 4.860095739364624, D loss 2: 0.7906611859798431, G loss: [3.942145347595215, 0.682774543762207, 3.259370803833008]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 9/43 [D loss 1: 4.84284234046936, D loss 2: 0.7864927053451538, G loss: [3.9409427642822266, 0.6821892261505127, 3.258753538131714]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 10/43 [D loss 1: 4.830370903015137, D loss 2: 0.7657702565193176, G loss: [3.9419467449188232, 0.6818628907203674, 3.2600839138031006]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 11/43 [D loss 1: 4.75626802444458, D loss 2: 0.7507375776767731, G loss: [3.9409122467041016, 0.6806734204292297, 3.2602388858795166]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 12/43 [D loss 1: 4.819644927978516, D loss 2: 0.7463622093200684, G loss: [3.940704584121704, 0.6791061758995056, 3.2615983486175537]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 13/43 [D loss 1: 4.7850682735443115, D loss 2: 0.7359172403812408, G loss: [3.9358577728271484, 0.6789857149124146, 3.2568719387054443]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 14/43 [D loss 1: 4.818985462188721, D loss 2: 0.7449143528938293, G loss: [3.9369213581085205, 0.6776390075683594, 3.259282350540161]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 15/43 [D loss 1: 4.767596483230591, D loss 2: 0.7248322069644928, G loss: [3.939115524291992, 0.6780299544334412, 3.2610855102539062]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 16/43 [D loss 1: 4.743318796157837, D loss 2: 0.7359158396720886, G loss: [3.937574863433838, 0.6769357919692993, 3.260639190673828]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 17/43 [D loss 1: 4.784788608551025, D loss 2: 0.7724103331565857, G loss: [3.9399352073669434, 0.6767373085021973, 3.263197898864746]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 18/43 [D loss 1: 4.7842116355896, D loss 2: 0.777816891670227, G loss: [3.9400033950805664, 0.6766061782836914, 3.263397216796875]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 19/43 [D loss 1: 4.817892789840698, D loss 2: 0.7937978506088257, G loss: [3.9377219676971436, 0.6756420731544495, 3.262079954147339]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 20/43 [D loss 1: 4.747493743896484, D loss 2: 0.7560080587863922, G loss: [3.939810037612915, 0.6751871705055237, 3.264622926712036]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 21/43 [D loss 1: 4.759363174438477, D loss 2: 0.7320486009120941, G loss: [3.9415171146392822, 0.6764867305755615, 3.2650303840637207]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 22/43 [D loss 1: 4.765618801116943, D loss 2: 0.7428616583347321, G loss: [3.933438777923584, 0.6737281680107117, 3.2597105503082275]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 23/43 [D loss 1: 4.787380933761597, D loss 2: 0.7566502094268799, G loss: [3.9395387172698975, 0.6751462817192078, 3.264392375946045]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 24/43 [D loss 1: 4.8054680824279785, D loss 2: 0.7907784283161163, G loss: [3.938406467437744, 0.6756632924079895, 3.2627432346343994]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 25/43 [D loss 1: 4.807112216949463, D loss 2: 0.7919360399246216, G loss: [3.935399055480957, 0.6732443571090698, 3.2621548175811768]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 26/43 [D loss 1: 4.770796537399292, D loss 2: 0.7652708292007446, G loss: [3.943080186843872, 0.6760971546173096, 3.2669830322265625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 27/43 [D loss 1: 4.686900854110718, D loss 2: 0.7383921444416046, G loss: [3.9395689964294434, 0.6747981309890747, 3.264770984649658]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 28/43 [D loss 1: 4.733527421951294, D loss 2: 0.7249582707881927, G loss: [3.9451186656951904, 0.6737236380577087, 3.271394968032837]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 29/43 [D loss 1: 4.704082250595093, D loss 2: 0.7287471294403076, G loss: [3.9416909217834473, 0.6719011068344116, 3.269789934158325]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 30/43 [D loss 1: 4.699909925460815, D loss 2: 0.7033473551273346, G loss: [3.9370062351226807, 0.6720938086509705, 3.2649123668670654]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 31/43 [D loss 1: 4.665231943130493, D loss 2: 0.6946789622306824, G loss: [3.939164638519287, 0.6712476015090942, 3.2679171562194824]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 32/43 [D loss 1: 4.6573731899261475, D loss 2: 0.7030405402183533, G loss: [3.941307544708252, 0.6716607213020325, 3.2696468830108643]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 33/43 [D loss 1: 4.670935153961182, D loss 2: 0.6853354573249817, G loss: [3.9404187202453613, 0.6711811423301697, 3.269237518310547]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 34/43 [D loss 1: 4.634076833724976, D loss 2: 0.7012009918689728, G loss: [3.9431912899017334, 0.6709563732147217, 3.2722349166870117]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 35/43 [D loss 1: 4.681121110916138, D loss 2: 0.7142938673496246, G loss: [3.937673568725586, 0.6722007989883423, 3.265472650527954]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 36/43 [D loss 1: 4.659386157989502, D loss 2: 0.6957628428936005, G loss: [3.938988208770752, 0.6715925335884094, 3.2673957347869873]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 37/43 [D loss 1: 4.529066324234009, D loss 2: 0.6283451616764069, G loss: [3.943176031112671, 0.6705668568611145, 3.272609233856201]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 38/43 [D loss 1: 4.590491056442261, D loss 2: 0.6540825963020325, G loss: [3.936997890472412, 0.6696285009384155, 3.267369270324707]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 39/43 [D loss 1: 4.646396160125732, D loss 2: 0.6629370748996735, G loss: [3.941500663757324, 0.6688576936721802, 3.2726430892944336]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 40/43 [D loss 1: 4.612391948699951, D loss 2: 0.6805338263511658, G loss: [3.944650411605835, 0.6683120131492615, 3.2763383388519287]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 41/43 [D loss 1: 4.6187756061553955, D loss 2: 0.652235209941864, G loss: [3.9449386596679688, 0.669043242931366, 3.275895357131958]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 42/43 [D loss 1: 4.624312877655029, D loss 2: 0.6892585754394531, G loss: [3.9389195442199707, 0.6682676076889038, 3.2706518173217773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 43/43 [D loss 1: 4.645179510116577, D loss 2: 0.6914825439453125, G loss: [3.9379210472106934, 0.6700860857963562, 3.2678349018096924]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 1/43 [D loss 1: 4.597984075546265, D loss 2: 0.701659619808197, G loss: [3.9453446865081787, 0.6686552166938782, 3.2766895294189453]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 2/43 [D loss 1: 4.579174518585205, D loss 2: 0.6677500903606415, G loss: [3.9432485103607178, 0.6689612865447998, 3.274287223815918]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 3/43 [D loss 1: 4.581939220428467, D loss 2: 0.6974158585071564, G loss: [3.937408924102783, 0.66766357421875, 3.269745349884033]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 4/43 [D loss 1: 4.5834736824035645, D loss 2: 0.6845941841602325, G loss: [3.944072723388672, 0.667485237121582, 3.27658748626709]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 5/43 [D loss 1: 4.645793914794922, D loss 2: 0.7234008312225342, G loss: [3.9534480571746826, 0.6659178137779236, 3.2875301837921143]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 6/43 [D loss 1: 4.584415435791016, D loss 2: 0.7069717943668365, G loss: [3.944124460220337, 0.6671634316444397, 3.276961088180542]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 7/43 [D loss 1: 4.58344030380249, D loss 2: 0.7177419364452362, G loss: [3.9404287338256836, 0.6664404273033142, 3.2739882469177246]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 8/43 [D loss 1: 4.572600364685059, D loss 2: 0.6935344338417053, G loss: [3.94962215423584, 0.667811930179596, 3.2818102836608887]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 9/43 [D loss 1: 4.597766399383545, D loss 2: 0.7343233525753021, G loss: [3.9364285469055176, 0.6660501956939697, 3.270378351211548]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 10/43 [D loss 1: 4.608603477478027, D loss 2: 0.7501964271068573, G loss: [3.952569007873535, 0.6695734262466431, 3.2829957008361816]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 11/43 [D loss 1: 4.626301288604736, D loss 2: 0.7561599612236023, G loss: [3.9526877403259277, 0.6680288314819336, 3.284658908843994]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 12/43 [D loss 1: 4.655248165130615, D loss 2: 0.7565250992774963, G loss: [3.957913637161255, 0.6684708595275879, 3.289442777633667]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 13/43 [D loss 1: 4.609448194503784, D loss 2: 0.7700924277305603, G loss: [3.951084613800049, 0.6672946214675903, 3.283790111541748]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 14/43 [D loss 1: 4.61267876625061, D loss 2: 0.7561737596988678, G loss: [3.9475748538970947, 0.6670463681221008, 3.2805285453796387]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 15/43 [D loss 1: 4.597261428833008, D loss 2: 0.7862518429756165, G loss: [3.9510176181793213, 0.6689128279685974, 3.282104730606079]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 16/43 [D loss 1: 4.588409185409546, D loss 2: 0.7674822807312012, G loss: [3.962750196456909, 0.6708889603614807, 3.2918612957000732]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 17/43 [D loss 1: 4.698210954666138, D loss 2: 0.848526656627655, G loss: [3.955287456512451, 0.6687352061271667, 3.2865521907806396]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 18/43 [D loss 1: 4.660617113113403, D loss 2: 0.8248028755187988, G loss: [3.963026285171509, 0.6669012904167175, 3.2961249351501465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 19/43 [D loss 1: 4.606550693511963, D loss 2: 0.8136124908924103, G loss: [3.962890386581421, 0.6724390387535095, 3.2904512882232666]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 20/43 [D loss 1: 4.601467132568359, D loss 2: 0.8063062727451324, G loss: [3.9595818519592285, 0.6725807189941406, 3.287001132965088]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 21/43 [D loss 1: 4.6366119384765625, D loss 2: 0.8023217618465424, G loss: [3.950254440307617, 0.6695722341537476, 3.28068208694458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 22/43 [D loss 1: 4.628800392150879, D loss 2: 0.8323771357536316, G loss: [3.957864284515381, 0.6721450686454773, 3.285719156265259]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 23/43 [D loss 1: 4.642289876937866, D loss 2: 0.838407427072525, G loss: [3.9555206298828125, 0.6718036532402039, 3.283716917037964]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 24/43 [D loss 1: 4.634688854217529, D loss 2: 0.8098551034927368, G loss: [3.964282989501953, 0.67592853307724, 3.2883543968200684]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 25/43 [D loss 1: 4.621780872344971, D loss 2: 0.8352424502372742, G loss: [3.9588799476623535, 0.6727767586708069, 3.2861032485961914]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 26/43 [D loss 1: 4.6294779777526855, D loss 2: 0.8418627083301544, G loss: [3.960911273956299, 0.6703875064849854, 3.2905237674713135]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 27/43 [D loss 1: 4.624643564224243, D loss 2: 0.8348740637302399, G loss: [3.970545768737793, 0.6684086322784424, 3.3021371364593506]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 28/43 [D loss 1: 4.5855560302734375, D loss 2: 0.817063570022583, G loss: [3.965841770172119, 0.6706946492195129, 3.295147180557251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 29/43 [D loss 1: 4.601551532745361, D loss 2: 0.8117220401763916, G loss: [3.968998908996582, 0.6720642447471619, 3.2969346046447754]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 30/43 [D loss 1: 4.633988857269287, D loss 2: 0.8283922672271729, G loss: [3.962944507598877, 0.6735861897468567, 3.289358377456665]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 31/43 [D loss 1: 4.609807729721069, D loss 2: 0.8334704041481018, G loss: [3.970200538635254, 0.678292453289032, 3.291908025741577]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 32/43 [D loss 1: 4.5454089641571045, D loss 2: 0.8019493520259857, G loss: [3.9727156162261963, 0.6728635430335999, 3.299852132797241]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 33/43 [D loss 1: 4.517362594604492, D loss 2: 0.7709218859672546, G loss: [3.9726603031158447, 0.6790807247161865, 3.293579578399658]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 34/43 [D loss 1: 4.529233932495117, D loss 2: 0.7638784348964691, G loss: [3.9883501529693604, 0.6806706190109253, 3.3076794147491455]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 35/43 [D loss 1: 4.540825366973877, D loss 2: 0.7720483541488647, G loss: [3.977139711380005, 0.6801956295967102, 3.2969441413879395]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 36/43 [D loss 1: 4.467360973358154, D loss 2: 0.7484900057315826, G loss: [3.987091064453125, 0.6801263093948364, 3.306964635848999]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 37/43 [D loss 1: 4.485657215118408, D loss 2: 0.7356580197811127, G loss: [3.9873440265655518, 0.6792259812355042, 3.3081181049346924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 38/43 [D loss 1: 4.476463079452515, D loss 2: 0.7365510165691376, G loss: [3.9699597358703613, 0.679696798324585, 3.2902629375457764]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 39/43 [D loss 1: 4.434771537780762, D loss 2: 0.6938517093658447, G loss: [3.988783359527588, 0.6792235374450684, 3.3095595836639404]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 40/43 [D loss 1: 4.4037206172943115, D loss 2: 0.6993520855903625, G loss: [3.984093189239502, 0.6842818856239319, 3.299811363220215]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 41/43 [D loss 1: 4.43590235710144, D loss 2: 0.7036978304386139, G loss: [3.9820632934570312, 0.6792423129081726, 3.302820920944214]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 42/43 [D loss 1: 4.3978376388549805, D loss 2: 0.6854125261306763, G loss: [3.9773850440979004, 0.6827100515365601, 3.294674873352051]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 43/43 [D loss 1: 4.358840823173523, D loss 2: 0.6855015158653259, G loss: [3.979038715362549, 0.6781919598579407, 3.300846815109253]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 1/43 [D loss 1: 4.373663663864136, D loss 2: 0.6881555914878845, G loss: [3.982062816619873, 0.6766807436943054, 3.305382013320923]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 2/43 [D loss 1: 4.403443455696106, D loss 2: 0.7025763392448425, G loss: [3.9787449836730957, 0.6738671660423279, 3.304877758026123]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 3/43 [D loss 1: 4.419425964355469, D loss 2: 0.7092121839523315, G loss: [3.9917585849761963, 0.6821235418319702, 3.3096354007720947]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 4/43 [D loss 1: 4.352279424667358, D loss 2: 0.694986879825592, G loss: [3.987779140472412, 0.6702260971069336, 3.3175530433654785]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 5/43 [D loss 1: 4.417563199996948, D loss 2: 0.7165413498878479, G loss: [3.991927146911621, 0.6794511079788208, 3.31247615814209]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 6/43 [D loss 1: 4.38386070728302, D loss 2: 0.7518024444580078, G loss: [3.986557960510254, 0.6731730699539185, 3.313385009765625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 7/43 [D loss 1: 4.355570316314697, D loss 2: 0.7370861172676086, G loss: [3.9882564544677734, 0.679893970489502, 3.3083627223968506]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 8/43 [D loss 1: 4.42660927772522, D loss 2: 0.7927375435829163, G loss: [4.013779163360596, 0.6848878264427185, 3.3288915157318115]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 9/43 [D loss 1: 4.507183313369751, D loss 2: 0.8725006580352783, G loss: [4.006386756896973, 0.683926522731781, 3.322460412979126]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 10/43 [D loss 1: 4.49257755279541, D loss 2: 0.8451419770717621, G loss: [4.003791809082031, 0.68621826171875, 3.3175737857818604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 11/43 [D loss 1: 4.548160076141357, D loss 2: 0.9028001427650452, G loss: [4.001100540161133, 0.6878042221069336, 3.3132965564727783]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 12/43 [D loss 1: 4.503247380256653, D loss 2: 0.8942381739616394, G loss: [4.000831604003906, 0.6845834255218506, 3.3162479400634766]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 13/43 [D loss 1: 4.481887102127075, D loss 2: 0.8568393588066101, G loss: [4.0073089599609375, 0.6913287043571472, 3.3159804344177246]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 14/43 [D loss 1: 4.414548397064209, D loss 2: 0.8293868601322174, G loss: [3.9973106384277344, 0.6870320439338684, 3.31027889251709]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 15/43 [D loss 1: 4.449601411819458, D loss 2: 0.8373588919639587, G loss: [4.011338233947754, 0.6900577545166016, 3.3212802410125732]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 16/43 [D loss 1: 4.457760334014893, D loss 2: 0.8531308770179749, G loss: [4.014747619628906, 0.6886811256408691, 3.326066255569458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 17/43 [D loss 1: 4.458022356033325, D loss 2: 0.8590887188911438, G loss: [4.030500411987305, 0.6926407814025879, 3.337859630584717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 18/43 [D loss 1: 4.38360333442688, D loss 2: 0.8350542783737183, G loss: [4.0072221755981445, 0.6914150714874268, 3.3158071041107178]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 19/43 [D loss 1: 4.380320310592651, D loss 2: 0.8196249604225159, G loss: [4.010223865509033, 0.6889923214912415, 3.3212316036224365]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 20/43 [D loss 1: 4.396154880523682, D loss 2: 0.8562540709972382, G loss: [4.022914886474609, 0.6902017593383789, 3.3327128887176514]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 21/43 [D loss 1: 4.395838141441345, D loss 2: 0.8450175523757935, G loss: [4.006133079528809, 0.6833035349845886, 3.3228297233581543]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 22/43 [D loss 1: 4.3665231466293335, D loss 2: 0.8301506638526917, G loss: [4.012240409851074, 0.6902287006378174, 3.322011947631836]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 23/43 [D loss 1: 4.3612518310546875, D loss 2: 0.8410300612449646, G loss: [4.0261406898498535, 0.6928794980049133, 3.333261013031006]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 24/43 [D loss 1: 4.402194857597351, D loss 2: 0.8331001698970795, G loss: [4.020416736602783, 0.6915480494499207, 3.328868865966797]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 25/43 [D loss 1: 4.336475610733032, D loss 2: 0.8240707814693451, G loss: [4.0082573890686035, 0.6954164505004883, 3.3128409385681152]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 26/43 [D loss 1: 4.330551266670227, D loss 2: 0.8231146335601807, G loss: [4.016005516052246, 0.6950288414955139, 3.320976734161377]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 27/43 [D loss 1: 4.385509967803955, D loss 2: 0.8506339490413666, G loss: [4.037339210510254, 0.6960027813911438, 3.341336488723755]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 28/43 [D loss 1: 4.3781352043151855, D loss 2: 0.8672372400760651, G loss: [4.022403717041016, 0.6939507722854614, 3.3284528255462646]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 29/43 [D loss 1: 4.386341333389282, D loss 2: 0.8731017708778381, G loss: [4.022605895996094, 0.6875107288360596, 3.3350954055786133]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 30/43 [D loss 1: 4.352695941925049, D loss 2: 0.8586775064468384, G loss: [4.018411159515381, 0.6912834644317627, 3.327127695083618]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 31/43 [D loss 1: 4.398108720779419, D loss 2: 0.909452885389328, G loss: [4.005258560180664, 0.6878569722175598, 3.31740140914917]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 32/43 [D loss 1: 4.413248538970947, D loss 2: 0.9157793521881104, G loss: [4.003527641296387, 0.6815061569213867, 3.322021245956421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 33/43 [D loss 1: 4.3311299085617065, D loss 2: 0.8800538778305054, G loss: [4.011931419372559, 0.6869534850120544, 3.3249778747558594]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 34/43 [D loss 1: 4.442421197891235, D loss 2: 0.9128846824169159, G loss: [4.014307498931885, 0.6801998019218445, 3.3341076374053955]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 35/43 [D loss 1: 4.366341829299927, D loss 2: 0.8969854116439819, G loss: [3.999386787414551, 0.6763184070587158, 3.323068380355835]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 36/43 [D loss 1: 4.381929397583008, D loss 2: 0.9119778573513031, G loss: [4.0034942626953125, 0.6789329051971436, 3.324561357498169]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 37/43 [D loss 1: 4.364712595939636, D loss 2: 0.9127121269702911, G loss: [4.003266334533691, 0.6680881977081299, 3.3351778984069824]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 38/43 [D loss 1: 4.365118861198425, D loss 2: 0.9498719274997711, G loss: [3.984997272491455, 0.6691372990608215, 3.315859794616699]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 39/43 [D loss 1: 4.39783239364624, D loss 2: 0.9451631307601929, G loss: [3.9955902099609375, 0.6746546030044556, 3.3209354877471924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 40/43 [D loss 1: 4.351737141609192, D loss 2: 0.914695680141449, G loss: [4.000185966491699, 0.674048125743866, 3.3261380195617676]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 41/43 [D loss 1: 4.33185076713562, D loss 2: 0.9114388525485992, G loss: [4.015169143676758, 0.6763516664505005, 3.3388173580169678]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 42/43 [D loss 1: 4.293276309967041, D loss 2: 0.8884410858154297, G loss: [3.9980099201202393, 0.6795349717140198, 3.318474769592285]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 43/43 [D loss 1: 4.231067419052124, D loss 2: 0.8533658981323242, G loss: [4.005630970001221, 0.6788656115531921, 3.326765298843384]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 1/43 [D loss 1: 4.2544344663619995, D loss 2: 0.8423746824264526, G loss: [3.9945125579833984, 0.6736536026000977, 3.320858955383301]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 2/43 [D loss 1: 4.22951340675354, D loss 2: 0.8360477685928345, G loss: [4.008101940155029, 0.681879997253418, 3.3262219429016113]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 3/43 [D loss 1: 4.194126129150391, D loss 2: 0.8338815569877625, G loss: [4.005125045776367, 0.6776587963104248, 3.3274664878845215]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 4/43 [D loss 1: 4.202982664108276, D loss 2: 0.8158175647258759, G loss: [4.001763343811035, 0.6714491844177246, 3.3303139209747314]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 5/43 [D loss 1: 4.123382329940796, D loss 2: 0.7920558750629425, G loss: [3.9997758865356445, 0.6724591255187988, 3.3273165225982666]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 6/43 [D loss 1: 4.135753750801086, D loss 2: 0.8169188797473907, G loss: [3.982945442199707, 0.6693977117538452, 3.3135478496551514]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 7/43 [D loss 1: 4.116321325302124, D loss 2: 0.7963284552097321, G loss: [3.985835552215576, 0.6728723049163818, 3.3129634857177734]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 8/43 [D loss 1: 4.065392374992371, D loss 2: 0.781928539276123, G loss: [3.9861092567443848, 0.6733179092407227, 3.312791109085083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 9/43 [D loss 1: 4.046787977218628, D loss 2: 0.7852844595909119, G loss: [3.976653575897217, 0.6752040982246399, 3.3014495372772217]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 10/43 [D loss 1: 4.047045946121216, D loss 2: 0.8046004772186279, G loss: [3.977041721343994, 0.671019971370697, 3.3060216903686523]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 11/43 [D loss 1: 4.036361575126648, D loss 2: 0.8049847185611725, G loss: [3.961948871612549, 0.6651324033737183, 3.296816349029541]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 12/43 [D loss 1: 3.979491949081421, D loss 2: 0.8283074498176575, G loss: [3.9838504791259766, 0.6675645112991333, 3.316286087036133]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 13/43 [D loss 1: 3.9754912853240967, D loss 2: 0.8024247288703918, G loss: [3.949127674102783, 0.6673444509506226, 3.281783103942871]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 14/43 [D loss 1: 3.914710760116577, D loss 2: 0.7863702178001404, G loss: [3.970275402069092, 0.6744281053543091, 3.2958474159240723]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 15/43 [D loss 1: 3.884441614151001, D loss 2: 0.7769342660903931, G loss: [3.932698965072632, 0.6671295166015625, 3.2655694484710693]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 16/43 [D loss 1: 3.9387084245681763, D loss 2: 0.8057944774627686, G loss: [3.9327564239501953, 0.6572967767715454, 3.2754595279693604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 17/43 [D loss 1: 3.7908931970596313, D loss 2: 0.7412859797477722, G loss: [3.922395706176758, 0.6738914251327515, 3.248504161834717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 18/43 [D loss 1: 3.7801620960235596, D loss 2: 0.7911359369754791, G loss: [3.9238786697387695, 0.6540050506591797, 3.26987361907959]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 19/43 [D loss 1: 3.7879180908203125, D loss 2: 0.8015595078468323, G loss: [3.8688724040985107, 0.6511021256446838, 3.2177703380584717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 20/43 [D loss 1: 3.70202100276947, D loss 2: 0.7759096026420593, G loss: [3.8678340911865234, 0.6456192135810852, 3.222214937210083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 21/43 [D loss 1: 3.663770079612732, D loss 2: 0.7673473954200745, G loss: [3.8712456226348877, 0.6573376655578613, 3.2139079570770264]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 22/43 [D loss 1: 3.7008044719696045, D loss 2: 0.8393977880477905, G loss: [3.836562156677246, 0.6461362838745117, 3.1904258728027344]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 23/43 [D loss 1: 3.59471595287323, D loss 2: 0.7414770424365997, G loss: [3.785770893096924, 0.6547909379005432, 3.1309800148010254]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 24/43 [D loss 1: 3.5222641229629517, D loss 2: 0.8102940618991852, G loss: [3.785726547241211, 0.6424347758293152, 3.143291711807251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 25/43 [D loss 1: 3.508444309234619, D loss 2: 0.7803544998168945, G loss: [3.7502353191375732, 0.6570368409156799, 3.093198537826538]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 26/43 [D loss 1: 3.478851079940796, D loss 2: 0.8280772268772125, G loss: [3.6937661170959473, 0.6353945732116699, 3.0583715438842773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 27/43 [D loss 1: 3.333854556083679, D loss 2: 0.7399417161941528, G loss: [3.6775259971618652, 0.6452388167381287, 3.032287120819092]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 28/43 [D loss 1: 3.3328038454055786, D loss 2: 0.8293476104736328, G loss: [3.6424508094787598, 0.6392431259155273, 3.0032076835632324]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 29/43 [D loss 1: 3.3354848623275757, D loss 2: 0.717313677072525, G loss: [3.635542392730713, 0.6579939723014832, 2.977548360824585]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 30/43 [D loss 1: 3.271485686302185, D loss 2: 0.7788122594356537, G loss: [3.573418140411377, 0.6395107507705688, 2.9339072704315186]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 31/43 [D loss 1: 3.2074555158615112, D loss 2: 0.7717618048191071, G loss: [3.5301544666290283, 0.6399485468864441, 2.8902058601379395]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 32/43 [D loss 1: 3.218937873840332, D loss 2: 0.7458419501781464, G loss: [3.4901304244995117, 0.6412101984024048, 2.8489201068878174]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 33/43 [D loss 1: 3.1236408948898315, D loss 2: 0.7819456458091736, G loss: [3.4287631511688232, 0.6266705989837646, 2.8020925521850586]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 34/43 [D loss 1: 3.0740723609924316, D loss 2: 0.7643399238586426, G loss: [3.4031777381896973, 0.6418179273605347, 2.761359930038452]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 35/43 [D loss 1: 3.055077910423279, D loss 2: 0.6977293789386749, G loss: [3.3267462253570557, 0.6374773979187012, 2.6892688274383545]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 36/43 [D loss 1: 3.0500491857528687, D loss 2: 0.7147528827190399, G loss: [3.2994890213012695, 0.6342195868492126, 2.665269374847412]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 37/43 [D loss 1: 2.989657998085022, D loss 2: 0.7521062195301056, G loss: [3.2366950511932373, 0.631417453289032, 2.6052775382995605]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 38/43 [D loss 1: 2.974358558654785, D loss 2: 0.6981134116649628, G loss: [3.191150188446045, 0.6245728731155396, 2.566577434539795]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 39/43 [D loss 1: 2.8904061317443848, D loss 2: 0.7682191431522369, G loss: [3.1419436931610107, 0.6206738948822021, 2.5212697982788086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 40/43 [D loss 1: 2.891229033470154, D loss 2: 0.7198057174682617, G loss: [3.094831943511963, 0.6165229082107544, 2.478309154510498]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 41/43 [D loss 1: 2.81144917011261, D loss 2: 0.7267067730426788, G loss: [3.0592000484466553, 0.622072160243988, 2.4371278285980225]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 42/43 [D loss 1: 2.7979131937026978, D loss 2: 0.6869189739227295, G loss: [3.032911539077759, 0.631696879863739, 2.401214599609375]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 43/43 [D loss 1: 2.735121726989746, D loss 2: 0.7754694521427155, G loss: [2.992255926132202, 0.6184570789337158, 2.3737988471984863]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 1/43 [D loss 1: 2.755034923553467, D loss 2: 0.7661198079586029, G loss: [2.9822981357574463, 0.6309349536895752, 2.351363182067871]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 2/43 [D loss 1: 2.7023967504501343, D loss 2: 0.7033170163631439, G loss: [2.931335926055908, 0.6256457567214966, 2.305690288543701]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 3/43 [D loss 1: 2.6658767461776733, D loss 2: 0.752815455198288, G loss: [2.8894617557525635, 0.6272035241127014, 2.262258291244507]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 4/43 [D loss 1: 2.6844273805618286, D loss 2: 0.7524745464324951, G loss: [2.8407530784606934, 0.6365647912025452, 2.204188346862793]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 5/43 [D loss 1: 2.663143038749695, D loss 2: 0.8152001202106476, G loss: [2.8338828086853027, 0.628180980682373, 2.2057018280029297]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 6/43 [D loss 1: 2.6321370601654053, D loss 2: 0.7029477059841156, G loss: [2.8074660301208496, 0.6554361581802368, 2.1520297527313232]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 7/43 [D loss 1: 2.6002215147018433, D loss 2: 0.7620640993118286, G loss: [2.790310859680176, 0.638214111328125, 2.152096748352051]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 8/43 [D loss 1: 2.605883479118347, D loss 2: 0.6425932347774506, G loss: [2.7478089332580566, 0.6481643915176392, 2.099644422531128]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 9/43 [D loss 1: 2.6175973415374756, D loss 2: 0.8711031675338745, G loss: [2.7134106159210205, 0.6233005523681641, 2.0901100635528564]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 10/43 [D loss 1: 2.6261366605758667, D loss 2: 0.5791057199239731, G loss: [2.663585901260376, 0.6509396433830261, 2.012646198272705]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 11/43 [D loss 1: 2.6338138580322266, D loss 2: 0.9201488792896271, G loss: [2.658010244369507, 0.6282238364219666, 2.0297863483428955]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 12/43 [D loss 1: 2.4492355585098267, D loss 2: 0.6536107957363129, G loss: [2.622779369354248, 0.6564117670059204, 1.966367483139038]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 13/43 [D loss 1: 2.489737391471863, D loss 2: 0.6646066606044769, G loss: [2.56148624420166, 0.6376429200172424, 1.923843264579773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 14/43 [D loss 1: 2.4673041105270386, D loss 2: 0.8554222881793976, G loss: [2.5178284645080566, 0.6280264854431152, 1.889802098274231]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 15/43 [D loss 1: 2.5013532638549805, D loss 2: 0.6249277293682098, G loss: [2.4848787784576416, 0.6559663414955139, 1.8289124965667725]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 16/43 [D loss 1: 2.439959764480591, D loss 2: 0.8060529232025146, G loss: [2.4653944969177246, 0.6303697824478149, 1.8350247144699097]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 17/43 [D loss 1: 2.422618865966797, D loss 2: 0.6683896481990814, G loss: [2.412339687347412, 0.6435807943344116, 1.768758773803711]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 18/43 [D loss 1: 2.4273314476013184, D loss 2: 0.816843569278717, G loss: [2.4112377166748047, 0.6252626776695251, 1.7859749794006348]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 19/43 [D loss 1: 2.4145073890686035, D loss 2: 0.6386728286743164, G loss: [2.3908884525299072, 0.653020977973938, 1.7378674745559692]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 20/43 [D loss 1: 2.384902060031891, D loss 2: 0.815075010061264, G loss: [2.3499202728271484, 0.6314993500709534, 1.7184208631515503]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 21/43 [D loss 1: 2.3364514112472534, D loss 2: 0.698426365852356, G loss: [2.3258273601531982, 0.6533212661743164, 1.6725060939788818]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 22/43 [D loss 1: 2.314793348312378, D loss 2: 0.7206433415412903, G loss: [2.2730860710144043, 0.6440277695655823, 1.6290583610534668]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 23/43 [D loss 1: 2.378523051738739, D loss 2: 0.7817190885543823, G loss: [2.2679481506347656, 0.636906623840332, 1.6310416460037231]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 24/43 [D loss 1: 2.2996007204055786, D loss 2: 0.6912529170513153, G loss: [2.242372751235962, 0.6431754231452942, 1.5991973876953125]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 25/43 [D loss 1: 2.2903682589530945, D loss 2: 0.832411527633667, G loss: [2.2138047218322754, 0.6429235339164734, 1.5708811283111572]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 26/43 [D loss 1: 2.286483407020569, D loss 2: 0.6985282003879547, G loss: [2.1754159927368164, 0.6598260402679443, 1.5155900716781616]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 27/43 [D loss 1: 2.291093111038208, D loss 2: 0.7494343221187592, G loss: [2.1464526653289795, 0.6490013003349304, 1.4974514245986938]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 28/43 [D loss 1: 2.269552707672119, D loss 2: 0.7771843373775482, G loss: [2.143981695175171, 0.6393894553184509, 1.5045922994613647]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 29/43 [D loss 1: 2.220482110977173, D loss 2: 0.7255133986473083, G loss: [2.0964949131011963, 0.6529226899147034, 1.4435722827911377]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 30/43 [D loss 1: 2.261390447616577, D loss 2: 0.7923220098018646, G loss: [2.0895581245422363, 0.6498252749443054, 1.4397327899932861]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 31/43 [D loss 1: 2.2039144039154053, D loss 2: 0.6810895800590515, G loss: [2.070728302001953, 0.6596585512161255, 1.4110697507858276]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 32/43 [D loss 1: 2.202182948589325, D loss 2: 0.7894690930843353, G loss: [2.0378518104553223, 0.6482738852500916, 1.389577865600586]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 33/43 [D loss 1: 2.196757435798645, D loss 2: 0.6816384792327881, G loss: [2.008333921432495, 0.6652175188064575, 1.3431164026260376]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 34/43 [D loss 1: 2.198372006416321, D loss 2: 0.8023935556411743, G loss: [2.008026599884033, 0.6559169888496399, 1.3521095514297485]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 35/43 [D loss 1: 2.1988377571105957, D loss 2: 0.6034373939037323, G loss: [1.961071491241455, 0.6614496111869812, 1.2996219396591187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 36/43 [D loss 1: 2.2936493158340454, D loss 2: 0.9359251856803894, G loss: [1.9882441759109497, 0.6355146169662476, 1.3527295589447021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 37/43 [D loss 1: 2.471311569213867, D loss 2: 0.4759421795606613, G loss: [1.9480582475662231, 0.6969118118286133, 1.2511464357376099]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 38/43 [D loss 1: 2.3771783113479614, D loss 2: 0.9619829654693604, G loss: [1.9217839241027832, 0.6059336066246033, 1.3158502578735352]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 39/43 [D loss 1: 2.118688225746155, D loss 2: 0.6642458140850067, G loss: [1.871199607849121, 0.6609563827514648, 1.2102432250976562]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 40/43 [D loss 1: 2.1453664302825928, D loss 2: 0.7169196605682373, G loss: [1.8761515617370605, 0.6666943430900574, 1.209457278251648]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 41/43 [D loss 1: 2.142163634300232, D loss 2: 0.8362297117710114, G loss: [1.8077304363250732, 0.6391295194625854, 1.1686009168624878]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 42/43 [D loss 1: 2.0916590690612793, D loss 2: 0.7555460929870605, G loss: [1.8164211511611938, 0.6513164043426514, 1.1651047468185425]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 43/43 [D loss 1: 2.1004804372787476, D loss 2: 0.7250344455242157, G loss: [1.7737102508544922, 0.6537391543388367, 1.1199711561203003]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 1/43 [D loss 1: 2.087941527366638, D loss 2: 0.7722101509571075, G loss: [1.7594428062438965, 0.6448076963424683, 1.1146351099014282]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 2/43 [D loss 1: 2.109066963195801, D loss 2: 0.7346806228160858, G loss: [1.7305200099945068, 0.6673351526260376, 1.0631848573684692]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 3/43 [D loss 1: 2.07627671957016, D loss 2: 0.7909389734268188, G loss: [1.703438401222229, 0.6511873006820679, 1.0522511005401611]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 4/43 [D loss 1: 2.0784939527511597, D loss 2: 0.7516616582870483, G loss: [1.6877415180206299, 0.6683340072631836, 1.0194075107574463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 5/43 [D loss 1: 2.0500649213790894, D loss 2: 0.7677445411682129, G loss: [1.6870489120483398, 0.6667147278785706, 1.020334243774414]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 6/43 [D loss 1: 2.0638004541397095, D loss 2: 0.7694892585277557, G loss: [1.663046956062317, 0.6573113203048706, 1.0057356357574463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 7/43 [D loss 1: 2.0782466530799866, D loss 2: 0.6754185259342194, G loss: [1.6332485675811768, 0.6503310203552246, 0.9829175472259521]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 8/43 [D loss 1: 2.075573980808258, D loss 2: 0.8036783039569855, G loss: [1.606102705001831, 0.6427164077758789, 0.9633862972259521]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 9/43 [D loss 1: 2.0428569316864014, D loss 2: 0.7208528816699982, G loss: [1.617969274520874, 0.6611344814300537, 0.9568347930908203]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 10/43 [D loss 1: 2.011373519897461, D loss 2: 0.7023442685604095, G loss: [1.5642205476760864, 0.6483995914459229, 0.9158209562301636]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 11/43 [D loss 1: 2.074004113674164, D loss 2: 0.8214608728885651, G loss: [1.5454511642456055, 0.6439377069473267, 0.9015134572982788]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 12/43 [D loss 1: 2.001237213611603, D loss 2: 0.7424117922782898, G loss: [1.5381132364273071, 0.6578961610794067, 0.8802170753479004]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 13/43 [D loss 1: 1.9562379121780396, D loss 2: 0.7705677449703217, G loss: [1.534883975982666, 0.6410576105117798, 0.8938263654708862]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 14/43 [D loss 1: 2.015584707260132, D loss 2: 0.669035941362381, G loss: [1.5110775232315063, 0.6452387571334839, 0.8658387660980225]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 15/43 [D loss 1: 2.0010223388671875, D loss 2: 0.7988221049308777, G loss: [1.481903314590454, 0.6333471536636353, 0.8485561013221741]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 16/43 [D loss 1: 2.0103918313980103, D loss 2: 0.7931253015995026, G loss: [1.4678070545196533, 0.6391561031341553, 0.828650951385498]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 17/43 [D loss 1: 2.0332785844802856, D loss 2: 0.6630061864852905, G loss: [1.4517033100128174, 0.6586966514587402, 0.7930067181587219]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 18/43 [D loss 1: 2.07503879070282, D loss 2: 0.8686671257019043, G loss: [1.434328317642212, 0.6251195669174194, 0.8092087507247925]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 19/43 [D loss 1: 1.935235619544983, D loss 2: 0.6791030466556549, G loss: [1.4262704849243164, 0.6522660255432129, 0.7740045189857483]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 20/43 [D loss 1: 1.9700654745101929, D loss 2: 0.840702086687088, G loss: [1.4121043682098389, 0.6336601972579956, 0.7784441709518433]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 21/43 [D loss 1: 1.9416512250900269, D loss 2: 0.7059975862503052, G loss: [1.4203767776489258, 0.6572256088256836, 0.763151228427887]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 22/43 [D loss 1: 2.011933922767639, D loss 2: 0.7477594614028931, G loss: [1.3943777084350586, 0.64838045835495, 0.7459972500801086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 23/43 [D loss 1: 1.9619499444961548, D loss 2: 0.7749089598655701, G loss: [1.3890873193740845, 0.6549704074859619, 0.7341169118881226]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 24/43 [D loss 1: 1.9795133471488953, D loss 2: 0.8039898872375488, G loss: [1.360194444656372, 0.6486713886260986, 0.7115231156349182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 25/43 [D loss 1: 1.9512619972229004, D loss 2: 0.6986000835895538, G loss: [1.3371374607086182, 0.6526902914047241, 0.6844472289085388]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 26/43 [D loss 1: 1.9852479696273804, D loss 2: 0.8284621834754944, G loss: [1.3223621845245361, 0.6227440237998962, 0.6996181607246399]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 27/43 [D loss 1: 1.9358327388763428, D loss 2: 0.6525574922561646, G loss: [1.3196520805358887, 0.6551591157913208, 0.6644929647445679]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 28/43 [D loss 1: 1.9849010705947876, D loss 2: 0.8573966324329376, G loss: [1.2851142883300781, 0.6406645178794861, 0.6444498300552368]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 29/43 [D loss 1: 1.8794505596160889, D loss 2: 0.7131164968013763, G loss: [1.2937943935394287, 0.6560041904449463, 0.6377901434898376]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 30/43 [D loss 1: 1.9513424634933472, D loss 2: 0.7453084588050842, G loss: [1.2566444873809814, 0.6341657042503357, 0.6224787831306458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 31/43 [D loss 1: 1.9387798309326172, D loss 2: 0.7634325325489044, G loss: [1.26856529712677, 0.640305757522583, 0.628259539604187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 32/43 [D loss 1: 1.9528908729553223, D loss 2: 0.8041618764400482, G loss: [1.2508342266082764, 0.6403567790985107, 0.6104775071144104]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 33/43 [D loss 1: 1.9053893089294434, D loss 2: 0.7001895010471344, G loss: [1.2327682971954346, 0.6512759327888489, 0.5814924240112305]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 34/43 [D loss 1: 1.9853414297103882, D loss 2: 0.8561705350875854, G loss: [1.2288211584091187, 0.6195201277732849, 0.6093010306358337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 35/43 [D loss 1: 1.9188839793205261, D loss 2: 0.60556660592556, G loss: [1.2253494262695312, 0.6409163475036621, 0.5844331383705139]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 36/43 [D loss 1: 1.977709174156189, D loss 2: 0.901260495185852, G loss: [1.2137279510498047, 0.6264976859092712, 0.5872302651405334]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 37/43 [D loss 1: 1.8851234316825867, D loss 2: 0.6546306312084198, G loss: [1.1899240016937256, 0.6512103080749512, 0.5387137532234192]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 38/43 [D loss 1: 1.8881497979164124, D loss 2: 0.7921743392944336, G loss: [1.1615557670593262, 0.6192752122879028, 0.5422805547714233]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 39/43 [D loss 1: 1.8847246170043945, D loss 2: 0.7592438459396362, G loss: [1.172060251235962, 0.64235919713974, 0.5297011137008667]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 40/43 [D loss 1: 1.9524157047271729, D loss 2: 0.780724048614502, G loss: [1.135827660560608, 0.6231492161750793, 0.5126784443855286]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 41/43 [D loss 1: 1.9293287992477417, D loss 2: 0.8068923652172089, G loss: [1.1359665393829346, 0.6113203167915344, 0.5246461629867554]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 42/43 [D loss 1: 1.8975951671600342, D loss 2: 0.7949745953083038, G loss: [1.1493972539901733, 0.6309219002723694, 0.518475353717804]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 43/43 [D loss 1: 1.9079289436340332, D loss 2: 0.750560849905014, G loss: [1.1051846742630005, 0.6279304027557373, 0.4772542715072632]]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 1/43 [D loss 1: 1.8669191598892212, D loss 2: 0.8055874407291412, G loss: [1.1135352849960327, 0.6272737383842468, 0.4862615764141083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 2/43 [D loss 1: 1.8670032024383545, D loss 2: 0.7396155297756195, G loss: [1.1182620525360107, 0.6419243216514587, 0.4763377010822296]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 3/43 [D loss 1: 1.8823598623275757, D loss 2: 0.8060621321201324, G loss: [1.097130298614502, 0.6216424703598022, 0.4754878878593445]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 4/43 [D loss 1: 1.8400911688804626, D loss 2: 0.7529380023479462, G loss: [1.0820233821868896, 0.6242490410804749, 0.45777428150177]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 5/43 [D loss 1: 1.87808758020401, D loss 2: 0.7690450251102448, G loss: [1.0818312168121338, 0.634959876537323, 0.44687139987945557]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 6/43 [D loss 1: 1.9230499863624573, D loss 2: 0.7870380580425262, G loss: [1.0580816268920898, 0.6280429363250732, 0.4300386607646942]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 7/43 [D loss 1: 1.8533477187156677, D loss 2: 0.7479755282402039, G loss: [1.058266282081604, 0.6194839477539062, 0.43878233432769775]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 8/43 [D loss 1: 1.8351450562477112, D loss 2: 0.7199176847934723, G loss: [1.0757439136505127, 0.6488706469535828, 0.4268733263015747]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 9/43 [D loss 1: 1.8422815203666687, D loss 2: 0.7131587564945221, G loss: [1.034024953842163, 0.6186196804046631, 0.4154052138328552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 10/43 [D loss 1: 1.9484126567840576, D loss 2: 0.8767369091510773, G loss: [1.0236454010009766, 0.617771327495575, 0.40587401390075684]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 11/43 [D loss 1: 1.835508018732071, D loss 2: 0.7612493634223938, G loss: [1.0206931829452515, 0.6198238134384155, 0.40086936950683594]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 12/43 [D loss 1: 1.8466260433197021, D loss 2: 0.7453805208206177, G loss: [1.034809947013855, 0.6323552131652832, 0.40245476365089417]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 13/43 [D loss 1: 1.8328603506088257, D loss 2: 0.7527886629104614, G loss: [1.0344831943511963, 0.6418571472167969, 0.392626017332077]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 14/43 [D loss 1: 1.8733252882957458, D loss 2: 0.8175574541091919, G loss: [1.0085660219192505, 0.6225773096084595, 0.3859887421131134]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 15/43 [D loss 1: 1.8121511936187744, D loss 2: 0.7358048260211945, G loss: [1.0036147832870483, 0.6262210011482239, 0.3773937523365021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 16/43 [D loss 1: 1.8851070404052734, D loss 2: 0.7920580208301544, G loss: [0.9927977323532104, 0.62924724817276, 0.36355048418045044]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 17/43 [D loss 1: 1.9110975861549377, D loss 2: 0.8938256204128265, G loss: [1.00752592086792, 0.6377182602882385, 0.36980772018432617]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 18/43 [D loss 1: 1.858847737312317, D loss 2: 0.6046361476182938, G loss: [0.9918704032897949, 0.6414027810096741, 0.35046759247779846]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 19/43 [D loss 1: 1.9685584902763367, D loss 2: 0.9413855373859406, G loss: [0.9860180616378784, 0.6094540953636169, 0.37656399607658386]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 20/43 [D loss 1: 1.7964401245117188, D loss 2: 0.6249818652868271, G loss: [0.9875273704528809, 0.6610220670700073, 0.3265053331851959]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 21/43 [D loss 1: 1.8725274205207825, D loss 2: 0.8308491408824921, G loss: [0.9710283875465393, 0.6396497488021851, 0.33137863874435425]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 22/43 [D loss 1: 1.8938166499137878, D loss 2: 0.8123546540737152, G loss: [0.9327107667922974, 0.6139203906059265, 0.31879037618637085]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 23/43 [D loss 1: 1.8655627369880676, D loss 2: 0.7979000508785248, G loss: [0.9467611312866211, 0.6278207898139954, 0.31894031167030334]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 24/43 [D loss 1: 1.8431596159934998, D loss 2: 0.8254736065864563, G loss: [0.9345020651817322, 0.619499921798706, 0.3150021433830261]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 25/43 [D loss 1: 1.8932576179504395, D loss 2: 0.887678474187851, G loss: [0.9563455581665039, 0.6417316198348999, 0.3146139681339264]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 26/43 [D loss 1: 1.837461918592453, D loss 2: 0.7630529701709747, G loss: [0.9448081851005554, 0.6427837014198303, 0.3020244836807251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 27/43 [D loss 1: 1.8428005874156952, D loss 2: 0.7934383451938629, G loss: [0.9237241744995117, 0.6370565891265869, 0.2866675853729248]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 28/43 [D loss 1: 1.8789138793945312, D loss 2: 0.8243609964847565, G loss: [0.9337028861045837, 0.6376146674156189, 0.29608821868896484]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 29/43 [D loss 1: 1.7938556969165802, D loss 2: 0.7588140070438385, G loss: [0.9454946517944336, 0.657065212726593, 0.28842946887016296]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 30/43 [D loss 1: 1.9070464968681335, D loss 2: 0.8739480078220367, G loss: [0.9487965703010559, 0.6509450078010559, 0.2978515625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 31/43 [D loss 1: 1.7879997491836548, D loss 2: 0.7239483594894409, G loss: [0.9332796335220337, 0.6529651880264282, 0.2803144156932831]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 32/43 [D loss 1: 1.8311635851860046, D loss 2: 0.8248659372329712, G loss: [0.9463552832603455, 0.667258083820343, 0.27909719944000244]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 33/43 [D loss 1: 1.7949832677841187, D loss 2: 0.7236174643039703, G loss: [0.9531057476997375, 0.6735645532608032, 0.2795411944389343]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 34/43 [D loss 1: 1.752824455499649, D loss 2: 0.7265817821025848, G loss: [0.9270728230476379, 0.6517965793609619, 0.275276243686676]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 35/43 [D loss 1: 1.9262471795082092, D loss 2: 0.9001048505306244, G loss: [0.9326790571212769, 0.656324565410614, 0.27635452151298523]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 36/43 [D loss 1: 1.7981088161468506, D loss 2: 0.6728216111660004, G loss: [0.9062274694442749, 0.6412028074264526, 0.26502469182014465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 37/43 [D loss 1: 1.936017394065857, D loss 2: 0.9399389624595642, G loss: [0.9202841520309448, 0.6459618210792542, 0.2743223309516907]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 38/43 [D loss 1: 1.7695072889328003, D loss 2: 0.6852979958057404, G loss: [0.9065102338790894, 0.6551553010940552, 0.25135496258735657]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 39/43 [D loss 1: 1.8798733949661255, D loss 2: 0.8411318063735962, G loss: [0.900032639503479, 0.6432381272315979, 0.2567945122718811]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 40/43 [D loss 1: 1.846584975719452, D loss 2: 0.8309854567050934, G loss: [0.8842452168464661, 0.6456192135810852, 0.23862600326538086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 41/43 [D loss 1: 1.8260865807533264, D loss 2: 0.802596390247345, G loss: [0.8873146176338196, 0.6505671739578247, 0.23674745857715607]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 42/43 [D loss 1: 1.7809830904006958, D loss 2: 0.7362532317638397, G loss: [0.8957158327102661, 0.6655123233795166, 0.2302035242319107]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 43/43 [D loss 1: 1.8091145753860474, D loss 2: 0.8274595737457275, G loss: [0.8917636871337891, 0.6483993530273438, 0.24336430430412292]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 1/43 [D loss 1: 1.8402597308158875, D loss 2: 0.8458036482334137, G loss: [0.8914787769317627, 0.6505990028381348, 0.24087980389595032]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 2/43 [D loss 1: 1.7556453049182892, D loss 2: 0.7712204158306122, G loss: [0.8921387791633606, 0.66293865442276, 0.22920013964176178]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 3/43 [D loss 1: 1.8349778056144714, D loss 2: 0.841178834438324, G loss: [0.8898292779922485, 0.6637805104255676, 0.2260487824678421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 4/43 [D loss 1: 1.7897595763206482, D loss 2: 0.7582050263881683, G loss: [0.8883450031280518, 0.6594656109809875, 0.22887936234474182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 5/43 [D loss 1: 1.9084312915802002, D loss 2: 0.8912112414836884, G loss: [0.8727648854255676, 0.6543617844581604, 0.21840311586856842]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 6/43 [D loss 1: 1.7737368941307068, D loss 2: 0.7752673327922821, G loss: [0.8872854709625244, 0.6753373146057129, 0.21194817125797272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 7/43 [D loss 1: 1.7713741958141327, D loss 2: 0.8175169229507446, G loss: [0.8793627619743347, 0.6665592193603516, 0.21280355751514435]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 8/43 [D loss 1: 1.7673170864582062, D loss 2: 0.7780359983444214, G loss: [0.8703840970993042, 0.6614108085632324, 0.20897327363491058]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 9/43 [D loss 1: 1.8105882108211517, D loss 2: 0.8177607655525208, G loss: [0.8741974234580994, 0.6729714274406433, 0.20122598111629486]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 10/43 [D loss 1: 1.7667303681373596, D loss 2: 0.7933732867240906, G loss: [0.8682281970977783, 0.658452570438385, 0.20977561175823212]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 11/43 [D loss 1: 1.7549498081207275, D loss 2: 0.7638983130455017, G loss: [0.8749902248382568, 0.6756875514984131, 0.19930267333984375]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 12/43 [D loss 1: 1.83994922041893, D loss 2: 0.833269476890564, G loss: [0.8647435903549194, 0.666658878326416, 0.19808469712734222]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 13/43 [D loss 1: 1.7205768823623657, D loss 2: 0.7222530245780945, G loss: [0.8619996309280396, 0.6603760123252869, 0.2016235888004303]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 14/43 [D loss 1: 1.8455491065979004, D loss 2: 0.8923244178295135, G loss: [0.8601408004760742, 0.6645958423614502, 0.19554492831230164]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 15/43 [D loss 1: 1.747382402420044, D loss 2: 0.6255773603916168, G loss: [0.8608014583587646, 0.659740686416626, 0.20106074213981628]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 16/43 [D loss 1: 1.8653151988983154, D loss 2: 0.8759492933750153, G loss: [0.8849194049835205, 0.6997594237327576, 0.18515998125076294]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 17/43 [D loss 1: 1.7927760481834412, D loss 2: 0.8257601857185364, G loss: [0.8609423041343689, 0.6717896461486816, 0.18915265798568726]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 18/43 [D loss 1: 1.7393414080142975, D loss 2: 0.6848233342170715, G loss: [0.8595100045204163, 0.6668857932090759, 0.19262421131134033]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 19/43 [D loss 1: 1.9176040291786194, D loss 2: 0.9153762459754944, G loss: [0.8762953877449036, 0.681711733341217, 0.19458366930484772]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 20/43 [D loss 1: 1.7428885102272034, D loss 2: 0.74803626537323, G loss: [0.8415921926498413, 0.6638088822364807, 0.1777832806110382]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 21/43 [D loss 1: 1.6986794769763947, D loss 2: 0.6771332025527954, G loss: [0.850959837436676, 0.6654634475708008, 0.18549638986587524]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 22/43 [D loss 1: 1.852162778377533, D loss 2: 0.823159784078598, G loss: [0.82225102186203, 0.6543596982955933, 0.16789130866527557]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 23/43 [D loss 1: 1.796493798494339, D loss 2: 0.7524102330207825, G loss: [0.8486813306808472, 0.6724742650985718, 0.17620709538459778]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 24/43 [D loss 1: 1.699842095375061, D loss 2: 0.743010550737381, G loss: [0.8451269865036011, 0.6636618375778198, 0.18146516382694244]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 25/43 [D loss 1: 1.7342790365219116, D loss 2: 0.7936651110649109, G loss: [0.8234867453575134, 0.6572531461715698, 0.1662335991859436]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 26/43 [D loss 1: 1.7496678531169891, D loss 2: 0.7473793923854828, G loss: [0.8327677249908447, 0.6697139739990234, 0.1630537509918213]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 27/43 [D loss 1: 1.6816373467445374, D loss 2: 0.6984413862228394, G loss: [0.8367882966995239, 0.6599736213684082, 0.1768147051334381]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 28/43 [D loss 1: 1.837787389755249, D loss 2: 0.8745092451572418, G loss: [0.8432314991950989, 0.6678587794303894, 0.17537270486354828]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 29/43 [D loss 1: 1.725169837474823, D loss 2: 0.6474595367908478, G loss: [0.836327314376831, 0.6687855124473572, 0.16754181683063507]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 30/43 [D loss 1: 1.8240928053855896, D loss 2: 0.8564231991767883, G loss: [0.8185438513755798, 0.6671102046966553, 0.15143363177776337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 31/43 [D loss 1: 1.7392553687095642, D loss 2: 0.808161735534668, G loss: [0.8324352502822876, 0.6788044571876526, 0.1536308228969574]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 32/43 [D loss 1: 1.6745060682296753, D loss 2: 0.6942162811756134, G loss: [0.8295547962188721, 0.6627293825149536, 0.16682541370391846]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 33/43 [D loss 1: 1.8369551301002502, D loss 2: 0.8816452324390411, G loss: [0.8230448961257935, 0.6631187796592712, 0.15992611646652222]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 34/43 [D loss 1: 1.6291632652282715, D loss 2: 0.6683166325092316, G loss: [0.831507682800293, 0.6866226196289062, 0.14488503336906433]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 35/43 [D loss 1: 1.7469860315322876, D loss 2: 0.7692644894123077, G loss: [0.8388233780860901, 0.6884594559669495, 0.15036393702030182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 36/43 [D loss 1: 1.7771152257919312, D loss 2: 0.8002570271492004, G loss: [0.8437581062316895, 0.6877270340919495, 0.1560310572385788]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 37/43 [D loss 1: 1.7767375707626343, D loss 2: 0.8218424618244171, G loss: [0.8244526982307434, 0.6726158857345581, 0.1518368273973465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 38/43 [D loss 1: 1.661433607339859, D loss 2: 0.7367301881313324, G loss: [0.8295122385025024, 0.6862461566925049, 0.14326611161231995]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 39/43 [D loss 1: 1.7571264505386353, D loss 2: 0.8333723247051239, G loss: [0.841681718826294, 0.6936948299407959, 0.14798685908317566]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 40/43 [D loss 1: 1.7115067839622498, D loss 2: 0.7735206186771393, G loss: [0.8599135875701904, 0.7175253629684448, 0.1423882395029068]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 41/43 [D loss 1: 1.7754001319408417, D loss 2: 0.825082927942276, G loss: [0.8409877419471741, 0.7011798620223999, 0.13980787992477417]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 42/43 [D loss 1: 1.6818299889564514, D loss 2: 0.7598327994346619, G loss: [0.8612741231918335, 0.7153556942939758, 0.14591845870018005]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 43/43 [D loss 1: 1.7587588429450989, D loss 2: 0.8003195524215698, G loss: [0.8688074350357056, 0.7211030125617981, 0.14770443737506866]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 1/43 [D loss 1: 1.692039042711258, D loss 2: 0.7549664080142975, G loss: [0.8731814026832581, 0.7147785425186157, 0.15840286016464233]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 2/43 [D loss 1: 1.8552005887031555, D loss 2: 0.8861204087734222, G loss: [0.8624323606491089, 0.724534273147583, 0.13789808750152588]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 3/43 [D loss 1: 1.6352953016757965, D loss 2: 0.6530478596687317, G loss: [0.8427923321723938, 0.7013233304023743, 0.14146901667118073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 4/43 [D loss 1: 1.806024968624115, D loss 2: 0.8808345198631287, G loss: [0.8683792352676392, 0.7362629771232605, 0.13211627304553986]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 5/43 [D loss 1: 1.7171461582183838, D loss 2: 0.765421599149704, G loss: [0.8585875034332275, 0.7116694450378418, 0.14691805839538574]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 6/43 [D loss 1: 1.732184499502182, D loss 2: 0.7996846139431, G loss: [0.8624136447906494, 0.7251202464103699, 0.13729341328144073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 7/43 [D loss 1: 1.66819566488266, D loss 2: 0.7765785753726959, G loss: [0.8667157292366028, 0.7332248091697693, 0.1334909349679947]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 8/43 [D loss 1: 1.6182207465171814, D loss 2: 0.6894863545894623, G loss: [0.8600940704345703, 0.7172197103500366, 0.1428743600845337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 9/43 [D loss 1: 1.8196901082992554, D loss 2: 0.8842717409133911, G loss: [0.8730615377426147, 0.7435332536697388, 0.12952826917171478]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 10/43 [D loss 1: 1.6255329847335815, D loss 2: 0.6592312157154083, G loss: [0.856249988079071, 0.717575192451477, 0.1386748105287552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 11/43 [D loss 1: 1.6834671795368195, D loss 2: 0.7865748107433319, G loss: [0.8381076455116272, 0.7107321619987488, 0.12737548351287842]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 12/43 [D loss 1: 1.6392661333084106, D loss 2: 0.6930260360240936, G loss: [0.8298035860061646, 0.6906940340995789, 0.1391095668077469]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 13/43 [D loss 1: 1.7175404131412506, D loss 2: 0.7513054609298706, G loss: [0.8412503600120544, 0.7147787809371948, 0.1264715939760208]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 14/43 [D loss 1: 1.7461284399032593, D loss 2: 0.8269615769386292, G loss: [0.8604218363761902, 0.7239787578582764, 0.13644307851791382]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 15/43 [D loss 1: 1.6746683418750763, D loss 2: 0.7066468000411987, G loss: [0.8137247562408447, 0.6892964243888855, 0.12442834675312042]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 16/43 [D loss 1: 1.6790272295475006, D loss 2: 0.7353771328926086, G loss: [0.8309539556503296, 0.7037126421928406, 0.127241313457489]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 17/43 [D loss 1: 1.699215829372406, D loss 2: 0.7623102366924286, G loss: [0.8323838114738464, 0.7078232169151306, 0.12456057965755463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 18/43 [D loss 1: 1.6642837226390839, D loss 2: 0.7382721602916718, G loss: [0.8226494789123535, 0.6990364789962769, 0.12361298501491547]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 19/43 [D loss 1: 1.6718375384807587, D loss 2: 0.7411293685436249, G loss: [0.8422046303749084, 0.7137971520423889, 0.12840749323368073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 20/43 [D loss 1: 1.6659037470817566, D loss 2: 0.7459340989589691, G loss: [0.8619253635406494, 0.7273573279380798, 0.1345680058002472]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 21/43 [D loss 1: 1.617406189441681, D loss 2: 0.6642006933689117, G loss: [0.8207270503044128, 0.6959471702575684, 0.12477990239858627]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 22/43 [D loss 1: 1.7538845539093018, D loss 2: 0.8761314153671265, G loss: [0.8617294430732727, 0.7452237010002136, 0.11650574207305908]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 23/43 [D loss 1: 1.581718772649765, D loss 2: 0.6396035254001617, G loss: [0.8302496671676636, 0.6824358701705933, 0.1478138118982315]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 24/43 [D loss 1: 1.946024775505066, D loss 2: 1.005948156118393, G loss: [0.8902836441993713, 0.759996235370636, 0.13028742372989655]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 25/43 [D loss 1: 1.5864135026931763, D loss 2: 0.5929377675056458, G loss: [0.8707258701324463, 0.7540464401245117, 0.11667941510677338]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 26/43 [D loss 1: 1.6785675883293152, D loss 2: 0.7040345072746277, G loss: [0.819218099117279, 0.7125279903411865, 0.10669011622667313]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 27/43 [D loss 1: 1.7086012363433838, D loss 2: 0.7869568467140198, G loss: [0.831816554069519, 0.7036072611808777, 0.12820927798748016]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 28/43 [D loss 1: 1.6156622171401978, D loss 2: 0.7141001224517822, G loss: [0.8471068739891052, 0.7268828749656677, 0.12022397667169571]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 29/43 [D loss 1: 1.579797625541687, D loss 2: 0.6759030520915985, G loss: [0.8311259150505066, 0.7109634876251221, 0.12016244977712631]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 30/43 [D loss 1: 1.7538753747940063, D loss 2: 0.8678408861160278, G loss: [0.8655475378036499, 0.744208574295044, 0.12133897095918655]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 31/43 [D loss 1: 1.5760282576084137, D loss 2: 0.6640399992465973, G loss: [0.8581468462944031, 0.7389408349990845, 0.1192060336470604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 32/43 [D loss 1: 1.6054053902626038, D loss 2: 0.6658749282360077, G loss: [0.8408334255218506, 0.7210169434547424, 0.11981645226478577]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 33/43 [D loss 1: 1.7459412813186646, D loss 2: 0.8576961159706116, G loss: [0.8416340351104736, 0.7175648808479309, 0.12406915426254272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 34/43 [D loss 1: 1.697385162115097, D loss 2: 0.7797947227954865, G loss: [0.8438769578933716, 0.7348222136497498, 0.10905477404594421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 35/43 [D loss 1: 1.5895203351974487, D loss 2: 0.669230580329895, G loss: [0.8662092089653015, 0.7426763772964478, 0.12353280931711197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 36/43 [D loss 1: 1.6292409002780914, D loss 2: 0.7734272181987762, G loss: [0.8336009383201599, 0.7179516553878784, 0.1156492754817009]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 37/43 [D loss 1: 1.5991036891937256, D loss 2: 0.6747349202632904, G loss: [0.8511424660682678, 0.7375535368919373, 0.11358890682458878]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 38/43 [D loss 1: 1.6444230377674103, D loss 2: 0.7132171988487244, G loss: [0.8409485816955566, 0.7194236516952515, 0.12152490019798279]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 39/43 [D loss 1: 1.5733997523784637, D loss 2: 0.6614870429039001, G loss: [0.8357939124107361, 0.7280052900314331, 0.10778862237930298]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 40/43 [D loss 1: 1.7362358570098877, D loss 2: 0.8391448855400085, G loss: [0.8633577227592468, 0.7523561716079712, 0.11100155860185623]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 41/43 [D loss 1: 1.6543315649032593, D loss 2: 0.7473129630088806, G loss: [0.8626539707183838, 0.7523617148399353, 0.11029224842786789]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 42/43 [D loss 1: 1.5990588665008545, D loss 2: 0.6929618418216705, G loss: [0.8526559472084045, 0.7375963926315308, 0.11505956202745438]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 43/43 [D loss 1: 1.7565415501594543, D loss 2: 0.897039920091629, G loss: [0.8577614426612854, 0.7461518049240112, 0.11160963773727417]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 1/43 [D loss 1: 1.6535817384719849, D loss 2: 0.735459178686142, G loss: [0.8872460722923279, 0.7782443165779114, 0.1090017706155777]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 2/43 [D loss 1: 1.5670222640037537, D loss 2: 0.656009703874588, G loss: [0.8845027685165405, 0.7661140561103821, 0.11838869750499725]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 3/43 [D loss 1: 1.7869721055030823, D loss 2: 0.8939661979675293, G loss: [0.8843799233436584, 0.7725469470024109, 0.11183296889066696]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 4/43 [D loss 1: 1.6656945049762726, D loss 2: 0.7479779720306396, G loss: [0.8860855102539062, 0.7707485556602478, 0.11533697694540024]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 5/43 [D loss 1: 1.6965810656547546, D loss 2: 0.8055711090564728, G loss: [0.9083513617515564, 0.8076576590538025, 0.1006937175989151]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 6/43 [D loss 1: 1.7008513808250427, D loss 2: 0.8113875389099121, G loss: [0.9428076148033142, 0.8239535093307495, 0.1188540980219841]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 7/43 [D loss 1: 1.5436462759971619, D loss 2: 0.6274878978729248, G loss: [0.8913621306419373, 0.777927577495575, 0.11343453824520111]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 8/43 [D loss 1: 1.7611858248710632, D loss 2: 0.8728926479816437, G loss: [0.9042130708694458, 0.7940987944602966, 0.11011426150798798]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 9/43 [D loss 1: 1.6422042548656464, D loss 2: 0.7485194504261017, G loss: [0.896252453327179, 0.789033055305481, 0.107219398021698]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 10/43 [D loss 1: 1.6958129107952118, D loss 2: 0.7818354070186615, G loss: [0.9044023752212524, 0.7925863862037659, 0.11181596666574478]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 11/43 [D loss 1: 1.6115809679031372, D loss 2: 0.719307541847229, G loss: [0.9233788251876831, 0.8085364103317261, 0.11484239995479584]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 12/43 [D loss 1: 1.6499479115009308, D loss 2: 0.8039478957653046, G loss: [0.9184897541999817, 0.8199905753135681, 0.09849916398525238]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 13/43 [D loss 1: 1.675185739994049, D loss 2: 0.7703932821750641, G loss: [0.9343026280403137, 0.8114493489265442, 0.12285330146551132]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 14/43 [D loss 1: 1.6987964510917664, D loss 2: 0.8584785163402557, G loss: [0.926916778087616, 0.8169852495193481, 0.10993150621652603]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 15/43 [D loss 1: 1.5544366836547852, D loss 2: 0.6853475570678711, G loss: [0.935566782951355, 0.8085920810699463, 0.1269747018814087]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 16/43 [D loss 1: 1.8801776766777039, D loss 2: 0.9715308248996735, G loss: [0.9540726542472839, 0.8459427356719971, 0.10812992602586746]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 17/43 [D loss 1: 1.6440937221050262, D loss 2: 0.7260192930698395, G loss: [0.933588445186615, 0.8116914629936218, 0.12189696729183197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 18/43 [D loss 1: 1.7093065977096558, D loss 2: 0.8342589437961578, G loss: [0.9690924286842346, 0.8610809445381165, 0.10801150649785995]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 19/43 [D loss 1: 1.6858243644237518, D loss 2: 0.8182710409164429, G loss: [0.9748712778091431, 0.8636594414710999, 0.1112118661403656]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 20/43 [D loss 1: 1.6396313905715942, D loss 2: 0.7466469407081604, G loss: [0.9577327966690063, 0.8416467308998108, 0.11608608812093735]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 21/43 [D loss 1: 1.6863506436347961, D loss 2: 0.8149302899837494, G loss: [0.9461106657981873, 0.8372885584831238, 0.10882212221622467]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 22/43 [D loss 1: 1.6482200026512146, D loss 2: 0.7485990524291992, G loss: [0.9510300159454346, 0.8450471758842468, 0.10598286241292953]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 23/43 [D loss 1: 1.6663945615291595, D loss 2: 0.7587076425552368, G loss: [0.965069055557251, 0.852101743221283, 0.11296732723712921]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 24/43 [D loss 1: 1.7154089212417603, D loss 2: 0.8283106684684753, G loss: [0.9865396618843079, 0.8712478876113892, 0.1152917742729187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 25/43 [D loss 1: 1.6249175071716309, D loss 2: 0.7414049506187439, G loss: [0.9809084534645081, 0.8414627313613892, 0.1394457370042801]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 26/43 [D loss 1: 1.954918384552002, D loss 2: 1.0540171563625336, G loss: [1.0447957515716553, 0.9398841857910156, 0.10491155833005905]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 27/43 [D loss 1: 1.549596130847931, D loss 2: 0.7200197577476501, G loss: [0.9987517595291138, 0.8865690231323242, 0.11218271404504776]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 28/43 [D loss 1: 1.5367779433727264, D loss 2: 0.6682182550430298, G loss: [1.0175163745880127, 0.8893577456474304, 0.1281585991382599]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 29/43 [D loss 1: 1.8687977194786072, D loss 2: 1.0076063573360443, G loss: [1.054382085800171, 0.9618639945983887, 0.09251805394887924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 30/43 [D loss 1: 1.4917097687721252, D loss 2: 0.6251477897167206, G loss: [1.0022635459899902, 0.8817328810691833, 0.12053069472312927]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 31/43 [D loss 1: 1.5870462954044342, D loss 2: 0.724706619977951, G loss: [0.988679826259613, 0.8778430819511414, 0.11083674430847168]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 32/43 [D loss 1: 1.7319979667663574, D loss 2: 0.8524730503559113, G loss: [0.9708737134933472, 0.865973174571991, 0.1049005389213562]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 33/43 [D loss 1: 1.6759561896324158, D loss 2: 0.7946515679359436, G loss: [0.9830236434936523, 0.88136225938797, 0.10166138410568237]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 34/43 [D loss 1: 1.596158117055893, D loss 2: 0.716495007276535, G loss: [0.9648115634918213, 0.8497090339660645, 0.11510251462459564]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 35/43 [D loss 1: 1.63313627243042, D loss 2: 0.7809310257434845, G loss: [0.9800891280174255, 0.875290036201477, 0.10479911416769028]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 36/43 [D loss 1: 1.6055120825767517, D loss 2: 0.7268995046615601, G loss: [0.9871436953544617, 0.884722888469696, 0.10242082178592682]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 37/43 [D loss 1: 1.5381822288036346, D loss 2: 0.6631147563457489, G loss: [0.9978348016738892, 0.8761278390884399, 0.12170694768428802]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 38/43 [D loss 1: 1.8070986866950989, D loss 2: 0.9082598090171814, G loss: [1.0004894733428955, 0.8958582878112793, 0.10463118553161621]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 39/43 [D loss 1: 1.622769445180893, D loss 2: 0.7647057175636292, G loss: [0.976351261138916, 0.8685257434844971, 0.10782553255558014]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 40/43 [D loss 1: 1.5947312414646149, D loss 2: 0.7168582677841187, G loss: [1.0128655433654785, 0.9054387211799622, 0.10742687433958054]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 41/43 [D loss 1: 1.5847695171833038, D loss 2: 0.7457383573055267, G loss: [1.0175068378448486, 0.9036036133766174, 0.11390317976474762]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 42/43 [D loss 1: 1.6375168859958649, D loss 2: 0.783266007900238, G loss: [0.9985141158103943, 0.8871641159057617, 0.11134998500347137]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 43/43 [D loss 1: 1.5352762937545776, D loss 2: 0.7023265361785889, G loss: [0.9944419860839844, 0.8816897869110107, 0.11275222152471542]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 1/43 [D loss 1: 1.5648506581783295, D loss 2: 0.7390713691711426, G loss: [0.9878362417221069, 0.8796943426132202, 0.10814188420772552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 2/43 [D loss 1: 1.635355681180954, D loss 2: 0.7502670288085938, G loss: [0.9985666275024414, 0.9003386497497559, 0.09822793304920197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 3/43 [D loss 1: 1.636286437511444, D loss 2: 0.7577040493488312, G loss: [0.9956918954849243, 0.8936219811439514, 0.1020699292421341]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 4/43 [D loss 1: 1.5538159012794495, D loss 2: 0.7075817584991455, G loss: [1.000744104385376, 0.8920411467552185, 0.1087028980255127]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 5/43 [D loss 1: 1.636974424123764, D loss 2: 0.7556804716587067, G loss: [0.9932881593704224, 0.8836796879768372, 0.10960845649242401]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 6/43 [D loss 1: 1.5882719159126282, D loss 2: 0.7651054859161377, G loss: [0.9753621816635132, 0.8758633136749268, 0.09949889779090881]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 7/43 [D loss 1: 1.613550066947937, D loss 2: 0.7496960759162903, G loss: [1.0001128911972046, 0.8927161693572998, 0.1073966696858406]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 8/43 [D loss 1: 1.5861807465553284, D loss 2: 0.759739339351654, G loss: [0.9960049390792847, 0.8940273523330688, 0.10197758674621582]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 9/43 [D loss 1: 1.724562168121338, D loss 2: 0.852980375289917, G loss: [1.0135101079940796, 0.9201017618179321, 0.09340829402208328]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 10/43 [D loss 1: 1.573344498872757, D loss 2: 0.7316723763942719, G loss: [0.9860031604766846, 0.8880048394203186, 0.09799829125404358]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 11/43 [D loss 1: 1.6891454458236694, D loss 2: 0.8271238207817078, G loss: [1.0191820859909058, 0.9242123365402222, 0.0949697494506836]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 12/43 [D loss 1: 1.5734758377075195, D loss 2: 0.7307780683040619, G loss: [1.0116562843322754, 0.899445652961731, 0.11221059411764145]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 13/43 [D loss 1: 1.6687240302562714, D loss 2: 0.8249054551124573, G loss: [1.055112361907959, 0.9660251140594482, 0.08908727020025253]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 14/43 [D loss 1: 1.6542952358722687, D loss 2: 0.8412657082080841, G loss: [1.0608365535736084, 0.9569755792617798, 0.1038610190153122]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 15/43 [D loss 1: 1.617776334285736, D loss 2: 0.7516493201255798, G loss: [1.055446743965149, 0.935697615146637, 0.11974907666444778]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 16/43 [D loss 1: 1.7683783769607544, D loss 2: 0.8882025182247162, G loss: [1.057342290878296, 0.9709916114807129, 0.08635067194700241]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 17/43 [D loss 1: 1.5735016465187073, D loss 2: 0.7202636897563934, G loss: [1.0714340209960938, 0.9596471190452576, 0.1117868423461914]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 18/43 [D loss 1: 1.6247412860393524, D loss 2: 0.7949364185333252, G loss: [1.0604429244995117, 0.9585257768630981, 0.10191714763641357]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 19/43 [D loss 1: 1.6339849829673767, D loss 2: 0.8156591355800629, G loss: [1.0632928609848022, 0.9620469212532043, 0.10124599188566208]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 20/43 [D loss 1: 1.5389695763587952, D loss 2: 0.7419953048229218, G loss: [1.0710898637771606, 0.946495771408081, 0.12459411472082138]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 21/43 [D loss 1: 1.7501757144927979, D loss 2: 0.8687408268451691, G loss: [1.0450283288955688, 0.9654683470726013, 0.07955998182296753]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 22/43 [D loss 1: 1.5493127703666687, D loss 2: 0.7326042652130127, G loss: [1.0553970336914062, 0.9443761706352234, 0.11102081090211868]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 23/43 [D loss 1: 1.642721563577652, D loss 2: 0.7712947130203247, G loss: [1.060319185256958, 0.9671029448509216, 0.09321625530719757]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 24/43 [D loss 1: 1.6102662086486816, D loss 2: 0.7680872082710266, G loss: [1.0517897605895996, 0.94954514503479, 0.10224461555480957]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 25/43 [D loss 1: 1.5917415618896484, D loss 2: 0.752830445766449, G loss: [1.0493583679199219, 0.93231600522995, 0.11704231053590775]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 26/43 [D loss 1: 1.6680595874786377, D loss 2: 0.8472044765949249, G loss: [1.044530987739563, 0.9514914155006409, 0.09303957968950272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 27/43 [D loss 1: 1.6331333220005035, D loss 2: 0.7613905072212219, G loss: [1.0483859777450562, 0.9486938118934631, 0.09969212114810944]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 28/43 [D loss 1: 1.5650735199451447, D loss 2: 0.7262989580631256, G loss: [1.0399655103683472, 0.933167576789856, 0.1067979708313942]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 29/43 [D loss 1: 1.6625617742538452, D loss 2: 0.8437951505184174, G loss: [1.0577106475830078, 0.9684457182884216, 0.08926496654748917]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 30/43 [D loss 1: 1.5950549840927124, D loss 2: 0.752576619386673, G loss: [1.0405417680740356, 0.9231998324394226, 0.11734197288751602]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 31/43 [D loss 1: 1.743180751800537, D loss 2: 0.8875152468681335, G loss: [1.0272225141525269, 0.9437661170959473, 0.08345640450716019]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 32/43 [D loss 1: 1.596357524394989, D loss 2: 0.7631303668022156, G loss: [1.044336199760437, 0.9369644522666931, 0.1073717251420021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 33/43 [D loss 1: 1.6277619898319244, D loss 2: 0.7964279651641846, G loss: [1.0303055047988892, 0.9360952973365784, 0.09421022981405258]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 34/43 [D loss 1: 1.5785943865776062, D loss 2: 0.7279449701309204, G loss: [1.0623888969421387, 0.9544757008552551, 0.10791320353746414]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 35/43 [D loss 1: 1.6545082032680511, D loss 2: 0.8234165012836456, G loss: [1.030653476715088, 0.9345077872276306, 0.09614570438861847]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 36/43 [D loss 1: 1.6503645777702332, D loss 2: 0.8027017116546631, G loss: [1.0257236957550049, 0.9320623278617859, 0.09366140514612198]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 37/43 [D loss 1: 1.6289423108100891, D loss 2: 0.8057739436626434, G loss: [1.0611428022384644, 0.9619004726409912, 0.09924229979515076]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 38/43 [D loss 1: 1.745169222354889, D loss 2: 0.8956911563873291, G loss: [1.058675765991211, 0.9597910642623901, 0.09888467192649841]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 39/43 [D loss 1: 1.6563555300235748, D loss 2: 0.8380565941333771, G loss: [1.0465563535690308, 0.9166548848152161, 0.1299014836549759]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 40/43 [D loss 1: 1.7655034065246582, D loss 2: 0.9221210181713104, G loss: [1.077305793762207, 0.9896154999732971, 0.08769028633832932]]\n",
      "18/65 [=======>......................] - ETA: 0s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate and train the DCGAN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m acgan \u001b[38;5;241m=\u001b[39m ACGAN(img_rows, img_cols, channels)\n\u001b[1;32m----> 6\u001b[0m \u001b[43macgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 128\u001b[0m, in \u001b[0;36mACGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval, gen_steps)\u001b[0m\n\u001b[0;32m    126\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim))\n\u001b[0;32m    127\u001b[0m gen_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, (batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 128\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m labels_fake \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    131\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(imgs, [labels_real, y_train[idx]])\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2254\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2255\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "acgan = ACGAN(img_rows, img_cols, channels)\n",
    "acgan.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0370418b23d3e6d627974f5b44612aacb169a42c01386bf7ba5dc9099819d8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
