{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)  # init TF ...\n",
    "# config=tf.ConfigProto(gpu_options=gpu_options)  # w/o taking ...\n",
    "# with tf.Session(config=config): pass            # all GPU memory\n",
    "\n",
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from 'c:\\\\Users\\\\p2308870\\\\Documents\\\\DELE_CA2\\\\RL\\\\helpers.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(st, model, eps):\n",
    "    if np.random.rand() > eps:\n",
    "        q_values = model.eval(np.stack([st]))\n",
    "        return np.argmax(q_values)\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, frames, gamma, eps_decay_steps, eps_target,\n",
    "               batch_size, model, mem, start_step=0,\n",
    "               callback=None, trace=None, render=False):\n",
    "    \"\"\"Q-Learning, supprots resume\n",
    "    \n",
    "    Note: If resuming, all parameters should be identical to original call, with\n",
    "        exception of 'start_step' and 'frames'.\n",
    "    \n",
    "    Params:\n",
    "        env - environment\n",
    "        frames - number of time steps to execute\n",
    "        gamma - discount factor [0..1]\n",
    "        eps_decay_steps - decay epsilon-greedy param over that many time steps\n",
    "        eps_target - epsilon-greedy param after decay\n",
    "        batch_size - neural network batch size from memory buffer\n",
    "        model      - function approximator, already initialised, with methods:\n",
    "                     eval(state, action) -> float\n",
    "                     train(state, target) -> None\n",
    "        mem - memory reply buffer\n",
    "        start_step - if continuning, pass in return value (tts_) here\n",
    "        callback - optional callback to execute\n",
    "        trace - this object handles data logging, plotting etc.\n",
    "        render - render openai gym environment?\n",
    "    \"\"\"\n",
    "    \n",
    "    def eps_schedule(tts, eps_decay_steps, eps_target):\n",
    "        if tts > eps_decay_steps:\n",
    "            return eps_target\n",
    "        else:\n",
    "            eps_per_step_change = (1-eps_target) / eps_decay_steps\n",
    "            return 1.0 - tts * eps_per_step_change\n",
    "    \n",
    "        \n",
    "    assert len(mem) >= batch_size\n",
    "    \n",
    "    tts_ = start_step                        # total time step\n",
    "    for _ in itertools.count():              # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        episode_reward = 0                   # purely for logging\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            eps = eps_schedule(tts_, eps_decay_steps, eps_target)\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            episode_reward += R\n",
    "            if render: env.render()\n",
    "            \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(tts_, t_, S, A, R, done, eps, episode_reward, model, mem, trace)\n",
    "            \n",
    "            states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "            targets = model.eval(n_states)\n",
    "            targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "            targets[dones] = rewards[dones]  # return of next-to-terminal state is just R\n",
    "            model.train(states, actions, targets)\n",
    "\n",
    "            S = S_\n",
    "            \n",
    "            tts_ += 1\n",
    "            if tts_ >= start_step + frames:\n",
    "                return tts_                  # so we can pick up where we left\n",
    "            \n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, model, frames=None, episodes=None, eps=0.0, render=False):\n",
    "    assert frames is not None or episodes is not None\n",
    "        \n",
    "    total_reward = 0\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        if episodes is not None and e_ >= episodes:\n",
    "            return total_reward\n",
    "        \n",
    "        S = env.reset()\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            total_reward += R\n",
    "            if render: env.render()\n",
    "    \n",
    "            S = S_\n",
    "            \n",
    "            tts_ += 1\n",
    "            if frames is not None and tts_ >= frames:\n",
    "                return\n",
    "            \n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_fill(env, mem, steps=None, episodes=None, render=False):\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    tts_ = 0\n",
    "    for e_ in itertools.count():\n",
    "        if episodes is not None and e_ >= episodes:\n",
    "            return\n",
    "        \n",
    "        S = env.reset()\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():\n",
    "        \n",
    "            A = env.action_space.sample()    # random policy\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            if render: env.render()\n",
    "                \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            S = S_\n",
    "            \n",
    "            tts_ += 1\n",
    "            if steps is not None and tts_ >= steps:\n",
    "                return\n",
    "            \n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNeuralNet():\n",
    "    def __init__(self, nb_in, nb_hid_1, nb_hid_2, nb_out, lr):\n",
    "        self.nb_in = nb_in\n",
    "        self.nb_hid_1 = nb_hid_1\n",
    "        self.nb_hid_2 = nb_hid_2\n",
    "        self.nb_out = nb_out\n",
    "              \n",
    "        self._x = tf.keras.Input(shape=(nb_in,), dtype=tf.float32, name='xx')\n",
    "        self._y = tf.keras.Input(shape=(nb_out,), dtype=tf.float32, name='yy')\n",
    "\n",
    "        # Define hidden layers using tf.keras.layers.Dense\n",
    "        self._h_hid_1 = tf.keras.layers.Dense(units=nb_hid_1, activation=tf.nn.relu, name='Hidden_1')(self._x)\n",
    "        self._h_hid_2 = tf.keras.layers.Dense(units=nb_hid_2, activation=tf.nn.relu, name='Hidden_2')(self._h_hid_1)\n",
    "        \n",
    "        # Define output layer\n",
    "        self._y_hat = tf.keras.layers.Dense(units=nb_out, activation=None, name='Output')(self._h_hid_2)\n",
    "        self._loss = tf.losses.mean_squared_error(self._y, self._y_hat)\n",
    "\n",
    "        self._optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "        trainable_vars = self.model.trainable_variables\n",
    "        self._train_op = self._optimizer.minimize(self._loss, var_list=trainable_vars)\n",
    "\n",
    "        self._sess = tf.Session()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def backward(self, x, y):\n",
    "        assert x.ndim == y.ndim == 2\n",
    "        _, y_hat, loss = self._sess.run([self._train_op, self._y_hat, self._loss],\n",
    "                                         feed_dict={self._x: x, self._y:y})\n",
    "        return y_hat, loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._sess.run(self._y_hat, feed_dict={self._x: x})\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self._sess, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self._sess, filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, rew_mean, rew_std, nb_actions):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: TFNeuralNet model\n",
    "        \"\"\"\n",
    "        st_low = np.array(st_low);\n",
    "        st_high = np.array(st_high)\n",
    "        self._model = model\n",
    "        \n",
    "        assert st_low.ndim == 1 and st_low.shape == st_high.shape\n",
    "        \n",
    "        if len(st_low) != model.nb_in:\n",
    "            raise ValueError('Input shape does not match state_space shape')\n",
    "\n",
    "        if nb_actions != model.nb_out:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "        \n",
    "        self._rew_mean = rew_mean\n",
    "        self._rew_std = rew_std\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "\n",
    "        y_hat = self._model.forward(inputs)\n",
    "        \n",
    "        #return y_hat\n",
    "        return y_hat*self._rew_std + self._rew_mean\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "        \n",
    "        \n",
    "        targets = (targets-self._rew_mean) / self._rew_std    # normalise\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "        all_targets = self._model.forward(inputs)       # this should normalised already\n",
    "        all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "        self._model.backward(inputs, all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Circular buffer for DQN memory reply. Fairly fast.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, state_shape, state_dtype):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_len: maximum capacity\n",
    "        \"\"\"\n",
    "        assert isinstance(max_len, int)\n",
    "        assert max_len > 0\n",
    "\n",
    "        self.max_len = max_len                      # maximum length        \n",
    "        self._curr_insert_ptr = 0                   # index to insert next data sample\n",
    "        self._curr_len = 0                          # number of currently stored elements\n",
    "\n",
    "        state_arr_shape = [max_len] + list(state_shape)\n",
    "\n",
    "        self._hist_St = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_At = np.zeros(max_len, dtype=int)\n",
    "        self._hist_Rt_1 = np.zeros(max_len, dtype=float)\n",
    "        self._hist_St_1 = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_done_1 = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    def append(self, St, At, Rt_1, St_1, done_1):\n",
    "        \"\"\"Add one sample to memory, override oldest if max_len reached.\n",
    "\n",
    "        Args:\n",
    "            St [np.ndarray]   - state\n",
    "            At [int]          - action\n",
    "            Rt_1 [float]      - reward\n",
    "            St_1 [np.ndarray] - next state\n",
    "            done_1 [bool]       - next state terminal?\n",
    "        \"\"\"\n",
    "        self._hist_St[self._curr_insert_ptr] = St\n",
    "        self._hist_At[self._curr_insert_ptr] = At\n",
    "        self._hist_Rt_1[self._curr_insert_ptr] = Rt_1\n",
    "        self._hist_St_1[self._curr_insert_ptr] = St_1\n",
    "        self._hist_done_1[self._curr_insert_ptr] = done_1\n",
    "        \n",
    "        if self._curr_len < self.max_len:                 # keep track of current length\n",
    "            self._curr_len += 1\n",
    "            \n",
    "        self._curr_insert_ptr += 1                        # increment insertion pointer\n",
    "        if self._curr_insert_ptr >= self.max_len:         # roll to zero if needed\n",
    "            self._curr_insert_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in memory, 0 <= length <= max_len\"\"\"\n",
    "        return self._curr_len\n",
    "\n",
    "    def get_batch(self, batch_len):\n",
    "        \"\"\"Sample batch of data, with repetition\n",
    "\n",
    "        Args:\n",
    "            batch_len: nb of samples to pick\n",
    "\n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, next_done, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert self._curr_len > 0\n",
    "        assert batch_len > 0\n",
    "\n",
    "        \n",
    "        indices = np.random.randint(        # randint much faster than np.random.sample\n",
    "            low=0, high=self._curr_len, size=batch_len, dtype=int)\n",
    "\n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "\n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "\n",
    "\n",
    "    \n",
    "    def pick_last(self, nb):\n",
    "        \"\"\"Pick last nb elements from memory\n",
    "        \n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, done_1, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert nb <= self._curr_len\n",
    "        \n",
    "        start = self._curr_insert_ptr - nb                # inclusive\n",
    "        end = self._curr_insert_ptr                       # not inclusive\n",
    "        indices = np.array(range(start,end), dtype=int)   # indices to pick, can be neg.\n",
    "        indices[indices < 0] += self._curr_len            # loop negative to positive\n",
    "        \n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapFrameSkip():\n",
    "    def __init__(self, env, frameskip):\n",
    "        assert frameskip >= 1\n",
    "        self._env = env\n",
    "        self._frameskip = frameskip\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "    \n",
    "    def reset(self):\n",
    "        return self._env.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        sum_rew = 0\n",
    "        for _ in range(self._frameskip):\n",
    "            obs, rew, done, info = self._env.step(action)\n",
    "            sum_rew += rew\n",
    "            if done: break\n",
    "        return obs, sum_rew, done, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self._env.render(mode=mode)\n",
    "        \n",
    "    def close(self):\n",
    "        self._env.close()               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace():\n",
    "    def __init__(self, eval_every, render=False, test_states=None, state_labels=None):\n",
    "        \n",
    "        if test_states is not None:\n",
    "            assert test_states.ndim == 2\n",
    "            \n",
    "        self.enable_plotting = False\n",
    "        \n",
    "        self.eval_every = eval_every\n",
    "        self.test_states = test_states\n",
    "        self.state_labels = state_labels\n",
    "        \n",
    "        self.tstep = 0\n",
    "        self.total_tstep = 0\n",
    "        \n",
    "        self.q_values = collections.OrderedDict()\n",
    "        self.ep_rewards = collections.defaultdict(float)\n",
    "        self.last_ep_reward = None\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []  # t+1\n",
    "        self.dones = []    # t+1\n",
    "        self.epsilons = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(total_time_step, tstep, st, act, rew_, done_,\n",
    "             eps, ep_reward, model, memory, trace):\n",
    "    \"\"\"Called from gradient_MC after every episode.\n",
    "    \n",
    "    Params:\n",
    "        episode [int] - episode number\n",
    "        tstep [int]   - timestep within episode\n",
    "        model [obj]   - function approximator\n",
    "        trace [list]  - list to write results to\"\"\"\n",
    "    \n",
    "    assert total_time_step == trace.total_tstep\n",
    "    \n",
    "    trace.tstep = tstep\n",
    "    \n",
    "    trace.states.append(st)\n",
    "    trace.actions.append(act)\n",
    "    trace.rewards.append(rew_)\n",
    "    trace.dones.append(done_)\n",
    "    trace.epsilons.append(eps)\n",
    "        \n",
    "    if done_:\n",
    "        trace.ep_rewards[total_time_step] = ep_reward\n",
    "        trace.last_ep_reward = ep_reward\n",
    "            \n",
    "    #\n",
    "    #   Print, Evaluate, Plot\n",
    "    #\n",
    "    if (trace.eval_every is not None) and (trace.total_tstep % trace.eval_every == 0):\n",
    "        \n",
    "        last_ep_rew = trace.last_ep_reward\n",
    "        reward_str = str(round(last_ep_rew, 3)) if last_ep_rew is not None else 'None'\n",
    "        print(f'wall: {datetime.datetime.now().strftime(\"%H:%M:%S\")}   '\n",
    "              f'ep: {len(trace.ep_rewards):3}   tstep: {tstep:4}   '\n",
    "              f'total tstep: {trace.total_tstep:6}   '\n",
    "              f'eps: {eps:5.3f}   reward: {reward_str}   ')\n",
    "\n",
    "        if len(st) == 2:\n",
    "            # We are working with 2D environment,\n",
    "            # eval. Q-Value function across whole state space\n",
    "            q_arr = helpers.eval_state_action_space(model, env, split=[128,128])\n",
    "            trace.q_values[trace.total_tstep] = q_arr\n",
    "        else:\n",
    "            # Environment is not 2D,\n",
    "            # eval. on pre-defined random sample of states\n",
    "            if trace.test_states is not None:\n",
    "                y_hat = model.eval(trace.test_states)\n",
    "                trace.q_values[trace.total_tstep] = y_hat\n",
    "\n",
    "        if trace.enable_plotting:\n",
    "            helpers.plot_all(env, model, memory, trace)\n",
    "            print('■'*80)\n",
    "\n",
    "    trace.total_tstep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pendulum2DEnv():\n",
    "    def __init__(self):\n",
    "        self._env = gym.make('Pendulum-v1')\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([-np.pi, -8.0]), high=np.array([np.pi, 8.0]), dtype=np.float32 )\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        \n",
    "    def reset(self):\n",
    "        cos, sin, vel = self._env.reset()\n",
    "        theta = np.arctan2(sin, cos)\n",
    "        return np.array([theta, vel])\n",
    "        \n",
    "    def step(self, action):\n",
    "        torques = [-2.0, 0.0, 2.0]\n",
    "        # torques = [-2.0, -.5, 0.0, .5, 2.0]\n",
    "        joint_effort = torques[action]\n",
    "        \n",
    "        obs, rew, done, _ = self._env.step([joint_effort])\n",
    "        cos, sin, vel = obs\n",
    "        theta = np.arctan2(sin, cos)\n",
    "        return np.array([theta, vel]), rew, done, obs\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self._env.render(mode=mode)\n",
    "        \n",
    "    def close(self):\n",
    "        self._env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pendulum():\n",
    "    neural_net = TFNeuralNet(nb_in=2, nb_hid_1=64, nb_hid_2=64, nb_out=3, lr=0.00025)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net,\n",
    "                          env.observation_space.low,\n",
    "                          env.observation_space.high,\n",
    "                          rew_mean=-210,\n",
    "                          rew_std=50,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    mem_fill(env, mem, steps=10000)\n",
    "    test_states, _, _, _, _, _ = mem.get_batch(10)\n",
    "    \n",
    "    trace = Trace(eval_every=1000, test_states=test_states)\n",
    "    \n",
    "    return trace, model, mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p2308870\\.conda\\envs\\gpu_env\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TFNeuralNet' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m Pendulum2DEnv()\n\u001b[1;32m----> 2\u001b[0m trace, model, mem \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment_pendulum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[117], line 2\u001b[0m, in \u001b[0;36mexperiment_pendulum\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexperiment_pendulum\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     neural_net \u001b[38;5;241m=\u001b[39m \u001b[43mTFNeuralNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_hid_1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_hid_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00025\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m TFFunctApprox(neural_net,\n\u001b[0;32m      5\u001b[0m                           env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mlow,\n\u001b[0;32m      6\u001b[0m                           env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mhigh,\n\u001b[0;32m      7\u001b[0m                           rew_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m210\u001b[39m,\n\u001b[0;32m      8\u001b[0m                           rew_std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m      9\u001b[0m                           nb_actions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m     11\u001b[0m     mem \u001b[38;5;241m=\u001b[39m Memory(max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, state_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,), state_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "Cell \u001b[1;32mIn[110], line 20\u001b[0m, in \u001b[0;36mTFNeuralNet.__init__\u001b[1;34m(self, nb_in, nb_hid_1, nb_hid_2, nb_out, lr)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mmean_squared_error(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y_hat)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m---> 20\u001b[0m trainable_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrainable_variables\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mminimize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss, var_list\u001b[38;5;241m=\u001b[39mtrainable_vars)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mSession()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TFNeuralNet' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "env = Pendulum2DEnv()\n",
    "trace, model, mem = experiment_pendulum()\n",
    "# trace.enable_plotting = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.8 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Program Files (x86)/Microsoft Visual Studio/Shared/Python37_64/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tts = q_learning(env, frames=25000, gamma=.99,\n",
    "                 eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "                 model=model, mem=mem, callback=callback, trace=trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.8 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Program Files (x86)/Microsoft Visual Studio/Shared/Python37_64/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tts = q_learning(env, frames=5000, gamma=.99,\n",
    "                 eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "                 model=model, mem=mem, start_step=tts, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.8 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Program Files (x86)/Microsoft Visual Studio/Shared/Python37_64/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "helpers.plot_all(env, model, mem, trace, print_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.8 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Program Files (x86)/Microsoft Visual Studio/Shared/Python37_64/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model._model.save('./tf_models/Pendulum.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.8 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Program Files (x86)/Microsoft Visual Studio/Shared/Python37_64/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model._model.load('./tf_models/Pendulum.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.8 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '\"c:/Program Files (x86)/Microsoft Visual Studio/Shared/Python37_64/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# In Jupyter, press squre '■' in top menu to quit animation\n",
    "try: evaluate(env, model, frames=float('inf'), eps=0.0, render=True)\n",
    "except KeyboardInterrupt: pass\n",
    "finally: env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "caafb4240398362289dee49613442c2e3dc97f83d57493b7ca8993fac85035ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
