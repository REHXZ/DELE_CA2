{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, Conv2DTranspose, PReLU\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Concatenate\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, HTML\n",
    "import glob\n",
    "from keras.layers import AveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Add\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import Input\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install scikit-image\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Reshape, UpSampling2D, \\\n",
    "    BatchNormalization, Activation, Input, LeakyReLU, ZeroPadding2D, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "#import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('emnist-letters-train.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51:24 Harry Potter Deathy Hallows 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {1: 0, \n",
    "           2: 1, \n",
    "           3: 2, \n",
    "           4: 3, \n",
    "           5: 4, \n",
    "           6: 5, \n",
    "           7: 6, \n",
    "           8: 7, \n",
    "           9: 8, \n",
    "           10: 9, \n",
    "           11: 10, \n",
    "           12: 11, \n",
    "           13: 12, \n",
    "           14: 13, \n",
    "           15: 14, \n",
    "           16: 15, \n",
    "           17: 16, \n",
    "           18: 17, \n",
    "           19: 18, \n",
    "           20: 19, \n",
    "           21: 20, \n",
    "           22: 21, \n",
    "           23: 22, \n",
    "           24: 23, \n",
    "           25: 24, \n",
    "           26: 25, \n",
    "           27: 26}\n",
    "\n",
    "        # Map the labels column to its corresponding value\n",
    "df[0] = df[0].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = np.array(df.iloc[:,0].values)\n",
    "y_pre = pd.Categorical(y_pre)\n",
    "X = np.array(df.iloc[:,1:].values)\n",
    "X = X.reshape(-1,28,28,1)\n",
    "preprocessed = []\n",
    "for image in X:\n",
    "    rotated_image = rotate(image, 90, reshape=False)\n",
    "    flipped_image = np.flipud(rotated_image)\n",
    "    preprocessed.append(flipped_image)\n",
    "X_pre = np.array(preprocessed)\n",
    "X = X_pre\n",
    "X = X.astype('float32')\n",
    "X_pre = (X - 127.5) / 127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pre\n",
      "[22, 6, 15, 14, 16, ..., 19, 8, 5, 11, 0]\n",
      "Length: 26\n",
      "Categories (26, int64): [0, 1, 2, 3, ..., 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f'y_pre\\n{y_pre.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = Model([z, label], valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = in_shape[0] * in_shape[1]\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        merge = Concatenate()([in_image, li])\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        fe = Dropout(0.4)(fe)\n",
    "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "        model = Model([in_image, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 128 * 7 * 7\n",
    "        gen = Dense(n_nodes)(in_lat)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Reshape((7, 7, 128))(gen) \n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(merge)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes  # Ensure labels are within valid range\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"CGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('CGAN_mnist', exist_ok=True)\n",
    "        fig.savefig(\"CGAN_mnist/CGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=1024, save_interval=1, gen_steps=3):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, y_train[idx]], labels_real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, gen_labels], labels_fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([noise, gen_labels], valid_y)\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "            if (epoch) % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 14ms/step\n",
      "Epoch 1/200, Batch 1/86 [D loss: 0.6843744516372681, acc.: 36.13%] [G loss: 0.6843607425689697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 2/86 [D loss: 0.6472448408603668, acc.: 49.95%] [G loss: 0.6532118320465088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 3/86 [D loss: 0.6312700659036636, acc.: 50.00%] [G loss: 0.5709606409072876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 4/86 [D loss: 0.6696633696556091, acc.: 50.00%] [G loss: 0.4677077531814575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 5/86 [D loss: 0.7232813537120819, acc.: 50.00%] [G loss: 0.4666253328323364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 6/86 [D loss: 0.6931503415107727, acc.: 50.00%] [G loss: 0.5488387942314148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 7/86 [D loss: 0.6357947289943695, acc.: 50.00%] [G loss: 0.6639143228530884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 8/86 [D loss: 0.5850180983543396, acc.: 65.97%] [G loss: 0.7854741811752319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 9/86 [D loss: 0.5425160527229309, acc.: 98.83%] [G loss: 0.9078989028930664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 10/86 [D loss: 0.5038425028324127, acc.: 97.66%] [G loss: 1.0349065065383911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 11/86 [D loss: 0.4597938060760498, acc.: 95.80%] [G loss: 1.1641602516174316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 12/86 [D loss: 0.4226006269454956, acc.: 95.17%] [G loss: 1.291378378868103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 13/86 [D loss: 0.37975287437438965, acc.: 94.63%] [G loss: 1.4101618528366089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 14/86 [D loss: 0.3441447764635086, acc.: 95.07%] [G loss: 1.5246968269348145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 15/86 [D loss: 0.3032737225294113, acc.: 95.61%] [G loss: 1.6412315368652344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 16/86 [D loss: 0.27400603890419006, acc.: 95.75%] [G loss: 1.7542510032653809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 17/86 [D loss: 0.23742827773094177, acc.: 96.83%] [G loss: 1.8869426250457764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 18/86 [D loss: 0.2055785059928894, acc.: 97.46%] [G loss: 2.0132811069488525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 19/86 [D loss: 0.17868835479021072, acc.: 97.85%] [G loss: 2.1379661560058594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 20/86 [D loss: 0.16592943668365479, acc.: 97.80%] [G loss: 2.247358798980713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 21/86 [D loss: 0.14511944353580475, acc.: 97.90%] [G loss: 2.3616209030151367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 22/86 [D loss: 0.12883297726511955, acc.: 98.24%] [G loss: 2.4597182273864746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 23/86 [D loss: 0.11740795150399208, acc.: 98.39%] [G loss: 2.5688869953155518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 24/86 [D loss: 0.10012057423591614, acc.: 98.97%] [G loss: 2.6741292476654053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 25/86 [D loss: 0.09996860474348068, acc.: 98.68%] [G loss: 2.7626945972442627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 26/86 [D loss: 0.08389072120189667, acc.: 98.83%] [G loss: 2.8687360286712646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 27/86 [D loss: 0.08171207830309868, acc.: 98.93%] [G loss: 2.9271657466888428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 28/86 [D loss: 0.07350500300526619, acc.: 99.12%] [G loss: 3.0070440769195557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 29/86 [D loss: 0.07256411015987396, acc.: 98.78%] [G loss: 3.0514919757843018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 30/86 [D loss: 0.0678798258304596, acc.: 99.02%] [G loss: 3.117177963256836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 31/86 [D loss: 0.05459318868815899, acc.: 99.46%] [G loss: 3.2003307342529297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 32/86 [D loss: 0.05900950729846954, acc.: 99.22%] [G loss: 3.268747091293335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 33/86 [D loss: 0.04969479702413082, acc.: 99.46%] [G loss: 3.346803903579712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 34/86 [D loss: 0.04414178989827633, acc.: 99.46%] [G loss: 3.4336462020874023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 35/86 [D loss: 0.04308951087296009, acc.: 99.37%] [G loss: 3.484795093536377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 36/86 [D loss: 0.04095294140279293, acc.: 99.32%] [G loss: 3.5616447925567627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 37/86 [D loss: 0.0387207567691803, acc.: 99.61%] [G loss: 3.6294283866882324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 38/86 [D loss: 0.036865889094769955, acc.: 99.22%] [G loss: 3.682116746902466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 39/86 [D loss: 0.03432478755712509, acc.: 99.61%] [G loss: 3.7498106956481934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 40/86 [D loss: 0.030515982769429684, acc.: 99.85%] [G loss: 3.8097898960113525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 41/86 [D loss: 0.030354971066117287, acc.: 99.56%] [G loss: 3.8426096439361572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 42/86 [D loss: 0.025042014196515083, acc.: 99.80%] [G loss: 3.931689739227295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 43/86 [D loss: 0.027507374063134193, acc.: 99.71%] [G loss: 3.9825072288513184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 44/86 [D loss: 0.026540763676166534, acc.: 99.51%] [G loss: 4.022153377532959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 45/86 [D loss: 0.0245148204267025, acc.: 99.51%] [G loss: 4.064769744873047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 46/86 [D loss: 0.02213070821017027, acc.: 99.76%] [G loss: 4.132532596588135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 47/86 [D loss: 0.022009918466210365, acc.: 99.66%] [G loss: 4.153740406036377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 48/86 [D loss: 0.020495052449405193, acc.: 99.76%] [G loss: 4.197888374328613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 49/86 [D loss: 0.023612369783222675, acc.: 99.61%] [G loss: 4.22275447845459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 50/86 [D loss: 0.019400100223720074, acc.: 99.61%] [G loss: 4.271001815795898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 51/86 [D loss: 0.021695095114409924, acc.: 99.80%] [G loss: 4.277644634246826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 52/86 [D loss: 0.016008672770112753, acc.: 99.90%] [G loss: 4.336175918579102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 53/86 [D loss: 0.016430001240223646, acc.: 99.90%] [G loss: 4.384870529174805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 54/86 [D loss: 0.016663344111293554, acc.: 99.85%] [G loss: 4.4175519943237305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 55/86 [D loss: 0.01690288633108139, acc.: 99.76%] [G loss: 4.46785831451416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 56/86 [D loss: 0.014909102581441402, acc.: 99.76%] [G loss: 4.521358013153076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 57/86 [D loss: 0.013350313995033503, acc.: 99.85%] [G loss: 4.580972194671631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 58/86 [D loss: 0.014981928747147322, acc.: 99.85%] [G loss: 4.586328029632568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 59/86 [D loss: 0.014197521843016148, acc.: 99.85%] [G loss: 4.641758441925049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 60/86 [D loss: 0.011466732248663902, acc.: 99.80%] [G loss: 4.700331211090088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 61/86 [D loss: 0.010636062826961279, acc.: 99.95%] [G loss: 4.760668754577637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 62/86 [D loss: 0.012714382726699114, acc.: 99.76%] [G loss: 4.761786460876465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 63/86 [D loss: 0.010428211651742458, acc.: 99.90%] [G loss: 4.815974235534668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 64/86 [D loss: 0.011236957274377346, acc.: 99.80%] [G loss: 4.862083435058594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 65/86 [D loss: 0.009809961076825857, acc.: 99.85%] [G loss: 4.8758440017700195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 66/86 [D loss: 0.009422156028449535, acc.: 99.90%] [G loss: 4.935000419616699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 67/86 [D loss: 0.008270870428532362, acc.: 100.00%] [G loss: 5.009754180908203]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 1/200, Batch 68/86 [D loss: 0.007164902985095978, acc.: 99.90%] [G loss: 5.065746784210205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 69/86 [D loss: 0.009095206391066313, acc.: 99.85%] [G loss: 5.048742294311523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 70/86 [D loss: 0.01050555519759655, acc.: 99.85%] [G loss: 5.022345542907715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 71/86 [D loss: 0.00733816740103066, acc.: 99.95%] [G loss: 5.105802536010742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 72/86 [D loss: 0.007994797313585877, acc.: 99.90%] [G loss: 5.138138771057129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 73/86 [D loss: 0.00818606885150075, acc.: 99.90%] [G loss: 5.195821762084961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 74/86 [D loss: 0.007799332495778799, acc.: 99.95%] [G loss: 5.204370021820068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 75/86 [D loss: 0.0072398202028125525, acc.: 99.90%] [G loss: 5.218662261962891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 76/86 [D loss: 0.0063089788891375065, acc.: 99.95%] [G loss: 5.254570960998535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 77/86 [D loss: 0.004968453664332628, acc.: 100.00%] [G loss: 5.3659868240356445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 78/86 [D loss: 0.007733698934316635, acc.: 99.80%] [G loss: 5.296210289001465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 79/86 [D loss: 0.00751259783282876, acc.: 99.90%] [G loss: 5.339451789855957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 80/86 [D loss: 0.006476095179095864, acc.: 100.00%] [G loss: 5.345212936401367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 81/86 [D loss: 0.00693338830024004, acc.: 99.95%] [G loss: 5.372895240783691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 82/86 [D loss: 0.008449672721326351, acc.: 99.80%] [G loss: 5.3966383934021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 83/86 [D loss: 0.006916114827618003, acc.: 99.90%] [G loss: 5.411507606506348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 84/86 [D loss: 0.005672375904396176, acc.: 99.95%] [G loss: 5.460792541503906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 85/86 [D loss: 0.0064325216226279736, acc.: 99.90%] [G loss: 5.4629106521606445]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 1/200, Batch 86/86 [D loss: 0.00438577588647604, acc.: 100.00%] [G loss: 5.556465148925781]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 1/86 [D loss: 0.006322537548840046, acc.: 99.90%] [G loss: 5.526697158813477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 2/86 [D loss: 0.0066340044140815735, acc.: 99.85%] [G loss: 5.520747184753418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 3/86 [D loss: 0.0050969182047992945, acc.: 99.90%] [G loss: 5.59625768661499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 4/86 [D loss: 0.00407660286873579, acc.: 99.95%] [G loss: 5.675650596618652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 5/86 [D loss: 0.004202851559966803, acc.: 99.95%] [G loss: 5.73507022857666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 6/86 [D loss: 0.006527571473270655, acc.: 99.85%] [G loss: 5.676181793212891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 7/86 [D loss: 0.005147137213498354, acc.: 99.95%] [G loss: 5.693634033203125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 8/86 [D loss: 0.00507733877748251, acc.: 99.95%] [G loss: 5.708062171936035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 9/86 [D loss: 0.00424381229095161, acc.: 100.00%] [G loss: 5.752164840698242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 10/86 [D loss: 0.004648477770388126, acc.: 100.00%] [G loss: 5.767320156097412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 11/86 [D loss: 0.005467241397127509, acc.: 99.85%] [G loss: 5.76779317855835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 12/86 [D loss: 0.005303374491631985, acc.: 99.95%] [G loss: 5.740049362182617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 13/86 [D loss: 0.004757708637043834, acc.: 99.95%] [G loss: 5.775241374969482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 14/86 [D loss: 0.003666760283522308, acc.: 99.95%] [G loss: 5.8300957679748535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 15/86 [D loss: 0.003779844380915165, acc.: 100.00%] [G loss: 5.894669532775879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 16/86 [D loss: 0.004488426726311445, acc.: 99.95%] [G loss: 5.9150190353393555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 17/86 [D loss: 0.0039269214030355215, acc.: 99.90%] [G loss: 5.936250686645508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 18/86 [D loss: 0.003482108237221837, acc.: 100.00%] [G loss: 5.9561285972595215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 19/86 [D loss: 0.0030260594794526696, acc.: 100.00%] [G loss: 5.99251127243042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 20/86 [D loss: 0.0035678547574207187, acc.: 100.00%] [G loss: 6.013273239135742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 21/86 [D loss: 0.00447773071937263, acc.: 99.95%] [G loss: 5.995145797729492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 22/86 [D loss: 0.004252942744642496, acc.: 100.00%] [G loss: 5.9269866943359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 23/86 [D loss: 0.002661807229742408, acc.: 100.00%] [G loss: 6.042697906494141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 24/86 [D loss: 0.0030424357391893864, acc.: 99.95%] [G loss: 6.10184907913208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 25/86 [D loss: 0.003137686289846897, acc.: 100.00%] [G loss: 6.120270729064941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 26/86 [D loss: 0.003004079684615135, acc.: 100.00%] [G loss: 6.1313252449035645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 27/86 [D loss: 0.0025827428326010704, acc.: 100.00%] [G loss: 6.171393394470215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 28/86 [D loss: 0.0027477353578433394, acc.: 99.95%] [G loss: 6.186715126037598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 29/86 [D loss: 0.0029498660005629063, acc.: 100.00%] [G loss: 6.241786956787109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 30/86 [D loss: 0.0028793533565476537, acc.: 99.95%] [G loss: 6.225800037384033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 31/86 [D loss: 0.0023078357335180044, acc.: 100.00%] [G loss: 6.258840084075928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 32/86 [D loss: 0.0023420125944539905, acc.: 100.00%] [G loss: 6.318511486053467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 33/86 [D loss: 0.0026289602974429727, acc.: 100.00%] [G loss: 6.309065818786621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 34/86 [D loss: 0.0026938183000311255, acc.: 99.95%] [G loss: 6.315834045410156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 35/86 [D loss: 0.002395699848420918, acc.: 99.95%] [G loss: 6.363033771514893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 36/86 [D loss: 0.0027207332896068692, acc.: 100.00%] [G loss: 6.347765922546387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 37/86 [D loss: 0.0016703151050023735, acc.: 100.00%] [G loss: 6.393945693969727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 38/86 [D loss: 0.004096998833119869, acc.: 99.95%] [G loss: 6.357494354248047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 39/86 [D loss: 0.002009156800340861, acc.: 100.00%] [G loss: 6.389778137207031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 40/86 [D loss: 0.002828907803632319, acc.: 99.95%] [G loss: 6.428722381591797]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 41/86 [D loss: 0.002877450780943036, acc.: 100.00%] [G loss: 6.329463958740234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 42/86 [D loss: 0.0029724370688199997, acc.: 99.90%] [G loss: 6.337357997894287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 43/86 [D loss: 0.002422029501758516, acc.: 99.95%] [G loss: 6.3952717781066895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 44/86 [D loss: 0.001824502949602902, acc.: 100.00%] [G loss: 6.447620391845703]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 45/86 [D loss: 0.0021237987093627453, acc.: 99.95%] [G loss: 6.509335041046143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 46/86 [D loss: 0.002798969973810017, acc.: 99.90%] [G loss: 6.4690141677856445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 47/86 [D loss: 0.0023993435315787792, acc.: 99.95%] [G loss: 6.4876861572265625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 48/86 [D loss: 0.00253215862903744, acc.: 99.95%] [G loss: 6.4803571701049805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 49/86 [D loss: 0.001340879243798554, acc.: 100.00%] [G loss: 6.592660903930664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 50/86 [D loss: 0.0019154767505824566, acc.: 100.00%] [G loss: 6.617110252380371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 51/86 [D loss: 0.003464498440735042, acc.: 99.90%] [G loss: 6.512373447418213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 52/86 [D loss: 0.001982104149647057, acc.: 100.00%] [G loss: 6.571736812591553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 53/86 [D loss: 0.0019114435999654233, acc.: 100.00%] [G loss: 6.650727272033691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 54/86 [D loss: 0.0022417972795665264, acc.: 100.00%] [G loss: 6.606807708740234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 55/86 [D loss: 0.0013621701509691775, acc.: 100.00%] [G loss: 6.654891490936279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 56/86 [D loss: 0.00344599672826007, acc.: 99.90%] [G loss: 6.646916389465332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 57/86 [D loss: 0.0017647637869231403, acc.: 99.95%] [G loss: 6.6484880447387695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 58/86 [D loss: 0.0018457093392498791, acc.: 99.90%] [G loss: 6.7154459953308105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 59/86 [D loss: 0.0017567038303241134, acc.: 99.95%] [G loss: 6.74012565612793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 60/86 [D loss: 0.0023679790319874883, acc.: 99.90%] [G loss: 6.6918625831604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 61/86 [D loss: 0.0016625682474114, acc.: 99.95%] [G loss: 6.730464935302734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 62/86 [D loss: 0.0014775313320569694, acc.: 100.00%] [G loss: 6.758740425109863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 63/86 [D loss: 0.0018263686797581613, acc.: 99.95%] [G loss: 6.782157897949219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 64/86 [D loss: 0.0024083027383312583, acc.: 99.90%] [G loss: 6.7867279052734375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 65/86 [D loss: 0.0016748880152590573, acc.: 100.00%] [G loss: 6.798181533813477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 66/86 [D loss: 0.0014956267550587654, acc.: 100.00%] [G loss: 6.777487277984619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 67/86 [D loss: 0.0015917517594061792, acc.: 100.00%] [G loss: 6.834196090698242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 68/86 [D loss: 0.0010598924709483981, acc.: 100.00%] [G loss: 6.886941432952881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 69/86 [D loss: 0.001435745507478714, acc.: 100.00%] [G loss: 6.859969139099121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 70/86 [D loss: 0.0016691519413143396, acc.: 99.90%] [G loss: 6.9057207107543945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 71/86 [D loss: 0.0015613748109899461, acc.: 99.95%] [G loss: 6.926985740661621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 72/86 [D loss: 0.0013970783329568803, acc.: 100.00%] [G loss: 6.892761707305908]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 73/86 [D loss: 0.0011718064779415727, acc.: 100.00%] [G loss: 6.943523406982422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 74/86 [D loss: 0.0023799369228072464, acc.: 99.95%] [G loss: 6.86308479309082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 75/86 [D loss: 0.0013397273723967373, acc.: 100.00%] [G loss: 6.875626087188721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 76/86 [D loss: 0.0011933247442357242, acc.: 100.00%] [G loss: 6.947142601013184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 77/86 [D loss: 0.001539583783596754, acc.: 99.95%] [G loss: 7.024969100952148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 78/86 [D loss: 0.0010888771212194115, acc.: 100.00%] [G loss: 7.007329940795898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 79/86 [D loss: 0.0016873724525794387, acc.: 99.95%] [G loss: 7.004203796386719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 80/86 [D loss: 0.0014658705913461745, acc.: 99.95%] [G loss: 6.973730087280273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 81/86 [D loss: 0.0010494523157831281, acc.: 100.00%] [G loss: 7.038501739501953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 82/86 [D loss: 0.0010361687745898962, acc.: 100.00%] [G loss: 7.100917816162109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 83/86 [D loss: 0.002085805172100663, acc.: 99.95%] [G loss: 6.991379737854004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 84/86 [D loss: 0.0014808604028075933, acc.: 100.00%] [G loss: 7.004693508148193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 85/86 [D loss: 0.0015812055789865553, acc.: 100.00%] [G loss: 7.007798194885254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 86/86 [D loss: 0.0013877626624889672, acc.: 100.00%] [G loss: 7.048687934875488]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 1/86 [D loss: 0.001025052013574168, acc.: 100.00%] [G loss: 7.107865333557129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 2/86 [D loss: 0.0024900443386286497, acc.: 99.90%] [G loss: 6.976144790649414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 3/86 [D loss: 0.0012269251747056842, acc.: 100.00%] [G loss: 6.987934589385986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 4/86 [D loss: 0.0014377945335581899, acc.: 100.00%] [G loss: 7.014926910400391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 5/86 [D loss: 0.001015008514514193, acc.: 100.00%] [G loss: 7.105334758758545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 6/86 [D loss: 0.0010862076887860894, acc.: 100.00%] [G loss: 7.141167640686035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 7/86 [D loss: 0.0008161104342434555, acc.: 100.00%] [G loss: 7.2266926765441895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 8/86 [D loss: 0.0013912241556681693, acc.: 99.95%] [G loss: 7.186036109924316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 9/86 [D loss: 0.0012934441328980029, acc.: 99.95%] [G loss: 7.198051929473877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 10/86 [D loss: 0.0019478301983326674, acc.: 99.95%] [G loss: 7.088866710662842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 11/86 [D loss: 0.0008679563761688769, acc.: 100.00%] [G loss: 7.143706321716309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 12/86 [D loss: 0.0020233229151926935, acc.: 99.95%] [G loss: 7.106462478637695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 13/86 [D loss: 0.0014250145759433508, acc.: 99.95%] [G loss: 7.189169406890869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 14/86 [D loss: 0.0013474835432134569, acc.: 100.00%] [G loss: 7.175421714782715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 15/86 [D loss: 0.0015699403011240065, acc.: 99.95%] [G loss: 7.116225242614746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 16/86 [D loss: 0.0009348848543595523, acc.: 100.00%] [G loss: 7.213132858276367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 17/86 [D loss: 0.001282010751310736, acc.: 100.00%] [G loss: 7.243024826049805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 18/86 [D loss: 0.0008953935757745057, acc.: 100.00%] [G loss: 7.26475191116333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 19/86 [D loss: 0.000771305407397449, acc.: 100.00%] [G loss: 7.3618927001953125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 20/86 [D loss: 0.001449880248401314, acc.: 99.95%] [G loss: 7.295546531677246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 21/86 [D loss: 0.002475498942658305, acc.: 99.95%] [G loss: 7.230152606964111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 22/86 [D loss: 0.0008083920110948384, acc.: 100.00%] [G loss: 7.257115840911865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 23/86 [D loss: 0.0014545412850566208, acc.: 99.95%] [G loss: 7.227184772491455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 24/86 [D loss: 0.0008815325563773513, acc.: 100.00%] [G loss: 7.312597751617432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 25/86 [D loss: 0.0019640253158286214, acc.: 99.95%] [G loss: 7.208128929138184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 26/86 [D loss: 0.0022100298083387315, acc.: 99.90%] [G loss: 7.121264934539795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 27/86 [D loss: 0.002200581890065223, acc.: 99.95%] [G loss: 7.032689094543457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 28/86 [D loss: 0.0008484953141305596, acc.: 100.00%] [G loss: 7.1802802085876465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 29/86 [D loss: 0.000646288288407959, acc.: 100.00%] [G loss: 7.327549934387207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 30/86 [D loss: 0.0007103768002707511, acc.: 100.00%] [G loss: 7.4417572021484375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 31/86 [D loss: 0.001427671842975542, acc.: 99.95%] [G loss: 7.400232791900635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 32/86 [D loss: 0.0006016638653818518, acc.: 100.00%] [G loss: 7.454308032989502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 33/86 [D loss: 0.0006383330910466611, acc.: 100.00%] [G loss: 7.477501392364502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 3/200, Batch 34/86 [D loss: 0.002672271919436753, acc.: 99.90%] [G loss: 7.352287769317627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 35/86 [D loss: 0.0014252533437684178, acc.: 99.95%] [G loss: 7.260993957519531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 36/86 [D loss: 0.0006502985197585076, acc.: 100.00%] [G loss: 7.305091857910156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 37/86 [D loss: 0.0014258176379371434, acc.: 99.90%] [G loss: 7.319857120513916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 38/86 [D loss: 0.0007615031208842993, acc.: 100.00%] [G loss: 7.360860824584961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 39/86 [D loss: 0.0017504314891994, acc.: 99.90%] [G loss: 7.3054094314575195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 40/86 [D loss: 0.000635681688436307, acc.: 100.00%] [G loss: 7.39210319519043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 41/86 [D loss: 0.0006312261248240247, acc.: 100.00%] [G loss: 7.486988544464111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 42/86 [D loss: 0.00088569286162965, acc.: 100.00%] [G loss: 7.5002522468566895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 43/86 [D loss: 0.001462300802813843, acc.: 99.95%] [G loss: 7.4686198234558105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 44/86 [D loss: 0.000707228435203433, acc.: 100.00%] [G loss: 7.510146617889404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 45/86 [D loss: 0.0007036018650978804, acc.: 100.00%] [G loss: 7.497005462646484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 46/86 [D loss: 0.0008618577558081597, acc.: 100.00%] [G loss: 7.577625274658203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 47/86 [D loss: 0.0006511015526484698, acc.: 100.00%] [G loss: 7.604902267456055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 48/86 [D loss: 0.0005794811440864578, acc.: 100.00%] [G loss: 7.664900302886963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 49/86 [D loss: 0.0006244134856387973, acc.: 100.00%] [G loss: 7.719455718994141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 50/86 [D loss: 0.0006195041642058641, acc.: 100.00%] [G loss: 7.641685485839844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 51/86 [D loss: 0.0005259981116978452, acc.: 100.00%] [G loss: 7.761628150939941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 52/86 [D loss: 0.0008048945455811918, acc.: 100.00%] [G loss: 7.668691158294678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 53/86 [D loss: 0.0010276298562530428, acc.: 100.00%] [G loss: 7.628453731536865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 54/86 [D loss: 0.000693062727805227, acc.: 100.00%] [G loss: 7.627018451690674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 55/86 [D loss: 0.000740483490517363, acc.: 100.00%] [G loss: 7.666400909423828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 56/86 [D loss: 0.0006115062860772014, acc.: 100.00%] [G loss: 7.681948661804199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 57/86 [D loss: 0.0015608783869538456, acc.: 99.95%] [G loss: 7.5293450355529785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 58/86 [D loss: 0.0007804887136444449, acc.: 100.00%] [G loss: 7.502202033996582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 59/86 [D loss: 0.0005235186399659142, acc.: 100.00%] [G loss: 7.603847026824951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 60/86 [D loss: 0.00044617279490921646, acc.: 100.00%] [G loss: 7.685579299926758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 61/86 [D loss: 0.0004582309629768133, acc.: 100.00%] [G loss: 7.775516033172607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 62/86 [D loss: 0.0006762343109585345, acc.: 100.00%] [G loss: 7.795411109924316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 63/86 [D loss: 0.000669585628202185, acc.: 100.00%] [G loss: 7.792068958282471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 64/86 [D loss: 0.001004313409794122, acc.: 99.95%] [G loss: 7.726524353027344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 65/86 [D loss: 0.0012808788742404431, acc.: 99.95%] [G loss: 7.621917724609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 66/86 [D loss: 0.0005049618921475485, acc.: 100.00%] [G loss: 7.663170337677002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 67/86 [D loss: 0.0009513366385363042, acc.: 99.95%] [G loss: 7.641522407531738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 68/86 [D loss: 0.0014540210831910372, acc.: 99.95%] [G loss: 7.598517417907715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 69/86 [D loss: 0.0005084555450594053, acc.: 100.00%] [G loss: 7.690768241882324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 70/86 [D loss: 0.0006787510646972805, acc.: 100.00%] [G loss: 7.744787216186523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 71/86 [D loss: 0.0005864079867023975, acc.: 100.00%] [G loss: 7.740763187408447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 72/86 [D loss: 0.0003625777389970608, acc.: 100.00%] [G loss: 7.798297882080078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 73/86 [D loss: 0.0004683477454818785, acc.: 100.00%] [G loss: 7.854727268218994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 74/86 [D loss: 0.0009103106276597828, acc.: 100.00%] [G loss: 7.824126243591309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 75/86 [D loss: 0.0005589151987805963, acc.: 100.00%] [G loss: 7.822296142578125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 76/86 [D loss: 0.0005064354627393186, acc.: 100.00%] [G loss: 7.8270697593688965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 77/86 [D loss: 0.00034980815689777955, acc.: 100.00%] [G loss: 7.902528762817383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 78/86 [D loss: 0.0008169718785211444, acc.: 99.95%] [G loss: 7.854394435882568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 79/86 [D loss: 0.00038261468580458313, acc.: 100.00%] [G loss: 7.90531587600708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 80/86 [D loss: 0.0006947205984033644, acc.: 100.00%] [G loss: 7.865934371948242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 81/86 [D loss: 0.0008283419883809984, acc.: 100.00%] [G loss: 7.828268051147461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 82/86 [D loss: 0.0006357722741086036, acc.: 100.00%] [G loss: 7.8548583984375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 83/86 [D loss: 0.0003119581742794253, acc.: 100.00%] [G loss: 7.948520660400391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 84/86 [D loss: 0.0013767104246653616, acc.: 99.95%] [G loss: 7.749872207641602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 85/86 [D loss: 0.0006325393915176392, acc.: 100.00%] [G loss: 7.735695838928223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 86/86 [D loss: 0.0005152336088940501, acc.: 100.00%] [G loss: 7.827827453613281]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 1/86 [D loss: 0.00095370999770239, acc.: 100.00%] [G loss: 7.845944404602051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 2/86 [D loss: 0.0004102754028281197, acc.: 100.00%] [G loss: 7.848820209503174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 3/86 [D loss: 0.0005955726664979011, acc.: 100.00%] [G loss: 7.86616849899292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 4/86 [D loss: 0.00043790112249553204, acc.: 100.00%] [G loss: 7.97750186920166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 5/86 [D loss: 0.00045264937216416, acc.: 100.00%] [G loss: 7.971776008605957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 6/86 [D loss: 0.002220686583314091, acc.: 99.90%] [G loss: 7.662679672241211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 7/86 [D loss: 0.0007910136773716658, acc.: 100.00%] [G loss: 7.657337665557861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 8/86 [D loss: 0.0005484654539031908, acc.: 100.00%] [G loss: 7.737407207489014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 9/86 [D loss: 0.00042097789992112666, acc.: 100.00%] [G loss: 7.853851318359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 10/86 [D loss: 0.0003914155167876743, acc.: 100.00%] [G loss: 7.970659255981445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 11/86 [D loss: 0.0005273477290757, acc.: 100.00%] [G loss: 7.962586879730225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 12/86 [D loss: 0.0008978697005659342, acc.: 99.95%] [G loss: 7.957464218139648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 13/86 [D loss: 0.00030604194034822285, acc.: 100.00%] [G loss: 7.971641540527344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 14/86 [D loss: 0.001215168711496517, acc.: 99.95%] [G loss: 7.928713321685791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 15/86 [D loss: 0.0003980968031100929, acc.: 100.00%] [G loss: 7.98323917388916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 16/86 [D loss: 0.0003306219878140837, acc.: 100.00%] [G loss: 8.048721313476562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 17/86 [D loss: 0.0018109214724972844, acc.: 99.95%] [G loss: 8.012142181396484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 18/86 [D loss: 0.0008082939893938601, acc.: 100.00%] [G loss: 7.935809135437012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 19/86 [D loss: 0.0008392947784159333, acc.: 100.00%] [G loss: 7.889301300048828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 20/86 [D loss: 0.0003764994180528447, acc.: 100.00%] [G loss: 7.959567070007324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 21/86 [D loss: 0.0005204635963309556, acc.: 100.00%] [G loss: 7.97237491607666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 22/86 [D loss: 0.0003667824203148484, acc.: 100.00%] [G loss: 8.019882202148438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 23/86 [D loss: 0.00034700640389928594, acc.: 100.00%] [G loss: 8.051371574401855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 24/86 [D loss: 0.0008699546742718667, acc.: 99.95%] [G loss: 8.042387962341309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 25/86 [D loss: 0.00030554065233445726, acc.: 100.00%] [G loss: 8.053495407104492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 26/86 [D loss: 0.0011655447888188064, acc.: 99.95%] [G loss: 8.00916576385498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 27/86 [D loss: 0.0017328400281257927, acc.: 99.95%] [G loss: 7.8938679695129395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 28/86 [D loss: 0.0013083252124488354, acc.: 99.95%] [G loss: 7.806698322296143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 29/86 [D loss: 0.001596647489350289, acc.: 99.95%] [G loss: 7.808686256408691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 30/86 [D loss: 0.0008306643285322934, acc.: 100.00%] [G loss: 7.710535526275635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 31/86 [D loss: 0.0006245144759304821, acc.: 100.00%] [G loss: 7.825827121734619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 32/86 [D loss: 0.00037084268842590973, acc.: 100.00%] [G loss: 7.915348052978516]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 33/86 [D loss: 0.0003010964937857352, acc.: 100.00%] [G loss: 7.996933937072754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 34/86 [D loss: 0.0002616283509269124, acc.: 100.00%] [G loss: 8.115067481994629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 35/86 [D loss: 0.001478524529375136, acc.: 99.90%] [G loss: 7.879274845123291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 36/86 [D loss: 0.00035455413853924256, acc.: 100.00%] [G loss: 7.93132209777832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 37/86 [D loss: 0.0004236494714859873, acc.: 100.00%] [G loss: 8.044143676757812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 38/86 [D loss: 0.00025014638595166616, acc.: 100.00%] [G loss: 8.084763526916504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 39/86 [D loss: 0.0015638259064871818, acc.: 99.90%] [G loss: 7.930688381195068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 40/86 [D loss: 0.00045122579467715696, acc.: 100.00%] [G loss: 7.966796875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 41/86 [D loss: 0.0003552993803168647, acc.: 100.00%] [G loss: 8.065719604492188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 42/86 [D loss: 0.00028162196394987404, acc.: 100.00%] [G loss: 8.169292449951172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 43/86 [D loss: 0.0002846502684406005, acc.: 100.00%] [G loss: 8.29202651977539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 44/86 [D loss: 0.000254585003858665, acc.: 100.00%] [G loss: 8.301311492919922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 45/86 [D loss: 0.00026509635063121095, acc.: 100.00%] [G loss: 8.391141891479492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 46/86 [D loss: 0.00035814834700431675, acc.: 100.00%] [G loss: 8.364880561828613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 47/86 [D loss: 0.00017968052634387277, acc.: 100.00%] [G loss: 8.408273696899414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 48/86 [D loss: 0.0004281736182747409, acc.: 100.00%] [G loss: 8.4281644821167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 49/86 [D loss: 0.000554313519387506, acc.: 100.00%] [G loss: 8.322531700134277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 50/86 [D loss: 0.0007400668109767139, acc.: 99.95%] [G loss: 8.244341850280762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 51/86 [D loss: 0.0003216269251424819, acc.: 100.00%] [G loss: 8.279147148132324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 52/86 [D loss: 0.00024173127530957572, acc.: 100.00%] [G loss: 8.304877281188965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 53/86 [D loss: 0.00022604981495533139, acc.: 100.00%] [G loss: 8.397744178771973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 54/86 [D loss: 0.0002443958292133175, acc.: 100.00%] [G loss: 8.382036209106445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 55/86 [D loss: 0.00029930695018265396, acc.: 100.00%] [G loss: 8.457448959350586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 56/86 [D loss: 0.00046988940448500216, acc.: 100.00%] [G loss: 8.417400360107422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 57/86 [D loss: 0.0009925049089360982, acc.: 99.95%] [G loss: 8.277997016906738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 58/86 [D loss: 0.00037691496254410595, acc.: 100.00%] [G loss: 8.277606964111328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 59/86 [D loss: 0.00026850214271689765, acc.: 100.00%] [G loss: 8.33662223815918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 60/86 [D loss: 0.00023160648925113492, acc.: 100.00%] [G loss: 8.41856861114502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 61/86 [D loss: 0.0011197864077985287, acc.: 99.95%] [G loss: 8.305337905883789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 62/86 [D loss: 0.0003578682226361707, acc.: 100.00%] [G loss: 8.303194999694824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 63/86 [D loss: 0.0002538992775953375, acc.: 100.00%] [G loss: 8.364142417907715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 64/86 [D loss: 0.00036230676050763577, acc.: 100.00%] [G loss: 8.343633651733398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 65/86 [D loss: 0.000506757220136933, acc.: 100.00%] [G loss: 8.33631706237793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 66/86 [D loss: 0.00022169445946929045, acc.: 100.00%] [G loss: 8.42172908782959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 67/86 [D loss: 0.00025563351664459333, acc.: 100.00%] [G loss: 8.48023509979248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 68/86 [D loss: 0.00017275218488066457, acc.: 100.00%] [G loss: 8.509848594665527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 69/86 [D loss: 0.00034979262272827327, acc.: 100.00%] [G loss: 8.488791465759277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 70/86 [D loss: 0.00017178595226141624, acc.: 100.00%] [G loss: 8.540722846984863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 71/86 [D loss: 0.00026183306908933446, acc.: 100.00%] [G loss: 8.577152252197266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 72/86 [D loss: 0.0005208405491430312, acc.: 100.00%] [G loss: 8.50772476196289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 73/86 [D loss: 0.0001887166581582278, acc.: 100.00%] [G loss: 8.541253089904785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 74/86 [D loss: 0.0005079589318484068, acc.: 100.00%] [G loss: 8.476247787475586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 75/86 [D loss: 0.0002962149737868458, acc.: 100.00%] [G loss: 8.45883846282959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 76/86 [D loss: 0.0002122303267242387, acc.: 100.00%] [G loss: 8.517061233520508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 77/86 [D loss: 0.00034581124782562256, acc.: 100.00%] [G loss: 8.526515007019043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 78/86 [D loss: 0.0001934660722326953, acc.: 100.00%] [G loss: 8.549379348754883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 79/86 [D loss: 0.0008206715137930587, acc.: 99.95%] [G loss: 8.4453706741333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 80/86 [D loss: 0.00023890019656391814, acc.: 100.00%] [G loss: 8.43980884552002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 81/86 [D loss: 0.00019313157099531963, acc.: 100.00%] [G loss: 8.53933048248291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 82/86 [D loss: 0.0006049406219972298, acc.: 99.95%] [G loss: 8.515238761901855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 83/86 [D loss: 0.00019408811203902587, acc.: 100.00%] [G loss: 8.521071434020996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 84/86 [D loss: 0.0003471442323643714, acc.: 100.00%] [G loss: 8.48906421661377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 85/86 [D loss: 0.0002863724221242592, acc.: 100.00%] [G loss: 8.525178909301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 86/86 [D loss: 0.0002564229289419018, acc.: 100.00%] [G loss: 8.566169738769531]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 1/86 [D loss: 0.000328935420839116, acc.: 100.00%] [G loss: 8.57240104675293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 2/86 [D loss: 0.00022849823290016502, acc.: 100.00%] [G loss: 8.653210639953613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 3/86 [D loss: 0.0005472596239997074, acc.: 99.95%] [G loss: 8.5345458984375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 4/86 [D loss: 0.00023408764536725357, acc.: 100.00%] [G loss: 8.59139633178711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 5/86 [D loss: 0.00047964148689061403, acc.: 100.00%] [G loss: 8.582413673400879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 6/86 [D loss: 0.00020498467347351834, acc.: 100.00%] [G loss: 8.598248481750488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 7/86 [D loss: 0.00034836219856515527, acc.: 100.00%] [G loss: 8.585832595825195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 8/86 [D loss: 0.00017751624545780942, acc.: 100.00%] [G loss: 8.61557674407959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 9/86 [D loss: 0.00025568944693077356, acc.: 100.00%] [G loss: 8.632200241088867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 10/86 [D loss: 0.0003347193996887654, acc.: 100.00%] [G loss: 8.632912635803223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 11/86 [D loss: 0.00037160434294492006, acc.: 100.00%] [G loss: 8.569116592407227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 12/86 [D loss: 0.00020722484987345524, acc.: 100.00%] [G loss: 8.603236198425293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 13/86 [D loss: 0.00019136860282742418, acc.: 100.00%] [G loss: 8.63966178894043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 14/86 [D loss: 0.0012756745127262548, acc.: 99.95%] [G loss: 8.53956413269043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 15/86 [D loss: 0.00035953018232248724, acc.: 100.00%] [G loss: 8.47075080871582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 16/86 [D loss: 0.00020414228856679983, acc.: 100.00%] [G loss: 8.523356437683105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 17/86 [D loss: 0.00023330518888542429, acc.: 100.00%] [G loss: 8.578792572021484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 18/86 [D loss: 0.0001948402168636676, acc.: 100.00%] [G loss: 8.640506744384766]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 5/200, Batch 19/86 [D loss: 0.000181504638021579, acc.: 100.00%] [G loss: 8.676408767700195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 20/86 [D loss: 0.0001689218588580843, acc.: 100.00%] [G loss: 8.764678955078125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 21/86 [D loss: 0.0001536821946501732, acc.: 100.00%] [G loss: 8.742122650146484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 22/86 [D loss: 0.0006883060996187851, acc.: 99.95%] [G loss: 8.642865180969238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 23/86 [D loss: 0.0006463806057581678, acc.: 99.95%] [G loss: 8.518431663513184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 24/86 [D loss: 0.00031926050723996013, acc.: 100.00%] [G loss: 8.47834300994873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 25/86 [D loss: 0.00023882300592958927, acc.: 100.00%] [G loss: 8.58365249633789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 26/86 [D loss: 0.00020458573999349028, acc.: 100.00%] [G loss: 8.619824409484863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 27/86 [D loss: 0.00021476184338098392, acc.: 100.00%] [G loss: 8.663421630859375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 28/86 [D loss: 0.0002738455368671566, acc.: 100.00%] [G loss: 8.665624618530273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 29/86 [D loss: 0.00015581579646095634, acc.: 100.00%] [G loss: 8.806336402893066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 30/86 [D loss: 0.00016796885029179975, acc.: 100.00%] [G loss: 8.826839447021484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 31/86 [D loss: 0.00022997530322754756, acc.: 100.00%] [G loss: 8.77444839477539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 32/86 [D loss: 0.00017128718172898516, acc.: 100.00%] [G loss: 8.850601196289062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 33/86 [D loss: 0.0002085494779748842, acc.: 100.00%] [G loss: 8.857034683227539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 34/86 [D loss: 0.00026043527032015845, acc.: 100.00%] [G loss: 8.842105865478516]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 35/86 [D loss: 0.00017240610031876713, acc.: 100.00%] [G loss: 8.862459182739258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 36/86 [D loss: 0.00011274656844761921, acc.: 100.00%] [G loss: 8.8590726852417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 37/86 [D loss: 0.00015478529894608073, acc.: 100.00%] [G loss: 8.93376636505127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 38/86 [D loss: 0.00020003148529212922, acc.: 100.00%] [G loss: 8.920013427734375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 39/86 [D loss: 0.00024961492454167455, acc.: 100.00%] [G loss: 8.9404935836792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 40/86 [D loss: 0.0015188427350949496, acc.: 99.95%] [G loss: 8.654184341430664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 41/86 [D loss: 0.00022664490825263783, acc.: 100.00%] [G loss: 8.581329345703125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 42/86 [D loss: 0.0001577558014105307, acc.: 100.00%] [G loss: 8.702516555786133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 43/86 [D loss: 0.00018129741147276945, acc.: 100.00%] [G loss: 8.782369613647461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 44/86 [D loss: 0.0004653903015423566, acc.: 100.00%] [G loss: 8.707355499267578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 45/86 [D loss: 0.00021414089133031666, acc.: 100.00%] [G loss: 8.735897064208984]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 46/86 [D loss: 0.00027216151647735387, acc.: 100.00%] [G loss: 8.712179183959961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 47/86 [D loss: 0.0001839666801970452, acc.: 100.00%] [G loss: 8.802720069885254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 48/86 [D loss: 0.00017857500643003732, acc.: 100.00%] [G loss: 8.833402633666992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 49/86 [D loss: 0.00012835191728299833, acc.: 100.00%] [G loss: 8.881564140319824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 50/86 [D loss: 0.00011034735643988824, acc.: 100.00%] [G loss: 8.954432487487793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 51/86 [D loss: 0.0005288225947879255, acc.: 100.00%] [G loss: 8.813765525817871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 52/86 [D loss: 0.00048342140507884324, acc.: 100.00%] [G loss: 8.72838306427002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 53/86 [D loss: 0.003084482275880873, acc.: 99.90%] [G loss: 6.931099891662598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 54/86 [D loss: 4.67923463810439, acc.: 82.03%] [G loss: 5.513712264537673e-10]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 55/86 [D loss: 16.129744611680508, acc.: 48.68%] [G loss: 1.3088537343719508e-05]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 56/86 [D loss: 5.378089874982834, acc.: 42.58%] [G loss: 2.5752399324119324e-06]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 57/86 [D loss: 5.440449878573418, acc.: 47.71%] [G loss: 0.10506229102611542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 58/86 [D loss: 0.11643738579005003, acc.: 98.68%] [G loss: 2.9593021869659424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 59/86 [D loss: 0.08262539003044367, acc.: 99.90%] [G loss: 0.15977345407009125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 60/86 [D loss: 1.8182751387357712, acc.: 50.05%] [G loss: 0.11049206554889679]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 5/200, Batch 61/86 [D loss: 0.9124861024320126, acc.: 50.15%] [G loss: 0.6880348920822144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 62/86 [D loss: 0.8830775618553162, acc.: 35.25%] [G loss: 0.6490174531936646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 63/86 [D loss: 0.834776908159256, acc.: 40.87%] [G loss: 0.6401664018630981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 64/86 [D loss: 0.7504842430353165, acc.: 43.41%] [G loss: 0.41296273469924927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 65/86 [D loss: 1.022341936826706, acc.: 39.45%] [G loss: 0.37107598781585693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 66/86 [D loss: 1.0240072906017303, acc.: 36.13%] [G loss: 0.5065934062004089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 67/86 [D loss: 0.9743789732456207, acc.: 26.56%] [G loss: 0.6292171478271484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 68/86 [D loss: 0.8836184144020081, acc.: 23.00%] [G loss: 0.7049891948699951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 69/86 [D loss: 0.8041408360004425, acc.: 29.54%] [G loss: 0.712921142578125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 70/86 [D loss: 0.7986633777618408, acc.: 31.15%] [G loss: 0.6764807105064392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 71/86 [D loss: 0.809000551700592, acc.: 31.84%] [G loss: 0.6778702139854431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 72/86 [D loss: 0.7965993881225586, acc.: 30.76%] [G loss: 0.6909490823745728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 73/86 [D loss: 0.7772140502929688, acc.: 30.57%] [G loss: 0.6913356184959412]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 5/200, Batch 74/86 [D loss: 0.7616634666919708, acc.: 31.49%] [G loss: 0.6859670877456665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 75/86 [D loss: 0.741452544927597, acc.: 37.21%] [G loss: 0.6618365049362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 76/86 [D loss: 0.7568790316581726, acc.: 39.60%] [G loss: 0.6066824793815613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 77/86 [D loss: 0.8023898303508759, acc.: 37.11%] [G loss: 0.7136309146881104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 78/86 [D loss: 0.7357547283172607, acc.: 43.26%] [G loss: 0.7255679965019226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 79/86 [D loss: 0.7800320386886597, acc.: 32.62%] [G loss: 0.6152973771095276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 80/86 [D loss: 0.8060762882232666, acc.: 27.00%] [G loss: 0.6534273028373718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 81/86 [D loss: 0.7375167906284332, acc.: 39.60%] [G loss: 0.6469097137451172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 82/86 [D loss: 0.7713287770748138, acc.: 39.89%] [G loss: 0.5051464438438416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 83/86 [D loss: 0.885929137468338, acc.: 35.94%] [G loss: 0.5823061466217041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 84/86 [D loss: 0.7963565587997437, acc.: 31.93%] [G loss: 0.6714027523994446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 85/86 [D loss: 0.8063388764858246, acc.: 23.58%] [G loss: 0.6509683132171631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 86/86 [D loss: 0.815616101026535, acc.: 20.21%] [G loss: 0.668405294418335]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 6/200, Batch 1/86 [D loss: 0.7843489944934845, acc.: 25.39%] [G loss: 0.7370272874832153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 2/86 [D loss: 0.734550952911377, acc.: 40.14%] [G loss: 0.8029500246047974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 3/86 [D loss: 0.7021917402744293, acc.: 51.81%] [G loss: 0.7880886793136597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 4/86 [D loss: 0.7118901014328003, acc.: 47.90%] [G loss: 0.7036144137382507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 5/86 [D loss: 0.7574913501739502, acc.: 36.96%] [G loss: 0.6680169701576233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 6/86 [D loss: 0.7711029052734375, acc.: 32.71%] [G loss: 0.6862295269966125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 7/86 [D loss: 0.7760708332061768, acc.: 31.25%] [G loss: 0.7059506177902222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 8/86 [D loss: 0.7926318347454071, acc.: 25.68%] [G loss: 0.695828914642334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 9/86 [D loss: 0.7902995645999908, acc.: 26.17%] [G loss: 0.6865439414978027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 10/86 [D loss: 0.7920242846012115, acc.: 23.49%] [G loss: 0.6681590676307678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 11/86 [D loss: 0.8000234663486481, acc.: 24.17%] [G loss: 0.6541889309883118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 12/86 [D loss: 0.8056654930114746, acc.: 24.32%] [G loss: 0.642769992351532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 13/86 [D loss: 0.8195734024047852, acc.: 21.88%] [G loss: 0.6419216394424438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 14/86 [D loss: 0.8144127428531647, acc.: 20.80%] [G loss: 0.6678332090377808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 15/86 [D loss: 0.7976718544960022, acc.: 23.10%] [G loss: 0.715790331363678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 16/86 [D loss: 0.7698773145675659, acc.: 29.15%] [G loss: 0.7547265291213989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 17/86 [D loss: 0.73832768201828, acc.: 39.01%] [G loss: 0.7731531262397766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 18/86 [D loss: 0.714439332485199, acc.: 46.34%] [G loss: 0.7583814263343811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 19/86 [D loss: 0.6993915736675262, acc.: 49.22%] [G loss: 0.745011031627655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 20/86 [D loss: 0.6882547438144684, acc.: 52.29%] [G loss: 0.7011807560920715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 21/86 [D loss: 0.7141807973384857, acc.: 46.09%] [G loss: 0.6484462022781372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 22/86 [D loss: 0.7454731464385986, acc.: 43.07%] [G loss: 0.6622388958930969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 23/86 [D loss: 0.7239475846290588, acc.: 47.07%] [G loss: 0.7046816349029541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 24/86 [D loss: 0.734218418598175, acc.: 43.07%] [G loss: 0.7085394859313965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 25/86 [D loss: 0.7598830461502075, acc.: 35.06%] [G loss: 0.6751818060874939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 26/86 [D loss: 0.7717798352241516, acc.: 33.69%] [G loss: 0.6490412950515747]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 6/200, Batch 27/86 [D loss: 0.7796362936496735, acc.: 30.42%] [G loss: 0.6419944167137146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 28/86 [D loss: 0.768513023853302, acc.: 33.11%] [G loss: 0.6331685185432434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 29/86 [D loss: 0.7689737379550934, acc.: 33.59%] [G loss: 0.6428366303443909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 30/86 [D loss: 0.7757772207260132, acc.: 33.01%] [G loss: 0.6835417747497559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 31/86 [D loss: 0.7566533386707306, acc.: 33.74%] [G loss: 0.714968740940094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 32/86 [D loss: 0.7519344091415405, acc.: 34.38%] [G loss: 0.7301730513572693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 33/86 [D loss: 0.7430253326892853, acc.: 37.01%] [G loss: 0.7350707650184631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 34/86 [D loss: 0.7429034113883972, acc.: 34.96%] [G loss: 0.7149732112884521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 35/86 [D loss: 0.7429687082767487, acc.: 35.74%] [G loss: 0.7130064368247986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 36/86 [D loss: 0.7360548973083496, acc.: 37.11%] [G loss: 0.7085148096084595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 37/86 [D loss: 0.7347031533718109, acc.: 36.77%] [G loss: 0.7001481652259827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 38/86 [D loss: 0.7409813404083252, acc.: 34.81%] [G loss: 0.6906954646110535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 39/86 [D loss: 0.7395075857639313, acc.: 35.60%] [G loss: 0.6876379251480103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 40/86 [D loss: 0.7407616376876831, acc.: 35.40%] [G loss: 0.7074315547943115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 41/86 [D loss: 0.7382977902889252, acc.: 36.23%] [G loss: 0.7296594977378845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 42/86 [D loss: 0.7308882474899292, acc.: 37.84%] [G loss: 0.7485617399215698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 43/86 [D loss: 0.7329543232917786, acc.: 37.11%] [G loss: 0.7490620613098145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 44/86 [D loss: 0.7386700809001923, acc.: 35.84%] [G loss: 0.7256208062171936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 45/86 [D loss: 0.7419416010379791, acc.: 32.81%] [G loss: 0.7062652111053467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 46/86 [D loss: 0.7372832894325256, acc.: 34.03%] [G loss: 0.7050576210021973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 47/86 [D loss: 0.7346533536911011, acc.: 35.35%] [G loss: 0.7042950391769409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 48/86 [D loss: 0.7318398058414459, acc.: 35.60%] [G loss: 0.6907656192779541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 49/86 [D loss: 0.7342721819877625, acc.: 35.21%] [G loss: 0.6784676313400269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 50/86 [D loss: 0.7359273135662079, acc.: 37.01%] [G loss: 0.6820701360702515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 51/86 [D loss: 0.7365586459636688, acc.: 38.18%] [G loss: 0.6932498812675476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 52/86 [D loss: 0.7402185797691345, acc.: 36.13%] [G loss: 0.7265899777412415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 53/86 [D loss: 0.7320113182067871, acc.: 38.92%] [G loss: 0.758675754070282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 54/86 [D loss: 0.7234064936637878, acc.: 40.72%] [G loss: 0.7565486431121826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 55/86 [D loss: 0.7238967716693878, acc.: 40.28%] [G loss: 0.7349848747253418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 56/86 [D loss: 0.7271120250225067, acc.: 39.16%] [G loss: 0.727024257183075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 57/86 [D loss: 0.7251092791557312, acc.: 37.35%] [G loss: 0.7211667895317078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 58/86 [D loss: 0.716264933347702, acc.: 41.75%] [G loss: 0.7160419821739197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 6/200, Batch 59/86 [D loss: 0.7041080296039581, acc.: 46.88%] [G loss: 0.7173779010772705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 60/86 [D loss: 0.6969449818134308, acc.: 50.78%] [G loss: 0.6981619596481323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 61/86 [D loss: 0.7113897800445557, acc.: 45.17%] [G loss: 0.6697685718536377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 62/86 [D loss: 0.7243821024894714, acc.: 45.12%] [G loss: 0.6605644822120667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 63/86 [D loss: 0.728548139333725, acc.: 41.89%] [G loss: 0.7075662612915039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 64/86 [D loss: 0.713081419467926, acc.: 45.12%] [G loss: 0.795777440071106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 65/86 [D loss: 0.6934005618095398, acc.: 49.80%] [G loss: 0.8329238295555115]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 6/200, Batch 66/86 [D loss: 0.6927379667758942, acc.: 51.61%] [G loss: 0.7965829968452454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 67/86 [D loss: 0.7214208543300629, acc.: 40.77%] [G loss: 0.7323678135871887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 68/86 [D loss: 0.7339690625667572, acc.: 34.67%] [G loss: 0.7075958847999573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 69/86 [D loss: 0.725334107875824, acc.: 36.28%] [G loss: 0.7064318656921387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 70/86 [D loss: 0.7049249708652496, acc.: 44.43%] [G loss: 0.7222974300384521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 71/86 [D loss: 0.6865586638450623, acc.: 54.49%] [G loss: 0.7179186940193176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 72/86 [D loss: 0.6848740577697754, acc.: 54.59%] [G loss: 0.6740062832832336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 73/86 [D loss: 0.7043309211730957, acc.: 47.80%] [G loss: 0.6297411918640137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 74/86 [D loss: 0.7417231500148773, acc.: 40.58%] [G loss: 0.6039097905158997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 75/86 [D loss: 0.7600930631160736, acc.: 38.23%] [G loss: 0.6684423685073853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 76/86 [D loss: 0.7270522117614746, acc.: 39.70%] [G loss: 0.7884541153907776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 77/86 [D loss: 0.6943262815475464, acc.: 50.34%] [G loss: 0.8265029788017273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 78/86 [D loss: 0.705740749835968, acc.: 47.31%] [G loss: 0.7855181694030762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 79/86 [D loss: 0.7194617986679077, acc.: 42.72%] [G loss: 0.7416879534721375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 80/86 [D loss: 0.7317591905593872, acc.: 38.33%] [G loss: 0.7131284475326538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 81/86 [D loss: 0.7268674075603485, acc.: 37.06%] [G loss: 0.706346333026886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 82/86 [D loss: 0.7149326503276825, acc.: 42.14%] [G loss: 0.7121504545211792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 83/86 [D loss: 0.7029838860034943, acc.: 46.34%] [G loss: 0.7013522982597351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 84/86 [D loss: 0.6966249942779541, acc.: 48.68%] [G loss: 0.6796889305114746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 85/86 [D loss: 0.7126166224479675, acc.: 44.97%] [G loss: 0.6451179385185242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 86/86 [D loss: 0.735410749912262, acc.: 39.94%] [G loss: 0.6340001821517944]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 1/86 [D loss: 0.7386612892150879, acc.: 38.72%] [G loss: 0.6870777010917664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 2/86 [D loss: 0.7207909822463989, acc.: 42.48%] [G loss: 0.7682023644447327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 3/86 [D loss: 0.7052383422851562, acc.: 46.48%] [G loss: 0.7964796423912048]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 4/86 [D loss: 0.7032665908336639, acc.: 48.10%] [G loss: 0.769244372844696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 5/86 [D loss: 0.7188715934753418, acc.: 41.70%] [G loss: 0.741104245185852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 6/86 [D loss: 0.7332549393177032, acc.: 37.70%] [G loss: 0.721174418926239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 7/86 [D loss: 0.7241978049278259, acc.: 38.48%] [G loss: 0.7177598476409912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 8/86 [D loss: 0.7084831893444061, acc.: 43.51%] [G loss: 0.7071139216423035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 9/86 [D loss: 0.7058519124984741, acc.: 44.09%] [G loss: 0.6935381293296814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 10/86 [D loss: 0.7143969535827637, acc.: 41.16%] [G loss: 0.6632802486419678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 11/86 [D loss: 0.7271320819854736, acc.: 38.23%] [G loss: 0.6515697836875916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 12/86 [D loss: 0.7356036603450775, acc.: 38.13%] [G loss: 0.6788363456726074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 13/86 [D loss: 0.7290902733802795, acc.: 38.72%] [G loss: 0.7354788184165955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 14/86 [D loss: 0.7152468860149384, acc.: 42.33%] [G loss: 0.763117253780365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 15/86 [D loss: 0.7171734571456909, acc.: 41.36%] [G loss: 0.7536446452140808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 16/86 [D loss: 0.7244633436203003, acc.: 40.72%] [G loss: 0.7386722564697266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 17/86 [D loss: 0.7270491123199463, acc.: 37.99%] [G loss: 0.7328590154647827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 18/86 [D loss: 0.716608464717865, acc.: 40.38%] [G loss: 0.734770655632019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 19/86 [D loss: 0.7079002261161804, acc.: 44.58%] [G loss: 0.7296990752220154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 20/86 [D loss: 0.7044278979301453, acc.: 45.85%] [G loss: 0.7228268384933472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 21/86 [D loss: 0.707452803850174, acc.: 45.17%] [G loss: 0.7059725522994995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 22/86 [D loss: 0.7078951597213745, acc.: 44.38%] [G loss: 0.7127923369407654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 23/86 [D loss: 0.7070591747760773, acc.: 47.27%] [G loss: 0.7355698347091675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 24/86 [D loss: 0.7055144608020782, acc.: 45.65%] [G loss: 0.7545242309570312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 25/86 [D loss: 0.7006419003009796, acc.: 49.37%] [G loss: 0.7568258047103882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 26/86 [D loss: 0.7053482830524445, acc.: 47.27%] [G loss: 0.7381495833396912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 27/86 [D loss: 0.7178157567977905, acc.: 41.89%] [G loss: 0.7190220952033997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 28/86 [D loss: 0.7209257781505585, acc.: 39.55%] [G loss: 0.7024949193000793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 29/86 [D loss: 0.7240145802497864, acc.: 37.21%] [G loss: 0.6916411519050598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 30/86 [D loss: 0.72048619389534, acc.: 38.77%] [G loss: 0.677940309047699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 31/86 [D loss: 0.7277442812919617, acc.: 35.94%] [G loss: 0.6625522375106812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 32/86 [D loss: 0.7285104990005493, acc.: 37.26%] [G loss: 0.6606583595275879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 33/86 [D loss: 0.7328998446464539, acc.: 36.18%] [G loss: 0.6697374582290649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 34/86 [D loss: 0.7272043228149414, acc.: 36.87%] [G loss: 0.686939537525177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 35/86 [D loss: 0.7183151543140411, acc.: 39.84%] [G loss: 0.7192305326461792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 7/200, Batch 36/86 [D loss: 0.7155066728591919, acc.: 41.85%] [G loss: 0.729393720626831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 37/86 [D loss: 0.7072462737560272, acc.: 45.21%] [G loss: 0.738056480884552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 38/86 [D loss: 0.707588404417038, acc.: 46.00%] [G loss: 0.7322154641151428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 39/86 [D loss: 0.6981906890869141, acc.: 50.98%] [G loss: 0.7311154007911682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 40/86 [D loss: 0.6988429725170135, acc.: 48.63%] [G loss: 0.7166502475738525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 41/86 [D loss: 0.6977483630180359, acc.: 48.49%] [G loss: 0.7105557918548584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 42/86 [D loss: 0.7008745968341827, acc.: 46.83%] [G loss: 0.694808304309845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 43/86 [D loss: 0.7054418921470642, acc.: 46.09%] [G loss: 0.6838490962982178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 44/86 [D loss: 0.712630957365036, acc.: 41.46%] [G loss: 0.6795538067817688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 45/86 [D loss: 0.7151502668857574, acc.: 42.63%] [G loss: 0.6839450597763062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 46/86 [D loss: 0.7185731530189514, acc.: 41.65%] [G loss: 0.6978037357330322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 47/86 [D loss: 0.7194381952285767, acc.: 41.11%] [G loss: 0.7093038558959961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 48/86 [D loss: 0.7221965193748474, acc.: 38.82%] [G loss: 0.7048402428627014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 49/86 [D loss: 0.7290470898151398, acc.: 35.99%] [G loss: 0.6936130523681641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 50/86 [D loss: 0.7351408302783966, acc.: 32.91%] [G loss: 0.6903938055038452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 51/86 [D loss: 0.7321158051490784, acc.: 33.84%] [G loss: 0.6866283416748047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 52/86 [D loss: 0.7302876114845276, acc.: 33.54%] [G loss: 0.6864371299743652]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 7/200, Batch 53/86 [D loss: 0.7225092053413391, acc.: 38.28%] [G loss: 0.697540819644928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 54/86 [D loss: 0.7138764262199402, acc.: 42.72%] [G loss: 0.7137365341186523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 55/86 [D loss: 0.7053819894790649, acc.: 46.29%] [G loss: 0.7422263026237488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 56/86 [D loss: 0.6847656071186066, acc.: 55.32%] [G loss: 0.7662244439125061]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 57/86 [D loss: 0.6833710968494415, acc.: 56.79%] [G loss: 0.7787110805511475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 58/86 [D loss: 0.6768536269664764, acc.: 57.62%] [G loss: 0.7754318118095398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 59/86 [D loss: 0.6826068758964539, acc.: 56.59%] [G loss: 0.7555297613143921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 60/86 [D loss: 0.6938478648662567, acc.: 49.90%] [G loss: 0.7394994497299194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 61/86 [D loss: 0.6997113823890686, acc.: 48.88%] [G loss: 0.7184395790100098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 62/86 [D loss: 0.7054004669189453, acc.: 46.92%] [G loss: 0.701600193977356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 63/86 [D loss: 0.7147716581821442, acc.: 41.60%] [G loss: 0.6850407123565674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 64/86 [D loss: 0.7189333736896515, acc.: 40.48%] [G loss: 0.6745673418045044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 65/86 [D loss: 0.7270746827125549, acc.: 35.69%] [G loss: 0.6625540256500244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 66/86 [D loss: 0.7304176688194275, acc.: 35.40%] [G loss: 0.6479230523109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 67/86 [D loss: 0.7390972971916199, acc.: 30.62%] [G loss: 0.645393967628479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 68/86 [D loss: 0.7388493418693542, acc.: 32.28%] [G loss: 0.6498709917068481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 69/86 [D loss: 0.7294147908687592, acc.: 35.16%] [G loss: 0.6734906435012817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 70/86 [D loss: 0.7176161110401154, acc.: 38.23%] [G loss: 0.7120943665504456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 71/86 [D loss: 0.6903619766235352, acc.: 53.08%] [G loss: 0.7581323385238647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 72/86 [D loss: 0.6712948679924011, acc.: 64.94%] [G loss: 0.7789916396141052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 73/86 [D loss: 0.6593851149082184, acc.: 69.68%] [G loss: 0.7755659222602844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 74/86 [D loss: 0.659545361995697, acc.: 68.65%] [G loss: 0.7581142783164978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 75/86 [D loss: 0.659866452217102, acc.: 68.21%] [G loss: 0.732719361782074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 76/86 [D loss: 0.6705620288848877, acc.: 60.99%] [G loss: 0.6871013641357422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 77/86 [D loss: 0.6958029866218567, acc.: 52.00%] [G loss: 0.6403795480728149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 78/86 [D loss: 0.7208026349544525, acc.: 46.88%] [G loss: 0.6468498706817627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 79/86 [D loss: 0.7143925726413727, acc.: 46.53%] [G loss: 0.7065653800964355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 80/86 [D loss: 0.6937538981437683, acc.: 51.32%] [G loss: 0.7422046065330505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 81/86 [D loss: 0.6916669011116028, acc.: 52.69%] [G loss: 0.7203967571258545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 82/86 [D loss: 0.7207094132900238, acc.: 41.11%] [G loss: 0.6862242221832275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 83/86 [D loss: 0.7330330908298492, acc.: 33.06%] [G loss: 0.6705089807510376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 84/86 [D loss: 0.7315706610679626, acc.: 32.81%] [G loss: 0.662002444267273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 85/86 [D loss: 0.7289077937602997, acc.: 35.21%] [G loss: 0.6519669890403748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 86/86 [D loss: 0.7226645350456238, acc.: 39.06%] [G loss: 0.6434545516967773]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 8/200, Batch 1/86 [D loss: 0.7329729497432709, acc.: 37.35%] [G loss: 0.623208224773407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 2/86 [D loss: 0.7428292632102966, acc.: 37.70%] [G loss: 0.6225378513336182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 3/86 [D loss: 0.7394639253616333, acc.: 37.30%] [G loss: 0.6555072069168091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 4/86 [D loss: 0.7266861498355865, acc.: 39.31%] [G loss: 0.7006876468658447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 5/86 [D loss: 0.7097684144973755, acc.: 43.26%] [G loss: 0.7517991065979004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 6/86 [D loss: 0.6971345841884613, acc.: 48.39%] [G loss: 0.781853199005127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 7/86 [D loss: 0.6928597688674927, acc.: 51.95%] [G loss: 0.7834967374801636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 8/86 [D loss: 0.6938169896602631, acc.: 52.64%] [G loss: 0.7813947796821594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 9/86 [D loss: 0.6943078935146332, acc.: 53.37%] [G loss: 0.7633604407310486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 10/86 [D loss: 0.6992237567901611, acc.: 49.56%] [G loss: 0.7389824986457825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 11/86 [D loss: 0.7035373151302338, acc.: 47.31%] [G loss: 0.7193112969398499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 12/86 [D loss: 0.710013210773468, acc.: 43.02%] [G loss: 0.705195963382721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 13/86 [D loss: 0.7136213481426239, acc.: 42.33%] [G loss: 0.6946601867675781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 14/86 [D loss: 0.7143877148628235, acc.: 40.92%] [G loss: 0.6932135224342346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 15/86 [D loss: 0.7199400663375854, acc.: 38.23%] [G loss: 0.6860880255699158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 16/86 [D loss: 0.724162369966507, acc.: 36.28%] [G loss: 0.6814837455749512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 17/86 [D loss: 0.7273947298526764, acc.: 35.55%] [G loss: 0.6763139367103577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 18/86 [D loss: 0.7300411760807037, acc.: 35.89%] [G loss: 0.6738298535346985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 19/86 [D loss: 0.7251231372356415, acc.: 35.69%] [G loss: 0.6837652921676636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 20/86 [D loss: 0.7201856672763824, acc.: 38.28%] [G loss: 0.69412761926651]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 21/86 [D loss: 0.7070541679859161, acc.: 43.41%] [G loss: 0.7172859311103821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 22/86 [D loss: 0.6956987380981445, acc.: 50.73%] [G loss: 0.7389715909957886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 23/86 [D loss: 0.6832903027534485, acc.: 58.11%] [G loss: 0.7430669665336609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 24/86 [D loss: 0.6795973181724548, acc.: 58.84%] [G loss: 0.7434635758399963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 25/86 [D loss: 0.6829763650894165, acc.: 56.40%] [G loss: 0.7273101806640625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 26/86 [D loss: 0.685779333114624, acc.: 54.54%] [G loss: 0.7153239846229553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 27/86 [D loss: 0.6931082904338837, acc.: 51.76%] [G loss: 0.7058468461036682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 28/86 [D loss: 0.7037701606750488, acc.: 45.95%] [G loss: 0.6888297200202942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 29/86 [D loss: 0.7052482962608337, acc.: 44.53%] [G loss: 0.6875772476196289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 30/86 [D loss: 0.7132847011089325, acc.: 42.19%] [G loss: 0.681084156036377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 31/86 [D loss: 0.7180849015712738, acc.: 39.89%] [G loss: 0.6761340498924255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 32/86 [D loss: 0.7175519466400146, acc.: 39.40%] [G loss: 0.6748927235603333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 33/86 [D loss: 0.7194958925247192, acc.: 39.11%] [G loss: 0.6788511872291565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 34/86 [D loss: 0.7180615067481995, acc.: 39.21%] [G loss: 0.6788762807846069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 35/86 [D loss: 0.7198777496814728, acc.: 39.31%] [G loss: 0.6869181990623474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 36/86 [D loss: 0.7165632545948029, acc.: 42.09%] [G loss: 0.6963316202163696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 37/86 [D loss: 0.7114339172840118, acc.: 43.26%] [G loss: 0.7081074118614197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 38/86 [D loss: 0.7115786969661713, acc.: 41.70%] [G loss: 0.7121411561965942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 39/86 [D loss: 0.7046380341053009, acc.: 46.29%] [G loss: 0.7212234139442444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 40/86 [D loss: 0.7038524746894836, acc.: 47.51%] [G loss: 0.7304518222808838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 41/86 [D loss: 0.7017135620117188, acc.: 46.04%] [G loss: 0.7317960262298584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 8/200, Batch 42/86 [D loss: 0.7022371888160706, acc.: 47.85%] [G loss: 0.7279418110847473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 43/86 [D loss: 0.7024583518505096, acc.: 48.05%] [G loss: 0.7228975892066956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 44/86 [D loss: 0.7023960053920746, acc.: 47.66%] [G loss: 0.7180138826370239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 45/86 [D loss: 0.7042026817798615, acc.: 45.85%] [G loss: 0.7108882665634155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 46/86 [D loss: 0.7068986892700195, acc.: 44.78%] [G loss: 0.7016682624816895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 47/86 [D loss: 0.7086777091026306, acc.: 42.33%] [G loss: 0.7009938955307007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 48/86 [D loss: 0.712136834859848, acc.: 42.72%] [G loss: 0.6977502703666687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 49/86 [D loss: 0.7148309648036957, acc.: 40.19%] [G loss: 0.7004814743995667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 50/86 [D loss: 0.714728593826294, acc.: 39.84%] [G loss: 0.7057929635047913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 51/86 [D loss: 0.7089081406593323, acc.: 43.85%] [G loss: 0.70882248878479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 52/86 [D loss: 0.7110022604465485, acc.: 41.60%] [G loss: 0.7097243070602417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 53/86 [D loss: 0.706974983215332, acc.: 44.24%] [G loss: 0.708813488483429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 54/86 [D loss: 0.7089737355709076, acc.: 43.21%] [G loss: 0.708389163017273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 55/86 [D loss: 0.7077818214893341, acc.: 42.19%] [G loss: 0.7028439044952393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 56/86 [D loss: 0.7046575248241425, acc.: 43.26%] [G loss: 0.6982250213623047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 57/86 [D loss: 0.7043558955192566, acc.: 45.17%] [G loss: 0.6968110799789429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 58/86 [D loss: 0.7048738300800323, acc.: 45.12%] [G loss: 0.686663806438446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 59/86 [D loss: 0.7032346725463867, acc.: 44.87%] [G loss: 0.67802894115448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 60/86 [D loss: 0.7071192562580109, acc.: 45.51%] [G loss: 0.6753668785095215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 61/86 [D loss: 0.7124887108802795, acc.: 42.53%] [G loss: 0.680774986743927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 62/86 [D loss: 0.7110174596309662, acc.: 43.46%] [G loss: 0.6917719841003418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 63/86 [D loss: 0.7095524668693542, acc.: 43.70%] [G loss: 0.7118996381759644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 64/86 [D loss: 0.7026465833187103, acc.: 45.70%] [G loss: 0.7217663526535034]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 65/86 [D loss: 0.700049638748169, acc.: 48.05%] [G loss: 0.7257434725761414]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 8/200, Batch 66/86 [D loss: 0.7013272643089294, acc.: 45.80%] [G loss: 0.7170040607452393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 67/86 [D loss: 0.6979019045829773, acc.: 47.90%] [G loss: 0.7071191072463989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 68/86 [D loss: 0.6940982043743134, acc.: 50.49%] [G loss: 0.7004424929618835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 69/86 [D loss: 0.689011812210083, acc.: 51.56%] [G loss: 0.6933016180992126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 70/86 [D loss: 0.6824747920036316, acc.: 56.98%] [G loss: 0.687855064868927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 71/86 [D loss: 0.6933082342147827, acc.: 52.64%] [G loss: 0.6539531350135803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 72/86 [D loss: 0.7106750309467316, acc.: 47.36%] [G loss: 0.6205547451972961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 73/86 [D loss: 0.7304661273956299, acc.: 42.19%] [G loss: 0.6267995834350586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 74/86 [D loss: 0.7330518364906311, acc.: 39.60%] [G loss: 0.671642541885376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 75/86 [D loss: 0.7120345532894135, acc.: 44.19%] [G loss: 0.71803879737854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 76/86 [D loss: 0.7052172124385834, acc.: 44.38%] [G loss: 0.718117892742157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 77/86 [D loss: 0.7166031897068024, acc.: 38.09%] [G loss: 0.7037163376808167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 78/86 [D loss: 0.721294105052948, acc.: 35.64%] [G loss: 0.6918274164199829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 79/86 [D loss: 0.7212927937507629, acc.: 34.28%] [G loss: 0.6956106424331665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 80/86 [D loss: 0.7138889133930206, acc.: 38.48%] [G loss: 0.7027239799499512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 81/86 [D loss: 0.7044914960861206, acc.: 44.97%] [G loss: 0.7082412242889404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 82/86 [D loss: 0.6962786912918091, acc.: 49.41%] [G loss: 0.7030007839202881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 83/86 [D loss: 0.6935014128684998, acc.: 52.20%] [G loss: 0.6940432786941528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 84/86 [D loss: 0.6958112120628357, acc.: 50.93%] [G loss: 0.6928576827049255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 85/86 [D loss: 0.7011958360671997, acc.: 48.78%] [G loss: 0.6964828372001648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 86/86 [D loss: 0.6964023113250732, acc.: 50.88%] [G loss: 0.7256040573120117]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 1/86 [D loss: 0.694321483373642, acc.: 49.56%] [G loss: 0.7427380084991455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 2/86 [D loss: 0.6889537572860718, acc.: 53.03%] [G loss: 0.7376493811607361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 3/86 [D loss: 0.6968032717704773, acc.: 47.75%] [G loss: 0.7272990942001343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 4/86 [D loss: 0.7007144093513489, acc.: 47.07%] [G loss: 0.709145724773407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 5/86 [D loss: 0.7092005014419556, acc.: 41.80%] [G loss: 0.6982507109642029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 6/86 [D loss: 0.710283637046814, acc.: 41.80%] [G loss: 0.6880602836608887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 7/86 [D loss: 0.7102563381195068, acc.: 40.23%] [G loss: 0.6806718707084656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 8/86 [D loss: 0.7111206352710724, acc.: 41.50%] [G loss: 0.666262149810791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 9/86 [D loss: 0.7163135409355164, acc.: 41.16%] [G loss: 0.6581701636314392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 10/86 [D loss: 0.7182523012161255, acc.: 39.79%] [G loss: 0.6631404161453247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 11/86 [D loss: 0.719278872013092, acc.: 39.55%] [G loss: 0.6808359026908875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 12/86 [D loss: 0.7124070227146149, acc.: 42.63%] [G loss: 0.7052385807037354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 13/86 [D loss: 0.7043699622154236, acc.: 46.09%] [G loss: 0.7310059070587158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 14/86 [D loss: 0.6965529024600983, acc.: 48.97%] [G loss: 0.7395588159561157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 15/86 [D loss: 0.6975604891777039, acc.: 48.49%] [G loss: 0.739514946937561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 16/86 [D loss: 0.6971400380134583, acc.: 49.80%] [G loss: 0.7301310896873474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 17/86 [D loss: 0.697769284248352, acc.: 49.80%] [G loss: 0.7176494002342224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 18/86 [D loss: 0.6985197365283966, acc.: 48.10%] [G loss: 0.7105531692504883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 19/86 [D loss: 0.6954725086688995, acc.: 49.37%] [G loss: 0.6991913318634033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 20/86 [D loss: 0.6986952424049377, acc.: 48.19%] [G loss: 0.6829909086227417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 21/86 [D loss: 0.7058872580528259, acc.: 45.70%] [G loss: 0.6661547422409058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 22/86 [D loss: 0.7146724462509155, acc.: 41.80%] [G loss: 0.6606225371360779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 23/86 [D loss: 0.7173624038696289, acc.: 41.46%] [G loss: 0.6720954775810242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 24/86 [D loss: 0.7140542566776276, acc.: 42.53%] [G loss: 0.6860500574111938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 25/86 [D loss: 0.7098718881607056, acc.: 42.77%] [G loss: 0.6998688578605652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 26/86 [D loss: 0.713228702545166, acc.: 40.14%] [G loss: 0.706561267375946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 27/86 [D loss: 0.70777428150177, acc.: 43.21%] [G loss: 0.7132485508918762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 28/86 [D loss: 0.7060319185256958, acc.: 43.60%] [G loss: 0.7137628197669983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 29/86 [D loss: 0.7003017961978912, acc.: 48.10%] [G loss: 0.7208056449890137]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 9/200, Batch 30/86 [D loss: 0.6951811015605927, acc.: 50.54%] [G loss: 0.7149764895439148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 31/86 [D loss: 0.6986566781997681, acc.: 47.27%] [G loss: 0.711570680141449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 32/86 [D loss: 0.6947706043720245, acc.: 51.17%] [G loss: 0.7000146508216858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 33/86 [D loss: 0.6988324522972107, acc.: 47.75%] [G loss: 0.6905362010002136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 34/86 [D loss: 0.7016497850418091, acc.: 46.44%] [G loss: 0.6859134435653687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 35/86 [D loss: 0.7044359445571899, acc.: 43.80%] [G loss: 0.6895358562469482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 36/86 [D loss: 0.7099276781082153, acc.: 41.11%] [G loss: 0.6887456774711609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 37/86 [D loss: 0.7117548286914825, acc.: 40.58%] [G loss: 0.6914348602294922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 38/86 [D loss: 0.7119252979755402, acc.: 41.06%] [G loss: 0.6953750848770142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 39/86 [D loss: 0.711310088634491, acc.: 39.45%] [G loss: 0.7043110728263855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 40/86 [D loss: 0.7091203033924103, acc.: 41.21%] [G loss: 0.705768346786499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 41/86 [D loss: 0.7061158716678619, acc.: 43.46%] [G loss: 0.7097223401069641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 42/86 [D loss: 0.7034288644790649, acc.: 46.00%] [G loss: 0.7180884480476379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 43/86 [D loss: 0.7001423239707947, acc.: 46.92%] [G loss: 0.7261264324188232]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 44/86 [D loss: 0.6962784826755524, acc.: 49.07%] [G loss: 0.7389849424362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 45/86 [D loss: 0.6911814212799072, acc.: 53.76%] [G loss: 0.7339669466018677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 46/86 [D loss: 0.6890966296195984, acc.: 54.74%] [G loss: 0.7380972504615784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 47/86 [D loss: 0.6867187023162842, acc.: 55.13%] [G loss: 0.7402172684669495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 48/86 [D loss: 0.6889399290084839, acc.: 54.74%] [G loss: 0.7337299585342407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 49/86 [D loss: 0.6899006962776184, acc.: 52.98%] [G loss: 0.7269189953804016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 50/86 [D loss: 0.6958314776420593, acc.: 49.02%] [G loss: 0.7202560901641846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 51/86 [D loss: 0.6990845799446106, acc.: 47.85%] [G loss: 0.7075759172439575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 52/86 [D loss: 0.6999286413192749, acc.: 46.78%] [G loss: 0.7022911310195923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 53/86 [D loss: 0.7043817937374115, acc.: 43.31%] [G loss: 0.6956334114074707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 54/86 [D loss: 0.7080814242362976, acc.: 42.24%] [G loss: 0.695853590965271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 55/86 [D loss: 0.7084399163722992, acc.: 43.21%] [G loss: 0.695392906665802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 56/86 [D loss: 0.7089675962924957, acc.: 41.41%] [G loss: 0.6936935186386108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 57/86 [D loss: 0.7061757743358612, acc.: 43.31%] [G loss: 0.6957172155380249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 58/86 [D loss: 0.7036111652851105, acc.: 44.43%] [G loss: 0.6990153789520264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 59/86 [D loss: 0.7054310739040375, acc.: 42.43%] [G loss: 0.7015855312347412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 60/86 [D loss: 0.7025107741355896, acc.: 44.14%] [G loss: 0.7049068212509155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 61/86 [D loss: 0.6998481154441833, acc.: 45.61%] [G loss: 0.7067933678627014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 62/86 [D loss: 0.6973092257976532, acc.: 47.51%] [G loss: 0.7104906439781189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 63/86 [D loss: 0.6947671473026276, acc.: 49.37%] [G loss: 0.7078448534011841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 64/86 [D loss: 0.6893959939479828, acc.: 52.64%] [G loss: 0.7115451097488403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 65/86 [D loss: 0.6920289397239685, acc.: 52.05%] [G loss: 0.707679271697998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 66/86 [D loss: 0.693597137928009, acc.: 50.68%] [G loss: 0.7020790576934814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 67/86 [D loss: 0.6955073177814484, acc.: 49.41%] [G loss: 0.7020284533500671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 68/86 [D loss: 0.6986860930919647, acc.: 47.56%] [G loss: 0.6960152983665466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 69/86 [D loss: 0.6973874270915985, acc.: 48.78%] [G loss: 0.6902931332588196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 70/86 [D loss: 0.697599858045578, acc.: 48.39%] [G loss: 0.6879331469535828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 71/86 [D loss: 0.7021211981773376, acc.: 46.44%] [G loss: 0.6839936971664429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 72/86 [D loss: 0.7062325775623322, acc.: 44.87%] [G loss: 0.6851938962936401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 73/86 [D loss: 0.7077434659004211, acc.: 42.33%] [G loss: 0.6806206703186035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 74/86 [D loss: 0.7089907824993134, acc.: 43.85%] [G loss: 0.681122899055481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 75/86 [D loss: 0.7089400887489319, acc.: 42.63%] [G loss: 0.6792759895324707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 76/86 [D loss: 0.7101955413818359, acc.: 42.53%] [G loss: 0.6823717951774597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 77/86 [D loss: 0.7115562856197357, acc.: 41.89%] [G loss: 0.6831463575363159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 78/86 [D loss: 0.707778126001358, acc.: 42.97%] [G loss: 0.6860412359237671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 79/86 [D loss: 0.7070594131946564, acc.: 44.63%] [G loss: 0.6881863474845886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 80/86 [D loss: 0.7093375623226166, acc.: 42.77%] [G loss: 0.6964241862297058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 81/86 [D loss: 0.7054937481880188, acc.: 44.48%] [G loss: 0.6950165629386902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 82/86 [D loss: 0.7056312561035156, acc.: 44.19%] [G loss: 0.7001458406448364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 83/86 [D loss: 0.7024756968021393, acc.: 45.41%] [G loss: 0.7027156352996826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 84/86 [D loss: 0.7007694244384766, acc.: 47.71%] [G loss: 0.7036978006362915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 85/86 [D loss: 0.6983267068862915, acc.: 48.14%] [G loss: 0.7058790326118469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 86/86 [D loss: 0.7009140253067017, acc.: 46.73%] [G loss: 0.7109785079956055]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 1/86 [D loss: 0.6995630860328674, acc.: 46.88%] [G loss: 0.7074161767959595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 2/86 [D loss: 0.70054891705513, acc.: 46.97%] [G loss: 0.7069122195243835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 3/86 [D loss: 0.7024169564247131, acc.: 45.31%] [G loss: 0.7050086259841919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 4/86 [D loss: 0.698891669511795, acc.: 47.85%] [G loss: 0.7076303362846375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 5/86 [D loss: 0.7030036151409149, acc.: 46.00%] [G loss: 0.7050623893737793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 6/86 [D loss: 0.7019992470741272, acc.: 46.44%] [G loss: 0.7045846581459045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 7/86 [D loss: 0.6995497047901154, acc.: 47.07%] [G loss: 0.7033460140228271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 8/86 [D loss: 0.7027262449264526, acc.: 45.31%] [G loss: 0.7032878994941711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 9/86 [D loss: 0.7056103050708771, acc.: 44.14%] [G loss: 0.7067016959190369]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 10/200, Batch 10/86 [D loss: 0.7038258016109467, acc.: 43.75%] [G loss: 0.7076133489608765]\n",
      "32/32 [==============================] - 2s 60ms/step\n",
      "Epoch 10/200, Batch 11/86 [D loss: 0.7041929066181183, acc.: 45.36%] [G loss: 0.7039856910705566]\n",
      "32/32 [==============================] - 2s 58ms/step\n",
      "Epoch 10/200, Batch 12/86 [D loss: 0.7033000290393829, acc.: 45.17%] [G loss: 0.7073473930358887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 13/86 [D loss: 0.7055693864822388, acc.: 43.80%] [G loss: 0.7082622051239014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 14/86 [D loss: 0.7020625174045563, acc.: 45.85%] [G loss: 0.7063277959823608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 15/86 [D loss: 0.6994374096393585, acc.: 47.22%] [G loss: 0.7070775032043457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 16/86 [D loss: 0.6992773413658142, acc.: 47.22%] [G loss: 0.709243893623352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 17/86 [D loss: 0.7014337182044983, acc.: 45.21%] [G loss: 0.7124485373497009]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 18/86 [D loss: 0.702081948518753, acc.: 45.75%] [G loss: 0.7133119106292725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 19/86 [D loss: 0.6987132728099823, acc.: 48.83%] [G loss: 0.7128466963768005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 20/86 [D loss: 0.7003255784511566, acc.: 47.17%] [G loss: 0.713761031627655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 21/86 [D loss: 0.6964805424213409, acc.: 49.85%] [G loss: 0.7105048894882202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 22/86 [D loss: 0.6990861296653748, acc.: 47.12%] [G loss: 0.7105642557144165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 23/86 [D loss: 0.701409786939621, acc.: 46.24%] [G loss: 0.7108978033065796]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 24/86 [D loss: 0.7003372311592102, acc.: 47.95%] [G loss: 0.7087738513946533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 25/86 [D loss: 0.6992431282997131, acc.: 47.46%] [G loss: 0.7108681201934814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 26/86 [D loss: 0.7012373208999634, acc.: 45.56%] [G loss: 0.7116385698318481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 27/86 [D loss: 0.703591912984848, acc.: 42.87%] [G loss: 0.7077733278274536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 28/86 [D loss: 0.7024245858192444, acc.: 45.21%] [G loss: 0.7099374532699585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 29/86 [D loss: 0.6999199986457825, acc.: 46.48%] [G loss: 0.7131068110466003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 30/86 [D loss: 0.6970163881778717, acc.: 49.07%] [G loss: 0.7126748561859131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 31/86 [D loss: 0.6983227431774139, acc.: 47.27%] [G loss: 0.7143281698226929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 32/86 [D loss: 0.6996820867061615, acc.: 48.88%] [G loss: 0.7172487378120422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 33/86 [D loss: 0.6987020075321198, acc.: 46.78%] [G loss: 0.7178063988685608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 34/86 [D loss: 0.7007512152194977, acc.: 46.68%] [G loss: 0.714728832244873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 35/86 [D loss: 0.6984733939170837, acc.: 48.39%] [G loss: 0.7163432836532593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 36/86 [D loss: 0.6983102560043335, acc.: 48.00%] [G loss: 0.7178618907928467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 37/86 [D loss: 0.7005418241024017, acc.: 47.31%] [G loss: 0.7153017520904541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 38/86 [D loss: 0.6997734308242798, acc.: 47.51%] [G loss: 0.713385283946991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 39/86 [D loss: 0.7002542614936829, acc.: 47.12%] [G loss: 0.7128976583480835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 40/86 [D loss: 0.7024770975112915, acc.: 45.65%] [G loss: 0.7107288837432861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 41/86 [D loss: 0.7031796276569366, acc.: 43.41%] [G loss: 0.7078059911727905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 42/86 [D loss: 0.7043278813362122, acc.: 44.58%] [G loss: 0.707697331905365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 43/86 [D loss: 0.7014959752559662, acc.: 45.41%] [G loss: 0.7104066014289856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 44/86 [D loss: 0.705230712890625, acc.: 42.77%] [G loss: 0.7037639021873474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 45/86 [D loss: 0.702444463968277, acc.: 45.46%] [G loss: 0.7021656036376953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 46/86 [D loss: 0.7042303085327148, acc.: 44.73%] [G loss: 0.7010545134544373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 47/86 [D loss: 0.7034801840782166, acc.: 43.90%] [G loss: 0.7063833475112915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 48/86 [D loss: 0.7030700445175171, acc.: 44.14%] [G loss: 0.7087690830230713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 49/86 [D loss: 0.7026038765907288, acc.: 43.60%] [G loss: 0.7068005204200745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 50/86 [D loss: 0.6998341083526611, acc.: 46.48%] [G loss: 0.7051100730895996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 51/86 [D loss: 0.7004857063293457, acc.: 45.95%] [G loss: 0.7092934846878052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 52/86 [D loss: 0.7000470459461212, acc.: 46.00%] [G loss: 0.7111577391624451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 53/86 [D loss: 0.7013912796974182, acc.: 44.97%] [G loss: 0.7088354229927063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 54/86 [D loss: 0.6998324692249298, acc.: 46.73%] [G loss: 0.7090907096862793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 55/86 [D loss: 0.6975668370723724, acc.: 48.78%] [G loss: 0.7103002071380615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 56/86 [D loss: 0.6982063353061676, acc.: 47.71%] [G loss: 0.7061548829078674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 57/86 [D loss: 0.7006930708885193, acc.: 44.24%] [G loss: 0.7077500224113464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 58/86 [D loss: 0.6999541521072388, acc.: 46.68%] [G loss: 0.7095103859901428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 59/86 [D loss: 0.697391927242279, acc.: 48.97%] [G loss: 0.7139236927032471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 60/86 [D loss: 0.699187695980072, acc.: 46.24%] [G loss: 0.7063589096069336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 61/86 [D loss: 0.700916975736618, acc.: 46.34%] [G loss: 0.7057281136512756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 62/86 [D loss: 0.7018623352050781, acc.: 45.46%] [G loss: 0.7032459378242493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 63/86 [D loss: 0.7004635334014893, acc.: 46.58%] [G loss: 0.7032065987586975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 64/86 [D loss: 0.7025181651115417, acc.: 45.21%] [G loss: 0.6959387063980103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 65/86 [D loss: 0.6996127068996429, acc.: 47.07%] [G loss: 0.6958792209625244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 66/86 [D loss: 0.703930139541626, acc.: 45.90%] [G loss: 0.6948839426040649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 67/86 [D loss: 0.7021072208881378, acc.: 46.00%] [G loss: 0.6973450779914856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 68/86 [D loss: 0.7011395692825317, acc.: 46.63%] [G loss: 0.6968555450439453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 69/86 [D loss: 0.7040361166000366, acc.: 45.12%] [G loss: 0.7007680535316467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 70/86 [D loss: 0.7047854363918304, acc.: 43.36%] [G loss: 0.7021650075912476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 71/86 [D loss: 0.7025843858718872, acc.: 44.82%] [G loss: 0.7030947208404541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 72/86 [D loss: 0.7022465169429779, acc.: 45.75%] [G loss: 0.7064080238342285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 73/86 [D loss: 0.7035209238529205, acc.: 43.90%] [G loss: 0.700311005115509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 74/86 [D loss: 0.702060341835022, acc.: 46.68%] [G loss: 0.6994511485099792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 75/86 [D loss: 0.7025206685066223, acc.: 45.07%] [G loss: 0.6996261477470398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 76/86 [D loss: 0.7004658281803131, acc.: 47.36%] [G loss: 0.7025235295295715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 77/86 [D loss: 0.7028810679912567, acc.: 43.60%] [G loss: 0.6967459917068481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 78/86 [D loss: 0.7019818723201752, acc.: 45.56%] [G loss: 0.7055106163024902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 79/86 [D loss: 0.6997296512126923, acc.: 47.02%] [G loss: 0.7022768259048462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 80/86 [D loss: 0.701411098241806, acc.: 45.90%] [G loss: 0.7047160267829895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 81/86 [D loss: 0.6986913681030273, acc.: 47.56%] [G loss: 0.7036942839622498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 82/86 [D loss: 0.7009730637073517, acc.: 46.83%] [G loss: 0.7050747871398926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 83/86 [D loss: 0.7024860382080078, acc.: 45.65%] [G loss: 0.70578533411026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 84/86 [D loss: 0.7030442953109741, acc.: 44.24%] [G loss: 0.7026642560958862]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 85/86 [D loss: 0.7011873424053192, acc.: 45.31%] [G loss: 0.7003480792045593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 86/86 [D loss: 0.7026530504226685, acc.: 44.38%] [G loss: 0.6977207064628601]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 11/200, Batch 1/86 [D loss: 0.7033281922340393, acc.: 43.95%] [G loss: 0.6970577836036682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 2/86 [D loss: 0.701416552066803, acc.: 45.17%] [G loss: 0.696946918964386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 3/86 [D loss: 0.7019715309143066, acc.: 45.56%] [G loss: 0.6994670629501343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 4/86 [D loss: 0.703578919172287, acc.: 44.58%] [G loss: 0.6971958875656128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 5/86 [D loss: 0.7039009034633636, acc.: 44.48%] [G loss: 0.6997293829917908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 6/86 [D loss: 0.7016987502574921, acc.: 45.36%] [G loss: 0.7030150890350342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 7/86 [D loss: 0.6998928189277649, acc.: 48.00%] [G loss: 0.706136167049408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 8/86 [D loss: 0.7026731371879578, acc.: 44.82%] [G loss: 0.7003025412559509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 9/86 [D loss: 0.7037895321846008, acc.: 43.95%] [G loss: 0.7007194757461548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 10/86 [D loss: 0.6986425518989563, acc.: 46.29%] [G loss: 0.7041722536087036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 11/86 [D loss: 0.7017573118209839, acc.: 46.09%] [G loss: 0.6989466547966003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 12/86 [D loss: 0.7033914923667908, acc.: 44.14%] [G loss: 0.7050068378448486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 13/86 [D loss: 0.7009359300136566, acc.: 45.70%] [G loss: 0.7032891511917114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 14/86 [D loss: 0.7017487585544586, acc.: 45.75%] [G loss: 0.7048016786575317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 15/86 [D loss: 0.7018245160579681, acc.: 44.97%] [G loss: 0.7045430541038513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 16/86 [D loss: 0.7006012499332428, acc.: 45.65%] [G loss: 0.7007420063018799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 17/86 [D loss: 0.702490895986557, acc.: 43.80%] [G loss: 0.7005679607391357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 18/86 [D loss: 0.7027062177658081, acc.: 44.73%] [G loss: 0.7010046243667603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 19/86 [D loss: 0.7001837193965912, acc.: 47.02%] [G loss: 0.7043272256851196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 20/86 [D loss: 0.7033757567405701, acc.: 43.95%] [G loss: 0.7075209617614746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 21/86 [D loss: 0.7029996514320374, acc.: 43.65%] [G loss: 0.7029638886451721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 22/86 [D loss: 0.7020392715930939, acc.: 45.41%] [G loss: 0.704614520072937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 23/86 [D loss: 0.6981802582740784, acc.: 49.12%] [G loss: 0.7020168900489807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 24/86 [D loss: 0.7017079889774323, acc.: 45.85%] [G loss: 0.705115795135498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 25/86 [D loss: 0.7004383504390717, acc.: 46.44%] [G loss: 0.7083428502082825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 26/86 [D loss: 0.6984475553035736, acc.: 47.07%] [G loss: 0.706790566444397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 27/86 [D loss: 0.6993346214294434, acc.: 46.58%] [G loss: 0.7080005407333374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 28/86 [D loss: 0.6990669071674347, acc.: 46.97%] [G loss: 0.703972578048706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 29/86 [D loss: 0.700869232416153, acc.: 46.48%] [G loss: 0.7075761556625366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 30/86 [D loss: 0.6973659694194794, acc.: 47.36%] [G loss: 0.7067996859550476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 31/86 [D loss: 0.7032630741596222, acc.: 43.65%] [G loss: 0.7017458081245422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 32/86 [D loss: 0.698427826166153, acc.: 48.14%] [G loss: 0.7076401114463806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 33/86 [D loss: 0.7005880177021027, acc.: 46.39%] [G loss: 0.7035694718360901]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 34/86 [D loss: 0.699579119682312, acc.: 46.88%] [G loss: 0.703415036201477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 35/86 [D loss: 0.7008353769779205, acc.: 46.29%] [G loss: 0.7014645338058472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 36/86 [D loss: 0.7008143067359924, acc.: 45.46%] [G loss: 0.6992334127426147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 37/86 [D loss: 0.703416496515274, acc.: 44.04%] [G loss: 0.7028694152832031]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 38/86 [D loss: 0.7014731168746948, acc.: 46.83%] [G loss: 0.7050827741622925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 39/86 [D loss: 0.7017516493797302, acc.: 44.97%] [G loss: 0.7054047584533691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 40/86 [D loss: 0.7010959386825562, acc.: 46.04%] [G loss: 0.7068955302238464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 41/86 [D loss: 0.6983376443386078, acc.: 46.58%] [G loss: 0.7013903856277466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 42/86 [D loss: 0.7001032829284668, acc.: 46.88%] [G loss: 0.7023757696151733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 43/86 [D loss: 0.7017586529254913, acc.: 45.90%] [G loss: 0.7019039392471313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 44/86 [D loss: 0.6971665918827057, acc.: 49.51%] [G loss: 0.7032296061515808]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 45/86 [D loss: 0.7007216215133667, acc.: 47.17%] [G loss: 0.705989420413971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 46/86 [D loss: 0.701244443655014, acc.: 45.51%] [G loss: 0.705188512802124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 47/86 [D loss: 0.7014445066452026, acc.: 46.19%] [G loss: 0.7069011330604553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 48/86 [D loss: 0.7020805478096008, acc.: 45.70%] [G loss: 0.7014554142951965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 49/86 [D loss: 0.7018904387950897, acc.: 46.19%] [G loss: 0.705642580986023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 50/86 [D loss: 0.7029414474964142, acc.: 43.95%] [G loss: 0.7019087672233582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 51/86 [D loss: 0.7019264101982117, acc.: 45.36%] [G loss: 0.7019715309143066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 52/86 [D loss: 0.7018373310565948, acc.: 45.31%] [G loss: 0.704382061958313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 53/86 [D loss: 0.6989255845546722, acc.: 47.07%] [G loss: 0.7064073085784912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 54/86 [D loss: 0.6989932656288147, acc.: 47.12%] [G loss: 0.7070842385292053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 55/86 [D loss: 0.6997170746326447, acc.: 46.14%] [G loss: 0.7059327960014343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 56/86 [D loss: 0.6984895467758179, acc.: 46.78%] [G loss: 0.7071046233177185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 57/86 [D loss: 0.6996640563011169, acc.: 47.12%] [G loss: 0.7065390348434448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 58/86 [D loss: 0.6939189434051514, acc.: 50.20%] [G loss: 0.7085907459259033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 59/86 [D loss: 0.6962372064590454, acc.: 50.15%] [G loss: 0.7089939713478088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 60/86 [D loss: 0.6975938677787781, acc.: 48.34%] [G loss: 0.7060061097145081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 61/86 [D loss: 0.6969262957572937, acc.: 49.46%] [G loss: 0.7079519033432007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 62/86 [D loss: 0.6965034306049347, acc.: 50.34%] [G loss: 0.7048748731613159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 63/86 [D loss: 0.6953162848949432, acc.: 49.41%] [G loss: 0.7043635249137878]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 64/86 [D loss: 0.6955331265926361, acc.: 48.97%] [G loss: 0.7043111324310303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 65/86 [D loss: 0.6972818374633789, acc.: 48.05%] [G loss: 0.7061630487442017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 66/86 [D loss: 0.6964053213596344, acc.: 48.49%] [G loss: 0.7037534713745117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 67/86 [D loss: 0.6951791346073151, acc.: 50.78%] [G loss: 0.7025052905082703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 68/86 [D loss: 0.6976014375686646, acc.: 46.68%] [G loss: 0.7049376964569092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 69/86 [D loss: 0.700348436832428, acc.: 45.61%] [G loss: 0.7012980580329895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 70/86 [D loss: 0.6973917186260223, acc.: 48.83%] [G loss: 0.6998296976089478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 71/86 [D loss: 0.695830225944519, acc.: 49.80%] [G loss: 0.6985898017883301]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 72/86 [D loss: 0.6963843703269958, acc.: 49.02%] [G loss: 0.6982617974281311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 73/86 [D loss: 0.6982443928718567, acc.: 47.95%] [G loss: 0.6970188617706299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 74/86 [D loss: 0.6980533301830292, acc.: 47.80%] [G loss: 0.7001315951347351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 75/86 [D loss: 0.6989138424396515, acc.: 46.92%] [G loss: 0.7012798190116882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 76/86 [D loss: 0.6971572935581207, acc.: 48.34%] [G loss: 0.6965872049331665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 77/86 [D loss: 0.6998071074485779, acc.: 46.00%] [G loss: 0.6983070969581604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 78/86 [D loss: 0.6991763114929199, acc.: 46.88%] [G loss: 0.7012009620666504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 79/86 [D loss: 0.7006209194660187, acc.: 44.78%] [G loss: 0.6973254680633545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 80/86 [D loss: 0.6996193528175354, acc.: 45.70%] [G loss: 0.6994779109954834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 81/86 [D loss: 0.7012260258197784, acc.: 46.00%] [G loss: 0.6937539577484131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 82/86 [D loss: 0.7007302939891815, acc.: 45.02%] [G loss: 0.6964665055274963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 83/86 [D loss: 0.7011922597885132, acc.: 45.26%] [G loss: 0.6961924433708191]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 84/86 [D loss: 0.7004522085189819, acc.: 47.51%] [G loss: 0.6974858045578003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 85/86 [D loss: 0.7001366913318634, acc.: 46.39%] [G loss: 0.6988404989242554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 86/86 [D loss: 0.7016837894916534, acc.: 45.31%] [G loss: 0.6994875073432922]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 1/86 [D loss: 0.6986111402511597, acc.: 46.63%] [G loss: 0.7015718221664429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 2/86 [D loss: 0.7022460103034973, acc.: 45.80%] [G loss: 0.7031537294387817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 3/86 [D loss: 0.6996124982833862, acc.: 47.22%] [G loss: 0.7014219760894775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 4/86 [D loss: 0.7034489512443542, acc.: 43.07%] [G loss: 0.6974545121192932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 5/86 [D loss: 0.7018760144710541, acc.: 44.53%] [G loss: 0.6984193921089172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 6/86 [D loss: 0.7003399133682251, acc.: 45.56%] [G loss: 0.6989037394523621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 7/86 [D loss: 0.7020159959793091, acc.: 45.21%] [G loss: 0.7000221014022827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 8/86 [D loss: 0.6990828812122345, acc.: 45.95%] [G loss: 0.7029075026512146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 9/86 [D loss: 0.7012490928173065, acc.: 45.85%] [G loss: 0.7001747488975525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 10/86 [D loss: 0.7009192109107971, acc.: 44.48%] [G loss: 0.7026663422584534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 11/86 [D loss: 0.7024838924407959, acc.: 43.85%] [G loss: 0.7018099427223206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 12/86 [D loss: 0.7014475464820862, acc.: 44.87%] [G loss: 0.7024151682853699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 13/86 [D loss: 0.6997603178024292, acc.: 46.88%] [G loss: 0.7010316848754883]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 14/86 [D loss: 0.7004221379756927, acc.: 45.90%] [G loss: 0.7052172422409058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 15/86 [D loss: 0.6993699669837952, acc.: 47.41%] [G loss: 0.703808605670929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 16/86 [D loss: 0.6989193558692932, acc.: 45.56%] [G loss: 0.7021923065185547]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 17/86 [D loss: 0.7003768980503082, acc.: 45.61%] [G loss: 0.7023862600326538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 18/86 [D loss: 0.6990326642990112, acc.: 47.56%] [G loss: 0.7059832811355591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 19/86 [D loss: 0.6995702981948853, acc.: 46.44%] [G loss: 0.7025079131126404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 20/86 [D loss: 0.6981735527515411, acc.: 47.51%] [G loss: 0.7035729885101318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 21/86 [D loss: 0.6981581151485443, acc.: 45.90%] [G loss: 0.7023848295211792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 22/86 [D loss: 0.6991413235664368, acc.: 46.78%] [G loss: 0.702362060546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 23/86 [D loss: 0.701214998960495, acc.: 45.31%] [G loss: 0.7056332230567932]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 24/86 [D loss: 0.6979618966579437, acc.: 49.07%] [G loss: 0.7050115466117859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 25/86 [D loss: 0.7019200623035431, acc.: 44.63%] [G loss: 0.7036521434783936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 26/86 [D loss: 0.6981277167797089, acc.: 47.02%] [G loss: 0.7006410360336304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 27/86 [D loss: 0.6991210579872131, acc.: 46.88%] [G loss: 0.7029509544372559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 28/86 [D loss: 0.6999168992042542, acc.: 46.34%] [G loss: 0.7007052898406982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 29/86 [D loss: 0.6998418271541595, acc.: 46.09%] [G loss: 0.7041347622871399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 30/86 [D loss: 0.6992238759994507, acc.: 48.00%] [G loss: 0.7071912884712219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 31/86 [D loss: 0.7013961672782898, acc.: 44.97%] [G loss: 0.7051757574081421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 32/86 [D loss: 0.6991747617721558, acc.: 46.44%] [G loss: 0.704695463180542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 33/86 [D loss: 0.6983008980751038, acc.: 48.24%] [G loss: 0.7032313346862793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 34/86 [D loss: 0.697214663028717, acc.: 48.29%] [G loss: 0.7082390785217285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 35/86 [D loss: 0.696582704782486, acc.: 49.51%] [G loss: 0.7034944891929626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 36/86 [D loss: 0.6992168724536896, acc.: 47.90%] [G loss: 0.7040368318557739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 37/86 [D loss: 0.6981644630432129, acc.: 47.56%] [G loss: 0.7062983512878418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 38/86 [D loss: 0.7000032961368561, acc.: 46.44%] [G loss: 0.7027884721755981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 39/86 [D loss: 0.6985578536987305, acc.: 47.61%] [G loss: 0.7041857242584229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 40/86 [D loss: 0.6979463398456573, acc.: 47.41%] [G loss: 0.7039657831192017]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 41/86 [D loss: 0.699240118265152, acc.: 46.78%] [G loss: 0.70135098695755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 42/86 [D loss: 0.6992773413658142, acc.: 47.36%] [G loss: 0.701496422290802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 43/86 [D loss: 0.6989427804946899, acc.: 46.19%] [G loss: 0.702018678188324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 44/86 [D loss: 0.7007355988025665, acc.: 46.34%] [G loss: 0.7041639685630798]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 45/86 [D loss: 0.7000078856945038, acc.: 46.83%] [G loss: 0.699246883392334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 46/86 [D loss: 0.6993762850761414, acc.: 45.75%] [G loss: 0.702060341835022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 47/86 [D loss: 0.698093056678772, acc.: 48.29%] [G loss: 0.6972905397415161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 48/86 [D loss: 0.6997994184494019, acc.: 46.63%] [G loss: 0.7018529176712036]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 49/86 [D loss: 0.7008909285068512, acc.: 45.85%] [G loss: 0.6997745633125305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 50/86 [D loss: 0.7005562484264374, acc.: 46.78%] [G loss: 0.7037465572357178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 51/86 [D loss: 0.6993387937545776, acc.: 47.85%] [G loss: 0.7031159400939941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 52/86 [D loss: 0.6997535824775696, acc.: 45.61%] [G loss: 0.6993642449378967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 53/86 [D loss: 0.7006455063819885, acc.: 45.51%] [G loss: 0.6990522742271423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 54/86 [D loss: 0.6978001296520233, acc.: 47.07%] [G loss: 0.7010257840156555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 55/86 [D loss: 0.700158566236496, acc.: 45.75%] [G loss: 0.7030528783798218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 56/86 [D loss: 0.698358565568924, acc.: 47.56%] [G loss: 0.7053409814834595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 57/86 [D loss: 0.6986202299594879, acc.: 47.75%] [G loss: 0.704828679561615]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 58/86 [D loss: 0.7009957134723663, acc.: 43.85%] [G loss: 0.7041086554527283]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 59/86 [D loss: 0.6992196440696716, acc.: 46.04%] [G loss: 0.7044690847396851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 60/86 [D loss: 0.7004478275775909, acc.: 45.46%] [G loss: 0.6980429291725159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 61/86 [D loss: 0.6979309618473053, acc.: 46.44%] [G loss: 0.704541802406311]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 62/86 [D loss: 0.6983993649482727, acc.: 46.48%] [G loss: 0.7095968723297119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 63/86 [D loss: 0.6964633762836456, acc.: 47.80%] [G loss: 0.7070350646972656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 64/86 [D loss: 0.6982006728649139, acc.: 47.31%] [G loss: 0.7094042301177979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 65/86 [D loss: 0.6963994204998016, acc.: 48.44%] [G loss: 0.706055223941803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 66/86 [D loss: 0.6984462738037109, acc.: 46.53%] [G loss: 0.7060062885284424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 67/86 [D loss: 0.6982649266719818, acc.: 47.07%] [G loss: 0.7046065330505371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 68/86 [D loss: 0.6977695226669312, acc.: 46.88%] [G loss: 0.701793372631073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 69/86 [D loss: 0.698973149061203, acc.: 47.17%] [G loss: 0.7044543027877808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 70/86 [D loss: 0.697721540927887, acc.: 47.27%] [G loss: 0.7090763449668884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 71/86 [D loss: 0.6952590644359589, acc.: 48.05%] [G loss: 0.7104650735855103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 72/86 [D loss: 0.697719156742096, acc.: 47.46%] [G loss: 0.7077640295028687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 73/86 [D loss: 0.6963202357292175, acc.: 48.68%] [G loss: 0.7077366709709167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 74/86 [D loss: 0.6981225609779358, acc.: 48.19%] [G loss: 0.7040125727653503]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 75/86 [D loss: 0.6976079642772675, acc.: 47.71%] [G loss: 0.7042093276977539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 76/86 [D loss: 0.6986233294010162, acc.: 47.07%] [G loss: 0.7013764381408691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 77/86 [D loss: 0.7001945972442627, acc.: 45.36%] [G loss: 0.7056376338005066]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 78/86 [D loss: 0.7008131444454193, acc.: 44.24%] [G loss: 0.7012221217155457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 79/86 [D loss: 0.6984944343566895, acc.: 46.97%] [G loss: 0.702653169631958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 80/86 [D loss: 0.7004894316196442, acc.: 44.97%] [G loss: 0.7011947631835938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 81/86 [D loss: 0.6985785961151123, acc.: 47.61%] [G loss: 0.7004990577697754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 82/86 [D loss: 0.6997538208961487, acc.: 45.65%] [G loss: 0.7032507061958313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 83/86 [D loss: 0.6997762620449066, acc.: 46.39%] [G loss: 0.7058966755867004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 84/86 [D loss: 0.6998796164989471, acc.: 46.04%] [G loss: 0.6994717121124268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 85/86 [D loss: 0.6990821361541748, acc.: 46.19%] [G loss: 0.700359582901001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 86/86 [D loss: 0.6989651620388031, acc.: 46.63%] [G loss: 0.7022233605384827]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 1/86 [D loss: 0.7007230222225189, acc.: 44.73%] [G loss: 0.7045252323150635]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 2/86 [D loss: 0.698569655418396, acc.: 46.83%] [G loss: 0.7017102241516113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 3/86 [D loss: 0.6969620585441589, acc.: 48.58%] [G loss: 0.7006178498268127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 4/86 [D loss: 0.701583206653595, acc.: 44.34%] [G loss: 0.6991284489631653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 5/86 [D loss: 0.6989199817180634, acc.: 46.58%] [G loss: 0.7028907537460327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 6/86 [D loss: 0.6981055438518524, acc.: 48.19%] [G loss: 0.6980356574058533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 7/86 [D loss: 0.7007980048656464, acc.: 45.51%] [G loss: 0.7032275199890137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 8/86 [D loss: 0.6992764472961426, acc.: 46.34%] [G loss: 0.7039005160331726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 9/86 [D loss: 0.6980485916137695, acc.: 46.63%] [G loss: 0.6990702152252197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 10/86 [D loss: 0.6976599991321564, acc.: 47.75%] [G loss: 0.7016102075576782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 11/86 [D loss: 0.7009157538414001, acc.: 45.26%] [G loss: 0.7019911408424377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 12/86 [D loss: 0.6994068026542664, acc.: 46.14%] [G loss: 0.6993398666381836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 13/86 [D loss: 0.7000510692596436, acc.: 45.65%] [G loss: 0.704419732093811]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 14/86 [D loss: 0.6994507014751434, acc.: 46.78%] [G loss: 0.699386715888977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 15/86 [D loss: 0.6986513435840607, acc.: 47.71%] [G loss: 0.7007519602775574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 16/86 [D loss: 0.6997427642345428, acc.: 46.09%] [G loss: 0.7036600708961487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 17/86 [D loss: 0.7006005942821503, acc.: 46.58%] [G loss: 0.7010733485221863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 18/86 [D loss: 0.7027620375156403, acc.: 43.75%] [G loss: 0.6995006799697876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 19/86 [D loss: 0.7003688216209412, acc.: 44.09%] [G loss: 0.704644501209259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 20/86 [D loss: 0.7006160616874695, acc.: 46.04%] [G loss: 0.7030155062675476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 21/86 [D loss: 0.6981633901596069, acc.: 46.58%] [G loss: 0.7057443857192993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 22/86 [D loss: 0.6964699625968933, acc.: 47.22%] [G loss: 0.7098627686500549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 23/86 [D loss: 0.6987706124782562, acc.: 46.58%] [G loss: 0.7096115350723267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 24/86 [D loss: 0.6995434165000916, acc.: 45.85%] [G loss: 0.7110320329666138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 25/86 [D loss: 0.697813093662262, acc.: 47.12%] [G loss: 0.7108592391014099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 26/86 [D loss: 0.6982917785644531, acc.: 47.56%] [G loss: 0.7119359374046326]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 27/86 [D loss: 0.6987664997577667, acc.: 47.12%] [G loss: 0.7069135308265686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 28/86 [D loss: 0.6972997486591339, acc.: 47.61%] [G loss: 0.7077445983886719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 29/86 [D loss: 0.7000087201595306, acc.: 46.53%] [G loss: 0.7081706523895264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 30/86 [D loss: 0.6979303956031799, acc.: 46.39%] [G loss: 0.7083752751350403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 31/86 [D loss: 0.6979727149009705, acc.: 47.31%] [G loss: 0.7055267691612244]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 32/86 [D loss: 0.6961842775344849, acc.: 49.56%] [G loss: 0.7111725211143494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 33/86 [D loss: 0.6958873271942139, acc.: 49.17%] [G loss: 0.70926833152771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 34/86 [D loss: 0.6961833238601685, acc.: 48.54%] [G loss: 0.712058424949646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 35/86 [D loss: 0.6989333629608154, acc.: 44.78%] [G loss: 0.7114838361740112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 36/86 [D loss: 0.6986785531044006, acc.: 46.73%] [G loss: 0.7099058032035828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 37/86 [D loss: 0.6993289589881897, acc.: 46.19%] [G loss: 0.7052640318870544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 38/86 [D loss: 0.698992520570755, acc.: 47.07%] [G loss: 0.7053990364074707]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 39/86 [D loss: 0.6971131563186646, acc.: 49.37%] [G loss: 0.7056006789207458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 40/86 [D loss: 0.6983166933059692, acc.: 47.56%] [G loss: 0.7053120732307434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 41/86 [D loss: 0.6994925439357758, acc.: 45.56%] [G loss: 0.706108808517456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 42/86 [D loss: 0.6994720697402954, acc.: 45.90%] [G loss: 0.7061761021614075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 43/86 [D loss: 0.700778067111969, acc.: 45.70%] [G loss: 0.7044205665588379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 44/86 [D loss: 0.6990830302238464, acc.: 47.75%] [G loss: 0.7034739851951599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 45/86 [D loss: 0.6977332532405853, acc.: 47.17%] [G loss: 0.7000295519828796]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 46/86 [D loss: 0.6984796822071075, acc.: 46.19%] [G loss: 0.7067042589187622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 47/86 [D loss: 0.7002849280834198, acc.: 45.26%] [G loss: 0.7058308124542236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 48/86 [D loss: 0.7011221647262573, acc.: 45.41%] [G loss: 0.7020807266235352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 49/86 [D loss: 0.6998420357704163, acc.: 45.90%] [G loss: 0.6996183395385742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 50/86 [D loss: 0.7016249895095825, acc.: 44.82%] [G loss: 0.7027550339698792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 51/86 [D loss: 0.702000081539154, acc.: 43.12%] [G loss: 0.7017658948898315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 52/86 [D loss: 0.6982958018779755, acc.: 46.97%] [G loss: 0.7015297412872314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 53/86 [D loss: 0.7010540962219238, acc.: 44.63%] [G loss: 0.6971014738082886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 54/86 [D loss: 0.6975729465484619, acc.: 48.10%] [G loss: 0.7036826610565186]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 55/86 [D loss: 0.6991877257823944, acc.: 46.83%] [G loss: 0.701546311378479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 56/86 [D loss: 0.697778970003128, acc.: 46.92%] [G loss: 0.705333411693573]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 57/86 [D loss: 0.6982226669788361, acc.: 48.58%] [G loss: 0.7072191834449768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 58/86 [D loss: 0.6976033747196198, acc.: 46.63%] [G loss: 0.7042334079742432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 59/86 [D loss: 0.6991167962551117, acc.: 46.63%] [G loss: 0.7039969563484192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 60/86 [D loss: 0.6976273655891418, acc.: 46.58%] [G loss: 0.7021957635879517]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 61/86 [D loss: 0.6962849199771881, acc.: 48.78%] [G loss: 0.7027252912521362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 62/86 [D loss: 0.6985260546207428, acc.: 46.04%] [G loss: 0.7017232179641724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 63/86 [D loss: 0.6976893842220306, acc.: 48.24%] [G loss: 0.7032342553138733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 64/86 [D loss: 0.6971194744110107, acc.: 46.48%] [G loss: 0.7082366347312927]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 65/86 [D loss: 0.6968971788883209, acc.: 48.19%] [G loss: 0.7084806561470032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 66/86 [D loss: 0.6978965103626251, acc.: 46.83%] [G loss: 0.7068912982940674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 67/86 [D loss: 0.6986528635025024, acc.: 46.83%] [G loss: 0.7054846286773682]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 68/86 [D loss: 0.6979588568210602, acc.: 47.51%] [G loss: 0.7048962116241455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 69/86 [D loss: 0.698565810918808, acc.: 47.41%] [G loss: 0.7045212984085083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 70/86 [D loss: 0.6968798637390137, acc.: 47.31%] [G loss: 0.7028111219406128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 71/86 [D loss: 0.6983572244644165, acc.: 47.31%] [G loss: 0.7033387422561646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 72/86 [D loss: 0.6982057094573975, acc.: 46.68%] [G loss: 0.7031093835830688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 73/86 [D loss: 0.6987890899181366, acc.: 46.14%] [G loss: 0.7046563029289246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 74/86 [D loss: 0.6990414559841156, acc.: 46.44%] [G loss: 0.7030938863754272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 75/86 [D loss: 0.6974457800388336, acc.: 47.22%] [G loss: 0.7057845592498779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 76/86 [D loss: 0.6994937062263489, acc.: 45.80%] [G loss: 0.7018755674362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 77/86 [D loss: 0.6983686089515686, acc.: 47.66%] [G loss: 0.7028062343597412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 78/86 [D loss: 0.6997521221637726, acc.: 45.21%] [G loss: 0.7003949880599976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 79/86 [D loss: 0.6981463730335236, acc.: 46.48%] [G loss: 0.7002636194229126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 80/86 [D loss: 0.6986328363418579, acc.: 47.46%] [G loss: 0.70208340883255]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 81/86 [D loss: 0.6962327063083649, acc.: 50.00%] [G loss: 0.7047019004821777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 82/86 [D loss: 0.6991427540779114, acc.: 46.39%] [G loss: 0.7008421421051025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 83/86 [D loss: 0.6987746357917786, acc.: 46.04%] [G loss: 0.6960210800170898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 84/86 [D loss: 0.6978915631771088, acc.: 47.56%] [G loss: 0.6979706287384033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 85/86 [D loss: 0.6977404654026031, acc.: 47.66%] [G loss: 0.7020623683929443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 86/86 [D loss: 0.6995657086372375, acc.: 46.68%] [G loss: 0.7026642560958862]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 14/200, Batch 1/86 [D loss: 0.6986300945281982, acc.: 46.83%] [G loss: 0.7015112638473511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 2/86 [D loss: 0.698479413986206, acc.: 47.66%] [G loss: 0.7041595578193665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 3/86 [D loss: 0.6974312663078308, acc.: 48.19%] [G loss: 0.7077649831771851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 4/86 [D loss: 0.6946792602539062, acc.: 49.46%] [G loss: 0.7067893743515015]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 5/86 [D loss: 0.697492390871048, acc.: 48.78%] [G loss: 0.70722496509552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 6/86 [D loss: 0.6970573663711548, acc.: 48.78%] [G loss: 0.7092421650886536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 7/86 [D loss: 0.698852002620697, acc.: 45.51%] [G loss: 0.7106910347938538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 8/86 [D loss: 0.69708052277565, acc.: 47.17%] [G loss: 0.7087469100952148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 9/86 [D loss: 0.6995545625686646, acc.: 45.61%] [G loss: 0.7079026699066162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 10/86 [D loss: 0.696365624666214, acc.: 48.10%] [G loss: 0.713092029094696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 11/86 [D loss: 0.6971211433410645, acc.: 48.54%] [G loss: 0.70918869972229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 12/86 [D loss: 0.6970374882221222, acc.: 48.00%] [G loss: 0.7099342346191406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 13/86 [D loss: 0.6982378959655762, acc.: 46.44%] [G loss: 0.7107423543930054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 14/86 [D loss: 0.6990640461444855, acc.: 47.46%] [G loss: 0.7076735496520996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 15/86 [D loss: 0.6975178718566895, acc.: 47.46%] [G loss: 0.7111278176307678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 16/86 [D loss: 0.6957306861877441, acc.: 49.17%] [G loss: 0.7124876379966736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 17/86 [D loss: 0.6950612664222717, acc.: 50.54%] [G loss: 0.7085649967193604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 18/86 [D loss: 0.6964022815227509, acc.: 48.68%] [G loss: 0.708763599395752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 19/86 [D loss: 0.6971904635429382, acc.: 48.93%] [G loss: 0.7083039283752441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 20/86 [D loss: 0.6986807286739349, acc.: 45.41%] [G loss: 0.7101491093635559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 21/86 [D loss: 0.6991293132305145, acc.: 47.12%] [G loss: 0.7086260318756104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 22/86 [D loss: 0.6984283924102783, acc.: 46.19%] [G loss: 0.7117395997047424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 23/86 [D loss: 0.6997140944004059, acc.: 44.92%] [G loss: 0.707116425037384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 24/86 [D loss: 0.6957187056541443, acc.: 48.54%] [G loss: 0.7117425203323364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 25/86 [D loss: 0.6966174840927124, acc.: 47.46%] [G loss: 0.7086439728736877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 26/86 [D loss: 0.6956881880760193, acc.: 47.71%] [G loss: 0.7059152722358704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 27/86 [D loss: 0.6971204280853271, acc.: 47.07%] [G loss: 0.7059888243675232]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 28/86 [D loss: 0.6978006362915039, acc.: 47.90%] [G loss: 0.7091976404190063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 29/86 [D loss: 0.6961270868778229, acc.: 47.61%] [G loss: 0.7069894075393677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 30/86 [D loss: 0.698111891746521, acc.: 47.27%] [G loss: 0.7090550065040588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 31/86 [D loss: 0.6972408592700958, acc.: 46.97%] [G loss: 0.7082079648971558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 32/86 [D loss: 0.6956658959388733, acc.: 50.24%] [G loss: 0.7113550305366516]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 33/86 [D loss: 0.6977330148220062, acc.: 47.36%] [G loss: 0.7063044309616089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 34/86 [D loss: 0.6973109245300293, acc.: 47.07%] [G loss: 0.7023188471794128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 35/86 [D loss: 0.6963794231414795, acc.: 48.29%] [G loss: 0.7047870755195618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 36/86 [D loss: 0.6963823735713959, acc.: 48.58%] [G loss: 0.7052880525588989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 37/86 [D loss: 0.6970155835151672, acc.: 48.54%] [G loss: 0.7063117623329163]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 38/86 [D loss: 0.6944291889667511, acc.: 49.66%] [G loss: 0.7062953114509583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 39/86 [D loss: 0.6979519426822662, acc.: 47.56%] [G loss: 0.7039021253585815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 40/86 [D loss: 0.6972894668579102, acc.: 47.71%] [G loss: 0.7056207656860352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 41/86 [D loss: 0.6981241405010223, acc.: 47.27%] [G loss: 0.7036638259887695]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 42/86 [D loss: 0.6954576969146729, acc.: 49.12%] [G loss: 0.7025591135025024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 43/86 [D loss: 0.6977708339691162, acc.: 46.88%] [G loss: 0.7037401795387268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 44/86 [D loss: 0.696012020111084, acc.: 48.68%] [G loss: 0.7036980390548706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 45/86 [D loss: 0.698016881942749, acc.: 46.97%] [G loss: 0.7035058736801147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 46/86 [D loss: 0.6978961229324341, acc.: 47.07%] [G loss: 0.704762876033783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 47/86 [D loss: 0.696893721818924, acc.: 47.80%] [G loss: 0.7019141316413879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 48/86 [D loss: 0.6991480886936188, acc.: 46.39%] [G loss: 0.7043877243995667]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 49/86 [D loss: 0.6971751749515533, acc.: 47.95%] [G loss: 0.7028031945228577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 50/86 [D loss: 0.6980015635490417, acc.: 47.46%] [G loss: 0.7042946219444275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 51/86 [D loss: 0.6989628970623016, acc.: 46.29%] [G loss: 0.7083144187927246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 52/86 [D loss: 0.696609228849411, acc.: 48.24%] [G loss: 0.7079476118087769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 53/86 [D loss: 0.6970618069171906, acc.: 47.95%] [G loss: 0.7077370882034302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 54/86 [D loss: 0.6980451941490173, acc.: 48.19%] [G loss: 0.7072064876556396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 55/86 [D loss: 0.6962895393371582, acc.: 48.24%] [G loss: 0.7040511965751648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 56/86 [D loss: 0.7004727721214294, acc.: 43.90%] [G loss: 0.7072994112968445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 57/86 [D loss: 0.6986431777477264, acc.: 46.09%] [G loss: 0.7054044008255005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 58/86 [D loss: 0.7000102698802948, acc.: 46.09%] [G loss: 0.706584632396698]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 59/86 [D loss: 0.6977782249450684, acc.: 45.95%] [G loss: 0.7052738666534424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 60/86 [D loss: 0.6986573338508606, acc.: 47.71%] [G loss: 0.7041576504707336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 61/86 [D loss: 0.6991383135318756, acc.: 46.09%] [G loss: 0.7070544958114624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 62/86 [D loss: 0.6975354850292206, acc.: 48.44%] [G loss: 0.708326518535614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 63/86 [D loss: 0.6981163024902344, acc.: 47.36%] [G loss: 0.7068917155265808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 64/86 [D loss: 0.697152316570282, acc.: 47.80%] [G loss: 0.7050755023956299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 65/86 [D loss: 0.6983451247215271, acc.: 46.44%] [G loss: 0.703425407409668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 66/86 [D loss: 0.6996408700942993, acc.: 45.12%] [G loss: 0.7035229802131653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 67/86 [D loss: 0.6975610554218292, acc.: 48.24%] [G loss: 0.7026481032371521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 68/86 [D loss: 0.6981514096260071, acc.: 47.22%] [G loss: 0.704159140586853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 69/86 [D loss: 0.6981364488601685, acc.: 45.75%] [G loss: 0.7065441012382507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 70/86 [D loss: 0.6986739039421082, acc.: 46.92%] [G loss: 0.7099165916442871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 71/86 [D loss: 0.6976202428340912, acc.: 47.22%] [G loss: 0.7048646211624146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 72/86 [D loss: 0.6975536942481995, acc.: 48.54%] [G loss: 0.7059815526008606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 73/86 [D loss: 0.6995227634906769, acc.: 45.90%] [G loss: 0.7029227018356323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 74/86 [D loss: 0.6973224878311157, acc.: 48.97%] [G loss: 0.7056871652603149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 75/86 [D loss: 0.6976270079612732, acc.: 46.63%] [G loss: 0.706897497177124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 76/86 [D loss: 0.6986408829689026, acc.: 46.00%] [G loss: 0.7074525952339172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 77/86 [D loss: 0.6993181109428406, acc.: 45.61%] [G loss: 0.7065304517745972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 78/86 [D loss: 0.6982783675193787, acc.: 47.22%] [G loss: 0.70798259973526]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 79/86 [D loss: 0.6974433362483978, acc.: 47.80%] [G loss: 0.708451509475708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 80/86 [D loss: 0.6987610161304474, acc.: 46.53%] [G loss: 0.7069322466850281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 81/86 [D loss: 0.700418621301651, acc.: 44.68%] [G loss: 0.7071707248687744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 82/86 [D loss: 0.697313666343689, acc.: 48.83%] [G loss: 0.708834707736969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 83/86 [D loss: 0.6978846192359924, acc.: 47.51%] [G loss: 0.7107448577880859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 84/86 [D loss: 0.6975434124469757, acc.: 46.83%] [G loss: 0.7104910612106323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 85/86 [D loss: 0.6975483298301697, acc.: 46.63%] [G loss: 0.7088000178337097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 86/86 [D loss: 0.6976208388805389, acc.: 46.63%] [G loss: 0.7123507261276245]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 1/86 [D loss: 0.6987632215023041, acc.: 46.24%] [G loss: 0.7110409736633301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 2/86 [D loss: 0.6969544589519501, acc.: 47.95%] [G loss: 0.711781919002533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 3/86 [D loss: 0.6972898542881012, acc.: 48.44%] [G loss: 0.7098172307014465]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 4/86 [D loss: 0.6976149678230286, acc.: 47.07%] [G loss: 0.7117033004760742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 5/86 [D loss: 0.6957378685474396, acc.: 48.54%] [G loss: 0.7136248350143433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 6/86 [D loss: 0.6970629990100861, acc.: 46.88%] [G loss: 0.7104848623275757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 7/86 [D loss: 0.6978842914104462, acc.: 47.66%] [G loss: 0.7118057012557983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 8/86 [D loss: 0.6990240514278412, acc.: 45.46%] [G loss: 0.7091577053070068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 9/86 [D loss: 0.6975877285003662, acc.: 47.75%] [G loss: 0.7094006538391113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 10/86 [D loss: 0.6973102390766144, acc.: 46.48%] [G loss: 0.7086220979690552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 11/86 [D loss: 0.6986383497714996, acc.: 45.70%] [G loss: 0.7105159163475037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 12/86 [D loss: 0.6982286274433136, acc.: 46.34%] [G loss: 0.7078733444213867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 13/86 [D loss: 0.6983204483985901, acc.: 46.92%] [G loss: 0.7097582817077637]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 14/86 [D loss: 0.6977346837520599, acc.: 48.24%] [G loss: 0.7082762718200684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 15/86 [D loss: 0.6960800588130951, acc.: 48.73%] [G loss: 0.708226203918457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 16/86 [D loss: 0.698523074388504, acc.: 46.24%] [G loss: 0.7036747336387634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 17/86 [D loss: 0.6977447867393494, acc.: 47.61%] [G loss: 0.7029021382331848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 18/86 [D loss: 0.6968439817428589, acc.: 48.34%] [G loss: 0.7055252194404602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 19/86 [D loss: 0.6980621218681335, acc.: 47.41%] [G loss: 0.7044463753700256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 20/86 [D loss: 0.6989395320415497, acc.: 46.68%] [G loss: 0.7045769691467285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 21/86 [D loss: 0.6970367431640625, acc.: 47.95%] [G loss: 0.7037948966026306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 22/86 [D loss: 0.6988368630409241, acc.: 45.56%] [G loss: 0.702160656452179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 23/86 [D loss: 0.6986435055732727, acc.: 46.58%] [G loss: 0.7019233703613281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 24/86 [D loss: 0.6979960203170776, acc.: 47.36%] [G loss: 0.7056494951248169]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 25/86 [D loss: 0.6974854469299316, acc.: 48.83%] [G loss: 0.7056472301483154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 26/86 [D loss: 0.6981540620326996, acc.: 47.17%] [G loss: 0.7050896883010864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 27/86 [D loss: 0.6964814364910126, acc.: 48.10%] [G loss: 0.7030467987060547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 28/86 [D loss: 0.698749303817749, acc.: 46.19%] [G loss: 0.7010205984115601]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 29/86 [D loss: 0.7001327872276306, acc.: 45.12%] [G loss: 0.7040410041809082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 30/86 [D loss: 0.6964639127254486, acc.: 47.75%] [G loss: 0.7057030200958252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 31/86 [D loss: 0.698731929063797, acc.: 46.48%] [G loss: 0.7035756707191467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 32/86 [D loss: 0.698305606842041, acc.: 46.39%] [G loss: 0.7031003832817078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 33/86 [D loss: 0.6987135410308838, acc.: 47.02%] [G loss: 0.7024809718132019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 34/86 [D loss: 0.6981582939624786, acc.: 45.56%] [G loss: 0.7074285745620728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 35/86 [D loss: 0.6976496875286102, acc.: 46.73%] [G loss: 0.7059119343757629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 36/86 [D loss: 0.6994211673736572, acc.: 47.36%] [G loss: 0.7052742838859558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 37/86 [D loss: 0.6958514750003815, acc.: 49.37%] [G loss: 0.7091926336288452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 38/86 [D loss: 0.697546124458313, acc.: 47.61%] [G loss: 0.7063335180282593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 39/86 [D loss: 0.6979503035545349, acc.: 46.24%] [G loss: 0.7082995176315308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 40/86 [D loss: 0.6980322599411011, acc.: 47.36%] [G loss: 0.709432065486908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 41/86 [D loss: 0.696956217288971, acc.: 48.19%] [G loss: 0.7069839835166931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 42/86 [D loss: 0.6968375146389008, acc.: 48.10%] [G loss: 0.7091540098190308]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 43/86 [D loss: 0.6977109313011169, acc.: 46.97%] [G loss: 0.706721305847168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 44/86 [D loss: 0.6981484293937683, acc.: 44.53%] [G loss: 0.7076340913772583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 45/86 [D loss: 0.6975581347942352, acc.: 45.85%] [G loss: 0.7118450403213501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 46/86 [D loss: 0.6972205638885498, acc.: 47.22%] [G loss: 0.7072570323944092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 47/86 [D loss: 0.6987249851226807, acc.: 46.00%] [G loss: 0.707720160484314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 48/86 [D loss: 0.6972515285015106, acc.: 47.75%] [G loss: 0.7113661766052246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 49/86 [D loss: 0.6977607011795044, acc.: 46.68%] [G loss: 0.7088520526885986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 50/86 [D loss: 0.6986953914165497, acc.: 45.90%] [G loss: 0.7080824971199036]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 51/86 [D loss: 0.6989201009273529, acc.: 46.68%] [G loss: 0.7049050331115723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 52/86 [D loss: 0.6971166431903839, acc.: 46.34%] [G loss: 0.7059783935546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 53/86 [D loss: 0.6963799595832825, acc.: 47.12%] [G loss: 0.7085757255554199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 54/86 [D loss: 0.6968169212341309, acc.: 47.56%] [G loss: 0.7043407559394836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 55/86 [D loss: 0.6973760724067688, acc.: 47.66%] [G loss: 0.7069775462150574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 56/86 [D loss: 0.6974837481975555, acc.: 46.00%] [G loss: 0.7072345018386841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 57/86 [D loss: 0.6999680399894714, acc.: 46.24%] [G loss: 0.7056117653846741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 58/86 [D loss: 0.6964861154556274, acc.: 47.12%] [G loss: 0.705228328704834]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 59/86 [D loss: 0.697799801826477, acc.: 46.78%] [G loss: 0.7065580487251282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 60/86 [D loss: 0.6975165009498596, acc.: 47.02%] [G loss: 0.7062865495681763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 61/86 [D loss: 0.6966160535812378, acc.: 49.41%] [G loss: 0.7056581974029541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 62/86 [D loss: 0.6966190934181213, acc.: 48.49%] [G loss: 0.7052094340324402]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 63/86 [D loss: 0.6962219178676605, acc.: 48.14%] [G loss: 0.7041434049606323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 64/86 [D loss: 0.7000924050807953, acc.: 46.44%] [G loss: 0.7080036401748657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 65/86 [D loss: 0.698510468006134, acc.: 46.92%] [G loss: 0.7074195146560669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 66/86 [D loss: 0.6975043416023254, acc.: 47.90%] [G loss: 0.7054926156997681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 67/86 [D loss: 0.6969550549983978, acc.: 48.54%] [G loss: 0.7037181854248047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 68/86 [D loss: 0.6978407502174377, acc.: 47.07%] [G loss: 0.7049670219421387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 69/86 [D loss: 0.6988397538661957, acc.: 46.04%] [G loss: 0.7086620330810547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 70/86 [D loss: 0.6987218856811523, acc.: 46.29%] [G loss: 0.7064107060432434]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 71/86 [D loss: 0.6967902183532715, acc.: 46.92%] [G loss: 0.7070903778076172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 72/86 [D loss: 0.6973379552364349, acc.: 46.29%] [G loss: 0.7034763097763062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 73/86 [D loss: 0.6980287134647369, acc.: 46.68%] [G loss: 0.7052355408668518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 74/86 [D loss: 0.6978217959403992, acc.: 47.22%] [G loss: 0.7034618854522705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 75/86 [D loss: 0.7005136907100677, acc.: 45.36%] [G loss: 0.7043017745018005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 76/86 [D loss: 0.6990121006965637, acc.: 46.44%] [G loss: 0.7047221660614014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 77/86 [D loss: 0.6972098052501678, acc.: 47.46%] [G loss: 0.7065140008926392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 78/86 [D loss: 0.6969240605831146, acc.: 47.31%] [G loss: 0.7067786455154419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 79/86 [D loss: 0.6974901258945465, acc.: 47.12%] [G loss: 0.7053608894348145]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 80/86 [D loss: 0.6973984837532043, acc.: 48.29%] [G loss: 0.7025097012519836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 81/86 [D loss: 0.6994935274124146, acc.: 45.90%] [G loss: 0.700161337852478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 82/86 [D loss: 0.6991444528102875, acc.: 45.95%] [G loss: 0.70653235912323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 83/86 [D loss: 0.6972235441207886, acc.: 47.61%] [G loss: 0.7045295238494873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 84/86 [D loss: 0.697366863489151, acc.: 46.83%] [G loss: 0.7042436599731445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 85/86 [D loss: 0.6988576650619507, acc.: 45.70%] [G loss: 0.7049951553344727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 86/86 [D loss: 0.6983380615711212, acc.: 45.70%] [G loss: 0.7017682790756226]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 1/86 [D loss: 0.6991772949695587, acc.: 45.56%] [G loss: 0.7045168280601501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 2/86 [D loss: 0.6990072131156921, acc.: 47.41%] [G loss: 0.7014698386192322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 3/86 [D loss: 0.6971404552459717, acc.: 46.63%] [G loss: 0.7032781839370728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 4/86 [D loss: 0.697581946849823, acc.: 47.90%] [G loss: 0.7008834481239319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 5/86 [D loss: 0.6990650296211243, acc.: 43.90%] [G loss: 0.7034854888916016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 6/86 [D loss: 0.6989061832427979, acc.: 46.53%] [G loss: 0.7008232474327087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 7/86 [D loss: 0.697699099779129, acc.: 46.58%] [G loss: 0.6992613077163696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 8/86 [D loss: 0.6999421417713165, acc.: 44.78%] [G loss: 0.7003353834152222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 9/86 [D loss: 0.6970944106578827, acc.: 46.58%] [G loss: 0.7042301297187805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 10/86 [D loss: 0.6981422305107117, acc.: 45.90%] [G loss: 0.7047039270401001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 11/86 [D loss: 0.699613630771637, acc.: 45.46%] [G loss: 0.7008805871009827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 12/86 [D loss: 0.6999377906322479, acc.: 45.17%] [G loss: 0.7030197381973267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 13/86 [D loss: 0.6985859870910645, acc.: 45.80%] [G loss: 0.7045294046401978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 14/86 [D loss: 0.6975104212760925, acc.: 47.02%] [G loss: 0.7041832208633423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 15/86 [D loss: 0.6983849406242371, acc.: 46.44%] [G loss: 0.7073677182197571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 16/86 [D loss: 0.6969887018203735, acc.: 47.56%] [G loss: 0.7030384540557861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 17/86 [D loss: 0.6984007954597473, acc.: 46.58%] [G loss: 0.7034301161766052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 18/86 [D loss: 0.6974146366119385, acc.: 47.31%] [G loss: 0.7054275870323181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 19/86 [D loss: 0.6968564391136169, acc.: 48.44%] [G loss: 0.7025229930877686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 20/86 [D loss: 0.6982307136058807, acc.: 46.34%] [G loss: 0.7074623703956604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 21/86 [D loss: 0.697145938873291, acc.: 47.56%] [G loss: 0.7079432606697083]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 22/86 [D loss: 0.6979818046092987, acc.: 46.09%] [G loss: 0.7044155597686768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 23/86 [D loss: 0.6978359520435333, acc.: 46.04%] [G loss: 0.7070893049240112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 24/86 [D loss: 0.698072075843811, acc.: 46.39%] [G loss: 0.7046670317649841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 25/86 [D loss: 0.6980486810207367, acc.: 47.75%] [G loss: 0.70534747838974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 26/86 [D loss: 0.6979798972606659, acc.: 47.75%] [G loss: 0.7060310244560242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 27/86 [D loss: 0.6987270712852478, acc.: 44.97%] [G loss: 0.705594003200531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 28/86 [D loss: 0.6981765925884247, acc.: 46.48%] [G loss: 0.7060147523880005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 29/86 [D loss: 0.6976290643215179, acc.: 46.39%] [G loss: 0.7033646702766418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 30/86 [D loss: 0.6976039111614227, acc.: 47.02%] [G loss: 0.7062504291534424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 31/86 [D loss: 0.696558266878128, acc.: 48.05%] [G loss: 0.7050380706787109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 32/86 [D loss: 0.6983168125152588, acc.: 46.88%] [G loss: 0.7033594846725464]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 33/86 [D loss: 0.6994539201259613, acc.: 44.78%] [G loss: 0.7033048272132874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 34/86 [D loss: 0.6974767744541168, acc.: 46.58%] [G loss: 0.7032088041305542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 35/86 [D loss: 0.6970259547233582, acc.: 47.41%] [G loss: 0.7035258412361145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 36/86 [D loss: 0.6973161697387695, acc.: 45.85%] [G loss: 0.7026599645614624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 37/86 [D loss: 0.6983141005039215, acc.: 46.14%] [G loss: 0.7006649971008301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 38/86 [D loss: 0.6996996104717255, acc.: 45.85%] [G loss: 0.7052751183509827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 39/86 [D loss: 0.6992955207824707, acc.: 45.61%] [G loss: 0.7046319842338562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 40/86 [D loss: 0.696447342634201, acc.: 47.90%] [G loss: 0.7021900415420532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 41/86 [D loss: 0.6989125609397888, acc.: 46.88%] [G loss: 0.7028087377548218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 42/86 [D loss: 0.698406994342804, acc.: 44.63%] [G loss: 0.6993046402931213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 43/86 [D loss: 0.6983157396316528, acc.: 46.24%] [G loss: 0.7006137371063232]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 44/86 [D loss: 0.6968556642532349, acc.: 48.29%] [G loss: 0.698539674282074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 45/86 [D loss: 0.6992088556289673, acc.: 45.36%] [G loss: 0.7001274824142456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 46/86 [D loss: 0.6993376314640045, acc.: 44.78%] [G loss: 0.6987380981445312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 47/86 [D loss: 0.6993865370750427, acc.: 46.00%] [G loss: 0.701508641242981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 48/86 [D loss: 0.6990628242492676, acc.: 45.17%] [G loss: 0.7003048062324524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 49/86 [D loss: 0.6992116272449493, acc.: 45.65%] [G loss: 0.7009016871452332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 50/86 [D loss: 0.697441041469574, acc.: 46.04%] [G loss: 0.699776291847229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 51/86 [D loss: 0.6994274854660034, acc.: 45.21%] [G loss: 0.7003099322319031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 52/86 [D loss: 0.7001534998416901, acc.: 46.00%] [G loss: 0.7024747729301453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 53/86 [D loss: 0.6992901861667633, acc.: 45.65%] [G loss: 0.7014251947402954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 54/86 [D loss: 0.6978138089179993, acc.: 45.85%] [G loss: 0.7032522559165955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 55/86 [D loss: 0.6980507671833038, acc.: 46.88%] [G loss: 0.7002443671226501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 56/86 [D loss: 0.6992294490337372, acc.: 45.41%] [G loss: 0.7005364298820496]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 57/86 [D loss: 0.6973719596862793, acc.: 46.39%] [G loss: 0.7026641368865967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 58/86 [D loss: 0.6972230076789856, acc.: 47.51%] [G loss: 0.7001340389251709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 59/86 [D loss: 0.6976824700832367, acc.: 47.56%] [G loss: 0.7003554105758667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 60/86 [D loss: 0.6992155611515045, acc.: 44.53%] [G loss: 0.7035255432128906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 61/86 [D loss: 0.6995564997196198, acc.: 43.65%] [G loss: 0.6979513764381409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 62/86 [D loss: 0.6991045475006104, acc.: 45.07%] [G loss: 0.7002241015434265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 63/86 [D loss: 0.699191153049469, acc.: 45.41%] [G loss: 0.6984716653823853]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 64/86 [D loss: 0.6983238458633423, acc.: 45.26%] [G loss: 0.7010864019393921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 65/86 [D loss: 0.6995531320571899, acc.: 45.17%] [G loss: 0.6993274688720703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 66/86 [D loss: 0.6995751857757568, acc.: 46.19%] [G loss: 0.7031485438346863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 67/86 [D loss: 0.6976602077484131, acc.: 46.09%] [G loss: 0.7008234262466431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 68/86 [D loss: 0.6978136897087097, acc.: 47.75%] [G loss: 0.7019327878952026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 69/86 [D loss: 0.6979497373104095, acc.: 47.02%] [G loss: 0.7023043632507324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 70/86 [D loss: 0.697567343711853, acc.: 47.46%] [G loss: 0.7010817527770996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 71/86 [D loss: 0.6987778544425964, acc.: 46.24%] [G loss: 0.7013081908226013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 72/86 [D loss: 0.6984145939350128, acc.: 45.56%] [G loss: 0.7027051448822021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 73/86 [D loss: 0.6991878747940063, acc.: 47.07%] [G loss: 0.7037076950073242]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 74/86 [D loss: 0.7001106142997742, acc.: 43.99%] [G loss: 0.7008544206619263]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 75/86 [D loss: 0.6994956433773041, acc.: 43.95%] [G loss: 0.701378345489502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 76/86 [D loss: 0.6997798979282379, acc.: 43.46%] [G loss: 0.7055652737617493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 77/86 [D loss: 0.6977827847003937, acc.: 46.92%] [G loss: 0.7042757868766785]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 78/86 [D loss: 0.6979205310344696, acc.: 46.34%] [G loss: 0.7039340734481812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 79/86 [D loss: 0.6995013356208801, acc.: 43.95%] [G loss: 0.7050524950027466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 80/86 [D loss: 0.6983447968959808, acc.: 45.95%] [G loss: 0.6998063325881958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 81/86 [D loss: 0.6991824805736542, acc.: 45.07%] [G loss: 0.7050096392631531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 82/86 [D loss: 0.6993832588195801, acc.: 45.07%] [G loss: 0.7077739834785461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 83/86 [D loss: 0.698833167552948, acc.: 44.48%] [G loss: 0.7055891156196594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 84/86 [D loss: 0.697516143321991, acc.: 45.90%] [G loss: 0.703176736831665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 85/86 [D loss: 0.6982482671737671, acc.: 46.34%] [G loss: 0.7008812427520752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 86/86 [D loss: 0.69777911901474, acc.: 46.00%] [G loss: 0.7038140296936035]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 1/86 [D loss: 0.699517011642456, acc.: 45.21%] [G loss: 0.7070785164833069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 2/86 [D loss: 0.6981417238712311, acc.: 46.53%] [G loss: 0.7065884470939636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 3/86 [D loss: 0.6976865828037262, acc.: 46.83%] [G loss: 0.7031447291374207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 4/86 [D loss: 0.6976653039455414, acc.: 46.14%] [G loss: 0.6980358958244324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 5/86 [D loss: 0.7003459334373474, acc.: 44.63%] [G loss: 0.7013636827468872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 6/86 [D loss: 0.699182391166687, acc.: 45.85%] [G loss: 0.7040737867355347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 7/86 [D loss: 0.6966147422790527, acc.: 47.02%] [G loss: 0.7078313827514648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 8/86 [D loss: 0.6992913484573364, acc.: 45.12%] [G loss: 0.7044394612312317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 9/86 [D loss: 0.6980840563774109, acc.: 45.02%] [G loss: 0.6977834105491638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 10/86 [D loss: 0.7003847062587738, acc.: 44.58%] [G loss: 0.6971519589424133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 11/86 [D loss: 0.7001111805438995, acc.: 44.63%] [G loss: 0.7036470770835876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 12/86 [D loss: 0.7007418572902679, acc.: 44.63%] [G loss: 0.7018521428108215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 13/86 [D loss: 0.6971765756607056, acc.: 48.24%] [G loss: 0.7021144032478333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 14/86 [D loss: 0.6980630159378052, acc.: 46.24%] [G loss: 0.7007248401641846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 15/86 [D loss: 0.6994422078132629, acc.: 44.24%] [G loss: 0.6984168291091919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 16/86 [D loss: 0.6991605162620544, acc.: 46.19%] [G loss: 0.7026647329330444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 17/86 [D loss: 0.6985483765602112, acc.: 45.21%] [G loss: 0.7040582895278931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 18/86 [D loss: 0.6987113952636719, acc.: 45.07%] [G loss: 0.703092634677887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 19/86 [D loss: 0.6980274319648743, acc.: 45.85%] [G loss: 0.7010810971260071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 20/86 [D loss: 0.6989135444164276, acc.: 45.46%] [G loss: 0.7010588049888611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 21/86 [D loss: 0.6995630860328674, acc.: 44.63%] [G loss: 0.702010989189148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 22/86 [D loss: 0.6973112225532532, acc.: 46.58%] [G loss: 0.7033309936523438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 23/86 [D loss: 0.6973274052143097, acc.: 46.39%] [G loss: 0.7023882269859314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 24/86 [D loss: 0.6982079148292542, acc.: 47.27%] [G loss: 0.701420783996582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 25/86 [D loss: 0.6982385516166687, acc.: 46.63%] [G loss: 0.7025071382522583]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 26/86 [D loss: 0.6964754462242126, acc.: 47.90%] [G loss: 0.706086277961731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 27/86 [D loss: 0.6992656886577606, acc.: 44.24%] [G loss: 0.704578697681427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 28/86 [D loss: 0.6973766088485718, acc.: 46.73%] [G loss: 0.7016662955284119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 29/86 [D loss: 0.6989800035953522, acc.: 44.92%] [G loss: 0.7019467353820801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 30/86 [D loss: 0.6994199156761169, acc.: 44.78%] [G loss: 0.6994471549987793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 31/86 [D loss: 0.6995209157466888, acc.: 44.34%] [G loss: 0.7021567821502686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 32/86 [D loss: 0.6979009509086609, acc.: 45.17%] [G loss: 0.7022906541824341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 33/86 [D loss: 0.6973276138305664, acc.: 47.36%] [G loss: 0.7002781629562378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 34/86 [D loss: 0.6972415447235107, acc.: 47.07%] [G loss: 0.7022944092750549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 35/86 [D loss: 0.6991377174854279, acc.: 44.78%] [G loss: 0.7007772326469421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 36/86 [D loss: 0.699570894241333, acc.: 45.56%] [G loss: 0.7009967565536499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 37/86 [D loss: 0.6973831355571747, acc.: 47.22%] [G loss: 0.6998416781425476]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 38/86 [D loss: 0.699511706829071, acc.: 45.02%] [G loss: 0.7025434970855713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 39/86 [D loss: 0.6978378891944885, acc.: 46.14%] [G loss: 0.7019422054290771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 40/86 [D loss: 0.6981271207332611, acc.: 45.61%] [G loss: 0.7011370658874512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 41/86 [D loss: 0.6985988020896912, acc.: 44.24%] [G loss: 0.7014030814170837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 42/86 [D loss: 0.698614776134491, acc.: 46.09%] [G loss: 0.700821042060852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 43/86 [D loss: 0.6997739374637604, acc.: 44.53%] [G loss: 0.7002480030059814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 44/86 [D loss: 0.6981441676616669, acc.: 46.14%] [G loss: 0.7033014297485352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 45/86 [D loss: 0.697751522064209, acc.: 46.24%] [G loss: 0.7015708088874817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 46/86 [D loss: 0.6978855133056641, acc.: 46.53%] [G loss: 0.7013232707977295]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 47/86 [D loss: 0.7008211016654968, acc.: 43.41%] [G loss: 0.6995185613632202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 48/86 [D loss: 0.6974307596683502, acc.: 45.65%] [G loss: 0.7014617919921875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 49/86 [D loss: 0.6982187330722809, acc.: 44.82%] [G loss: 0.7032047510147095]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 50/86 [D loss: 0.6969780623912811, acc.: 47.90%] [G loss: 0.7014473676681519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 51/86 [D loss: 0.6984014213085175, acc.: 44.82%] [G loss: 0.7022762894630432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 52/86 [D loss: 0.6991026401519775, acc.: 45.26%] [G loss: 0.7016695141792297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 53/86 [D loss: 0.698270320892334, acc.: 46.19%] [G loss: 0.7003012895584106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 54/86 [D loss: 0.6988065838813782, acc.: 46.29%] [G loss: 0.7004818916320801]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 55/86 [D loss: 0.6963615715503693, acc.: 47.85%] [G loss: 0.7025890350341797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 56/86 [D loss: 0.6986058354377747, acc.: 45.51%] [G loss: 0.7015416622161865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 57/86 [D loss: 0.6986491084098816, acc.: 44.38%] [G loss: 0.702479898929596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 58/86 [D loss: 0.6983362436294556, acc.: 45.21%] [G loss: 0.7044634819030762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 59/86 [D loss: 0.697860062122345, acc.: 46.58%] [G loss: 0.70133376121521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 60/86 [D loss: 0.6998481452465057, acc.: 46.00%] [G loss: 0.7018465399742126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 61/86 [D loss: 0.7004962861537933, acc.: 44.48%] [G loss: 0.7061030864715576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 62/86 [D loss: 0.6990762650966644, acc.: 44.97%] [G loss: 0.70014888048172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 63/86 [D loss: 0.6988031268119812, acc.: 44.97%] [G loss: 0.7013786435127258]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 64/86 [D loss: 0.6968649327754974, acc.: 47.02%] [G loss: 0.7031562924385071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 65/86 [D loss: 0.6987980902194977, acc.: 44.73%] [G loss: 0.7064216136932373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 66/86 [D loss: 0.6972484886646271, acc.: 45.65%] [G loss: 0.7046716213226318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 67/86 [D loss: 0.6973288059234619, acc.: 47.27%] [G loss: 0.7035315036773682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 68/86 [D loss: 0.6992588043212891, acc.: 44.63%] [G loss: 0.7037570476531982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 69/86 [D loss: 0.6985632181167603, acc.: 44.04%] [G loss: 0.7029809355735779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 70/86 [D loss: 0.6973214447498322, acc.: 46.09%] [G loss: 0.7033780217170715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 71/86 [D loss: 0.6985029876232147, acc.: 45.75%] [G loss: 0.703567385673523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 72/86 [D loss: 0.6991652250289917, acc.: 44.68%] [G loss: 0.7050513029098511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 73/86 [D loss: 0.6991503834724426, acc.: 45.61%] [G loss: 0.6999726295471191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 74/86 [D loss: 0.6984285414218903, acc.: 45.90%] [G loss: 0.7031307816505432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 75/86 [D loss: 0.6989723742008209, acc.: 45.90%] [G loss: 0.7019214630126953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 76/86 [D loss: 0.6977306604385376, acc.: 46.00%] [G loss: 0.7023360729217529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 77/86 [D loss: 0.7005151510238647, acc.: 43.21%] [G loss: 0.7021757960319519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 78/86 [D loss: 0.6973573863506317, acc.: 47.61%] [G loss: 0.7026734352111816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 79/86 [D loss: 0.6972449123859406, acc.: 46.92%] [G loss: 0.7008600234985352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 80/86 [D loss: 0.6989416480064392, acc.: 45.02%] [G loss: 0.7011510133743286]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 81/86 [D loss: 0.6979875266551971, acc.: 46.19%] [G loss: 0.7019425630569458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 82/86 [D loss: 0.6980539858341217, acc.: 45.41%] [G loss: 0.6998465657234192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 83/86 [D loss: 0.7004271149635315, acc.: 43.95%] [G loss: 0.7021616697311401]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 84/86 [D loss: 0.6983073353767395, acc.: 45.75%] [G loss: 0.7026652097702026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 85/86 [D loss: 0.6975915431976318, acc.: 48.19%] [G loss: 0.6988565921783447]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 86/86 [D loss: 0.6993881464004517, acc.: 43.41%] [G loss: 0.6991029977798462]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 18/200, Batch 1/86 [D loss: 0.6996645033359528, acc.: 45.61%] [G loss: 0.699725866317749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 2/86 [D loss: 0.6978113949298859, acc.: 45.95%] [G loss: 0.7007485628128052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 3/86 [D loss: 0.698266476392746, acc.: 43.70%] [G loss: 0.6999747157096863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 4/86 [D loss: 0.6985687911510468, acc.: 44.78%] [G loss: 0.6999579668045044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 5/86 [D loss: 0.6983029544353485, acc.: 44.14%] [G loss: 0.6994068026542664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 6/86 [D loss: 0.6978170871734619, acc.: 45.21%] [G loss: 0.7027503848075867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 7/86 [D loss: 0.7002871930599213, acc.: 43.55%] [G loss: 0.7028740644454956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 8/86 [D loss: 0.6978688836097717, acc.: 45.46%] [G loss: 0.7007099986076355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 9/86 [D loss: 0.6984816193580627, acc.: 45.17%] [G loss: 0.7010471224784851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 10/86 [D loss: 0.6993403136730194, acc.: 45.41%] [G loss: 0.6994525194168091]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 11/86 [D loss: 0.6982318460941315, acc.: 45.80%] [G loss: 0.6987630128860474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 12/86 [D loss: 0.6986376643180847, acc.: 44.34%] [G loss: 0.7031402587890625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 13/86 [D loss: 0.6982826292514801, acc.: 45.65%] [G loss: 0.7005208134651184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 14/86 [D loss: 0.6977512836456299, acc.: 44.92%] [G loss: 0.6986101269721985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 15/86 [D loss: 0.6990609765052795, acc.: 44.87%] [G loss: 0.6988220810890198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 16/86 [D loss: 0.6993290781974792, acc.: 44.43%] [G loss: 0.7007132172584534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 17/86 [D loss: 0.6977552175521851, acc.: 47.31%] [G loss: 0.7011011838912964]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 18/86 [D loss: 0.6979784369468689, acc.: 45.61%] [G loss: 0.7021576166152954]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 19/86 [D loss: 0.6983928084373474, acc.: 44.48%] [G loss: 0.6976915001869202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 20/86 [D loss: 0.7009865641593933, acc.: 43.60%] [G loss: 0.6981213688850403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 21/86 [D loss: 0.6997874081134796, acc.: 45.12%] [G loss: 0.7016423940658569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 22/86 [D loss: 0.6974453926086426, acc.: 46.48%] [G loss: 0.7016940116882324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 23/86 [D loss: 0.6994850933551788, acc.: 44.24%] [G loss: 0.7011781930923462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 24/86 [D loss: 0.6993271112442017, acc.: 43.99%] [G loss: 0.7001258134841919]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 25/86 [D loss: 0.7004033327102661, acc.: 43.51%] [G loss: 0.6953691244125366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 26/86 [D loss: 0.698029488325119, acc.: 45.41%] [G loss: 0.6993919014930725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 27/86 [D loss: 0.6979537308216095, acc.: 45.61%] [G loss: 0.7023835182189941]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 28/86 [D loss: 0.69715616106987, acc.: 46.53%] [G loss: 0.6989672183990479]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 29/86 [D loss: 0.6977652013301849, acc.: 46.09%] [G loss: 0.6983530521392822]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 30/86 [D loss: 0.6996704936027527, acc.: 43.95%] [G loss: 0.6961923837661743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 31/86 [D loss: 0.7003122866153717, acc.: 45.56%] [G loss: 0.7016352415084839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 32/86 [D loss: 0.6977260112762451, acc.: 46.73%] [G loss: 0.7026928067207336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 33/86 [D loss: 0.6988584995269775, acc.: 46.58%] [G loss: 0.7020798325538635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 34/86 [D loss: 0.6980913281440735, acc.: 45.65%] [G loss: 0.6986779570579529]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 35/86 [D loss: 0.7007901072502136, acc.: 43.51%] [G loss: 0.6977823972702026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 36/86 [D loss: 0.7004808783531189, acc.: 44.87%] [G loss: 0.7044559121131897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 37/86 [D loss: 0.6973251104354858, acc.: 46.04%] [G loss: 0.7002565264701843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 38/86 [D loss: 0.6978021562099457, acc.: 45.65%] [G loss: 0.7027061581611633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 39/86 [D loss: 0.6988964080810547, acc.: 44.63%] [G loss: 0.696563720703125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 40/86 [D loss: 0.6995627284049988, acc.: 45.02%] [G loss: 0.6981943249702454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 41/86 [D loss: 0.6985491812229156, acc.: 46.19%] [G loss: 0.7045510411262512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 42/86 [D loss: 0.6988138854503632, acc.: 44.78%] [G loss: 0.7054872512817383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 43/86 [D loss: 0.6971902251243591, acc.: 46.34%] [G loss: 0.7041422128677368]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 44/86 [D loss: 0.6989467740058899, acc.: 44.63%] [G loss: 0.6995874643325806]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 45/86 [D loss: 0.7002457976341248, acc.: 42.87%] [G loss: 0.7018241286277771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 46/86 [D loss: 0.6986055672168732, acc.: 45.85%] [G loss: 0.7030434608459473]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 47/86 [D loss: 0.6979663074016571, acc.: 45.51%] [G loss: 0.7048307657241821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 48/86 [D loss: 0.6973532140254974, acc.: 46.24%] [G loss: 0.7026352286338806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 49/86 [D loss: 0.698161780834198, acc.: 44.38%] [G loss: 0.6996674537658691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 50/86 [D loss: 0.6995201706886292, acc.: 45.31%] [G loss: 0.7025861144065857]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 51/86 [D loss: 0.6990551650524139, acc.: 45.36%] [G loss: 0.7064767479896545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 52/86 [D loss: 0.6975246965885162, acc.: 46.83%] [G loss: 0.7019802331924438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 53/86 [D loss: 0.6962564289569855, acc.: 48.19%] [G loss: 0.7005866765975952]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 54/86 [D loss: 0.6987640559673309, acc.: 45.31%] [G loss: 0.6972986459732056]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 55/86 [D loss: 0.6994317471981049, acc.: 44.53%] [G loss: 0.7027378082275391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 56/86 [D loss: 0.6987424790859222, acc.: 46.14%] [G loss: 0.703938364982605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 57/86 [D loss: 0.6978494226932526, acc.: 44.58%] [G loss: 0.7044312953948975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 58/86 [D loss: 0.6966586709022522, acc.: 48.10%] [G loss: 0.7006667852401733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 59/86 [D loss: 0.7001293897628784, acc.: 43.46%] [G loss: 0.6980002522468567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 60/86 [D loss: 0.7013725340366364, acc.: 42.68%] [G loss: 0.7004229426383972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 61/86 [D loss: 0.6988945007324219, acc.: 45.26%] [G loss: 0.7047407031059265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 62/86 [D loss: 0.696731835603714, acc.: 47.12%] [G loss: 0.7023835778236389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 63/86 [D loss: 0.6997600197792053, acc.: 45.61%] [G loss: 0.7009290456771851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 64/86 [D loss: 0.6984034776687622, acc.: 46.39%] [G loss: 0.6962045431137085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 65/86 [D loss: 0.6999499797821045, acc.: 44.43%] [G loss: 0.6945585012435913]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 66/86 [D loss: 0.7012375593185425, acc.: 44.29%] [G loss: 0.703324019908905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 67/86 [D loss: 0.6970925033092499, acc.: 45.75%] [G loss: 0.7038207650184631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 68/86 [D loss: 0.6973586082458496, acc.: 45.46%] [G loss: 0.7025482654571533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 69/86 [D loss: 0.6987152695655823, acc.: 44.68%] [G loss: 0.6989849805831909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 70/86 [D loss: 0.6989214420318604, acc.: 47.12%] [G loss: 0.6950664520263672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 71/86 [D loss: 0.699300080537796, acc.: 45.61%] [G loss: 0.7051171064376831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 72/86 [D loss: 0.6962820589542389, acc.: 47.66%] [G loss: 0.7031415700912476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 73/86 [D loss: 0.6973105669021606, acc.: 46.48%] [G loss: 0.7045223712921143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 74/86 [D loss: 0.6984211802482605, acc.: 45.46%] [G loss: 0.7024260759353638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 75/86 [D loss: 0.6994675397872925, acc.: 43.95%] [G loss: 0.697584331035614]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 76/86 [D loss: 0.6981328725814819, acc.: 44.82%] [G loss: 0.7031187415122986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 77/86 [D loss: 0.6990168988704681, acc.: 44.97%] [G loss: 0.7037461400032043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 78/86 [D loss: 0.6982050836086273, acc.: 45.51%] [G loss: 0.7047119140625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 79/86 [D loss: 0.697626382112503, acc.: 46.78%] [G loss: 0.7042152285575867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 80/86 [D loss: 0.7003097236156464, acc.: 44.38%] [G loss: 0.6999711394309998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 81/86 [D loss: 0.6994679272174835, acc.: 44.38%] [G loss: 0.6988481283187866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 82/86 [D loss: 0.6979172825813293, acc.: 45.95%] [G loss: 0.7038490772247314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 83/86 [D loss: 0.6971602141857147, acc.: 47.22%] [G loss: 0.7038662433624268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 84/86 [D loss: 0.6993399560451508, acc.: 43.60%] [G loss: 0.7011429071426392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 85/86 [D loss: 0.699419766664505, acc.: 44.78%] [G loss: 0.7009751796722412]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 86/86 [D loss: 0.6978047490119934, acc.: 46.44%] [G loss: 0.7007246017456055]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 1/86 [D loss: 0.6978490352630615, acc.: 46.78%] [G loss: 0.7016857862472534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 2/86 [D loss: 0.6977448463439941, acc.: 45.85%] [G loss: 0.7023954391479492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 3/86 [D loss: 0.6981105208396912, acc.: 45.56%] [G loss: 0.7004555463790894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 4/86 [D loss: 0.6976024806499481, acc.: 45.70%] [G loss: 0.6988126039505005]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 5/86 [D loss: 0.6990506649017334, acc.: 43.99%] [G loss: 0.7011622190475464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 6/86 [D loss: 0.7004857063293457, acc.: 43.55%] [G loss: 0.7029728889465332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 7/86 [D loss: 0.6992021203041077, acc.: 43.65%] [G loss: 0.7038300037384033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 8/86 [D loss: 0.6991870105266571, acc.: 45.02%] [G loss: 0.6985906362533569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 9/86 [D loss: 0.6991665065288544, acc.: 44.68%] [G loss: 0.7003646492958069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 10/86 [D loss: 0.6988269090652466, acc.: 45.41%] [G loss: 0.7046072483062744]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 11/86 [D loss: 0.6967534720897675, acc.: 46.97%] [G loss: 0.7049189209938049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 12/86 [D loss: 0.6966164112091064, acc.: 45.95%] [G loss: 0.703584611415863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 13/86 [D loss: 0.6977525353431702, acc.: 47.56%] [G loss: 0.7007354497909546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 14/86 [D loss: 0.7002420425415039, acc.: 43.99%] [G loss: 0.6994920372962952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 15/86 [D loss: 0.698377251625061, acc.: 46.97%] [G loss: 0.7058670520782471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 16/86 [D loss: 0.699178546667099, acc.: 44.24%] [G loss: 0.7047823071479797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 17/86 [D loss: 0.6977637112140656, acc.: 45.80%] [G loss: 0.7042898535728455]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 18/86 [D loss: 0.6977787017822266, acc.: 46.29%] [G loss: 0.6989470720291138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 19/86 [D loss: 0.7010743319988251, acc.: 44.48%] [G loss: 0.6980539560317993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 20/86 [D loss: 0.6998492181301117, acc.: 44.82%] [G loss: 0.7054383754730225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 21/86 [D loss: 0.6976669132709503, acc.: 46.29%] [G loss: 0.7033738493919373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 22/86 [D loss: 0.6996727883815765, acc.: 44.24%] [G loss: 0.7012247443199158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 23/86 [D loss: 0.6971464455127716, acc.: 45.80%] [G loss: 0.697475790977478]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 24/86 [D loss: 0.7015025019645691, acc.: 44.09%] [G loss: 0.689541220664978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 25/86 [D loss: 0.7006178498268127, acc.: 45.21%] [G loss: 0.7068314552307129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 26/86 [D loss: 0.6946022212505341, acc.: 48.24%] [G loss: 0.700505256652832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 27/86 [D loss: 0.7010886371135712, acc.: 42.04%] [G loss: 0.7000018358230591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 28/86 [D loss: 0.6989828050136566, acc.: 43.80%] [G loss: 0.6982476711273193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 29/86 [D loss: 0.6977440714836121, acc.: 45.36%] [G loss: 0.6865275502204895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 30/86 [D loss: 0.7039072215557098, acc.: 42.72%] [G loss: 0.6969566345214844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 31/86 [D loss: 0.6951094269752502, acc.: 49.07%] [G loss: 0.7044833302497864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 32/86 [D loss: 0.6993028223514557, acc.: 43.36%] [G loss: 0.7010815143585205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 33/86 [D loss: 0.6997136473655701, acc.: 45.02%] [G loss: 0.7005345225334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 34/86 [D loss: 0.6981372237205505, acc.: 46.83%] [G loss: 0.6917714476585388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 35/86 [D loss: 0.700760155916214, acc.: 43.95%] [G loss: 0.6924445629119873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 36/86 [D loss: 0.6988163888454437, acc.: 44.58%] [G loss: 0.7054930925369263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 37/86 [D loss: 0.6975786685943604, acc.: 46.14%] [G loss: 0.702457845211029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 38/86 [D loss: 0.698582798242569, acc.: 45.56%] [G loss: 0.7003641724586487]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 39/86 [D loss: 0.6985326409339905, acc.: 44.73%] [G loss: 0.6966051459312439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 40/86 [D loss: 0.700878769159317, acc.: 43.95%] [G loss: 0.6943712830543518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 41/86 [D loss: 0.7003738284111023, acc.: 45.02%] [G loss: 0.7010138630867004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 42/86 [D loss: 0.6954023241996765, acc.: 47.61%] [G loss: 0.7031227946281433]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 43/86 [D loss: 0.6976318061351776, acc.: 45.56%] [G loss: 0.7014641165733337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 44/86 [D loss: 0.699463278055191, acc.: 43.55%] [G loss: 0.7028242945671082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 45/86 [D loss: 0.6985255181789398, acc.: 44.78%] [G loss: 0.6986784934997559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 46/86 [D loss: 0.6999513804912567, acc.: 44.63%] [G loss: 0.7021012306213379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 47/86 [D loss: 0.6995596289634705, acc.: 45.07%] [G loss: 0.7044524550437927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 48/86 [D loss: 0.6976848244667053, acc.: 45.26%] [G loss: 0.704758882522583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 49/86 [D loss: 0.698188066482544, acc.: 45.46%] [G loss: 0.7026536464691162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 50/86 [D loss: 0.6996899843215942, acc.: 43.95%] [G loss: 0.7002153992652893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 51/86 [D loss: 0.7004713714122772, acc.: 41.70%] [G loss: 0.7028377056121826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 52/86 [D loss: 0.697986900806427, acc.: 45.61%] [G loss: 0.7056463360786438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 53/86 [D loss: 0.6979226768016815, acc.: 45.17%] [G loss: 0.7043785452842712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 54/86 [D loss: 0.6991772651672363, acc.: 45.21%] [G loss: 0.7032556533813477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 55/86 [D loss: 0.6977637708187103, acc.: 46.14%] [G loss: 0.7002476453781128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 56/86 [D loss: 0.6987016499042511, acc.: 44.24%] [G loss: 0.7028508186340332]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 57/86 [D loss: 0.6993902027606964, acc.: 44.53%] [G loss: 0.7026749849319458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 58/86 [D loss: 0.696925550699234, acc.: 45.36%] [G loss: 0.7035006880760193]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 59/86 [D loss: 0.697757363319397, acc.: 45.70%] [G loss: 0.7054197192192078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 60/86 [D loss: 0.6983542144298553, acc.: 44.19%] [G loss: 0.7053394317626953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 61/86 [D loss: 0.6984519958496094, acc.: 46.00%] [G loss: 0.7029966711997986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 62/86 [D loss: 0.6973069608211517, acc.: 46.88%] [G loss: 0.7015571594238281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 63/86 [D loss: 0.6991516053676605, acc.: 43.85%] [G loss: 0.7036989331245422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 64/86 [D loss: 0.6990927755832672, acc.: 44.73%] [G loss: 0.7042421698570251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 65/86 [D loss: 0.6984068751335144, acc.: 45.70%] [G loss: 0.7016503214836121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 66/86 [D loss: 0.696957916021347, acc.: 46.34%] [G loss: 0.7017099857330322]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 67/86 [D loss: 0.698261171579361, acc.: 45.36%] [G loss: 0.7024375796318054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 68/86 [D loss: 0.699979841709137, acc.: 43.16%] [G loss: 0.7029047012329102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 69/86 [D loss: 0.6977812349796295, acc.: 47.22%] [G loss: 0.7040180563926697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 70/86 [D loss: 0.6967889666557312, acc.: 47.22%] [G loss: 0.7022086381912231]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 71/86 [D loss: 0.6971619725227356, acc.: 47.41%] [G loss: 0.6997137069702148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 72/86 [D loss: 0.6979185044765472, acc.: 46.00%] [G loss: 0.6992840766906738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 73/86 [D loss: 0.6985773146152496, acc.: 44.97%] [G loss: 0.7042540907859802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 74/86 [D loss: 0.6981601715087891, acc.: 44.09%] [G loss: 0.7030745148658752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 75/86 [D loss: 0.6984694004058838, acc.: 44.14%] [G loss: 0.7028969526290894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 76/86 [D loss: 0.6976224184036255, acc.: 46.48%] [G loss: 0.6981640458106995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 77/86 [D loss: 0.6989411115646362, acc.: 45.02%] [G loss: 0.7010858058929443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 78/86 [D loss: 0.6990495920181274, acc.: 43.02%] [G loss: 0.7043476104736328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 79/86 [D loss: 0.6956536173820496, acc.: 48.73%] [G loss: 0.7016417980194092]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 80/86 [D loss: 0.6968701481819153, acc.: 47.02%] [G loss: 0.7026088237762451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 81/86 [D loss: 0.6972706317901611, acc.: 45.46%] [G loss: 0.6965973973274231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 82/86 [D loss: 0.6999769508838654, acc.: 43.95%] [G loss: 0.6953569650650024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 83/86 [D loss: 0.70103919506073, acc.: 44.78%] [G loss: 0.7024399042129517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 84/86 [D loss: 0.697666734457016, acc.: 44.34%] [G loss: 0.7023369073867798]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 85/86 [D loss: 0.6979377865791321, acc.: 44.92%] [G loss: 0.7024059891700745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 86/86 [D loss: 0.6987200379371643, acc.: 45.02%] [G loss: 0.7002612948417664]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 1/86 [D loss: 0.6996003091335297, acc.: 44.73%] [G loss: 0.6950069665908813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 2/86 [D loss: 0.7002163529396057, acc.: 44.24%] [G loss: 0.7017542719841003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 3/86 [D loss: 0.6974258720874786, acc.: 46.78%] [G loss: 0.7015337347984314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 4/86 [D loss: 0.6987908482551575, acc.: 44.48%] [G loss: 0.7023473381996155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 5/86 [D loss: 0.697676032781601, acc.: 45.75%] [G loss: 0.6962090730667114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 6/86 [D loss: 0.7012318670749664, acc.: 41.94%] [G loss: 0.6941241025924683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 7/86 [D loss: 0.7001471817493439, acc.: 45.21%] [G loss: 0.6991284489631653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 8/86 [D loss: 0.6960834264755249, acc.: 47.17%] [G loss: 0.7013819813728333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 9/86 [D loss: 0.6962748765945435, acc.: 47.36%] [G loss: 0.7000353932380676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 10/86 [D loss: 0.6987428069114685, acc.: 44.53%] [G loss: 0.697394609451294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 11/86 [D loss: 0.698859304189682, acc.: 44.24%] [G loss: 0.6983458995819092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 12/86 [D loss: 0.699155867099762, acc.: 46.14%] [G loss: 0.6979271769523621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 13/86 [D loss: 0.6976643800735474, acc.: 45.17%] [G loss: 0.7024767398834229]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 14/86 [D loss: 0.6991557478904724, acc.: 43.07%] [G loss: 0.7022554874420166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 15/86 [D loss: 0.6977234184741974, acc.: 47.36%] [G loss: 0.7002969980239868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 16/86 [D loss: 0.6976401209831238, acc.: 45.26%] [G loss: 0.6964560747146606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 17/86 [D loss: 0.7006067633628845, acc.: 43.75%] [G loss: 0.6985041499137878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 18/86 [D loss: 0.6958474516868591, acc.: 48.29%] [G loss: 0.7009280920028687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 19/86 [D loss: 0.697148859500885, acc.: 47.02%] [G loss: 0.7028412222862244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 20/86 [D loss: 0.6970078349113464, acc.: 46.88%] [G loss: 0.6990443468093872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 21/86 [D loss: 0.6973443925380707, acc.: 46.68%] [G loss: 0.6994485855102539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 22/86 [D loss: 0.7004311382770538, acc.: 43.31%] [G loss: 0.7001591324806213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 23/86 [D loss: 0.6975221335887909, acc.: 47.12%] [G loss: 0.7026661038398743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 24/86 [D loss: 0.698830246925354, acc.: 45.41%] [G loss: 0.7009361982345581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 25/86 [D loss: 0.6991391181945801, acc.: 45.65%] [G loss: 0.701429545879364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 26/86 [D loss: 0.6974653005599976, acc.: 46.24%] [G loss: 0.7009099125862122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 27/86 [D loss: 0.6982080340385437, acc.: 45.51%] [G loss: 0.7016888856887817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 28/86 [D loss: 0.6985822021961212, acc.: 45.70%] [G loss: 0.700753927230835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 29/86 [D loss: 0.6989506781101227, acc.: 44.19%] [G loss: 0.7008141875267029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 30/86 [D loss: 0.6958662569522858, acc.: 48.34%] [G loss: 0.701511025428772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 31/86 [D loss: 0.6970938742160797, acc.: 46.58%] [G loss: 0.7000053524971008]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 32/86 [D loss: 0.6996555924415588, acc.: 43.07%] [G loss: 0.6980077028274536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 33/86 [D loss: 0.6984860002994537, acc.: 44.19%] [G loss: 0.7013906240463257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 34/86 [D loss: 0.6974993050098419, acc.: 45.70%] [G loss: 0.7017607688903809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 35/86 [D loss: 0.69835364818573, acc.: 46.04%] [G loss: 0.7009754180908203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 36/86 [D loss: 0.6980636417865753, acc.: 45.56%] [G loss: 0.7011754512786865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 37/86 [D loss: 0.6994243860244751, acc.: 44.87%] [G loss: 0.6997100710868835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 38/86 [D loss: 0.6980307400226593, acc.: 44.04%] [G loss: 0.702120304107666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 39/86 [D loss: 0.6977305114269257, acc.: 45.51%] [G loss: 0.701164186000824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 40/86 [D loss: 0.6976374089717865, acc.: 44.87%] [G loss: 0.6989059448242188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 41/86 [D loss: 0.6966060101985931, acc.: 46.44%] [G loss: 0.6989549994468689]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 42/86 [D loss: 0.6981830894947052, acc.: 45.90%] [G loss: 0.700244128704071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 43/86 [D loss: 0.6975359916687012, acc.: 47.31%] [G loss: 0.7027151584625244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 44/86 [D loss: 0.6985874772071838, acc.: 44.87%] [G loss: 0.7020179629325867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 45/86 [D loss: 0.6969419717788696, acc.: 46.73%] [G loss: 0.7004377841949463]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 46/86 [D loss: 0.6973219513893127, acc.: 46.83%] [G loss: 0.6987223625183105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 47/86 [D loss: 0.6982395052909851, acc.: 46.14%] [G loss: 0.6998688578605652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 48/86 [D loss: 0.6982065141201019, acc.: 46.83%] [G loss: 0.7020564675331116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 49/86 [D loss: 0.6969736516475677, acc.: 48.34%] [G loss: 0.7016831636428833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 50/86 [D loss: 0.6972425580024719, acc.: 47.31%] [G loss: 0.700752854347229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 51/86 [D loss: 0.6984237730503082, acc.: 46.39%] [G loss: 0.7005112171173096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 52/86 [D loss: 0.6986124515533447, acc.: 44.63%] [G loss: 0.6994199752807617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 53/86 [D loss: 0.6973700821399689, acc.: 45.95%] [G loss: 0.7009328007698059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 54/86 [D loss: 0.6970311403274536, acc.: 46.92%] [G loss: 0.6997413635253906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 55/86 [D loss: 0.6965019702911377, acc.: 46.83%] [G loss: 0.6996177434921265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 56/86 [D loss: 0.6980219483375549, acc.: 45.41%] [G loss: 0.6999369859695435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 57/86 [D loss: 0.6981185972690582, acc.: 45.80%] [G loss: 0.6992068290710449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 58/86 [D loss: 0.6978503167629242, acc.: 46.00%] [G loss: 0.6987046003341675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 59/86 [D loss: 0.6980473399162292, acc.: 45.17%] [G loss: 0.7002991437911987]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 60/86 [D loss: 0.6968500316143036, acc.: 45.75%] [G loss: 0.6996051669120789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 61/86 [D loss: 0.6981363594532013, acc.: 44.78%] [G loss: 0.6982332468032837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 62/86 [D loss: 0.6974563002586365, acc.: 46.68%] [G loss: 0.7009152770042419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 63/86 [D loss: 0.6966218054294586, acc.: 47.17%] [G loss: 0.6997440457344055]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 64/86 [D loss: 0.6982341706752777, acc.: 44.73%] [G loss: 0.7007309198379517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 65/86 [D loss: 0.6977647542953491, acc.: 44.82%] [G loss: 0.7001141905784607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 66/86 [D loss: 0.6959353089332581, acc.: 48.44%] [G loss: 0.6990194320678711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 67/86 [D loss: 0.6983573734760284, acc.: 46.14%] [G loss: 0.6980903744697571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 68/86 [D loss: 0.699009358882904, acc.: 45.70%] [G loss: 0.7003283500671387]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 69/86 [D loss: 0.6987218856811523, acc.: 45.36%] [G loss: 0.6992999315261841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 70/86 [D loss: 0.6967057287693024, acc.: 47.56%] [G loss: 0.6993475556373596]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 71/86 [D loss: 0.6962209045886993, acc.: 47.85%] [G loss: 0.6983349919319153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 72/86 [D loss: 0.6977694034576416, acc.: 45.80%] [G loss: 0.696465015411377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 73/86 [D loss: 0.6980702877044678, acc.: 44.87%] [G loss: 0.6999197006225586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 74/86 [D loss: 0.6977055370807648, acc.: 44.78%] [G loss: 0.7000499963760376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 75/86 [D loss: 0.6971107423305511, acc.: 46.04%] [G loss: 0.698195219039917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 76/86 [D loss: 0.6976476907730103, acc.: 44.87%] [G loss: 0.6975130438804626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 77/86 [D loss: 0.6972261667251587, acc.: 47.71%] [G loss: 0.6960406303405762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 78/86 [D loss: 0.6993091702461243, acc.: 45.21%] [G loss: 0.6963099241256714]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 79/86 [D loss: 0.695555180311203, acc.: 47.22%] [G loss: 0.6990489959716797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 80/86 [D loss: 0.6952523291110992, acc.: 48.78%] [G loss: 0.7017463445663452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 81/86 [D loss: 0.696277529001236, acc.: 47.61%] [G loss: 0.7004718780517578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 82/86 [D loss: 0.6972029507160187, acc.: 46.58%] [G loss: 0.6967952847480774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 83/86 [D loss: 0.698968917131424, acc.: 44.53%] [G loss: 0.6970233917236328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 84/86 [D loss: 0.6978346705436707, acc.: 45.26%] [G loss: 0.7017560005187988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 85/86 [D loss: 0.6963966190814972, acc.: 46.58%] [G loss: 0.7000846266746521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 86/86 [D loss: 0.6970667243003845, acc.: 46.39%] [G loss: 0.698523223400116]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 1/86 [D loss: 0.6970346570014954, acc.: 46.58%] [G loss: 0.6981525421142578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 2/86 [D loss: 0.6985265016555786, acc.: 45.36%] [G loss: 0.6988657116889954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 3/86 [D loss: 0.6979061663150787, acc.: 45.90%] [G loss: 0.7020477056503296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 4/86 [D loss: 0.6982200741767883, acc.: 45.51%] [G loss: 0.7000836133956909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 5/86 [D loss: 0.6961441338062286, acc.: 47.31%] [G loss: 0.7018877863883972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 6/86 [D loss: 0.6971514225006104, acc.: 45.36%] [G loss: 0.6995421051979065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 7/86 [D loss: 0.6980277299880981, acc.: 46.00%] [G loss: 0.6988744139671326]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 8/86 [D loss: 0.6979009509086609, acc.: 47.71%] [G loss: 0.6998395919799805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 9/86 [D loss: 0.6974615156650543, acc.: 44.78%] [G loss: 0.7024375200271606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 10/86 [D loss: 0.6966783702373505, acc.: 46.97%] [G loss: 0.7017332315444946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 11/86 [D loss: 0.6982114911079407, acc.: 45.80%] [G loss: 0.6982248425483704]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 12/86 [D loss: 0.6996662020683289, acc.: 43.90%] [G loss: 0.699207603931427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 13/86 [D loss: 0.6993821561336517, acc.: 43.95%] [G loss: 0.7008477449417114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 14/86 [D loss: 0.6974211931228638, acc.: 47.07%] [G loss: 0.700209379196167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 15/86 [D loss: 0.6966311633586884, acc.: 46.34%] [G loss: 0.6999585628509521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 16/86 [D loss: 0.6977630853652954, acc.: 46.58%] [G loss: 0.6993198394775391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 17/86 [D loss: 0.6968503296375275, acc.: 46.58%] [G loss: 0.6987176537513733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 18/86 [D loss: 0.697528213262558, acc.: 47.22%] [G loss: 0.700249195098877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 19/86 [D loss: 0.6963109970092773, acc.: 47.27%] [G loss: 0.6985222697257996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 20/86 [D loss: 0.6977641582489014, acc.: 45.51%] [G loss: 0.7014341950416565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 21/86 [D loss: 0.6961746513843536, acc.: 47.85%] [G loss: 0.7004361748695374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 22/86 [D loss: 0.6977806389331818, acc.: 45.31%] [G loss: 0.6993637681007385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 23/86 [D loss: 0.6993772089481354, acc.: 44.63%] [G loss: 0.701686441898346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 24/86 [D loss: 0.6971602737903595, acc.: 46.39%] [G loss: 0.7045904397964478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 25/86 [D loss: 0.6970802843570709, acc.: 45.95%] [G loss: 0.7013388276100159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 26/86 [D loss: 0.6969499588012695, acc.: 46.24%] [G loss: 0.7011980414390564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 27/86 [D loss: 0.6984991431236267, acc.: 44.92%] [G loss: 0.6992901563644409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 28/86 [D loss: 0.6981946527957916, acc.: 45.61%] [G loss: 0.7042173147201538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 29/86 [D loss: 0.6977092027664185, acc.: 46.24%] [G loss: 0.7017545104026794]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 30/86 [D loss: 0.6978403329849243, acc.: 45.80%] [G loss: 0.7022470831871033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 31/86 [D loss: 0.6972240507602692, acc.: 46.14%] [G loss: 0.6995600461959839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 32/86 [D loss: 0.6962770521640778, acc.: 47.71%] [G loss: 0.7003377079963684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 33/86 [D loss: 0.6976663172245026, acc.: 45.85%] [G loss: 0.6997365951538086]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 34/86 [D loss: 0.6972211003303528, acc.: 46.39%] [G loss: 0.7010979056358337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 35/86 [D loss: 0.6976266503334045, acc.: 46.63%] [G loss: 0.699611246585846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 36/86 [D loss: 0.6958320140838623, acc.: 47.85%] [G loss: 0.6980369687080383]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 37/86 [D loss: 0.6989164054393768, acc.: 43.90%] [G loss: 0.6976912021636963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 38/86 [D loss: 0.6968256235122681, acc.: 45.61%] [G loss: 0.7005811333656311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 39/86 [D loss: 0.6981669366359711, acc.: 45.51%] [G loss: 0.7030901908874512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 40/86 [D loss: 0.6974913477897644, acc.: 45.90%] [G loss: 0.7004414796829224]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 41/86 [D loss: 0.6974228024482727, acc.: 45.95%] [G loss: 0.6977266669273376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 42/86 [D loss: 0.6979663074016571, acc.: 46.29%] [G loss: 0.6975847482681274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 43/86 [D loss: 0.6990126371383667, acc.: 44.58%] [G loss: 0.7013570070266724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 44/86 [D loss: 0.6949390172958374, acc.: 48.14%] [G loss: 0.7029732465744019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 45/86 [D loss: 0.6957411170005798, acc.: 48.05%] [G loss: 0.7013263702392578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 46/86 [D loss: 0.6986723244190216, acc.: 44.97%] [G loss: 0.6995763182640076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 47/86 [D loss: 0.6980929970741272, acc.: 44.92%] [G loss: 0.6987847089767456]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 48/86 [D loss: 0.6975250244140625, acc.: 45.36%] [G loss: 0.7018905282020569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 49/86 [D loss: 0.6974480748176575, acc.: 46.39%] [G loss: 0.7008209228515625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 50/86 [D loss: 0.6970674693584442, acc.: 46.00%] [G loss: 0.6994966864585876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 51/86 [D loss: 0.6974894106388092, acc.: 46.29%] [G loss: 0.6977067589759827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 52/86 [D loss: 0.6987300217151642, acc.: 44.14%] [G loss: 0.7004681825637817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 53/86 [D loss: 0.6987354159355164, acc.: 43.75%] [G loss: 0.699815571308136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 54/86 [D loss: 0.6976963579654694, acc.: 45.31%] [G loss: 0.6977423429489136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 55/86 [D loss: 0.698384553194046, acc.: 45.46%] [G loss: 0.6989585757255554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 56/86 [D loss: 0.6979925632476807, acc.: 44.14%] [G loss: 0.6973285675048828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 57/86 [D loss: 0.6977494359016418, acc.: 45.61%] [G loss: 0.7014007568359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 58/86 [D loss: 0.696467787027359, acc.: 47.66%] [G loss: 0.7002170085906982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 59/86 [D loss: 0.696575254201889, acc.: 46.09%] [G loss: 0.6992637515068054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 60/86 [D loss: 0.6965664625167847, acc.: 46.48%] [G loss: 0.6958560347557068]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 61/86 [D loss: 0.6972417831420898, acc.: 47.80%] [G loss: 0.6985799074172974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 62/86 [D loss: 0.6982617080211639, acc.: 46.04%] [G loss: 0.7018789649009705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 63/86 [D loss: 0.697736918926239, acc.: 44.87%] [G loss: 0.7000953555107117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 64/86 [D loss: 0.6967373788356781, acc.: 47.36%] [G loss: 0.7000017166137695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 65/86 [D loss: 0.6976548433303833, acc.: 45.56%] [G loss: 0.6984201073646545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 66/86 [D loss: 0.6967900991439819, acc.: 45.31%] [G loss: 0.6993303298950195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 67/86 [D loss: 0.6965261995792389, acc.: 47.36%] [G loss: 0.7023652791976929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 68/86 [D loss: 0.6965707838535309, acc.: 48.49%] [G loss: 0.7009291052818298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 69/86 [D loss: 0.6970668137073517, acc.: 47.51%] [G loss: 0.6993532776832581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 70/86 [D loss: 0.6975646913051605, acc.: 47.07%] [G loss: 0.6975471377372742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 71/86 [D loss: 0.6990398466587067, acc.: 43.80%] [G loss: 0.699303388595581]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 72/86 [D loss: 0.6976649165153503, acc.: 47.07%] [G loss: 0.7021863460540771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 73/86 [D loss: 0.6982189416885376, acc.: 45.85%] [G loss: 0.7009372115135193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 74/86 [D loss: 0.6984878480434418, acc.: 43.75%] [G loss: 0.6999384164810181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 75/86 [D loss: 0.6966403722763062, acc.: 46.83%] [G loss: 0.6993879079818726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 76/86 [D loss: 0.697473406791687, acc.: 45.85%] [G loss: 0.6992897391319275]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 77/86 [D loss: 0.6978742778301239, acc.: 46.14%] [G loss: 0.7031131386756897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 78/86 [D loss: 0.6969918012619019, acc.: 45.85%] [G loss: 0.703089714050293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 79/86 [D loss: 0.6967014074325562, acc.: 48.00%] [G loss: 0.698231041431427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 80/86 [D loss: 0.6976478099822998, acc.: 45.85%] [G loss: 0.6974431872367859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 81/86 [D loss: 0.696191668510437, acc.: 48.68%] [G loss: 0.701424241065979]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 82/86 [D loss: 0.6965348422527313, acc.: 47.07%] [G loss: 0.699641764163971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 83/86 [D loss: 0.6971181333065033, acc.: 46.92%] [G loss: 0.6999511122703552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 84/86 [D loss: 0.697319358587265, acc.: 45.70%] [G loss: 0.6984500288963318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 85/86 [D loss: 0.6980010867118835, acc.: 44.78%] [G loss: 0.7001190185546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 86/86 [D loss: 0.6964532732963562, acc.: 47.75%] [G loss: 0.7021823525428772]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 22/200, Batch 1/86 [D loss: 0.6964541673660278, acc.: 47.22%] [G loss: 0.7030397653579712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 2/86 [D loss: 0.6970846652984619, acc.: 46.44%] [G loss: 0.6975839138031006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 3/86 [D loss: 0.6974799036979675, acc.: 48.10%] [G loss: 0.6979257464408875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 4/86 [D loss: 0.697810024023056, acc.: 46.19%] [G loss: 0.6995594501495361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 5/86 [D loss: 0.6968058943748474, acc.: 45.75%] [G loss: 0.6994889378547668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 6/86 [D loss: 0.6968781352043152, acc.: 47.66%] [G loss: 0.6986681222915649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 7/86 [D loss: 0.6970263719558716, acc.: 46.83%] [G loss: 0.6971136331558228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 8/86 [D loss: 0.6982125341892242, acc.: 45.80%] [G loss: 0.6977109313011169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 9/86 [D loss: 0.6970260441303253, acc.: 46.92%] [G loss: 0.7027446627616882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 10/86 [D loss: 0.6962333917617798, acc.: 48.05%] [G loss: 0.6999849677085876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 11/86 [D loss: 0.6966589391231537, acc.: 47.75%] [G loss: 0.7003932595252991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 12/86 [D loss: 0.697285532951355, acc.: 46.48%] [G loss: 0.6962057948112488]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 13/86 [D loss: 0.696876734495163, acc.: 45.56%] [G loss: 0.6984990239143372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 14/86 [D loss: 0.69883131980896, acc.: 45.85%] [G loss: 0.7038032412528992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 15/86 [D loss: 0.6969993710517883, acc.: 46.04%] [G loss: 0.6995603442192078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 16/86 [D loss: 0.6985985338687897, acc.: 43.85%] [G loss: 0.7000682950019836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 17/86 [D loss: 0.6975928544998169, acc.: 45.80%] [G loss: 0.6977386474609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 18/86 [D loss: 0.6997037827968597, acc.: 44.24%] [G loss: 0.6984801888465881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 19/86 [D loss: 0.6960912346839905, acc.: 47.12%] [G loss: 0.7030961513519287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 20/86 [D loss: 0.6959527432918549, acc.: 47.66%] [G loss: 0.7013946175575256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 21/86 [D loss: 0.696145623922348, acc.: 46.78%] [G loss: 0.6985395550727844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 22/86 [D loss: 0.6973317861557007, acc.: 47.07%] [G loss: 0.6952612400054932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 23/86 [D loss: 0.697420746088028, acc.: 45.21%] [G loss: 0.6994502544403076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 24/86 [D loss: 0.6976523697376251, acc.: 44.97%] [G loss: 0.7016661167144775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 25/86 [D loss: 0.6958726644515991, acc.: 46.83%] [G loss: 0.7013081312179565]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 26/86 [D loss: 0.6965495347976685, acc.: 46.53%] [G loss: 0.6976834535598755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 27/86 [D loss: 0.6983615756034851, acc.: 46.14%] [G loss: 0.6989302039146423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 28/86 [D loss: 0.6983547210693359, acc.: 44.38%] [G loss: 0.6999645829200745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 29/86 [D loss: 0.6962986588478088, acc.: 47.17%] [G loss: 0.6988599300384521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 30/86 [D loss: 0.6972529590129852, acc.: 45.26%] [G loss: 0.6991393566131592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 31/86 [D loss: 0.6965344250202179, acc.: 47.12%] [G loss: 0.6986890435218811]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 32/86 [D loss: 0.6968891024589539, acc.: 46.73%] [G loss: 0.6986526250839233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 33/86 [D loss: 0.69709911942482, acc.: 45.02%] [G loss: 0.7014550566673279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 34/86 [D loss: 0.6961453258991241, acc.: 47.27%] [G loss: 0.7018251419067383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 35/86 [D loss: 0.6966367661952972, acc.: 46.00%] [G loss: 0.698850154876709]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 36/86 [D loss: 0.6975226700305939, acc.: 45.80%] [G loss: 0.6980555057525635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 37/86 [D loss: 0.6985701620578766, acc.: 44.43%] [G loss: 0.6989786624908447]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 38/86 [D loss: 0.6972213387489319, acc.: 46.19%] [G loss: 0.7029292583465576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 39/86 [D loss: 0.695999950170517, acc.: 47.12%] [G loss: 0.7013965845108032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 40/86 [D loss: 0.6964833736419678, acc.: 47.27%] [G loss: 0.6992913484573364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 41/86 [D loss: 0.698511391878128, acc.: 45.95%] [G loss: 0.6986902356147766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 42/86 [D loss: 0.6974675357341766, acc.: 46.00%] [G loss: 0.703183114528656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 43/86 [D loss: 0.6964228749275208, acc.: 47.46%] [G loss: 0.7026359438896179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 44/86 [D loss: 0.6963959634304047, acc.: 45.41%] [G loss: 0.701856255531311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 45/86 [D loss: 0.6956060528755188, acc.: 48.63%] [G loss: 0.6972615718841553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 46/86 [D loss: 0.6990955770015717, acc.: 43.55%] [G loss: 0.6981569528579712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 47/86 [D loss: 0.6976754665374756, acc.: 45.65%] [G loss: 0.7006271481513977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 48/86 [D loss: 0.6963797211647034, acc.: 48.39%] [G loss: 0.700751543045044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 49/86 [D loss: 0.6959218382835388, acc.: 47.46%] [G loss: 0.7002506852149963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 50/86 [D loss: 0.6969951689243317, acc.: 44.38%] [G loss: 0.6969857811927795]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 51/86 [D loss: 0.6995733976364136, acc.: 43.80%] [G loss: 0.6968218088150024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 52/86 [D loss: 0.6972368955612183, acc.: 47.17%] [G loss: 0.7015068531036377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 53/86 [D loss: 0.6966820657253265, acc.: 46.83%] [G loss: 0.6982569694519043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 54/86 [D loss: 0.6964786350727081, acc.: 46.68%] [G loss: 0.698814868927002]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 55/86 [D loss: 0.6987411379814148, acc.: 45.51%] [G loss: 0.6989899277687073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 56/86 [D loss: 0.6987550556659698, acc.: 45.95%] [G loss: 0.6995788216590881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 57/86 [D loss: 0.6962198913097382, acc.: 47.07%] [G loss: 0.7014282941818237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 58/86 [D loss: 0.6970426142215729, acc.: 46.63%] [G loss: 0.6995570659637451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 59/86 [D loss: 0.6965427100658417, acc.: 47.36%] [G loss: 0.6993257403373718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 60/86 [D loss: 0.6973977982997894, acc.: 46.29%] [G loss: 0.6974749565124512]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 61/86 [D loss: 0.6967461109161377, acc.: 45.26%] [G loss: 0.700289249420166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 62/86 [D loss: 0.6951694190502167, acc.: 49.07%] [G loss: 0.7008520364761353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 63/86 [D loss: 0.6955638527870178, acc.: 48.54%] [G loss: 0.7009680867195129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 64/86 [D loss: 0.6968837380409241, acc.: 46.14%] [G loss: 0.6989554166793823]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 65/86 [D loss: 0.6967772543430328, acc.: 47.41%] [G loss: 0.6995024085044861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 66/86 [D loss: 0.6970600187778473, acc.: 47.90%] [G loss: 0.7014390826225281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 67/86 [D loss: 0.6970441341400146, acc.: 45.61%] [G loss: 0.700951099395752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 68/86 [D loss: 0.6967491805553436, acc.: 46.68%] [G loss: 0.6990736126899719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 69/86 [D loss: 0.6967921853065491, acc.: 45.80%] [G loss: 0.701371431350708]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 70/86 [D loss: 0.6965630054473877, acc.: 46.09%] [G loss: 0.7002314329147339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 71/86 [D loss: 0.6966369152069092, acc.: 46.09%] [G loss: 0.6996192932128906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 72/86 [D loss: 0.6954028308391571, acc.: 47.80%] [G loss: 0.6984897255897522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 73/86 [D loss: 0.6966294348239899, acc.: 47.02%] [G loss: 0.699923038482666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 74/86 [D loss: 0.6962157189846039, acc.: 48.05%] [G loss: 0.6987264752388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 75/86 [D loss: 0.696925163269043, acc.: 46.78%] [G loss: 0.6990671157836914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 76/86 [D loss: 0.6960894465446472, acc.: 48.93%] [G loss: 0.7002979516983032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 77/86 [D loss: 0.6967134773731232, acc.: 45.80%] [G loss: 0.699790358543396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 78/86 [D loss: 0.6975502371788025, acc.: 45.75%] [G loss: 0.69817054271698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 79/86 [D loss: 0.6996550559997559, acc.: 42.87%] [G loss: 0.6990631818771362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 80/86 [D loss: 0.6958224773406982, acc.: 48.24%] [G loss: 0.7019059658050537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 81/86 [D loss: 0.6956867873668671, acc.: 46.92%] [G loss: 0.6992961168289185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 82/86 [D loss: 0.6972421407699585, acc.: 47.61%] [G loss: 0.6976898312568665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 83/86 [D loss: 0.6977023482322693, acc.: 46.63%] [G loss: 0.7002776265144348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 84/86 [D loss: 0.6976073682308197, acc.: 45.56%] [G loss: 0.7018478512763977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 85/86 [D loss: 0.695588231086731, acc.: 47.46%] [G loss: 0.7001051902770996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 86/86 [D loss: 0.6967455148696899, acc.: 47.61%] [G loss: 0.6990712881088257]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 1/86 [D loss: 0.6970303058624268, acc.: 45.70%] [G loss: 0.6980147361755371]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 2/86 [D loss: 0.6984975636005402, acc.: 45.02%] [G loss: 0.6986738443374634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 3/86 [D loss: 0.6953091323375702, acc.: 48.24%] [G loss: 0.6999037265777588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 4/86 [D loss: 0.6965388059616089, acc.: 46.44%] [G loss: 0.6993105411529541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 5/86 [D loss: 0.6964944005012512, acc.: 47.61%] [G loss: 0.6998847126960754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 6/86 [D loss: 0.6976743936538696, acc.: 44.63%] [G loss: 0.6987171173095703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 7/86 [D loss: 0.6968773007392883, acc.: 47.02%] [G loss: 0.6986925601959229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 8/86 [D loss: 0.6970691680908203, acc.: 45.17%] [G loss: 0.6983837485313416]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 9/86 [D loss: 0.69754359126091, acc.: 45.12%] [G loss: 0.6971913576126099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 10/86 [D loss: 0.6964631676673889, acc.: 47.41%] [G loss: 0.6958457231521606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 11/86 [D loss: 0.6970526874065399, acc.: 46.73%] [G loss: 0.6984531879425049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 12/86 [D loss: 0.6961063146591187, acc.: 47.90%] [G loss: 0.698015034198761]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 13/86 [D loss: 0.6971253454685211, acc.: 45.56%] [G loss: 0.6971867680549622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 14/86 [D loss: 0.6971578001976013, acc.: 46.58%] [G loss: 0.6978705525398254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 15/86 [D loss: 0.6972644329071045, acc.: 45.21%] [G loss: 0.6993439197540283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 16/86 [D loss: 0.6963106691837311, acc.: 48.14%] [G loss: 0.699813187122345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 17/86 [D loss: 0.6962290108203888, acc.: 46.78%] [G loss: 0.6985134482383728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 18/86 [D loss: 0.6964218616485596, acc.: 46.04%] [G loss: 0.6987168788909912]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 19/86 [D loss: 0.6974355280399323, acc.: 45.36%] [G loss: 0.6957554221153259]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 20/86 [D loss: 0.6984843909740448, acc.: 44.48%] [G loss: 0.6977259516716003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 21/86 [D loss: 0.6953967809677124, acc.: 48.24%] [G loss: 0.7004091143608093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 22/86 [D loss: 0.6959341168403625, acc.: 48.78%] [G loss: 0.6999232769012451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 23/86 [D loss: 0.6955642104148865, acc.: 48.58%] [G loss: 0.698657214641571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 24/86 [D loss: 0.698311448097229, acc.: 44.97%] [G loss: 0.6983513236045837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 25/86 [D loss: 0.6981095373630524, acc.: 44.48%] [G loss: 0.7017086148262024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 26/86 [D loss: 0.6968146562576294, acc.: 45.75%] [G loss: 0.7007074952125549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 27/86 [D loss: 0.6947396099567413, acc.: 49.27%] [G loss: 0.6992405652999878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 28/86 [D loss: 0.6974062919616699, acc.: 44.92%] [G loss: 0.6978864669799805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 29/86 [D loss: 0.6974330544471741, acc.: 45.75%] [G loss: 0.6975233554840088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 30/86 [D loss: 0.696967750787735, acc.: 46.63%] [G loss: 0.7018199563026428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 31/86 [D loss: 0.6959964036941528, acc.: 48.10%] [G loss: 0.701077938079834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 32/86 [D loss: 0.6955545246601105, acc.: 48.44%] [G loss: 0.7008038759231567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 33/86 [D loss: 0.6971677243709564, acc.: 45.85%] [G loss: 0.6976369619369507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 34/86 [D loss: 0.6977881193161011, acc.: 46.00%] [G loss: 0.6994336843490601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 35/86 [D loss: 0.6974033713340759, acc.: 45.12%] [G loss: 0.7025370001792908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 36/86 [D loss: 0.696362316608429, acc.: 46.78%] [G loss: 0.7009287476539612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 37/86 [D loss: 0.6957159042358398, acc.: 47.56%] [G loss: 0.6992882490158081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 38/86 [D loss: 0.6968196928501129, acc.: 46.00%] [G loss: 0.6953830718994141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 39/86 [D loss: 0.6993538737297058, acc.: 44.53%] [G loss: 0.6993089914321899]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 40/86 [D loss: 0.6968012154102325, acc.: 46.04%] [G loss: 0.702491044998169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 41/86 [D loss: 0.6964750289916992, acc.: 46.48%] [G loss: 0.7004262208938599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 42/86 [D loss: 0.6963052749633789, acc.: 47.85%] [G loss: 0.6968137621879578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 43/86 [D loss: 0.6971331834793091, acc.: 46.97%] [G loss: 0.6922826766967773]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 44/86 [D loss: 0.7006831467151642, acc.: 43.46%] [G loss: 0.6962926983833313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 45/86 [D loss: 0.6942303478717804, acc.: 49.27%] [G loss: 0.7014598846435547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 46/86 [D loss: 0.6979367434978485, acc.: 44.78%] [G loss: 0.7006466388702393]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 47/86 [D loss: 0.6966277360916138, acc.: 46.68%] [G loss: 0.6985539793968201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 48/86 [D loss: 0.697111576795578, acc.: 46.04%] [G loss: 0.6928722262382507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 49/86 [D loss: 0.7002114951610565, acc.: 42.29%] [G loss: 0.6982298493385315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 50/86 [D loss: 0.6941810548305511, acc.: 50.54%] [G loss: 0.6996944546699524]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 51/86 [D loss: 0.6979396939277649, acc.: 43.41%] [G loss: 0.7000631093978882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 52/86 [D loss: 0.6969757080078125, acc.: 44.78%] [G loss: 0.6990157961845398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 53/86 [D loss: 0.6983310878276825, acc.: 45.85%] [G loss: 0.6943280696868896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 54/86 [D loss: 0.6990222930908203, acc.: 43.75%] [G loss: 0.7002913355827332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 55/86 [D loss: 0.6959426403045654, acc.: 47.80%] [G loss: 0.7009482979774475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 56/86 [D loss: 0.6962807476520538, acc.: 47.41%] [G loss: 0.6990929245948792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 57/86 [D loss: 0.696828305721283, acc.: 45.46%] [G loss: 0.6988004446029663]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 58/86 [D loss: 0.6979159712791443, acc.: 46.14%] [G loss: 0.69401615858078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 59/86 [D loss: 0.6999310255050659, acc.: 43.85%] [G loss: 0.701018214225769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 60/86 [D loss: 0.6954213976860046, acc.: 47.02%] [G loss: 0.7012269496917725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 61/86 [D loss: 0.6980366110801697, acc.: 44.63%] [G loss: 0.7001886367797852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 62/86 [D loss: 0.6969919800758362, acc.: 46.24%] [G loss: 0.6989975571632385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 63/86 [D loss: 0.6973957717418671, acc.: 46.19%] [G loss: 0.6988930106163025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 64/86 [D loss: 0.6978841423988342, acc.: 45.02%] [G loss: 0.6992987394332886]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 65/86 [D loss: 0.6973454356193542, acc.: 46.58%] [G loss: 0.6994092464447021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 66/86 [D loss: 0.6966709792613983, acc.: 45.26%] [G loss: 0.7003986239433289]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 67/86 [D loss: 0.6962926089763641, acc.: 47.61%] [G loss: 0.6999971270561218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 68/86 [D loss: 0.6970634460449219, acc.: 46.48%] [G loss: 0.699910581111908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 69/86 [D loss: 0.6959381997585297, acc.: 47.07%] [G loss: 0.7015411257743835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 70/86 [D loss: 0.6961731314659119, acc.: 47.12%] [G loss: 0.7014578580856323]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 71/86 [D loss: 0.6974330246448517, acc.: 45.12%] [G loss: 0.6982194781303406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 72/86 [D loss: 0.6960374414920807, acc.: 46.44%] [G loss: 0.7005606889724731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 73/86 [D loss: 0.6958852112293243, acc.: 47.56%] [G loss: 0.7005577683448792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 74/86 [D loss: 0.6971511542797089, acc.: 45.85%] [G loss: 0.702782928943634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 75/86 [D loss: 0.6950499713420868, acc.: 48.34%] [G loss: 0.7016212940216064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 76/86 [D loss: 0.6960079669952393, acc.: 46.48%] [G loss: 0.7014220952987671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 77/86 [D loss: 0.6970608234405518, acc.: 45.46%] [G loss: 0.6997436285018921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 78/86 [D loss: 0.6970526874065399, acc.: 44.78%] [G loss: 0.7014241218566895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 79/86 [D loss: 0.6965299844741821, acc.: 45.56%] [G loss: 0.7012521624565125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 80/86 [D loss: 0.6963033080101013, acc.: 46.63%] [G loss: 0.7029832005500793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 81/86 [D loss: 0.6966663300991058, acc.: 46.14%] [G loss: 0.6993283629417419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 82/86 [D loss: 0.6969527304172516, acc.: 45.65%] [G loss: 0.7010228633880615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 83/86 [D loss: 0.696119487285614, acc.: 46.34%] [G loss: 0.7005900144577026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 84/86 [D loss: 0.6960798501968384, acc.: 47.17%] [G loss: 0.7012139558792114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 85/86 [D loss: 0.6971246600151062, acc.: 46.97%] [G loss: 0.7008872032165527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 86/86 [D loss: 0.6969256699085236, acc.: 46.92%] [G loss: 0.7004542946815491]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 1/86 [D loss: 0.6955753862857819, acc.: 47.31%] [G loss: 0.7011101245880127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 2/86 [D loss: 0.6960816383361816, acc.: 48.39%] [G loss: 0.7017993330955505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 3/86 [D loss: 0.6960113048553467, acc.: 46.53%] [G loss: 0.6983950138092041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 4/86 [D loss: 0.6974507868289948, acc.: 45.07%] [G loss: 0.6993987560272217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 5/86 [D loss: 0.6967804133892059, acc.: 45.90%] [G loss: 0.6991549134254456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 6/86 [D loss: 0.6983484029769897, acc.: 44.19%] [G loss: 0.6993481516838074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 7/86 [D loss: 0.6961537301540375, acc.: 46.73%] [G loss: 0.6991050243377686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 8/86 [D loss: 0.6961172223091125, acc.: 46.39%] [G loss: 0.700282096862793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 9/86 [D loss: 0.6966642737388611, acc.: 44.48%] [G loss: 0.6983844637870789]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 10/86 [D loss: 0.695950835943222, acc.: 48.78%] [G loss: 0.6985538601875305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 11/86 [D loss: 0.6965213716030121, acc.: 47.17%] [G loss: 0.6981362104415894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 12/86 [D loss: 0.6961342692375183, acc.: 48.00%] [G loss: 0.6988804340362549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 13/86 [D loss: 0.6957427859306335, acc.: 48.05%] [G loss: 0.6965832114219666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 14/86 [D loss: 0.6983862817287445, acc.: 43.99%] [G loss: 0.6957182288169861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 15/86 [D loss: 0.6973085701465607, acc.: 46.09%] [G loss: 0.6983993649482727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 16/86 [D loss: 0.6969112157821655, acc.: 45.90%] [G loss: 0.6986367702484131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 17/86 [D loss: 0.6958471238613129, acc.: 46.19%] [G loss: 0.6976686120033264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 18/86 [D loss: 0.6959609091281891, acc.: 47.27%] [G loss: 0.6961718797683716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 19/86 [D loss: 0.6985151171684265, acc.: 44.68%] [G loss: 0.6970211267471313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 20/86 [D loss: 0.6972573697566986, acc.: 47.22%] [G loss: 0.696961522102356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 21/86 [D loss: 0.6966817677021027, acc.: 48.39%] [G loss: 0.6998311281204224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 22/86 [D loss: 0.6961873769760132, acc.: 46.39%] [G loss: 0.6978374123573303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 23/86 [D loss: 0.6968265175819397, acc.: 46.19%] [G loss: 0.696714460849762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 24/86 [D loss: 0.697054922580719, acc.: 45.85%] [G loss: 0.6983076333999634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 25/86 [D loss: 0.6961954534053802, acc.: 46.68%] [G loss: 0.6979338526725769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 26/86 [D loss: 0.6971833109855652, acc.: 45.90%] [G loss: 0.6999196410179138]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 27/86 [D loss: 0.6974832713603973, acc.: 45.31%] [G loss: 0.6982102394104004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 28/86 [D loss: 0.6960726082324982, acc.: 47.36%] [G loss: 0.6972441673278809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 29/86 [D loss: 0.6957012116909027, acc.: 47.71%] [G loss: 0.6987466812133789]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 30/86 [D loss: 0.6965154707431793, acc.: 47.02%] [G loss: 0.7001189589500427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 31/86 [D loss: 0.695019394159317, acc.: 49.02%] [G loss: 0.6991354823112488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 32/86 [D loss: 0.6953085064888, acc.: 47.85%] [G loss: 0.6992218494415283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 33/86 [D loss: 0.6962960660457611, acc.: 45.46%] [G loss: 0.6963075995445251]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 34/86 [D loss: 0.6983589828014374, acc.: 44.58%] [G loss: 0.6968818306922913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 35/86 [D loss: 0.6957471072673798, acc.: 47.31%] [G loss: 0.6997634768486023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 36/86 [D loss: 0.6958024501800537, acc.: 47.46%] [G loss: 0.6998552680015564]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 37/86 [D loss: 0.6976288557052612, acc.: 46.68%] [G loss: 0.6987545490264893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 38/86 [D loss: 0.6971939504146576, acc.: 44.82%] [G loss: 0.6983900666236877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 39/86 [D loss: 0.6969306170940399, acc.: 46.68%] [G loss: 0.6997027397155762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 40/86 [D loss: 0.6960205137729645, acc.: 46.68%] [G loss: 0.6986513733863831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 41/86 [D loss: 0.6957288086414337, acc.: 48.00%] [G loss: 0.6975680589675903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 42/86 [D loss: 0.6953431963920593, acc.: 47.95%] [G loss: 0.6963638663291931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 43/86 [D loss: 0.6964768469333649, acc.: 47.02%] [G loss: 0.69843590259552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 44/86 [D loss: 0.6963132321834564, acc.: 45.26%] [G loss: 0.7009112238883972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 45/86 [D loss: 0.6955176293849945, acc.: 46.83%] [G loss: 0.699455976486206]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 46/86 [D loss: 0.6960105001926422, acc.: 47.61%] [G loss: 0.6989975571632385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 47/86 [D loss: 0.6970301568508148, acc.: 45.41%] [G loss: 0.6979272365570068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 48/86 [D loss: 0.6976287066936493, acc.: 44.87%] [G loss: 0.6977035403251648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 49/86 [D loss: 0.6961245536804199, acc.: 47.36%] [G loss: 0.7001082897186279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 50/86 [D loss: 0.696562260389328, acc.: 46.09%] [G loss: 0.7000585794448853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 51/86 [D loss: 0.6968861818313599, acc.: 45.80%] [G loss: 0.6966833472251892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 52/86 [D loss: 0.6979119777679443, acc.: 45.41%] [G loss: 0.6968116760253906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 53/86 [D loss: 0.6965481340885162, acc.: 46.14%] [G loss: 0.7010168433189392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 54/86 [D loss: 0.6960960030555725, acc.: 46.04%] [G loss: 0.700320839881897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 55/86 [D loss: 0.6959617137908936, acc.: 47.71%] [G loss: 0.6980103850364685]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 56/86 [D loss: 0.6957575380802155, acc.: 45.95%] [G loss: 0.6993712186813354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 57/86 [D loss: 0.6967341899871826, acc.: 46.34%] [G loss: 0.699493408203125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 58/86 [D loss: 0.6968847215175629, acc.: 46.83%] [G loss: 0.7003641724586487]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 59/86 [D loss: 0.6957927346229553, acc.: 47.02%] [G loss: 0.7001223564147949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 60/86 [D loss: 0.6967666149139404, acc.: 45.17%] [G loss: 0.6976077556610107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 61/86 [D loss: 0.6988543570041656, acc.: 43.36%] [G loss: 0.6983066201210022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 62/86 [D loss: 0.6966659426689148, acc.: 47.61%] [G loss: 0.699705958366394]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 63/86 [D loss: 0.6960876882076263, acc.: 45.95%] [G loss: 0.6984407305717468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 64/86 [D loss: 0.6960805952548981, acc.: 46.29%] [G loss: 0.6978806257247925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 65/86 [D loss: 0.6967217028141022, acc.: 46.78%] [G loss: 0.6960963606834412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 66/86 [D loss: 0.6964878439903259, acc.: 46.63%] [G loss: 0.6975377202033997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 67/86 [D loss: 0.6958915293216705, acc.: 48.00%] [G loss: 0.7000359296798706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 68/86 [D loss: 0.6955130398273468, acc.: 46.73%] [G loss: 0.6985989809036255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 69/86 [D loss: 0.6967585682868958, acc.: 47.12%] [G loss: 0.6986715793609619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 70/86 [D loss: 0.6977318525314331, acc.: 45.07%] [G loss: 0.6958981156349182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 71/86 [D loss: 0.695919394493103, acc.: 48.73%] [G loss: 0.7012665867805481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 72/86 [D loss: 0.6954454481601715, acc.: 48.58%] [G loss: 0.7000966668128967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 73/86 [D loss: 0.6961765289306641, acc.: 45.85%] [G loss: 0.6991231441497803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 74/86 [D loss: 0.6950439810752869, acc.: 48.49%] [G loss: 0.6969089508056641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 75/86 [D loss: 0.6994915306568146, acc.: 44.09%] [G loss: 0.6990674734115601]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 76/86 [D loss: 0.6970738470554352, acc.: 45.85%] [G loss: 0.700907289981842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 77/86 [D loss: 0.6965208053588867, acc.: 45.31%] [G loss: 0.7003446221351624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 78/86 [D loss: 0.6958688497543335, acc.: 46.00%] [G loss: 0.6990234851837158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 79/86 [D loss: 0.6986321210861206, acc.: 43.80%] [G loss: 0.6946158409118652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 80/86 [D loss: 0.69801065325737, acc.: 43.75%] [G loss: 0.6978465914726257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 81/86 [D loss: 0.6955963671207428, acc.: 48.10%] [G loss: 0.6994682550430298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 82/86 [D loss: 0.6965139806270599, acc.: 46.29%] [G loss: 0.6997639536857605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 83/86 [D loss: 0.69510218501091, acc.: 47.95%] [G loss: 0.6981703042984009]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 84/86 [D loss: 0.6949933469295502, acc.: 49.76%] [G loss: 0.6936612129211426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 85/86 [D loss: 0.7000196278095245, acc.: 43.51%] [G loss: 0.7012126445770264]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 86/86 [D loss: 0.6942412853240967, acc.: 48.97%] [G loss: 0.6979725360870361]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 1/86 [D loss: 0.6971895694732666, acc.: 44.43%] [G loss: 0.7002313137054443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 2/86 [D loss: 0.6950414180755615, acc.: 47.85%] [G loss: 0.6967858672142029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 3/86 [D loss: 0.6970177590847015, acc.: 46.00%] [G loss: 0.6896556615829468]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 4/86 [D loss: 0.7007884085178375, acc.: 42.33%] [G loss: 0.7025104761123657]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 5/86 [D loss: 0.6949406266212463, acc.: 48.63%] [G loss: 0.6980777978897095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 6/86 [D loss: 0.6981258690357208, acc.: 42.24%] [G loss: 0.6988989114761353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 7/86 [D loss: 0.6962163746356964, acc.: 47.22%] [G loss: 0.6972192525863647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 8/86 [D loss: 0.6988524198532104, acc.: 43.80%] [G loss: 0.6884820461273193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 9/86 [D loss: 0.700815349817276, acc.: 43.95%] [G loss: 0.7015868425369263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 10/86 [D loss: 0.6945014595985413, acc.: 48.97%] [G loss: 0.6979990601539612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 11/86 [D loss: 0.6983107030391693, acc.: 43.90%] [G loss: 0.6977965235710144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 12/86 [D loss: 0.6971840262413025, acc.: 45.90%] [G loss: 0.6946855187416077]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 13/86 [D loss: 0.6977702677249908, acc.: 44.43%] [G loss: 0.6899276971817017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 14/86 [D loss: 0.7003675997257233, acc.: 43.55%] [G loss: 0.6988727450370789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 15/86 [D loss: 0.6937289237976074, acc.: 49.07%] [G loss: 0.7004197835922241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 16/86 [D loss: 0.6978761553764343, acc.: 45.26%] [G loss: 0.6997295618057251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 17/86 [D loss: 0.6963849365711212, acc.: 46.92%] [G loss: 0.6970496773719788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 18/86 [D loss: 0.6977485716342926, acc.: 43.46%] [G loss: 0.691849946975708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 19/86 [D loss: 0.6995601058006287, acc.: 42.63%] [G loss: 0.6971820592880249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 20/86 [D loss: 0.6957464814186096, acc.: 46.68%] [G loss: 0.7007219791412354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 21/86 [D loss: 0.6954815089702606, acc.: 48.34%] [G loss: 0.6995047926902771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 22/86 [D loss: 0.6951628923416138, acc.: 48.10%] [G loss: 0.6991310715675354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 23/86 [D loss: 0.6978604197502136, acc.: 43.80%] [G loss: 0.6974665522575378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 24/86 [D loss: 0.6973563730716705, acc.: 44.58%] [G loss: 0.6985293626785278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 25/86 [D loss: 0.6968501508235931, acc.: 46.73%] [G loss: 0.7019127607345581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 26/86 [D loss: 0.6954026520252228, acc.: 48.39%] [G loss: 0.7020900845527649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 27/86 [D loss: 0.696694552898407, acc.: 46.09%] [G loss: 0.7024607062339783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 28/86 [D loss: 0.6965646147727966, acc.: 46.92%] [G loss: 0.6974217891693115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 29/86 [D loss: 0.69814732670784, acc.: 44.78%] [G loss: 0.700671911239624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 30/86 [D loss: 0.6961942315101624, acc.: 47.51%] [G loss: 0.7012684345245361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 31/86 [D loss: 0.6958083510398865, acc.: 48.19%] [G loss: 0.7021266222000122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 32/86 [D loss: 0.696600615978241, acc.: 47.36%] [G loss: 0.7011797428131104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 33/86 [D loss: 0.6952179670333862, acc.: 47.56%] [G loss: 0.7002418041229248]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 34/86 [D loss: 0.6984047889709473, acc.: 43.26%] [G loss: 0.7005603313446045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 35/86 [D loss: 0.6950525343418121, acc.: 48.44%] [G loss: 0.7024160623550415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 36/86 [D loss: 0.6960485279560089, acc.: 46.68%] [G loss: 0.7019397616386414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 37/86 [D loss: 0.6961857676506042, acc.: 46.44%] [G loss: 0.7004036903381348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 38/86 [D loss: 0.6961078643798828, acc.: 45.95%] [G loss: 0.7000927925109863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 39/86 [D loss: 0.6971422433853149, acc.: 46.00%] [G loss: 0.7006512880325317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 40/86 [D loss: 0.6956750154495239, acc.: 47.80%] [G loss: 0.7018734216690063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 41/86 [D loss: 0.6966094672679901, acc.: 47.31%] [G loss: 0.7014176249504089]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 42/86 [D loss: 0.6949514150619507, acc.: 48.44%] [G loss: 0.7000662088394165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 43/86 [D loss: 0.6955403089523315, acc.: 47.75%] [G loss: 0.7001066207885742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 44/86 [D loss: 0.696059376001358, acc.: 46.88%] [G loss: 0.7016369104385376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 45/86 [D loss: 0.6967705190181732, acc.: 45.17%] [G loss: 0.7018190026283264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 46/86 [D loss: 0.6962129771709442, acc.: 46.83%] [G loss: 0.7009447813034058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 47/86 [D loss: 0.6960841715335846, acc.: 45.41%] [G loss: 0.7008390426635742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 48/86 [D loss: 0.6963496506214142, acc.: 46.04%] [G loss: 0.7008622884750366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 49/86 [D loss: 0.69615238904953, acc.: 48.14%] [G loss: 0.7017592191696167]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 50/86 [D loss: 0.6967711448669434, acc.: 45.51%] [G loss: 0.7020201683044434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 51/86 [D loss: 0.6966610848903656, acc.: 46.09%] [G loss: 0.6990413665771484]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 52/86 [D loss: 0.6963709592819214, acc.: 46.73%] [G loss: 0.7004200220108032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 53/86 [D loss: 0.6969522833824158, acc.: 45.41%] [G loss: 0.6994596719741821]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 54/86 [D loss: 0.6970553398132324, acc.: 45.90%] [G loss: 0.6994117498397827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 55/86 [D loss: 0.6960912346839905, acc.: 46.19%] [G loss: 0.700042724609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 56/86 [D loss: 0.6961149871349335, acc.: 47.27%] [G loss: 0.7012889981269836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 57/86 [D loss: 0.6951928436756134, acc.: 47.75%] [G loss: 0.6979376673698425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 58/86 [D loss: 0.6963222622871399, acc.: 46.34%] [G loss: 0.6956139802932739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 59/86 [D loss: 0.6975412964820862, acc.: 46.19%] [G loss: 0.7001193761825562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 60/86 [D loss: 0.6955394446849823, acc.: 46.39%] [G loss: 0.6995620131492615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 61/86 [D loss: 0.6961388289928436, acc.: 46.73%] [G loss: 0.6989533305168152]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 62/86 [D loss: 0.6967302560806274, acc.: 45.12%] [G loss: 0.696805477142334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 63/86 [D loss: 0.6964648962020874, acc.: 47.27%] [G loss: 0.6948873996734619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 64/86 [D loss: 0.6974895298480988, acc.: 45.41%] [G loss: 0.7004515528678894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 65/86 [D loss: 0.695546567440033, acc.: 47.07%] [G loss: 0.6992541551589966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 66/86 [D loss: 0.6969550251960754, acc.: 44.48%] [G loss: 0.6998684406280518]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 67/86 [D loss: 0.696437656879425, acc.: 46.53%] [G loss: 0.6963997483253479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 68/86 [D loss: 0.696265459060669, acc.: 47.71%] [G loss: 0.6909924149513245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 69/86 [D loss: 0.699436604976654, acc.: 44.24%] [G loss: 0.7032341957092285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 70/86 [D loss: 0.6930888593196869, acc.: 50.49%] [G loss: 0.6945807337760925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 71/86 [D loss: 0.6979705393314362, acc.: 43.60%] [G loss: 0.6967127323150635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 72/86 [D loss: 0.6960800290107727, acc.: 47.12%] [G loss: 0.6957768797874451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 73/86 [D loss: 0.6977805197238922, acc.: 44.29%] [G loss: 0.6873365640640259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 74/86 [D loss: 0.702177882194519, acc.: 41.70%] [G loss: 0.6982725858688354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 75/86 [D loss: 0.6926559209823608, acc.: 51.07%] [G loss: 0.6951834559440613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 76/86 [D loss: 0.7015663087368011, acc.: 37.16%] [G loss: 0.6928175687789917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 77/86 [D loss: 0.6979634761810303, acc.: 42.92%] [G loss: 0.6968333721160889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 78/86 [D loss: 0.6942036747932434, acc.: 49.71%] [G loss: 0.687152087688446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 79/86 [D loss: 0.7001394033432007, acc.: 45.07%] [G loss: 0.6844518780708313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 80/86 [D loss: 0.69992396235466, acc.: 43.31%] [G loss: 0.7018817663192749]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 81/86 [D loss: 0.692771703004837, acc.: 51.46%] [G loss: 0.6951450109481812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 82/86 [D loss: 0.6986996531486511, acc.: 42.19%] [G loss: 0.6946092844009399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 83/86 [D loss: 0.6976475417613983, acc.: 44.87%] [G loss: 0.6954997181892395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 84/86 [D loss: 0.6983532607555389, acc.: 44.68%] [G loss: 0.6885970234870911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 85/86 [D loss: 0.6997115612030029, acc.: 45.12%] [G loss: 0.6944800019264221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 86/86 [D loss: 0.6964044868946075, acc.: 46.00%] [G loss: 0.6990880370140076]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 1/86 [D loss: 0.6966104805469513, acc.: 45.75%] [G loss: 0.6982591152191162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 2/86 [D loss: 0.6967937648296356, acc.: 46.00%] [G loss: 0.698316216468811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 3/86 [D loss: 0.6962481439113617, acc.: 47.71%] [G loss: 0.696282148361206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 4/86 [D loss: 0.6970473229885101, acc.: 45.75%] [G loss: 0.6968477964401245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 5/86 [D loss: 0.6958483755588531, acc.: 47.12%] [G loss: 0.7010611295700073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 6/86 [D loss: 0.6958141922950745, acc.: 46.68%] [G loss: 0.7012923955917358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 7/86 [D loss: 0.6962161064147949, acc.: 47.27%] [G loss: 0.6996541619300842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 8/86 [D loss: 0.6964940130710602, acc.: 45.21%] [G loss: 0.7010789513587952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 9/86 [D loss: 0.6969953775405884, acc.: 45.90%] [G loss: 0.7004551291465759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 10/86 [D loss: 0.6953958570957184, acc.: 48.63%] [G loss: 0.7027242183685303]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 11/86 [D loss: 0.6955358684062958, acc.: 46.78%] [G loss: 0.7034845948219299]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 12/86 [D loss: 0.6972518265247345, acc.: 46.04%] [G loss: 0.7025850415229797]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 13/86 [D loss: 0.6959955096244812, acc.: 45.75%] [G loss: 0.7021288871765137]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 14/86 [D loss: 0.6954391002655029, acc.: 48.39%] [G loss: 0.7033683657646179]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 15/86 [D loss: 0.6962916254997253, acc.: 46.78%] [G loss: 0.7013154029846191]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 16/86 [D loss: 0.6973433196544647, acc.: 45.31%] [G loss: 0.7018439173698425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 17/86 [D loss: 0.695334792137146, acc.: 47.61%] [G loss: 0.701543927192688]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 18/86 [D loss: 0.6948380768299103, acc.: 48.00%] [G loss: 0.7020034790039062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 19/86 [D loss: 0.6944863200187683, acc.: 49.12%] [G loss: 0.7006407976150513]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 20/86 [D loss: 0.6961721181869507, acc.: 45.90%] [G loss: 0.7010411024093628]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 21/86 [D loss: 0.6948275566101074, acc.: 48.44%] [G loss: 0.7016779780387878]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 22/86 [D loss: 0.6954656541347504, acc.: 46.53%] [G loss: 0.7001780271530151]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 23/86 [D loss: 0.6963871419429779, acc.: 46.09%] [G loss: 0.7015417218208313]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 24/86 [D loss: 0.6963981986045837, acc.: 45.85%] [G loss: 0.7018437385559082]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 25/86 [D loss: 0.6954491436481476, acc.: 47.22%] [G loss: 0.7008469700813293]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 26/86 [D loss: 0.6945125758647919, acc.: 48.54%] [G loss: 0.7015813589096069]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 27/86 [D loss: 0.6952149271965027, acc.: 48.73%] [G loss: 0.6998581886291504]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 28/86 [D loss: 0.6957785487174988, acc.: 46.73%] [G loss: 0.6997617483139038]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 29/86 [D loss: 0.6958727240562439, acc.: 46.44%] [G loss: 0.6975365877151489]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 30/86 [D loss: 0.6965653002262115, acc.: 48.00%] [G loss: 0.7006405591964722]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 31/86 [D loss: 0.6951683163642883, acc.: 47.41%] [G loss: 0.7003558874130249]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 32/86 [D loss: 0.6953316628932953, acc.: 47.41%] [G loss: 0.7009256482124329]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 33/86 [D loss: 0.6952103674411774, acc.: 47.90%] [G loss: 0.696712076663971]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 34/86 [D loss: 0.6960592567920685, acc.: 46.68%] [G loss: 0.6907753348350525]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 35/86 [D loss: 0.6996257603168488, acc.: 44.09%] [G loss: 0.7013199925422668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 36/86 [D loss: 0.6948646008968353, acc.: 48.00%] [G loss: 0.6984854936599731]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 37/86 [D loss: 0.6987859308719635, acc.: 42.33%] [G loss: 0.6961479187011719]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 38/86 [D loss: 0.6957433521747589, acc.: 46.83%] [G loss: 0.6964111924171448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 39/86 [D loss: 0.6963837146759033, acc.: 46.29%] [G loss: 0.6836631298065186]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 40/86 [D loss: 0.7032508254051208, acc.: 41.16%] [G loss: 0.6863526105880737]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 41/86 [D loss: 0.6952100992202759, acc.: 47.75%] [G loss: 0.7045720219612122]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 42/86 [D loss: 0.6960318088531494, acc.: 45.95%] [G loss: 0.6872382164001465]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 43/86 [D loss: 0.7031185626983643, acc.: 34.67%] [G loss: 0.6946519017219543]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 44/86 [D loss: 0.6924188435077667, acc.: 51.86%] [G loss: 0.6954249739646912]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 45/86 [D loss: 0.6947767734527588, acc.: 48.54%] [G loss: 0.6721732020378113]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 26/200, Batch 46/86 [D loss: 0.7087530195713043, acc.: 40.33%] [G loss: 0.6804851293563843]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 47/86 [D loss: 0.6953956186771393, acc.: 48.39%] [G loss: 0.7045929431915283]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 48/86 [D loss: 0.6960964500904083, acc.: 45.85%] [G loss: 0.6879990100860596]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 49/86 [D loss: 0.7026335895061493, acc.: 36.08%] [G loss: 0.691266655921936]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 50/86 [D loss: 0.6980018615722656, acc.: 44.43%] [G loss: 0.6928485035896301]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 51/86 [D loss: 0.6949902474880219, acc.: 49.02%] [G loss: 0.6872032284736633]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 52/86 [D loss: 0.6986560225486755, acc.: 44.58%] [G loss: 0.684840202331543]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 53/86 [D loss: 0.6986281275749207, acc.: 44.48%] [G loss: 0.6936206817626953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 54/86 [D loss: 0.6956336200237274, acc.: 46.78%] [G loss: 0.6975637078285217]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 55/86 [D loss: 0.6962794363498688, acc.: 45.70%] [G loss: 0.697420597076416]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 56/86 [D loss: 0.6960678398609161, acc.: 46.78%] [G loss: 0.6981840133666992]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 57/86 [D loss: 0.6952923834323883, acc.: 48.24%] [G loss: 0.6970638036727905]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 58/86 [D loss: 0.6959179043769836, acc.: 46.29%] [G loss: 0.6969046592712402]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 59/86 [D loss: 0.6952817738056183, acc.: 48.49%] [G loss: 0.6988493204116821]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 60/86 [D loss: 0.6964620351791382, acc.: 45.12%] [G loss: 0.7016099691390991]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 61/86 [D loss: 0.6965069472789764, acc.: 46.29%] [G loss: 0.7015904188156128]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 62/86 [D loss: 0.6952422857284546, acc.: 47.56%] [G loss: 0.7025360465049744]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 63/86 [D loss: 0.696072906255722, acc.: 45.95%] [G loss: 0.7023805379867554]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 64/86 [D loss: 0.6964339911937714, acc.: 46.88%] [G loss: 0.7034502029418945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 65/86 [D loss: 0.695784866809845, acc.: 46.97%] [G loss: 0.7027220726013184]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 66/86 [D loss: 0.6959308981895447, acc.: 46.88%] [G loss: 0.702048659324646]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 67/86 [D loss: 0.6943228840827942, acc.: 49.56%] [G loss: 0.705691397190094]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 68/86 [D loss: 0.6947331428527832, acc.: 47.17%] [G loss: 0.7039818167686462]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 69/86 [D loss: 0.6953641772270203, acc.: 49.51%] [G loss: 0.7053572535514832]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 70/86 [D loss: 0.6945008337497711, acc.: 49.51%] [G loss: 0.7046024203300476]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 71/86 [D loss: 0.6955165863037109, acc.: 47.12%] [G loss: 0.7054203748703003]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 72/86 [D loss: 0.6951129734516144, acc.: 48.58%] [G loss: 0.7043043375015259]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 73/86 [D loss: 0.6954007744789124, acc.: 47.71%] [G loss: 0.7034814357757568]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 74/86 [D loss: 0.6949154436588287, acc.: 49.22%] [G loss: 0.7008004188537598]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 75/86 [D loss: 0.6945516467094421, acc.: 48.54%] [G loss: 0.7006632089614868]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 76/86 [D loss: 0.6966496109962463, acc.: 45.51%] [G loss: 0.7024394273757935]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 77/86 [D loss: 0.6945782005786896, acc.: 49.17%] [G loss: 0.7048189043998718]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 78/86 [D loss: 0.6942765712738037, acc.: 49.32%] [G loss: 0.7016411423683167]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 79/86 [D loss: 0.6963402628898621, acc.: 45.85%] [G loss: 0.7018017172813416]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 80/86 [D loss: 0.6956557929515839, acc.: 47.80%] [G loss: 0.7003462314605713]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 81/86 [D loss: 0.6959773004055023, acc.: 46.44%] [G loss: 0.6974034905433655]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 82/86 [D loss: 0.6956126689910889, acc.: 47.46%] [G loss: 0.7015163898468018]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 83/86 [D loss: 0.6937501430511475, acc.: 50.93%] [G loss: 0.7012409567832947]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 84/86 [D loss: 0.6954746544361115, acc.: 46.44%] [G loss: 0.6993719339370728]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 85/86 [D loss: 0.6960123479366302, acc.: 47.07%] [G loss: 0.6985912322998047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 86/86 [D loss: 0.6955534815788269, acc.: 47.75%] [G loss: 0.694773256778717]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 1/86 [D loss: 0.6986742317676544, acc.: 44.78%] [G loss: 0.69635009765625]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 2/86 [D loss: 0.6947857737541199, acc.: 48.29%] [G loss: 0.7024523019790649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 3/86 [D loss: 0.694507509469986, acc.: 47.75%] [G loss: 0.6980119347572327]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 4/86 [D loss: 0.6964819133281708, acc.: 47.71%] [G loss: 0.6975420117378235]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 5/86 [D loss: 0.6952225267887115, acc.: 47.07%] [G loss: 0.6956788301467896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 6/86 [D loss: 0.6957151889801025, acc.: 48.05%] [G loss: 0.6820145845413208]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 7/86 [D loss: 0.7028382122516632, acc.: 42.72%] [G loss: 0.6983547210693359]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 8/86 [D loss: 0.6908928453922272, acc.: 54.83%] [G loss: 0.7049123048782349]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 9/86 [D loss: 0.6993890106678009, acc.: 42.33%] [G loss: 0.6895612478256226]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 10/86 [D loss: 0.7005750238895416, acc.: 37.21%] [G loss: 0.6968330144882202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 11/86 [D loss: 0.6919856071472168, acc.: 52.15%] [G loss: 0.6957833766937256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 12/86 [D loss: 0.6939291656017303, acc.: 50.88%] [G loss: 0.6693863272666931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 13/86 [D loss: 0.7107594609260559, acc.: 39.31%] [G loss: 0.6818439960479736]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 14/86 [D loss: 0.6928234398365021, acc.: 52.00%] [G loss: 0.7095518708229065]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 15/86 [D loss: 0.6924936175346375, acc.: 52.29%] [G loss: 0.686386227607727]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 16/86 [D loss: 0.7041256427764893, acc.: 32.32%] [G loss: 0.6896501183509827]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 17/86 [D loss: 0.6980819404125214, acc.: 43.41%] [G loss: 0.6960803866386414]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 18/86 [D loss: 0.6942947208881378, acc.: 49.76%] [G loss: 0.6894391179084778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 19/86 [D loss: 0.6966035068035126, acc.: 47.27%] [G loss: 0.679321825504303]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 20/86 [D loss: 0.7015206515789032, acc.: 43.26%] [G loss: 0.6898789405822754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 21/86 [D loss: 0.6947458982467651, acc.: 49.76%] [G loss: 0.6955997347831726]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 22/86 [D loss: 0.6966538727283478, acc.: 45.75%] [G loss: 0.6963807940483093]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 23/86 [D loss: 0.6973840594291687, acc.: 44.63%] [G loss: 0.6959108710289001]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 24/86 [D loss: 0.6963604688644409, acc.: 48.14%] [G loss: 0.6967426538467407]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 25/86 [D loss: 0.6956015229225159, acc.: 47.31%] [G loss: 0.6962257027626038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 26/86 [D loss: 0.6970283389091492, acc.: 44.14%] [G loss: 0.696570873260498]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 27/86 [D loss: 0.6952924132347107, acc.: 46.58%] [G loss: 0.7003797888755798]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 28/86 [D loss: 0.6946116089820862, acc.: 48.14%] [G loss: 0.701557457447052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 29/86 [D loss: 0.6959445476531982, acc.: 46.88%] [G loss: 0.7010486721992493]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 30/86 [D loss: 0.6947658956050873, acc.: 48.97%] [G loss: 0.7016770839691162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 31/86 [D loss: 0.6957828998565674, acc.: 47.17%] [G loss: 0.7016025185585022]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 32/86 [D loss: 0.6958490312099457, acc.: 46.04%] [G loss: 0.6999698877334595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 33/86 [D loss: 0.6957425177097321, acc.: 46.83%] [G loss: 0.7023523449897766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 34/86 [D loss: 0.6950750946998596, acc.: 48.00%] [G loss: 0.7053967714309692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 35/86 [D loss: 0.6944926977157593, acc.: 48.05%] [G loss: 0.7050222158432007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 36/86 [D loss: 0.6951448917388916, acc.: 47.46%] [G loss: 0.7034748792648315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 37/86 [D loss: 0.6950569450855255, acc.: 47.66%] [G loss: 0.7026185393333435]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 38/86 [D loss: 0.6940697431564331, acc.: 49.22%] [G loss: 0.704737663269043]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 39/86 [D loss: 0.6946998536586761, acc.: 48.24%] [G loss: 0.7037898302078247]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 40/86 [D loss: 0.6947504580020905, acc.: 48.97%] [G loss: 0.7050603628158569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 41/86 [D loss: 0.6940940320491791, acc.: 48.68%] [G loss: 0.7037891149520874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 42/86 [D loss: 0.6943142414093018, acc.: 49.61%] [G loss: 0.7042503952980042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 43/86 [D loss: 0.6954315006732941, acc.: 48.10%] [G loss: 0.7030589580535889]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 44/86 [D loss: 0.694345086812973, acc.: 50.63%] [G loss: 0.7032802104949951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 45/86 [D loss: 0.6951135694980621, acc.: 47.41%] [G loss: 0.7018246054649353]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 46/86 [D loss: 0.6950638592243195, acc.: 47.61%] [G loss: 0.7027807235717773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 47/86 [D loss: 0.6954405903816223, acc.: 46.24%] [G loss: 0.7027515769004822]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 48/86 [D loss: 0.6945931017398834, acc.: 48.39%] [G loss: 0.7025785446166992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 49/86 [D loss: 0.6953462064266205, acc.: 47.12%] [G loss: 0.7011337280273438]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 50/86 [D loss: 0.6952562630176544, acc.: 47.61%] [G loss: 0.6984180212020874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 51/86 [D loss: 0.6944847106933594, acc.: 47.85%] [G loss: 0.6999943256378174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 52/86 [D loss: 0.6961510479450226, acc.: 46.78%] [G loss: 0.701149582862854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 53/86 [D loss: 0.6958278119564056, acc.: 47.02%] [G loss: 0.70048987865448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 54/86 [D loss: 0.6949244439601898, acc.: 48.19%] [G loss: 0.6998905539512634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 55/86 [D loss: 0.6956517398357391, acc.: 46.29%] [G loss: 0.6987406611442566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 56/86 [D loss: 0.6942503750324249, acc.: 49.17%] [G loss: 0.6973440647125244]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 57/86 [D loss: 0.6953616738319397, acc.: 47.46%] [G loss: 0.6981369256973267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 58/86 [D loss: 0.6948347389698029, acc.: 48.78%] [G loss: 0.7002201080322266]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 59/86 [D loss: 0.6942978501319885, acc.: 50.00%] [G loss: 0.6985660195350647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 60/86 [D loss: 0.6956481337547302, acc.: 47.27%] [G loss: 0.6969588398933411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 61/86 [D loss: 0.695676863193512, acc.: 47.07%] [G loss: 0.6973492503166199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 62/86 [D loss: 0.6956621706485748, acc.: 46.58%] [G loss: 0.6967887282371521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 63/86 [D loss: 0.6964133381843567, acc.: 46.24%] [G loss: 0.6989624500274658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 64/86 [D loss: 0.6945449113845825, acc.: 49.56%] [G loss: 0.7018036246299744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 65/86 [D loss: 0.6955862939357758, acc.: 47.90%] [G loss: 0.6981043815612793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 66/86 [D loss: 0.6951446831226349, acc.: 47.46%] [G loss: 0.6970858573913574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 67/86 [D loss: 0.6951178908348083, acc.: 47.22%] [G loss: 0.6942965984344482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 68/86 [D loss: 0.6964178383350372, acc.: 46.19%] [G loss: 0.6893925666809082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 69/86 [D loss: 0.6991873681545258, acc.: 44.43%] [G loss: 0.6979517936706543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 70/86 [D loss: 0.6924904882907867, acc.: 51.27%] [G loss: 0.700855016708374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 71/86 [D loss: 0.6965716183185577, acc.: 44.82%] [G loss: 0.6934348940849304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 72/86 [D loss: 0.6989979147911072, acc.: 40.82%] [G loss: 0.6958494782447815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 73/86 [D loss: 0.6939603686332703, acc.: 49.41%] [G loss: 0.6949548125267029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 74/86 [D loss: 0.6947961747646332, acc.: 51.37%] [G loss: 0.6776719689369202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 75/86 [D loss: 0.7041656076908112, acc.: 42.48%] [G loss: 0.6873303651809692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 76/86 [D loss: 0.692011684179306, acc.: 52.88%] [G loss: 0.7104114294052124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 77/86 [D loss: 0.6925645172595978, acc.: 51.81%] [G loss: 0.6870505809783936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 78/86 [D loss: 0.7032140791416168, acc.: 33.69%] [G loss: 0.690142810344696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 79/86 [D loss: 0.6952365338802338, acc.: 45.85%] [G loss: 0.6948606967926025]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 80/86 [D loss: 0.692767858505249, acc.: 52.15%] [G loss: 0.6860306262969971]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 81/86 [D loss: 0.6995278894901276, acc.: 45.51%] [G loss: 0.6752125024795532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 82/86 [D loss: 0.7025258243083954, acc.: 43.31%] [G loss: 0.6973413825035095]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 83/86 [D loss: 0.690838634967804, acc.: 54.64%] [G loss: 0.6995323300361633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 84/86 [D loss: 0.6986860930919647, acc.: 41.46%] [G loss: 0.6923018097877502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 85/86 [D loss: 0.6976402103900909, acc.: 44.24%] [G loss: 0.6940778493881226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 86/86 [D loss: 0.6957590579986572, acc.: 46.73%] [G loss: 0.694679856300354]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 28/200, Batch 1/86 [D loss: 0.6950562000274658, acc.: 47.61%] [G loss: 0.691918134689331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 2/86 [D loss: 0.6969692707061768, acc.: 44.53%] [G loss: 0.689094066619873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 3/86 [D loss: 0.6980930268764496, acc.: 44.82%] [G loss: 0.6949307918548584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 4/86 [D loss: 0.6953066885471344, acc.: 47.85%] [G loss: 0.6997254490852356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 5/86 [D loss: 0.6948711574077606, acc.: 48.24%] [G loss: 0.7009913921356201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 6/86 [D loss: 0.6968309879302979, acc.: 45.90%] [G loss: 0.7002205848693848]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 7/86 [D loss: 0.696203887462616, acc.: 46.09%] [G loss: 0.6975361704826355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 8/86 [D loss: 0.6951895654201508, acc.: 47.02%] [G loss: 0.6971450448036194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 9/86 [D loss: 0.6959876418113708, acc.: 45.95%] [G loss: 0.6974308490753174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 10/86 [D loss: 0.6946049332618713, acc.: 48.88%] [G loss: 0.7012007236480713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 11/86 [D loss: 0.6942649781703949, acc.: 49.32%] [G loss: 0.7026239037513733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 12/86 [D loss: 0.6950746178627014, acc.: 47.61%] [G loss: 0.7020970582962036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 13/86 [D loss: 0.6956081390380859, acc.: 45.90%] [G loss: 0.7018992900848389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 14/86 [D loss: 0.6947071254253387, acc.: 48.34%] [G loss: 0.7026015520095825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 15/86 [D loss: 0.6949321329593658, acc.: 46.78%] [G loss: 0.7006244659423828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 16/86 [D loss: 0.6945267915725708, acc.: 48.83%] [G loss: 0.7041405439376831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 17/86 [D loss: 0.6945136785507202, acc.: 48.00%] [G loss: 0.702812671661377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 18/86 [D loss: 0.6951365768909454, acc.: 47.61%] [G loss: 0.7031334638595581]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 19/86 [D loss: 0.695805013179779, acc.: 46.09%] [G loss: 0.7034968733787537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 20/86 [D loss: 0.6949409246444702, acc.: 48.29%] [G loss: 0.7036305665969849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 21/86 [D loss: 0.6949899196624756, acc.: 47.95%] [G loss: 0.7025889754295349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 22/86 [D loss: 0.6945953965187073, acc.: 47.12%] [G loss: 0.7008230686187744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 23/86 [D loss: 0.6945600211620331, acc.: 49.12%] [G loss: 0.7025820016860962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 24/86 [D loss: 0.6938124299049377, acc.: 49.61%] [G loss: 0.7028693556785583]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 25/86 [D loss: 0.6945314407348633, acc.: 47.71%] [G loss: 0.703700840473175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 26/86 [D loss: 0.6948548257350922, acc.: 47.95%] [G loss: 0.7029980421066284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 27/86 [D loss: 0.6943720579147339, acc.: 47.90%] [G loss: 0.7010631561279297]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 28/86 [D loss: 0.6943704187870026, acc.: 48.73%] [G loss: 0.7018026113510132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 29/86 [D loss: 0.6951701045036316, acc.: 47.61%] [G loss: 0.700312077999115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 30/86 [D loss: 0.6953850388526917, acc.: 47.56%] [G loss: 0.6990532875061035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 31/86 [D loss: 0.6941713392734528, acc.: 48.68%] [G loss: 0.7012959122657776]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 32/86 [D loss: 0.6941406726837158, acc.: 49.76%] [G loss: 0.7011702060699463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 33/86 [D loss: 0.6962122619152069, acc.: 45.65%] [G loss: 0.6999821662902832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 34/86 [D loss: 0.6955433487892151, acc.: 47.41%] [G loss: 0.6981428265571594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 35/86 [D loss: 0.6955229043960571, acc.: 46.48%] [G loss: 0.696959912776947]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 36/86 [D loss: 0.6955201923847198, acc.: 48.93%] [G loss: 0.7000384330749512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 37/86 [D loss: 0.6947342455387115, acc.: 48.54%] [G loss: 0.7021269202232361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 38/86 [D loss: 0.6948484778404236, acc.: 47.56%] [G loss: 0.6982322335243225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 39/86 [D loss: 0.6949003040790558, acc.: 48.05%] [G loss: 0.6986626386642456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 40/86 [D loss: 0.6947931051254272, acc.: 48.44%] [G loss: 0.6959531307220459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 41/86 [D loss: 0.6957080066204071, acc.: 46.39%] [G loss: 0.6881337761878967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 42/86 [D loss: 0.6983374953269958, acc.: 44.92%] [G loss: 0.6954584121704102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 43/86 [D loss: 0.6926307380199432, acc.: 51.22%] [G loss: 0.7038554549217224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 44/86 [D loss: 0.6953570544719696, acc.: 46.48%] [G loss: 0.693266749382019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 45/86 [D loss: 0.6983650624752045, acc.: 41.55%] [G loss: 0.6950991153717041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 46/86 [D loss: 0.6938466727733612, acc.: 48.78%] [G loss: 0.6964374780654907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 47/86 [D loss: 0.6938064992427826, acc.: 49.80%] [G loss: 0.6830757260322571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 48/86 [D loss: 0.7004714608192444, acc.: 43.80%] [G loss: 0.6811382174491882]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 49/86 [D loss: 0.6997350454330444, acc.: 44.43%] [G loss: 0.7057338356971741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 50/86 [D loss: 0.689513087272644, acc.: 54.88%] [G loss: 0.6941521167755127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 51/86 [D loss: 0.7008683085441589, acc.: 37.55%] [G loss: 0.6905124187469482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 52/86 [D loss: 0.6980287432670593, acc.: 41.50%] [G loss: 0.6937339305877686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 53/86 [D loss: 0.692705363035202, acc.: 50.83%] [G loss: 0.6905829906463623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 54/86 [D loss: 0.6952710449695587, acc.: 49.12%] [G loss: 0.6812175512313843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 55/86 [D loss: 0.700659304857254, acc.: 42.63%] [G loss: 0.6878793835639954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 56/86 [D loss: 0.6952491104602814, acc.: 48.88%] [G loss: 0.7000082731246948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 57/86 [D loss: 0.6931668519973755, acc.: 50.10%] [G loss: 0.6947358250617981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 58/86 [D loss: 0.6968187987804413, acc.: 45.51%] [G loss: 0.6951736807823181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 59/86 [D loss: 0.6958300769329071, acc.: 45.90%] [G loss: 0.6949227452278137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 60/86 [D loss: 0.6941079497337341, acc.: 49.56%] [G loss: 0.6930228471755981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 61/86 [D loss: 0.6953620314598083, acc.: 47.12%] [G loss: 0.6898231506347656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 62/86 [D loss: 0.6966309547424316, acc.: 46.53%] [G loss: 0.6948795914649963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 63/86 [D loss: 0.6944172978401184, acc.: 49.32%] [G loss: 0.699496865272522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 64/86 [D loss: 0.6938584446907043, acc.: 49.51%] [G loss: 0.6993663907051086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 65/86 [D loss: 0.6956528127193451, acc.: 46.34%] [G loss: 0.6987453103065491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 66/86 [D loss: 0.6948525607585907, acc.: 47.46%] [G loss: 0.69972825050354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 67/86 [D loss: 0.6943165063858032, acc.: 49.02%] [G loss: 0.6978726983070374]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 68/86 [D loss: 0.6952299475669861, acc.: 47.56%] [G loss: 0.6976451873779297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 69/86 [D loss: 0.6960141360759735, acc.: 45.70%] [G loss: 0.7008146047592163]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 70/86 [D loss: 0.6942769587039948, acc.: 48.44%] [G loss: 0.7016507983207703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 71/86 [D loss: 0.695372462272644, acc.: 47.66%] [G loss: 0.702485978603363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 72/86 [D loss: 0.6944054067134857, acc.: 48.54%] [G loss: 0.7018531560897827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 73/86 [D loss: 0.6943264603614807, acc.: 48.24%] [G loss: 0.7024719715118408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 74/86 [D loss: 0.6952691376209259, acc.: 46.53%] [G loss: 0.7014147639274597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 75/86 [D loss: 0.695776492357254, acc.: 46.78%] [G loss: 0.703119158744812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 76/86 [D loss: 0.6945559680461884, acc.: 48.34%] [G loss: 0.7045106291770935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 77/86 [D loss: 0.6949592530727386, acc.: 49.12%] [G loss: 0.7028236389160156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 78/86 [D loss: 0.6950839757919312, acc.: 47.31%] [G loss: 0.7022474408149719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 79/86 [D loss: 0.6942052841186523, acc.: 48.44%] [G loss: 0.703201949596405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 80/86 [D loss: 0.694422572851181, acc.: 48.49%] [G loss: 0.7011651396751404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 81/86 [D loss: 0.6941336691379547, acc.: 48.29%] [G loss: 0.7028085589408875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 82/86 [D loss: 0.6942248642444611, acc.: 48.54%] [G loss: 0.7025317549705505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 83/86 [D loss: 0.6934083700180054, acc.: 50.68%] [G loss: 0.7025330662727356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 84/86 [D loss: 0.6942793726921082, acc.: 48.34%] [G loss: 0.7009688019752502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 85/86 [D loss: 0.69539475440979, acc.: 47.12%] [G loss: 0.7005301713943481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 86/86 [D loss: 0.6944643259048462, acc.: 48.83%] [G loss: 0.6992615461349487]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 1/86 [D loss: 0.6952280700206757, acc.: 46.53%] [G loss: 0.6990076303482056]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 2/86 [D loss: 0.6950375437736511, acc.: 48.00%] [G loss: 0.7004597783088684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 3/86 [D loss: 0.6940864324569702, acc.: 50.24%] [G loss: 0.7000700235366821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 4/86 [D loss: 0.694656103849411, acc.: 48.44%] [G loss: 0.700416088104248]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 5/86 [D loss: 0.6942644417285919, acc.: 48.39%] [G loss: 0.7002646923065186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 6/86 [D loss: 0.6951006352901459, acc.: 48.24%] [G loss: 0.7002872824668884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 7/86 [D loss: 0.694812685251236, acc.: 47.46%] [G loss: 0.6972306966781616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 8/86 [D loss: 0.6962267160415649, acc.: 46.29%] [G loss: 0.6976209282875061]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 9/86 [D loss: 0.6943129897117615, acc.: 49.66%] [G loss: 0.7015225887298584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 10/86 [D loss: 0.6944881677627563, acc.: 48.83%] [G loss: 0.6987991333007812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 11/86 [D loss: 0.6942308843135834, acc.: 48.54%] [G loss: 0.6983801126480103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 12/86 [D loss: 0.6958998143672943, acc.: 46.19%] [G loss: 0.6976607441902161]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 13/86 [D loss: 0.6944663822650909, acc.: 49.85%] [G loss: 0.6920794248580933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 14/86 [D loss: 0.6982478499412537, acc.: 45.02%] [G loss: 0.6949468851089478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 15/86 [D loss: 0.6945250630378723, acc.: 48.44%] [G loss: 0.7042933702468872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 16/86 [D loss: 0.6946657299995422, acc.: 48.00%] [G loss: 0.6953367590904236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 17/86 [D loss: 0.6971612870693207, acc.: 42.53%] [G loss: 0.6957266926765442]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 18/86 [D loss: 0.6944575309753418, acc.: 47.56%] [G loss: 0.6970873475074768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 19/86 [D loss: 0.693355917930603, acc.: 50.54%] [G loss: 0.6914657354354858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 20/86 [D loss: 0.6976804733276367, acc.: 45.80%] [G loss: 0.6852883100509644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 21/86 [D loss: 0.6994275450706482, acc.: 44.78%] [G loss: 0.7003962993621826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 22/86 [D loss: 0.6914771497249603, acc.: 54.00%] [G loss: 0.6980257034301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 23/86 [D loss: 0.6972713470458984, acc.: 44.29%] [G loss: 0.6939203143119812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 24/86 [D loss: 0.6974546611309052, acc.: 43.90%] [G loss: 0.6947694420814514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 25/86 [D loss: 0.6937887668609619, acc.: 49.85%] [G loss: 0.6943110227584839]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 26/86 [D loss: 0.6943513453006744, acc.: 49.17%] [G loss: 0.6850587725639343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 27/86 [D loss: 0.6994153559207916, acc.: 45.17%] [G loss: 0.6873201131820679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 28/86 [D loss: 0.6961512565612793, acc.: 48.39%] [G loss: 0.7014778256416321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 29/86 [D loss: 0.6928602755069733, acc.: 50.49%] [G loss: 0.697598397731781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 30/86 [D loss: 0.6971757709980011, acc.: 43.51%] [G loss: 0.6955410242080688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 31/86 [D loss: 0.6963648796081543, acc.: 46.39%] [G loss: 0.6974274516105652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 32/86 [D loss: 0.6942387819290161, acc.: 48.68%] [G loss: 0.693584680557251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 33/86 [D loss: 0.6951903700828552, acc.: 47.85%] [G loss: 0.6922827959060669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 34/86 [D loss: 0.6962064206600189, acc.: 47.71%] [G loss: 0.6944403648376465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 35/86 [D loss: 0.6951333284378052, acc.: 49.46%] [G loss: 0.6996946334838867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 36/86 [D loss: 0.6948113739490509, acc.: 48.93%] [G loss: 0.6998974084854126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 37/86 [D loss: 0.6948978900909424, acc.: 46.73%] [G loss: 0.6998974680900574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 38/86 [D loss: 0.6952100694179535, acc.: 47.75%] [G loss: 0.6992362141609192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 39/86 [D loss: 0.6941401958465576, acc.: 50.20%] [G loss: 0.6985952854156494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 40/86 [D loss: 0.6949212849140167, acc.: 47.66%] [G loss: 0.6970781087875366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 41/86 [D loss: 0.6963168978691101, acc.: 44.63%] [G loss: 0.6993973851203918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 42/86 [D loss: 0.6932951807975769, acc.: 50.20%] [G loss: 0.7042235732078552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 43/86 [D loss: 0.6950626373291016, acc.: 47.85%] [G loss: 0.7041739225387573]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 44/86 [D loss: 0.6956590712070465, acc.: 47.36%] [G loss: 0.702567994594574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 45/86 [D loss: 0.6936502158641815, acc.: 50.00%] [G loss: 0.6998215913772583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 46/86 [D loss: 0.6946333050727844, acc.: 48.24%] [G loss: 0.7014929056167603]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 47/86 [D loss: 0.6949898600578308, acc.: 49.12%] [G loss: 0.7015832662582397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 48/86 [D loss: 0.694032609462738, acc.: 48.97%] [G loss: 0.7016385793685913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 49/86 [D loss: 0.6939947307109833, acc.: 48.78%] [G loss: 0.7036636471748352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 50/86 [D loss: 0.6946468949317932, acc.: 48.58%] [G loss: 0.7026010155677795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 51/86 [D loss: 0.6940330564975739, acc.: 50.63%] [G loss: 0.700905978679657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 52/86 [D loss: 0.6944661140441895, acc.: 50.88%] [G loss: 0.6986091136932373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 53/86 [D loss: 0.6957961916923523, acc.: 45.75%] [G loss: 0.701561689376831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 54/86 [D loss: 0.6946524083614349, acc.: 49.32%] [G loss: 0.7029558420181274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 55/86 [D loss: 0.6939854621887207, acc.: 49.02%] [G loss: 0.7018017768859863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 56/86 [D loss: 0.6951037347316742, acc.: 48.29%] [G loss: 0.700488269329071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 57/86 [D loss: 0.6947152614593506, acc.: 47.71%] [G loss: 0.6999067068099976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 58/86 [D loss: 0.6950558423995972, acc.: 46.68%] [G loss: 0.6999845504760742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 59/86 [D loss: 0.6952304840087891, acc.: 47.31%] [G loss: 0.69901442527771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 60/86 [D loss: 0.69482421875, acc.: 49.46%] [G loss: 0.7001221776008606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 61/86 [D loss: 0.6949052512645721, acc.: 45.75%] [G loss: 0.7028706073760986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 62/86 [D loss: 0.6951231062412262, acc.: 48.58%] [G loss: 0.7006898522377014]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 63/86 [D loss: 0.6940928101539612, acc.: 50.15%] [G loss: 0.6996374726295471]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 64/86 [D loss: 0.6946663558483124, acc.: 47.80%] [G loss: 0.7002019882202148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 65/86 [D loss: 0.6955846846103668, acc.: 46.44%] [G loss: 0.6980069875717163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 66/86 [D loss: 0.6937333941459656, acc.: 48.97%] [G loss: 0.6973546147346497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 67/86 [D loss: 0.6949475705623627, acc.: 47.31%] [G loss: 0.698523998260498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 68/86 [D loss: 0.6943167150020599, acc.: 48.78%] [G loss: 0.7013416290283203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 69/86 [D loss: 0.6942608952522278, acc.: 49.27%] [G loss: 0.6989863514900208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 70/86 [D loss: 0.694033682346344, acc.: 49.90%] [G loss: 0.6983755826950073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 71/86 [D loss: 0.6952024698257446, acc.: 46.39%] [G loss: 0.6973965167999268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 72/86 [D loss: 0.6938303411006927, acc.: 49.95%] [G loss: 0.6949933767318726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 73/86 [D loss: 0.6942616403102875, acc.: 50.59%] [G loss: 0.69767826795578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 74/86 [D loss: 0.6942269504070282, acc.: 49.17%] [G loss: 0.6993740200996399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 75/86 [D loss: 0.6952853202819824, acc.: 46.53%] [G loss: 0.6975539922714233]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 76/86 [D loss: 0.6952822804450989, acc.: 47.17%] [G loss: 0.698214590549469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 77/86 [D loss: 0.694545567035675, acc.: 47.90%] [G loss: 0.6947969198226929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 78/86 [D loss: 0.6950766444206238, acc.: 48.24%] [G loss: 0.6954441070556641]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 79/86 [D loss: 0.6950427889823914, acc.: 48.05%] [G loss: 0.6980265974998474]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 80/86 [D loss: 0.6935775578022003, acc.: 49.32%] [G loss: 0.6985546350479126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 81/86 [D loss: 0.6944478154182434, acc.: 48.63%] [G loss: 0.6984097957611084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 82/86 [D loss: 0.6947693228721619, acc.: 46.09%] [G loss: 0.696221113204956]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 83/86 [D loss: 0.6952112019062042, acc.: 46.73%] [G loss: 0.6952183246612549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 84/86 [D loss: 0.6949903666973114, acc.: 47.71%] [G loss: 0.6947422027587891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 85/86 [D loss: 0.6955604255199432, acc.: 46.58%] [G loss: 0.6993659138679504]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 86/86 [D loss: 0.6935863792896271, acc.: 49.76%] [G loss: 0.6981996893882751]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 1/86 [D loss: 0.6956599354743958, acc.: 45.90%] [G loss: 0.6990188956260681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 2/86 [D loss: 0.6940617263317108, acc.: 48.44%] [G loss: 0.697530210018158]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 3/86 [D loss: 0.6952333152294159, acc.: 47.66%] [G loss: 0.6979326009750366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 4/86 [D loss: 0.6943663954734802, acc.: 49.37%] [G loss: 0.6965105533599854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 5/86 [D loss: 0.696739137172699, acc.: 45.61%] [G loss: 0.6965909004211426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 6/86 [D loss: 0.6938580274581909, acc.: 49.85%] [G loss: 0.7002049088478088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 7/86 [D loss: 0.6949238181114197, acc.: 47.71%] [G loss: 0.6996431946754456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 8/86 [D loss: 0.6943500936031342, acc.: 48.14%] [G loss: 0.6983540654182434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 9/86 [D loss: 0.6938741505146027, acc.: 48.73%] [G loss: 0.6959748268127441]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 10/86 [D loss: 0.6954253017902374, acc.: 47.46%] [G loss: 0.6960282325744629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 11/86 [D loss: 0.6960688531398773, acc.: 45.51%] [G loss: 0.6974653005599976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 12/86 [D loss: 0.6941965222358704, acc.: 48.83%] [G loss: 0.701869785785675]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 13/86 [D loss: 0.6947633624076843, acc.: 48.19%] [G loss: 0.6991075873374939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 14/86 [D loss: 0.6948878765106201, acc.: 46.58%] [G loss: 0.6996748447418213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 15/86 [D loss: 0.6953220963478088, acc.: 47.36%] [G loss: 0.6983715295791626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 16/86 [D loss: 0.6946550607681274, acc.: 47.66%] [G loss: 0.6996148228645325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 17/86 [D loss: 0.6959919929504395, acc.: 45.80%] [G loss: 0.6963451504707336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 18/86 [D loss: 0.6953230798244476, acc.: 47.75%] [G loss: 0.7018357515335083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 19/86 [D loss: 0.6944906115531921, acc.: 48.97%] [G loss: 0.70119708776474]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 20/86 [D loss: 0.6948724985122681, acc.: 47.46%] [G loss: 0.699164867401123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 21/86 [D loss: 0.6945011913776398, acc.: 49.56%] [G loss: 0.6989853382110596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 22/86 [D loss: 0.6951664090156555, acc.: 47.36%] [G loss: 0.6975846290588379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 23/86 [D loss: 0.6952897906303406, acc.: 45.85%] [G loss: 0.6982481479644775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 24/86 [D loss: 0.6946362853050232, acc.: 48.63%] [G loss: 0.7003734111785889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 25/86 [D loss: 0.6948141157627106, acc.: 48.63%] [G loss: 0.6995498538017273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 26/86 [D loss: 0.6952303349971771, acc.: 47.51%] [G loss: 0.6994688510894775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 27/86 [D loss: 0.6948518753051758, acc.: 47.95%] [G loss: 0.6975346803665161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 28/86 [D loss: 0.6950793266296387, acc.: 48.14%] [G loss: 0.6946610808372498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 29/86 [D loss: 0.6965853869915009, acc.: 45.80%] [G loss: 0.6977956295013428]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 30/86 [D loss: 0.6942606568336487, acc.: 50.00%] [G loss: 0.7004319429397583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 31/86 [D loss: 0.6938583850860596, acc.: 48.83%] [G loss: 0.7004348635673523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 32/86 [D loss: 0.6953867673873901, acc.: 46.83%] [G loss: 0.6986506581306458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 33/86 [D loss: 0.6951954960823059, acc.: 46.53%] [G loss: 0.6978680491447449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 34/86 [D loss: 0.6947808265686035, acc.: 47.61%] [G loss: 0.695254921913147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 35/86 [D loss: 0.6951685547828674, acc.: 46.53%] [G loss: 0.6989983320236206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 36/86 [D loss: 0.6956725716590881, acc.: 47.51%] [G loss: 0.7009581327438354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 37/86 [D loss: 0.6952347159385681, acc.: 47.07%] [G loss: 0.7001621723175049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 38/86 [D loss: 0.695435643196106, acc.: 46.24%] [G loss: 0.6995362043380737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 39/86 [D loss: 0.6950761377811432, acc.: 46.63%] [G loss: 0.6977196335792542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 40/86 [D loss: 0.6955098211765289, acc.: 48.34%] [G loss: 0.6947944760322571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 41/86 [D loss: 0.6961935758590698, acc.: 45.95%] [G loss: 0.698652982711792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 42/86 [D loss: 0.6951633989810944, acc.: 47.75%] [G loss: 0.7002800703048706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 43/86 [D loss: 0.6951967477798462, acc.: 47.46%] [G loss: 0.7004575133323669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 44/86 [D loss: 0.6953163743019104, acc.: 46.63%] [G loss: 0.6997960805892944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 45/86 [D loss: 0.6953939199447632, acc.: 48.24%] [G loss: 0.6973713636398315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 46/86 [D loss: 0.6957646906375885, acc.: 47.07%] [G loss: 0.6940327286720276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 47/86 [D loss: 0.6964598298072815, acc.: 45.80%] [G loss: 0.6988425254821777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 48/86 [D loss: 0.6940021514892578, acc.: 49.37%] [G loss: 0.7012097239494324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 49/86 [D loss: 0.6952949464321136, acc.: 47.12%] [G loss: 0.6990365386009216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 50/86 [D loss: 0.6955519020557404, acc.: 46.92%] [G loss: 0.6995576620101929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 51/86 [D loss: 0.6942158639431, acc.: 49.22%] [G loss: 0.6981675624847412]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 52/86 [D loss: 0.6946631968021393, acc.: 48.49%] [G loss: 0.6942917108535767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 53/86 [D loss: 0.6968729197978973, acc.: 45.85%] [G loss: 0.6996708512306213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 54/86 [D loss: 0.6934637129306793, acc.: 50.20%] [G loss: 0.7016146183013916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 55/86 [D loss: 0.6939886808395386, acc.: 48.78%] [G loss: 0.7002366185188293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 56/86 [D loss: 0.6953610777854919, acc.: 46.48%] [G loss: 0.6988521814346313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 57/86 [D loss: 0.6948117911815643, acc.: 48.00%] [G loss: 0.698660135269165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 58/86 [D loss: 0.6960494518280029, acc.: 45.51%] [G loss: 0.6939892768859863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 59/86 [D loss: 0.696345865726471, acc.: 46.44%] [G loss: 0.6952176690101624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 60/86 [D loss: 0.6945363581180573, acc.: 50.05%] [G loss: 0.7019375562667847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 61/86 [D loss: 0.6946726739406586, acc.: 47.51%] [G loss: 0.7003881931304932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 62/86 [D loss: 0.6941110491752625, acc.: 48.93%] [G loss: 0.699253261089325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 63/86 [D loss: 0.6950597167015076, acc.: 47.17%] [G loss: 0.6979526877403259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 64/86 [D loss: 0.6951667070388794, acc.: 47.22%] [G loss: 0.6961618661880493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 65/86 [D loss: 0.6960252225399017, acc.: 46.04%] [G loss: 0.6954855918884277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 66/86 [D loss: 0.6953509747982025, acc.: 46.88%] [G loss: 0.7019181251525879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 67/86 [D loss: 0.6945557296276093, acc.: 48.39%] [G loss: 0.6990877389907837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 68/86 [D loss: 0.6956524550914764, acc.: 46.34%] [G loss: 0.6980291604995728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 69/86 [D loss: 0.6945606470108032, acc.: 49.22%] [G loss: 0.6984367966651917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 70/86 [D loss: 0.6946950852870941, acc.: 48.88%] [G loss: 0.6977518200874329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 71/86 [D loss: 0.694546103477478, acc.: 48.49%] [G loss: 0.6991294026374817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 72/86 [D loss: 0.6943648755550385, acc.: 49.37%] [G loss: 0.7010919451713562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 73/86 [D loss: 0.6937119960784912, acc.: 49.61%] [G loss: 0.7006095051765442]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 74/86 [D loss: 0.6952707469463348, acc.: 46.78%] [G loss: 0.6995960474014282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 75/86 [D loss: 0.6940171718597412, acc.: 48.34%] [G loss: 0.7007718086242676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 76/86 [D loss: 0.6948806345462799, acc.: 47.41%] [G loss: 0.6983426809310913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 77/86 [D loss: 0.6946207582950592, acc.: 47.61%] [G loss: 0.6971333026885986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 78/86 [D loss: 0.6957205533981323, acc.: 46.48%] [G loss: 0.6988625526428223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 79/86 [D loss: 0.6943980157375336, acc.: 48.44%] [G loss: 0.7002785205841064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 80/86 [D loss: 0.6948385238647461, acc.: 49.02%] [G loss: 0.7008752822875977]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 81/86 [D loss: 0.6951994299888611, acc.: 47.90%] [G loss: 0.6985309720039368]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 82/86 [D loss: 0.6952057778835297, acc.: 47.12%] [G loss: 0.6981784105300903]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 83/86 [D loss: 0.6958493292331696, acc.: 46.19%] [G loss: 0.6996017694473267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 84/86 [D loss: 0.6942262649536133, acc.: 50.39%] [G loss: 0.7019938230514526]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 85/86 [D loss: 0.6943391561508179, acc.: 49.90%] [G loss: 0.7021093368530273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 86/86 [D loss: 0.6953026354312897, acc.: 47.27%] [G loss: 0.7010080218315125]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 1/86 [D loss: 0.6950294077396393, acc.: 47.61%] [G loss: 0.6991572380065918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 2/86 [D loss: 0.6950317919254303, acc.: 47.90%] [G loss: 0.6975927352905273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 3/86 [D loss: 0.6946589946746826, acc.: 48.34%] [G loss: 0.6998611092567444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 4/86 [D loss: 0.6948122382164001, acc.: 48.10%] [G loss: 0.7004013061523438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 5/86 [D loss: 0.6943734288215637, acc.: 47.71%] [G loss: 0.7009398937225342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 6/86 [D loss: 0.6949786841869354, acc.: 47.85%] [G loss: 0.7018633484840393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 7/86 [D loss: 0.6954081654548645, acc.: 47.12%] [G loss: 0.7004621028900146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 8/86 [D loss: 0.6942834258079529, acc.: 47.75%] [G loss: 0.6981809139251709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 9/86 [D loss: 0.6952789723873138, acc.: 47.02%] [G loss: 0.6972931623458862]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 10/86 [D loss: 0.6949168741703033, acc.: 47.85%] [G loss: 0.7006140947341919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 11/86 [D loss: 0.6933970153331757, acc.: 50.24%] [G loss: 0.7005038857460022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 12/86 [D loss: 0.6945256888866425, acc.: 49.46%] [G loss: 0.6995408535003662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 13/86 [D loss: 0.6948599517345428, acc.: 48.49%] [G loss: 0.6994163990020752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 14/86 [D loss: 0.6960785090923309, acc.: 46.58%] [G loss: 0.6983100175857544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 15/86 [D loss: 0.6953803598880768, acc.: 48.44%] [G loss: 0.6964271664619446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 16/86 [D loss: 0.6963768899440765, acc.: 46.48%] [G loss: 0.6991830468177795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 17/86 [D loss: 0.6935674548149109, acc.: 49.51%] [G loss: 0.6996708512306213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 18/86 [D loss: 0.6940981447696686, acc.: 48.88%] [G loss: 0.699506402015686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 19/86 [D loss: 0.6942265927791595, acc.: 48.14%] [G loss: 0.698074221611023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 20/86 [D loss: 0.6954117119312286, acc.: 47.75%] [G loss: 0.696820080280304]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 21/86 [D loss: 0.6959349811077118, acc.: 46.63%] [G loss: 0.694690465927124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 22/86 [D loss: 0.6954202055931091, acc.: 47.41%] [G loss: 0.7004794478416443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 23/86 [D loss: 0.6939087212085724, acc.: 49.22%] [G loss: 0.7000123858451843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 24/86 [D loss: 0.6946992874145508, acc.: 48.00%] [G loss: 0.7004636526107788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 25/86 [D loss: 0.6942037642002106, acc.: 48.83%] [G loss: 0.6979814171791077]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 26/86 [D loss: 0.6954863965511322, acc.: 46.48%] [G loss: 0.6965981125831604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 27/86 [D loss: 0.694875955581665, acc.: 47.36%] [G loss: 0.6967340111732483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 28/86 [D loss: 0.6954377293586731, acc.: 47.90%] [G loss: 0.7019484043121338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 29/86 [D loss: 0.6946251392364502, acc.: 47.07%] [G loss: 0.7014142870903015]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 30/86 [D loss: 0.6949213743209839, acc.: 47.51%] [G loss: 0.6991259455680847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 31/86 [D loss: 0.6938671171665192, acc.: 50.29%] [G loss: 0.6990359425544739]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 32/86 [D loss: 0.6947650015354156, acc.: 47.02%] [G loss: 0.6960905194282532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 33/86 [D loss: 0.6960658729076385, acc.: 45.26%] [G loss: 0.6965614557266235]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 34/86 [D loss: 0.6951172351837158, acc.: 48.29%] [G loss: 0.7022614479064941]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 35/86 [D loss: 0.6941689848899841, acc.: 48.54%] [G loss: 0.699589192867279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 36/86 [D loss: 0.695633590221405, acc.: 46.78%] [G loss: 0.698701024055481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 37/86 [D loss: 0.6942736506462097, acc.: 49.46%] [G loss: 0.6989215016365051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 38/86 [D loss: 0.6957480609416962, acc.: 45.17%] [G loss: 0.6963040232658386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 39/86 [D loss: 0.6946389079093933, acc.: 49.76%] [G loss: 0.6973603963851929]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 40/86 [D loss: 0.6956256031990051, acc.: 46.97%] [G loss: 0.7030220031738281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 41/86 [D loss: 0.6929730772972107, acc.: 50.88%] [G loss: 0.7015153169631958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 42/86 [D loss: 0.6950734555721283, acc.: 46.97%] [G loss: 0.7004747986793518]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 43/86 [D loss: 0.6948267817497253, acc.: 47.02%] [G loss: 0.6991772651672363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 44/86 [D loss: 0.6950743198394775, acc.: 47.07%] [G loss: 0.6955112218856812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 45/86 [D loss: 0.6964443325996399, acc.: 46.24%] [G loss: 0.6981260180473328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 46/86 [D loss: 0.6942017078399658, acc.: 49.80%] [G loss: 0.7031418085098267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 47/86 [D loss: 0.6928707659244537, acc.: 50.98%] [G loss: 0.7022078633308411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 48/86 [D loss: 0.6949518024921417, acc.: 46.88%] [G loss: 0.7003447413444519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 49/86 [D loss: 0.6933301389217377, acc.: 50.10%] [G loss: 0.7012404799461365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 50/86 [D loss: 0.6944087743759155, acc.: 48.97%] [G loss: 0.6996708512306213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 51/86 [D loss: 0.6958065330982208, acc.: 47.02%] [G loss: 0.6974725127220154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 52/86 [D loss: 0.6948214769363403, acc.: 47.95%] [G loss: 0.7033594250679016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 53/86 [D loss: 0.6939694881439209, acc.: 49.71%] [G loss: 0.7016656398773193]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 54/86 [D loss: 0.6951710283756256, acc.: 47.61%] [G loss: 0.7016870379447937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 55/86 [D loss: 0.6942285597324371, acc.: 49.22%] [G loss: 0.7010797262191772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 56/86 [D loss: 0.6945919692516327, acc.: 48.58%] [G loss: 0.6958256363868713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 57/86 [D loss: 0.6958784759044647, acc.: 46.97%] [G loss: 0.6981425881385803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 58/86 [D loss: 0.6950721144676208, acc.: 47.80%] [G loss: 0.7020326256752014]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 59/86 [D loss: 0.693295806646347, acc.: 51.86%] [G loss: 0.7013271450996399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 60/86 [D loss: 0.6957930624485016, acc.: 46.04%] [G loss: 0.6997292041778564]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 61/86 [D loss: 0.6940384209156036, acc.: 47.75%] [G loss: 0.6997659206390381]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 62/86 [D loss: 0.6941189765930176, acc.: 49.46%] [G loss: 0.696682333946228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 63/86 [D loss: 0.6966036558151245, acc.: 45.70%] [G loss: 0.6958209872245789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 64/86 [D loss: 0.6948314309120178, acc.: 47.90%] [G loss: 0.7032247185707092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 65/86 [D loss: 0.6929021775722504, acc.: 51.07%] [G loss: 0.7002168893814087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 66/86 [D loss: 0.6952279508113861, acc.: 47.66%] [G loss: 0.6987606287002563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 67/86 [D loss: 0.6949320435523987, acc.: 47.46%] [G loss: 0.6981414556503296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 68/86 [D loss: 0.6949544548988342, acc.: 46.73%] [G loss: 0.6937997937202454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 69/86 [D loss: 0.6968812346458435, acc.: 45.85%] [G loss: 0.6955276727676392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 70/86 [D loss: 0.6951196491718292, acc.: 47.07%] [G loss: 0.7008678317070007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 71/86 [D loss: 0.6935549080371857, acc.: 50.29%] [G loss: 0.6988727450370789]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 72/86 [D loss: 0.6957007050514221, acc.: 47.27%] [G loss: 0.6980605721473694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 73/86 [D loss: 0.6939393579959869, acc.: 49.95%] [G loss: 0.6988112330436707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 74/86 [D loss: 0.6937783062458038, acc.: 49.85%] [G loss: 0.6942751407623291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 75/86 [D loss: 0.6964725852012634, acc.: 46.48%] [G loss: 0.6936405301094055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 76/86 [D loss: 0.6956340968608856, acc.: 46.00%] [G loss: 0.6995182633399963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 77/86 [D loss: 0.6943808495998383, acc.: 49.46%] [G loss: 0.6992923021316528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 78/86 [D loss: 0.6953614056110382, acc.: 46.63%] [G loss: 0.6979396939277649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 79/86 [D loss: 0.6951439678668976, acc.: 47.90%] [G loss: 0.6993051767349243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 80/86 [D loss: 0.6939407885074615, acc.: 49.80%] [G loss: 0.6964843273162842]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 81/86 [D loss: 0.6943531632423401, acc.: 48.44%] [G loss: 0.6961489319801331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 82/86 [D loss: 0.6947290897369385, acc.: 48.97%] [G loss: 0.7011774778366089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 83/86 [D loss: 0.6937239170074463, acc.: 49.95%] [G loss: 0.7003353834152222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 84/86 [D loss: 0.694559633731842, acc.: 48.73%] [G loss: 0.7000492215156555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 85/86 [D loss: 0.6949372589588165, acc.: 47.90%] [G loss: 0.7004948854446411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 86/86 [D loss: 0.6944769024848938, acc.: 47.66%] [G loss: 0.7002668380737305]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 1/86 [D loss: 0.6942344903945923, acc.: 47.51%] [G loss: 0.6986940503120422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 2/86 [D loss: 0.6948782503604889, acc.: 46.83%] [G loss: 0.7017678022384644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 3/86 [D loss: 0.6938656568527222, acc.: 49.02%] [G loss: 0.7026184797286987]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 4/86 [D loss: 0.6938799917697906, acc.: 49.46%] [G loss: 0.7014942169189453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 5/86 [D loss: 0.6942822933197021, acc.: 50.10%] [G loss: 0.7002336382865906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 6/86 [D loss: 0.6949716210365295, acc.: 48.10%] [G loss: 0.697973370552063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 7/86 [D loss: 0.6949684619903564, acc.: 47.31%] [G loss: 0.6990244388580322]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 8/86 [D loss: 0.6944350004196167, acc.: 49.12%] [G loss: 0.7015562653541565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 9/86 [D loss: 0.6930466294288635, acc.: 49.76%] [G loss: 0.7010462880134583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 10/86 [D loss: 0.694343626499176, acc.: 49.76%] [G loss: 0.7015836238861084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 11/86 [D loss: 0.6946999430656433, acc.: 49.22%] [G loss: 0.7009018659591675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 12/86 [D loss: 0.6943901777267456, acc.: 48.88%] [G loss: 0.6991321444511414]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 13/86 [D loss: 0.6952478587627411, acc.: 46.78%] [G loss: 0.6992907524108887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 14/86 [D loss: 0.6941930651664734, acc.: 48.58%] [G loss: 0.7017898559570312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 15/86 [D loss: 0.6942340433597565, acc.: 48.97%] [G loss: 0.6999720931053162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 16/86 [D loss: 0.695389598608017, acc.: 46.34%] [G loss: 0.7013571262359619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 17/86 [D loss: 0.6933813095092773, acc.: 49.95%] [G loss: 0.6999514102935791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 18/86 [D loss: 0.6955761015415192, acc.: 45.31%] [G loss: 0.6963412761688232]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 19/86 [D loss: 0.6961985230445862, acc.: 46.44%] [G loss: 0.6984982490539551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 20/86 [D loss: 0.6942042708396912, acc.: 48.58%] [G loss: 0.7006134986877441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 21/86 [D loss: 0.6948955059051514, acc.: 48.34%] [G loss: 0.6978719830513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 22/86 [D loss: 0.6947927474975586, acc.: 48.63%] [G loss: 0.7000818252563477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 23/86 [D loss: 0.6942743062973022, acc.: 47.46%] [G loss: 0.6967253088951111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 24/86 [D loss: 0.69538813829422, acc.: 46.44%] [G loss: 0.6971505880355835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 25/86 [D loss: 0.6955374479293823, acc.: 47.27%] [G loss: 0.6999425888061523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 26/86 [D loss: 0.6948077380657196, acc.: 48.73%] [G loss: 0.7009931206703186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 27/86 [D loss: 0.6937806904315948, acc.: 49.46%] [G loss: 0.6996956467628479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 28/86 [D loss: 0.694715827703476, acc.: 47.75%] [G loss: 0.699954092502594]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 29/86 [D loss: 0.6942299604415894, acc.: 48.68%] [G loss: 0.698023796081543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 30/86 [D loss: 0.6950976550579071, acc.: 47.12%] [G loss: 0.6965600848197937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 31/86 [D loss: 0.6949274837970734, acc.: 47.46%] [G loss: 0.7013185024261475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 32/86 [D loss: 0.6938717365264893, acc.: 48.88%] [G loss: 0.7015641331672668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 33/86 [D loss: 0.6944788098335266, acc.: 48.39%] [G loss: 0.6991640329360962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 34/86 [D loss: 0.6952251493930817, acc.: 48.05%] [G loss: 0.7005318999290466]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 35/86 [D loss: 0.6939496695995331, acc.: 49.02%] [G loss: 0.6980335116386414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 36/86 [D loss: 0.6950671374797821, acc.: 48.39%] [G loss: 0.6937388777732849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 37/86 [D loss: 0.6970081627368927, acc.: 45.26%] [G loss: 0.6983487606048584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 38/86 [D loss: 0.692540317773819, acc.: 51.56%] [G loss: 0.7018729448318481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 39/86 [D loss: 0.6944401264190674, acc.: 48.29%] [G loss: 0.6990212798118591]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 40/86 [D loss: 0.6958507597446442, acc.: 46.58%] [G loss: 0.6987385749816895]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 41/86 [D loss: 0.6943384408950806, acc.: 48.97%] [G loss: 0.696679413318634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 42/86 [D loss: 0.6950872242450714, acc.: 47.85%] [G loss: 0.6930080652236938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 43/86 [D loss: 0.6979749798774719, acc.: 43.51%] [G loss: 0.6983181238174438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 44/86 [D loss: 0.6935073435306549, acc.: 48.73%] [G loss: 0.701240062713623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 45/86 [D loss: 0.6947827339172363, acc.: 47.66%] [G loss: 0.6977547407150269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 46/86 [D loss: 0.6963753700256348, acc.: 45.31%] [G loss: 0.6980668306350708]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 47/86 [D loss: 0.6949760317802429, acc.: 46.58%] [G loss: 0.6965049505233765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 48/86 [D loss: 0.6932349503040314, acc.: 50.73%] [G loss: 0.693635106086731]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 49/86 [D loss: 0.6955589950084686, acc.: 47.07%] [G loss: 0.6959963440895081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 50/86 [D loss: 0.6945637166500092, acc.: 47.90%] [G loss: 0.7004542350769043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 51/86 [D loss: 0.6938747763633728, acc.: 49.66%] [G loss: 0.6982979774475098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 52/86 [D loss: 0.6948103606700897, acc.: 48.24%] [G loss: 0.6971278190612793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 53/86 [D loss: 0.6950741708278656, acc.: 48.05%] [G loss: 0.6976027488708496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 54/86 [D loss: 0.694716215133667, acc.: 47.61%] [G loss: 0.6958794593811035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 55/86 [D loss: 0.6959374845027924, acc.: 46.09%] [G loss: 0.6937945485115051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 56/86 [D loss: 0.6952175498008728, acc.: 47.56%] [G loss: 0.7015763521194458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 57/86 [D loss: 0.6936342716217041, acc.: 50.93%] [G loss: 0.6997491717338562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 58/86 [D loss: 0.6951155364513397, acc.: 46.73%] [G loss: 0.6992651224136353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 59/86 [D loss: 0.6951147317886353, acc.: 47.85%] [G loss: 0.6994430422782898]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 60/86 [D loss: 0.694437712430954, acc.: 49.12%] [G loss: 0.695527970790863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 61/86 [D loss: 0.6954493820667267, acc.: 46.29%] [G loss: 0.6968401074409485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 62/86 [D loss: 0.6955172717571259, acc.: 47.51%] [G loss: 0.7016226649284363]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 63/86 [D loss: 0.6926329731941223, acc.: 50.63%] [G loss: 0.7007564902305603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 64/86 [D loss: 0.6950300335884094, acc.: 48.05%] [G loss: 0.7003566026687622]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 65/86 [D loss: 0.6948091089725494, acc.: 47.17%] [G loss: 0.6985089182853699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 66/86 [D loss: 0.6947808563709259, acc.: 47.07%] [G loss: 0.6989921927452087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 67/86 [D loss: 0.6959321200847626, acc.: 45.65%] [G loss: 0.6969041228294373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 68/86 [D loss: 0.6947779953479767, acc.: 47.66%] [G loss: 0.7015101313591003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 69/86 [D loss: 0.6945672035217285, acc.: 48.63%] [G loss: 0.6999075412750244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 70/86 [D loss: 0.6949390172958374, acc.: 47.22%] [G loss: 0.700706422328949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 71/86 [D loss: 0.6946357488632202, acc.: 48.39%] [G loss: 0.6996530294418335]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 72/86 [D loss: 0.6945628821849823, acc.: 49.71%] [G loss: 0.6974626779556274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 73/86 [D loss: 0.6946148872375488, acc.: 48.93%] [G loss: 0.6968855261802673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 74/86 [D loss: 0.6960322558879852, acc.: 45.70%] [G loss: 0.699928879737854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 75/86 [D loss: 0.6939210891723633, acc.: 49.02%] [G loss: 0.7005919814109802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 76/86 [D loss: 0.6941957175731659, acc.: 49.61%] [G loss: 0.7006367444992065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 77/86 [D loss: 0.6944101452827454, acc.: 49.41%] [G loss: 0.6981179118156433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 78/86 [D loss: 0.6942225694656372, acc.: 49.32%] [G loss: 0.6975307464599609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 79/86 [D loss: 0.6960296332836151, acc.: 46.19%] [G loss: 0.6973050832748413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 80/86 [D loss: 0.6955525577068329, acc.: 46.88%] [G loss: 0.7001474499702454]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 81/86 [D loss: 0.6934362053871155, acc.: 50.39%] [G loss: 0.7011438012123108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 82/86 [D loss: 0.6947478652000427, acc.: 46.92%] [G loss: 0.6997820734977722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 83/86 [D loss: 0.6937247812747955, acc.: 49.80%] [G loss: 0.6985557079315186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 84/86 [D loss: 0.6941078305244446, acc.: 49.41%] [G loss: 0.6976817846298218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 85/86 [D loss: 0.694423645734787, acc.: 49.22%] [G loss: 0.6965437531471252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 86/86 [D loss: 0.6947381496429443, acc.: 48.34%] [G loss: 0.6992936134338379]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 1/86 [D loss: 0.6925784647464752, acc.: 52.10%] [G loss: 0.699409008026123]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 2/86 [D loss: 0.6940446197986603, acc.: 48.63%] [G loss: 0.7001970410346985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 3/86 [D loss: 0.6945909857749939, acc.: 47.71%] [G loss: 0.6982937455177307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 4/86 [D loss: 0.6943418383598328, acc.: 49.85%] [G loss: 0.6983378529548645]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 5/86 [D loss: 0.6948175430297852, acc.: 46.63%] [G loss: 0.6951028108596802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 6/86 [D loss: 0.6951315104961395, acc.: 47.80%] [G loss: 0.6977697014808655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 7/86 [D loss: 0.6945958733558655, acc.: 48.00%] [G loss: 0.700166642665863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 8/86 [D loss: 0.6946893334388733, acc.: 47.95%] [G loss: 0.699182391166687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 9/86 [D loss: 0.6947030425071716, acc.: 47.56%] [G loss: 0.6970556378364563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 10/86 [D loss: 0.6938931345939636, acc.: 49.71%] [G loss: 0.6960500478744507]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 11/86 [D loss: 0.6963109970092773, acc.: 46.29%] [G loss: 0.6957733631134033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 12/86 [D loss: 0.69509357213974, acc.: 47.02%] [G loss: 0.6981557607650757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 13/86 [D loss: 0.6929956078529358, acc.: 50.54%] [G loss: 0.6996825933456421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 14/86 [D loss: 0.6951605975627899, acc.: 46.78%] [G loss: 0.697746753692627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 15/86 [D loss: 0.6949253082275391, acc.: 48.19%] [G loss: 0.6971945762634277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 16/86 [D loss: 0.6942858397960663, acc.: 49.46%] [G loss: 0.694263219833374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 17/86 [D loss: 0.695794552564621, acc.: 46.29%] [G loss: 0.6942018270492554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 18/86 [D loss: 0.6952754855155945, acc.: 46.92%] [G loss: 0.7009366154670715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 19/86 [D loss: 0.6938841342926025, acc.: 49.22%] [G loss: 0.6996467113494873]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 20/86 [D loss: 0.6943097412586212, acc.: 49.56%] [G loss: 0.6988941431045532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 21/86 [D loss: 0.6950487196445465, acc.: 46.14%] [G loss: 0.6997266411781311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 22/86 [D loss: 0.6952293217182159, acc.: 47.90%] [G loss: 0.695285439491272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 23/86 [D loss: 0.6951986253261566, acc.: 46.68%] [G loss: 0.6960709095001221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 24/86 [D loss: 0.6944913268089294, acc.: 49.17%] [G loss: 0.701418399810791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 25/86 [D loss: 0.6937333941459656, acc.: 49.76%] [G loss: 0.6995488405227661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 26/86 [D loss: 0.6947835683822632, acc.: 48.44%] [G loss: 0.6978320479393005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 27/86 [D loss: 0.6939010322093964, acc.: 49.02%] [G loss: 0.6992736458778381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 28/86 [D loss: 0.6950919032096863, acc.: 48.00%] [G loss: 0.6962355375289917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 29/86 [D loss: 0.6962611973285675, acc.: 45.51%] [G loss: 0.6947633028030396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 30/86 [D loss: 0.6946457624435425, acc.: 49.02%] [G loss: 0.7008581757545471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 31/86 [D loss: 0.6928552985191345, acc.: 51.27%] [G loss: 0.7000454664230347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 32/86 [D loss: 0.6948077380657196, acc.: 47.90%] [G loss: 0.6997055411338806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 33/86 [D loss: 0.6949965059757233, acc.: 47.61%] [G loss: 0.6989893317222595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 34/86 [D loss: 0.6949813067913055, acc.: 47.66%] [G loss: 0.6962389945983887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 35/86 [D loss: 0.6964950561523438, acc.: 45.26%] [G loss: 0.6950128078460693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 36/86 [D loss: 0.6949827373027802, acc.: 48.10%] [G loss: 0.7007052302360535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 37/86 [D loss: 0.6939710080623627, acc.: 49.61%] [G loss: 0.7008463740348816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 38/86 [D loss: 0.6955374777317047, acc.: 46.83%] [G loss: 0.7000784277915955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 39/86 [D loss: 0.6942935287952423, acc.: 48.49%] [G loss: 0.6986208558082581]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 40/86 [D loss: 0.6927875280380249, acc.: 51.22%] [G loss: 0.6969923973083496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 41/86 [D loss: 0.6947961747646332, acc.: 48.54%] [G loss: 0.6963286399841309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 42/86 [D loss: 0.696555882692337, acc.: 45.17%] [G loss: 0.7022352814674377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 43/86 [D loss: 0.6943718492984772, acc.: 47.90%] [G loss: 0.7013562917709351]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 44/86 [D loss: 0.6948556303977966, acc.: 48.10%] [G loss: 0.6998403072357178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 45/86 [D loss: 0.6944009065628052, acc.: 47.31%] [G loss: 0.6998624205589294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 46/86 [D loss: 0.6942300796508789, acc.: 48.88%] [G loss: 0.6980569362640381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 47/86 [D loss: 0.6952938139438629, acc.: 46.73%] [G loss: 0.695608377456665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 48/86 [D loss: 0.6948002576828003, acc.: 47.07%] [G loss: 0.6995993852615356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 49/86 [D loss: 0.6946964263916016, acc.: 49.12%] [G loss: 0.7015923261642456]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 50/86 [D loss: 0.693846732378006, acc.: 49.61%] [G loss: 0.6988397836685181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 51/86 [D loss: 0.6945963501930237, acc.: 46.09%] [G loss: 0.6986605525016785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 52/86 [D loss: 0.6938464343547821, acc.: 50.73%] [G loss: 0.6985285878181458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 53/86 [D loss: 0.6946933567523956, acc.: 48.19%] [G loss: 0.6941267848014832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 54/86 [D loss: 0.6968016028404236, acc.: 45.21%] [G loss: 0.6988176703453064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 55/86 [D loss: 0.6933384239673615, acc.: 52.44%] [G loss: 0.701549232006073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 56/86 [D loss: 0.6951244473457336, acc.: 47.71%] [G loss: 0.6987637877464294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 57/86 [D loss: 0.6950816214084625, acc.: 47.85%] [G loss: 0.6983790993690491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 58/86 [D loss: 0.69427490234375, acc.: 49.27%] [G loss: 0.6972051858901978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 59/86 [D loss: 0.6939252018928528, acc.: 50.05%] [G loss: 0.6947450041770935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 60/86 [D loss: 0.6966141164302826, acc.: 44.82%] [G loss: 0.6952011585235596]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 61/86 [D loss: 0.6944937109947205, acc.: 47.56%] [G loss: 0.7007321119308472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 62/86 [D loss: 0.6945878565311432, acc.: 48.19%] [G loss: 0.6995747089385986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 63/86 [D loss: 0.6955222189426422, acc.: 46.83%] [G loss: 0.699386477470398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 64/86 [D loss: 0.6938779950141907, acc.: 48.88%] [G loss: 0.6983431577682495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 65/86 [D loss: 0.6941860914230347, acc.: 49.76%] [G loss: 0.6946629285812378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 66/86 [D loss: 0.6958938539028168, acc.: 45.61%] [G loss: 0.6964365243911743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 67/86 [D loss: 0.6947003304958344, acc.: 47.71%] [G loss: 0.7013943791389465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 68/86 [D loss: 0.6933447122573853, acc.: 49.22%] [G loss: 0.6993839740753174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 69/86 [D loss: 0.6946623027324677, acc.: 47.95%] [G loss: 0.6984742283821106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 70/86 [D loss: 0.6941916346549988, acc.: 49.02%] [G loss: 0.6997276544570923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 71/86 [D loss: 0.6934541165828705, acc.: 50.49%] [G loss: 0.6959118843078613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 72/86 [D loss: 0.6956564486026764, acc.: 46.09%] [G loss: 0.6950685977935791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 73/86 [D loss: 0.6960658133029938, acc.: 47.12%] [G loss: 0.7013635039329529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 74/86 [D loss: 0.6936769783496857, acc.: 49.07%] [G loss: 0.6994171738624573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 75/86 [D loss: 0.6954862773418427, acc.: 46.97%] [G loss: 0.6991010308265686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 76/86 [D loss: 0.6945262849330902, acc.: 47.27%] [G loss: 0.6994240283966064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 77/86 [D loss: 0.6940138339996338, acc.: 50.05%] [G loss: 0.6960633993148804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 78/86 [D loss: 0.695557028055191, acc.: 46.34%] [G loss: 0.6937214136123657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 79/86 [D loss: 0.6962263882160187, acc.: 45.70%] [G loss: 0.7003552913665771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 80/86 [D loss: 0.6925779283046722, acc.: 51.03%] [G loss: 0.7003511786460876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 81/86 [D loss: 0.6948003768920898, acc.: 47.90%] [G loss: 0.6980330348014832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 82/86 [D loss: 0.6945905089378357, acc.: 48.68%] [G loss: 0.7000895738601685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 83/86 [D loss: 0.6929661631584167, acc.: 51.66%] [G loss: 0.6971790790557861]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 84/86 [D loss: 0.6939685940742493, acc.: 48.78%] [G loss: 0.6953594088554382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 85/86 [D loss: 0.6955784857273102, acc.: 47.07%] [G loss: 0.6991294026374817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 86/86 [D loss: 0.6939046084880829, acc.: 48.78%] [G loss: 0.7023575901985168]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 1/86 [D loss: 0.6935293674468994, acc.: 49.66%] [G loss: 0.6998393535614014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 2/86 [D loss: 0.6950331032276154, acc.: 47.17%] [G loss: 0.6992368102073669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 3/86 [D loss: 0.6949228048324585, acc.: 47.46%] [G loss: 0.6990923285484314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 4/86 [D loss: 0.6950559020042419, acc.: 47.02%] [G loss: 0.6969619989395142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 5/86 [D loss: 0.6951631605625153, acc.: 47.12%] [G loss: 0.6980029940605164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 6/86 [D loss: 0.6943036913871765, acc.: 47.90%] [G loss: 0.7006741166114807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 7/86 [D loss: 0.6935924887657166, acc.: 49.56%] [G loss: 0.7005842924118042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 8/86 [D loss: 0.6944112181663513, acc.: 48.78%] [G loss: 0.6999039649963379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 9/86 [D loss: 0.6944986581802368, acc.: 47.95%] [G loss: 0.6988690495491028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 10/86 [D loss: 0.694754034280777, acc.: 47.31%] [G loss: 0.6987496614456177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 11/86 [D loss: 0.695574939250946, acc.: 46.14%] [G loss: 0.6954175233840942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 12/86 [D loss: 0.695195198059082, acc.: 46.78%] [G loss: 0.7014620304107666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 13/86 [D loss: 0.6929395794868469, acc.: 53.03%] [G loss: 0.7016741037368774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 14/86 [D loss: 0.6937934458255768, acc.: 48.97%] [G loss: 0.6996131539344788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 15/86 [D loss: 0.6949218213558197, acc.: 46.78%] [G loss: 0.6987130045890808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 16/86 [D loss: 0.6935029923915863, acc.: 50.44%] [G loss: 0.6971778869628906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 17/86 [D loss: 0.6945390105247498, acc.: 47.56%] [G loss: 0.6974633932113647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 18/86 [D loss: 0.6944147348403931, acc.: 48.78%] [G loss: 0.7025659084320068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 19/86 [D loss: 0.6931199133396149, acc.: 50.10%] [G loss: 0.7009513974189758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 20/86 [D loss: 0.6936508417129517, acc.: 49.46%] [G loss: 0.6999142169952393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 21/86 [D loss: 0.6948050558567047, acc.: 46.73%] [G loss: 0.6991540789604187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 22/86 [D loss: 0.6944603621959686, acc.: 48.34%] [G loss: 0.6960895657539368]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 23/86 [D loss: 0.6949955821037292, acc.: 47.22%] [G loss: 0.6966555118560791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 24/86 [D loss: 0.6956215500831604, acc.: 46.09%] [G loss: 0.6989567279815674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 25/86 [D loss: 0.6942425966262817, acc.: 49.71%] [G loss: 0.7011732459068298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 26/86 [D loss: 0.6941012740135193, acc.: 50.24%] [G loss: 0.6997039914131165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 27/86 [D loss: 0.6934955716133118, acc.: 50.29%] [G loss: 0.6995065212249756]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 28/86 [D loss: 0.6940532028675079, acc.: 50.24%] [G loss: 0.6961871385574341]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 29/86 [D loss: 0.6952653229236603, acc.: 48.05%] [G loss: 0.6970065832138062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 30/86 [D loss: 0.6942353248596191, acc.: 49.17%] [G loss: 0.6991991996765137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 31/86 [D loss: 0.6934249997138977, acc.: 50.05%] [G loss: 0.6987898349761963]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 32/86 [D loss: 0.6951568126678467, acc.: 46.88%] [G loss: 0.6981570720672607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 33/86 [D loss: 0.6940357387065887, acc.: 48.97%] [G loss: 0.6996796727180481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 34/86 [D loss: 0.694087564945221, acc.: 48.93%] [G loss: 0.6978877186775208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 35/86 [D loss: 0.6943715810775757, acc.: 47.75%] [G loss: 0.6984381079673767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 36/86 [D loss: 0.6942607462406158, acc.: 48.19%] [G loss: 0.7019073367118835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 37/86 [D loss: 0.6944348812103271, acc.: 48.10%] [G loss: 0.7009999752044678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 38/86 [D loss: 0.6942726075649261, acc.: 47.51%] [G loss: 0.700680136680603]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 39/86 [D loss: 0.6950001418590546, acc.: 47.80%] [G loss: 0.6992405652999878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 40/86 [D loss: 0.6938975155353546, acc.: 49.61%] [G loss: 0.6986585259437561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 41/86 [D loss: 0.6953647136688232, acc.: 45.65%] [G loss: 0.6985339522361755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 42/86 [D loss: 0.6950308978557587, acc.: 46.83%] [G loss: 0.6999944448471069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 43/86 [D loss: 0.6937803030014038, acc.: 50.10%] [G loss: 0.6997878551483154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 44/86 [D loss: 0.694545567035675, acc.: 48.39%] [G loss: 0.7009919881820679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 45/86 [D loss: 0.6937887668609619, acc.: 50.34%] [G loss: 0.6994854807853699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 46/86 [D loss: 0.6942758560180664, acc.: 49.41%] [G loss: 0.6977035999298096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 47/86 [D loss: 0.6952120959758759, acc.: 47.22%] [G loss: 0.6977334022521973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 48/86 [D loss: 0.693899929523468, acc.: 48.58%] [G loss: 0.7000300884246826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 49/86 [D loss: 0.6933186948299408, acc.: 50.73%] [G loss: 0.7011784315109253]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 50/86 [D loss: 0.6941056549549103, acc.: 48.58%] [G loss: 0.6989355087280273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 51/86 [D loss: 0.694596916437149, acc.: 48.00%] [G loss: 0.6978117227554321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 52/86 [D loss: 0.6936612129211426, acc.: 49.95%] [G loss: 0.6973049640655518]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 53/86 [D loss: 0.6946873068809509, acc.: 48.83%] [G loss: 0.6982066035270691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 54/86 [D loss: 0.6943987607955933, acc.: 48.19%] [G loss: 0.7004895210266113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 55/86 [D loss: 0.69355109333992, acc.: 49.80%] [G loss: 0.6989859342575073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 56/86 [D loss: 0.693822979927063, acc.: 49.27%] [G loss: 0.698959231376648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 57/86 [D loss: 0.6944716572761536, acc.: 49.02%] [G loss: 0.6982578635215759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 58/86 [D loss: 0.694758266210556, acc.: 48.05%] [G loss: 0.698898196220398]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 59/86 [D loss: 0.6950538158416748, acc.: 47.07%] [G loss: 0.6979118585586548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 60/86 [D loss: 0.6937032639980316, acc.: 49.02%] [G loss: 0.7006233334541321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 61/86 [D loss: 0.6933192312717438, acc.: 49.46%] [G loss: 0.7006071209907532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 62/86 [D loss: 0.6938555240631104, acc.: 49.41%] [G loss: 0.6991325616836548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 63/86 [D loss: 0.6941854655742645, acc.: 48.29%] [G loss: 0.699466347694397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 64/86 [D loss: 0.6948784589767456, acc.: 47.07%] [G loss: 0.6992144584655762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 65/86 [D loss: 0.6937210857868195, acc.: 49.07%] [G loss: 0.6972440481185913]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 66/86 [D loss: 0.695136159658432, acc.: 47.46%] [G loss: 0.69890958070755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 67/86 [D loss: 0.6929121613502502, acc.: 50.44%] [G loss: 0.7003117799758911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 68/86 [D loss: 0.6929312348365784, acc.: 51.95%] [G loss: 0.6997092962265015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 69/86 [D loss: 0.693085640668869, acc.: 51.32%] [G loss: 0.7006276845932007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 70/86 [D loss: 0.6941197514533997, acc.: 49.12%] [G loss: 0.6968340277671814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 71/86 [D loss: 0.6950802803039551, acc.: 48.00%] [G loss: 0.694770097732544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 72/86 [D loss: 0.6953303813934326, acc.: 46.73%] [G loss: 0.6995109915733337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 73/86 [D loss: 0.6940957009792328, acc.: 47.95%] [G loss: 0.700995922088623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 74/86 [D loss: 0.6947234272956848, acc.: 46.97%] [G loss: 0.6992888450622559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 75/86 [D loss: 0.6945805251598358, acc.: 47.31%] [G loss: 0.6989961862564087]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 76/86 [D loss: 0.6941336989402771, acc.: 49.66%] [G loss: 0.6965641975402832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 77/86 [D loss: 0.6952658295631409, acc.: 46.63%] [G loss: 0.6950393319129944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 78/86 [D loss: 0.695123702287674, acc.: 46.78%] [G loss: 0.7009140849113464]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 79/86 [D loss: 0.6945463120937347, acc.: 46.97%] [G loss: 0.7008161544799805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 80/86 [D loss: 0.6952282786369324, acc.: 47.51%] [G loss: 0.6995693445205688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 81/86 [D loss: 0.6950682997703552, acc.: 47.75%] [G loss: 0.6997678875923157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 82/86 [D loss: 0.694886714220047, acc.: 46.92%] [G loss: 0.6952732801437378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 83/86 [D loss: 0.6948042809963226, acc.: 47.61%] [G loss: 0.6961512565612793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 84/86 [D loss: 0.6960878372192383, acc.: 46.19%] [G loss: 0.7020507454872131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 85/86 [D loss: 0.6929237842559814, acc.: 51.07%] [G loss: 0.6998892426490784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 86/86 [D loss: 0.6953456997871399, acc.: 47.02%] [G loss: 0.6982244253158569]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 35/200, Batch 1/86 [D loss: 0.6949975788593292, acc.: 47.22%] [G loss: 0.6987850666046143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 2/86 [D loss: 0.6938016712665558, acc.: 49.76%] [G loss: 0.6965741515159607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 3/86 [D loss: 0.695330023765564, acc.: 48.34%] [G loss: 0.6946794390678406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 4/86 [D loss: 0.6969624757766724, acc.: 43.41%] [G loss: 0.7030413150787354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 5/86 [D loss: 0.6926993131637573, acc.: 51.61%] [G loss: 0.7006866931915283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 6/86 [D loss: 0.6960418820381165, acc.: 46.29%] [G loss: 0.6977738738059998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 7/86 [D loss: 0.6947380602359772, acc.: 47.56%] [G loss: 0.7005653977394104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 8/86 [D loss: 0.6931052803993225, acc.: 50.44%] [G loss: 0.6967797875404358]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 9/86 [D loss: 0.6959896087646484, acc.: 45.65%] [G loss: 0.694381058216095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 10/86 [D loss: 0.6971113681793213, acc.: 43.65%] [G loss: 0.7026453018188477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 11/86 [D loss: 0.6934348940849304, acc.: 50.00%] [G loss: 0.7012513875961304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 12/86 [D loss: 0.6947849094867706, acc.: 47.51%] [G loss: 0.6993111968040466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 13/86 [D loss: 0.6946261823177338, acc.: 48.05%] [G loss: 0.7006611227989197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 14/86 [D loss: 0.6937370598316193, acc.: 49.80%] [G loss: 0.6978100538253784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 15/86 [D loss: 0.6944057941436768, acc.: 49.17%] [G loss: 0.6955046653747559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 16/86 [D loss: 0.6948226690292358, acc.: 48.44%] [G loss: 0.7015580534934998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 17/86 [D loss: 0.6933575868606567, acc.: 51.12%] [G loss: 0.7005956768989563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 18/86 [D loss: 0.6945896744728088, acc.: 47.90%] [G loss: 0.7009462118148804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 19/86 [D loss: 0.6939881145954132, acc.: 49.22%] [G loss: 0.6994902491569519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 20/86 [D loss: 0.6939532458782196, acc.: 49.51%] [G loss: 0.697433352470398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 21/86 [D loss: 0.6955353617668152, acc.: 46.88%] [G loss: 0.6946805715560913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 22/86 [D loss: 0.696058601140976, acc.: 45.75%] [G loss: 0.7009981870651245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 23/86 [D loss: 0.6930332183837891, acc.: 50.83%] [G loss: 0.7014007568359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 24/86 [D loss: 0.6951866745948792, acc.: 47.22%] [G loss: 0.6989426016807556]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 25/86 [D loss: 0.6948356330394745, acc.: 47.02%] [G loss: 0.7007277011871338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 26/86 [D loss: 0.6944620907306671, acc.: 48.83%] [G loss: 0.6960747241973877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 27/86 [D loss: 0.6962515413761139, acc.: 45.70%] [G loss: 0.6948080062866211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 28/86 [D loss: 0.6960053443908691, acc.: 46.39%] [G loss: 0.702411949634552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 29/86 [D loss: 0.6926931142807007, acc.: 51.61%] [G loss: 0.7008554339408875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 30/86 [D loss: 0.6952985525131226, acc.: 47.07%] [G loss: 0.6983363032341003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 31/86 [D loss: 0.6942856907844543, acc.: 48.63%] [G loss: 0.6985009908676147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 32/86 [D loss: 0.6929424405097961, acc.: 51.90%] [G loss: 0.6954344511032104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 33/86 [D loss: 0.6956177055835724, acc.: 44.97%] [G loss: 0.6921366453170776]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 34/86 [D loss: 0.696713387966156, acc.: 45.80%] [G loss: 0.6992108821868896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 35/86 [D loss: 0.6920845806598663, acc.: 51.46%] [G loss: 0.7006366848945618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 36/86 [D loss: 0.6948013603687286, acc.: 47.75%] [G loss: 0.6974754333496094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 37/86 [D loss: 0.6951240301132202, acc.: 47.22%] [G loss: 0.698686420917511]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 38/86 [D loss: 0.6935781836509705, acc.: 50.78%] [G loss: 0.6968269348144531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 39/86 [D loss: 0.694496363401413, acc.: 48.05%] [G loss: 0.6931154727935791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 40/86 [D loss: 0.6965816915035248, acc.: 45.31%] [G loss: 0.6972383856773376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 41/86 [D loss: 0.6933587789535522, acc.: 48.88%] [G loss: 0.7019731998443604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 42/86 [D loss: 0.6943288147449493, acc.: 48.88%] [G loss: 0.699129581451416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 43/86 [D loss: 0.6952890157699585, acc.: 46.34%] [G loss: 0.6997662782669067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 44/86 [D loss: 0.694758802652359, acc.: 48.05%] [G loss: 0.6985849142074585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 45/86 [D loss: 0.6933857500553131, acc.: 49.85%] [G loss: 0.6960232853889465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 46/86 [D loss: 0.6947591006755829, acc.: 47.61%] [G loss: 0.6965944170951843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 47/86 [D loss: 0.6930137872695923, acc.: 50.88%] [G loss: 0.7017404437065125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 48/86 [D loss: 0.6939650475978851, acc.: 49.46%] [G loss: 0.700427234172821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 49/86 [D loss: 0.6947470307350159, acc.: 48.49%] [G loss: 0.6998116374015808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 50/86 [D loss: 0.693755567073822, acc.: 49.07%] [G loss: 0.7006663680076599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 51/86 [D loss: 0.6939134895801544, acc.: 48.68%] [G loss: 0.6968631744384766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 52/86 [D loss: 0.695618063211441, acc.: 45.95%] [G loss: 0.6978492140769958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 53/86 [D loss: 0.6942331194877625, acc.: 48.73%] [G loss: 0.7015302181243896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 54/86 [D loss: 0.6933738887310028, acc.: 49.02%] [G loss: 0.7002688646316528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 55/86 [D loss: 0.6945172548294067, acc.: 47.27%] [G loss: 0.6993595361709595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 56/86 [D loss: 0.6941634118556976, acc.: 49.51%] [G loss: 0.6987249255180359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 57/86 [D loss: 0.6948210000991821, acc.: 47.02%] [G loss: 0.6967777013778687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 58/86 [D loss: 0.6945099532604218, acc.: 48.78%] [G loss: 0.6973638534545898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 59/86 [D loss: 0.6946707963943481, acc.: 47.75%] [G loss: 0.7003833055496216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 60/86 [D loss: 0.6941137313842773, acc.: 49.27%] [G loss: 0.70121830701828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 61/86 [D loss: 0.6937184929847717, acc.: 48.88%] [G loss: 0.7006272077560425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 62/86 [D loss: 0.6941450238227844, acc.: 47.80%] [G loss: 0.7009154558181763]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 63/86 [D loss: 0.6938879787921906, acc.: 50.78%] [G loss: 0.6976454257965088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 64/86 [D loss: 0.6949229538440704, acc.: 47.95%] [G loss: 0.6952265501022339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 65/86 [D loss: 0.6947534084320068, acc.: 47.12%] [G loss: 0.6999956369400024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 66/86 [D loss: 0.6925174593925476, acc.: 52.25%] [G loss: 0.700991153717041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 67/86 [D loss: 0.6939131021499634, acc.: 49.61%] [G loss: 0.7000375986099243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 68/86 [D loss: 0.6941821575164795, acc.: 48.34%] [G loss: 0.6999301910400391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 69/86 [D loss: 0.6944696009159088, acc.: 48.39%] [G loss: 0.6978244781494141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 70/86 [D loss: 0.6947423219680786, acc.: 49.07%] [G loss: 0.6975365281105042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 71/86 [D loss: 0.6938144564628601, acc.: 50.20%] [G loss: 0.7011690139770508]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 72/86 [D loss: 0.6925225853919983, acc.: 52.20%] [G loss: 0.7001137733459473]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 73/86 [D loss: 0.6939063966274261, acc.: 50.00%] [G loss: 0.6999183297157288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 74/86 [D loss: 0.6939685344696045, acc.: 50.05%] [G loss: 0.700710117816925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 75/86 [D loss: 0.6943444609642029, acc.: 48.14%] [G loss: 0.6981459856033325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 76/86 [D loss: 0.6949380040168762, acc.: 47.22%] [G loss: 0.6944665312767029]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 77/86 [D loss: 0.6949949264526367, acc.: 47.41%] [G loss: 0.701747715473175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 78/86 [D loss: 0.6936248540878296, acc.: 49.41%] [G loss: 0.7004225850105286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 79/86 [D loss: 0.6944563090801239, acc.: 48.10%] [G loss: 0.6991251707077026]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 80/86 [D loss: 0.6946257054805756, acc.: 47.95%] [G loss: 0.7008088827133179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 81/86 [D loss: 0.6934959590435028, acc.: 50.05%] [G loss: 0.6963431239128113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 82/86 [D loss: 0.6955753862857819, acc.: 46.63%] [G loss: 0.6960384845733643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 83/86 [D loss: 0.6956369280815125, acc.: 46.34%] [G loss: 0.6998326778411865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 84/86 [D loss: 0.6932089030742645, acc.: 49.27%] [G loss: 0.7023584842681885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 85/86 [D loss: 0.6943617761135101, acc.: 48.54%] [G loss: 0.698085606098175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 86/86 [D loss: 0.6939417719841003, acc.: 47.36%] [G loss: 0.7009884119033813]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 1/86 [D loss: 0.693814605474472, acc.: 49.07%] [G loss: 0.6982062458992004]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 2/86 [D loss: 0.6948202550411224, acc.: 47.46%] [G loss: 0.6955289840698242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 3/86 [D loss: 0.6959442794322968, acc.: 45.07%] [G loss: 0.6981407403945923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 4/86 [D loss: 0.6938520669937134, acc.: 49.07%] [G loss: 0.7020286321640015]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 5/86 [D loss: 0.6940732598304749, acc.: 48.44%] [G loss: 0.6988381743431091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 6/86 [D loss: 0.6947092711925507, acc.: 47.17%] [G loss: 0.7006044983863831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 7/86 [D loss: 0.6942462921142578, acc.: 48.83%] [G loss: 0.6965246200561523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 8/86 [D loss: 0.6952612102031708, acc.: 47.56%] [G loss: 0.694975733757019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 9/86 [D loss: 0.6962119936943054, acc.: 44.38%] [G loss: 0.7007285356521606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 10/86 [D loss: 0.6929522454738617, acc.: 50.98%] [G loss: 0.701151430606842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 11/86 [D loss: 0.6948506534099579, acc.: 47.27%] [G loss: 0.6985514163970947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 12/86 [D loss: 0.6948589086532593, acc.: 48.34%] [G loss: 0.6995251774787903]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 13/86 [D loss: 0.6942940056324005, acc.: 48.44%] [G loss: 0.6983647346496582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 14/86 [D loss: 0.6947979032993317, acc.: 47.41%] [G loss: 0.6938415169715881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 15/86 [D loss: 0.6960830986499786, acc.: 45.17%] [G loss: 0.6995290517807007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 16/86 [D loss: 0.692034125328064, acc.: 52.29%] [G loss: 0.7018409371376038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 17/86 [D loss: 0.694439560174942, acc.: 48.93%] [G loss: 0.6994670629501343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 18/86 [D loss: 0.6948675811290741, acc.: 47.61%] [G loss: 0.7009169459342957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 19/86 [D loss: 0.6942216157913208, acc.: 49.02%] [G loss: 0.6986175179481506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 20/86 [D loss: 0.6938432157039642, acc.: 49.76%] [G loss: 0.6968555450439453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 21/86 [D loss: 0.6959621608257294, acc.: 45.56%] [G loss: 0.6980709433555603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 22/86 [D loss: 0.6933963596820831, acc.: 51.03%] [G loss: 0.700577437877655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 23/86 [D loss: 0.6933178305625916, acc.: 49.71%] [G loss: 0.7005054354667664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 24/86 [D loss: 0.6934974193572998, acc.: 51.07%] [G loss: 0.6992990970611572]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 25/86 [D loss: 0.6940023899078369, acc.: 49.71%] [G loss: 0.6989073157310486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 26/86 [D loss: 0.6939818859100342, acc.: 48.68%] [G loss: 0.6973814964294434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 27/86 [D loss: 0.6947032809257507, acc.: 47.80%] [G loss: 0.7002761363983154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 28/86 [D loss: 0.6943197250366211, acc.: 47.61%] [G loss: 0.7017273902893066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 29/86 [D loss: 0.6934820711612701, acc.: 50.59%] [G loss: 0.7014576196670532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 30/86 [D loss: 0.6945106685161591, acc.: 47.75%] [G loss: 0.7004565000534058]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 31/86 [D loss: 0.6934866309165955, acc.: 51.61%] [G loss: 0.7001208662986755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 32/86 [D loss: 0.6935390532016754, acc.: 50.29%] [G loss: 0.6988701820373535]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 33/86 [D loss: 0.6949106454849243, acc.: 48.49%] [G loss: 0.7011075615882874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 34/86 [D loss: 0.6935063600540161, acc.: 50.59%] [G loss: 0.7019017338752747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 35/86 [D loss: 0.6938577592372894, acc.: 48.29%] [G loss: 0.6997725963592529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 36/86 [D loss: 0.6940731406211853, acc.: 48.78%] [G loss: 0.7003136873245239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 37/86 [D loss: 0.6931066513061523, acc.: 50.29%] [G loss: 0.699108362197876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 38/86 [D loss: 0.6944540441036224, acc.: 48.63%] [G loss: 0.6960868239402771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 39/86 [D loss: 0.6957883238792419, acc.: 46.58%] [G loss: 0.6996716856956482]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 40/86 [D loss: 0.6937406063079834, acc.: 49.22%] [G loss: 0.7010090351104736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 41/86 [D loss: 0.6950710415840149, acc.: 46.92%] [G loss: 0.7000494003295898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 42/86 [D loss: 0.6948455274105072, acc.: 48.39%] [G loss: 0.6994316577911377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 43/86 [D loss: 0.6938557624816895, acc.: 47.95%] [G loss: 0.6988701820373535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 44/86 [D loss: 0.6946749985218048, acc.: 48.24%] [G loss: 0.6967716813087463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 45/86 [D loss: 0.6956994533538818, acc.: 46.09%] [G loss: 0.6996588110923767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 46/86 [D loss: 0.6943398714065552, acc.: 48.44%] [G loss: 0.7028775215148926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 47/86 [D loss: 0.6940862834453583, acc.: 47.80%] [G loss: 0.70060133934021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 48/86 [D loss: 0.6946655511856079, acc.: 47.17%] [G loss: 0.6999306678771973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 49/86 [D loss: 0.6938136219978333, acc.: 49.12%] [G loss: 0.7001523971557617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 50/86 [D loss: 0.694196343421936, acc.: 48.39%] [G loss: 0.6951340436935425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 51/86 [D loss: 0.6958386301994324, acc.: 46.73%] [G loss: 0.6973221302032471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 52/86 [D loss: 0.6937291324138641, acc.: 49.95%] [G loss: 0.7023499011993408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 53/86 [D loss: 0.694071501493454, acc.: 49.90%] [G loss: 0.6991084814071655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 54/86 [D loss: 0.6938141286373138, acc.: 49.80%] [G loss: 0.6995993256568909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 55/86 [D loss: 0.6946201622486115, acc.: 47.71%] [G loss: 0.6993662118911743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 56/86 [D loss: 0.6933077275753021, acc.: 50.88%] [G loss: 0.6974784135818481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 57/86 [D loss: 0.6957060396671295, acc.: 47.66%] [G loss: 0.6977002620697021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 58/86 [D loss: 0.69523486495018, acc.: 47.27%] [G loss: 0.7005212903022766]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 59/86 [D loss: 0.6927044987678528, acc.: 50.20%] [G loss: 0.6997805237770081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 60/86 [D loss: 0.6949129104614258, acc.: 48.49%] [G loss: 0.6993277668952942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 61/86 [D loss: 0.6935031414031982, acc.: 49.85%] [G loss: 0.6996742486953735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 62/86 [D loss: 0.6935563087463379, acc.: 49.80%] [G loss: 0.697913408279419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 63/86 [D loss: 0.6949678659439087, acc.: 47.17%] [G loss: 0.6981815695762634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 64/86 [D loss: 0.695021778345108, acc.: 47.12%] [G loss: 0.700375497341156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 65/86 [D loss: 0.6933884024620056, acc.: 50.44%] [G loss: 0.7008967399597168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 66/86 [D loss: 0.6940415799617767, acc.: 49.51%] [G loss: 0.6998384594917297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 67/86 [D loss: 0.6938379406929016, acc.: 49.66%] [G loss: 0.6998724341392517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 68/86 [D loss: 0.6936614811420441, acc.: 49.80%] [G loss: 0.6980345249176025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 69/86 [D loss: 0.6940035223960876, acc.: 49.90%] [G loss: 0.6990550756454468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 70/86 [D loss: 0.694257527589798, acc.: 47.75%] [G loss: 0.7003893852233887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 71/86 [D loss: 0.693211704492569, acc.: 49.71%] [G loss: 0.7002497315406799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 72/86 [D loss: 0.6948399543762207, acc.: 47.90%] [G loss: 0.7011913657188416]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 73/86 [D loss: 0.6939410865306854, acc.: 47.90%] [G loss: 0.69983971118927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 74/86 [D loss: 0.6937891244888306, acc.: 48.39%] [G loss: 0.6973021626472473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 75/86 [D loss: 0.6958142220973969, acc.: 45.95%] [G loss: 0.6988846063613892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 76/86 [D loss: 0.6940855979919434, acc.: 48.39%] [G loss: 0.7014604806900024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 77/86 [D loss: 0.6933726370334625, acc.: 49.80%] [G loss: 0.7006794810295105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 78/86 [D loss: 0.6941560804843903, acc.: 49.22%] [G loss: 0.6993994116783142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 79/86 [D loss: 0.6942464709281921, acc.: 47.85%] [G loss: 0.7004260420799255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 80/86 [D loss: 0.6930089890956879, acc.: 51.32%] [G loss: 0.696486234664917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 81/86 [D loss: 0.6960824728012085, acc.: 45.07%] [G loss: 0.6994192600250244]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 82/86 [D loss: 0.694165050983429, acc.: 48.10%] [G loss: 0.7031713724136353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 83/86 [D loss: 0.6934894621372223, acc.: 48.73%] [G loss: 0.7004458904266357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 84/86 [D loss: 0.694817066192627, acc.: 47.41%] [G loss: 0.700423538684845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 85/86 [D loss: 0.6934085786342621, acc.: 51.42%] [G loss: 0.700346052646637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 86/86 [D loss: 0.6938006579875946, acc.: 50.54%] [G loss: 0.6975036859512329]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 1/86 [D loss: 0.6962875127792358, acc.: 43.80%] [G loss: 0.6985499858856201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 2/86 [D loss: 0.6941010057926178, acc.: 47.22%] [G loss: 0.7041386961936951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 3/86 [D loss: 0.6930460035800934, acc.: 50.59%] [G loss: 0.7003589272499084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 4/86 [D loss: 0.6959935426712036, acc.: 44.53%] [G loss: 0.6993294954299927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 5/86 [D loss: 0.6944650113582611, acc.: 49.17%] [G loss: 0.6998060345649719]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 6/86 [D loss: 0.6939892172813416, acc.: 49.32%] [G loss: 0.6974751949310303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 7/86 [D loss: 0.6949910223484039, acc.: 48.24%] [G loss: 0.6909569501876831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 8/86 [D loss: 0.6957099437713623, acc.: 46.53%] [G loss: 0.704109787940979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 9/86 [D loss: 0.6917581856250763, acc.: 52.49%] [G loss: 0.7008233666419983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 10/86 [D loss: 0.6961683630943298, acc.: 45.70%] [G loss: 0.6973279714584351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 11/86 [D loss: 0.6955180466175079, acc.: 45.12%] [G loss: 0.6993969678878784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 12/86 [D loss: 0.6935285329818726, acc.: 51.46%] [G loss: 0.6984924674034119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 13/86 [D loss: 0.6947459280490875, acc.: 46.73%] [G loss: 0.6929695010185242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 14/86 [D loss: 0.6966589689254761, acc.: 43.75%] [G loss: 0.7002378702163696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 15/86 [D loss: 0.6921211183071136, acc.: 52.34%] [G loss: 0.702532172203064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 16/86 [D loss: 0.6954763829708099, acc.: 45.85%] [G loss: 0.6973949074745178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 17/86 [D loss: 0.6963973343372345, acc.: 44.09%] [G loss: 0.6985388994216919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 18/86 [D loss: 0.6937520802021027, acc.: 49.37%] [G loss: 0.6995338797569275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 19/86 [D loss: 0.693770706653595, acc.: 50.20%] [G loss: 0.694559633731842]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 20/86 [D loss: 0.6960818767547607, acc.: 46.63%] [G loss: 0.6968099474906921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 21/86 [D loss: 0.6942972242832184, acc.: 47.95%] [G loss: 0.7018890976905823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 22/86 [D loss: 0.6933545172214508, acc.: 49.56%] [G loss: 0.6989089250564575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 23/86 [D loss: 0.6940941214561462, acc.: 49.85%] [G loss: 0.7007176280021667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 24/86 [D loss: 0.6934657096862793, acc.: 50.10%] [G loss: 0.6990652084350586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 25/86 [D loss: 0.6942941844463348, acc.: 48.97%] [G loss: 0.698201596736908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 26/86 [D loss: 0.6933983564376831, acc.: 50.34%] [G loss: 0.6974436640739441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 27/86 [D loss: 0.6941027939319611, acc.: 48.29%] [G loss: 0.7015889883041382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 28/86 [D loss: 0.6935790181159973, acc.: 50.24%] [G loss: 0.701454758644104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 29/86 [D loss: 0.694346159696579, acc.: 48.93%] [G loss: 0.7009063959121704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 30/86 [D loss: 0.6941225826740265, acc.: 48.83%] [G loss: 0.6996161937713623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 31/86 [D loss: 0.6942099034786224, acc.: 48.05%] [G loss: 0.6992139220237732]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 32/86 [D loss: 0.6933998763561249, acc.: 50.88%] [G loss: 0.7007665038108826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 33/86 [D loss: 0.6929114162921906, acc.: 50.34%] [G loss: 0.7020465135574341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 34/86 [D loss: 0.6926083266735077, acc.: 51.12%] [G loss: 0.7024363875389099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 35/86 [D loss: 0.6934674382209778, acc.: 50.00%] [G loss: 0.701740026473999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 36/86 [D loss: 0.6940868198871613, acc.: 47.80%] [G loss: 0.7020822763442993]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 37/86 [D loss: 0.6931709945201874, acc.: 50.39%] [G loss: 0.7015798687934875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 38/86 [D loss: 0.6934293806552887, acc.: 50.68%] [G loss: 0.6993832588195801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 39/86 [D loss: 0.6940291523933411, acc.: 48.78%] [G loss: 0.7020299434661865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 40/86 [D loss: 0.6943075954914093, acc.: 47.41%] [G loss: 0.7026798725128174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 41/86 [D loss: 0.6942735612392426, acc.: 48.63%] [G loss: 0.7015334963798523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 42/86 [D loss: 0.6940064132213593, acc.: 50.15%] [G loss: 0.7011367082595825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 43/86 [D loss: 0.6935978531837463, acc.: 49.80%] [G loss: 0.700793981552124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 44/86 [D loss: 0.6941358745098114, acc.: 47.80%] [G loss: 0.7000241279602051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 45/86 [D loss: 0.6939319372177124, acc.: 49.41%] [G loss: 0.701565682888031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 46/86 [D loss: 0.6935136020183563, acc.: 49.32%] [G loss: 0.7015241980552673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 47/86 [D loss: 0.6938448846340179, acc.: 49.37%] [G loss: 0.700360119342804]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 48/86 [D loss: 0.6941822171211243, acc.: 48.97%] [G loss: 0.7003962397575378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 49/86 [D loss: 0.6928870975971222, acc.: 51.42%] [G loss: 0.6991496682167053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 50/86 [D loss: 0.6939749121665955, acc.: 48.93%] [G loss: 0.6975095272064209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 51/86 [D loss: 0.6945585310459137, acc.: 47.66%] [G loss: 0.6997731924057007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 52/86 [D loss: 0.6933186054229736, acc.: 49.32%] [G loss: 0.7016170620918274]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 53/86 [D loss: 0.6934648156166077, acc.: 50.39%] [G loss: 0.700324296951294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 54/86 [D loss: 0.693504273891449, acc.: 50.20%] [G loss: 0.7006011009216309]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 55/86 [D loss: 0.6926379203796387, acc.: 51.27%] [G loss: 0.699723482131958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 56/86 [D loss: 0.6934693455696106, acc.: 49.41%] [G loss: 0.6972571015357971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 57/86 [D loss: 0.6945920884609222, acc.: 48.14%] [G loss: 0.7003051042556763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 58/86 [D loss: 0.6942397356033325, acc.: 48.24%] [G loss: 0.7015864253044128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 59/86 [D loss: 0.6930323243141174, acc.: 50.24%] [G loss: 0.6998785138130188]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 60/86 [D loss: 0.693503737449646, acc.: 48.54%] [G loss: 0.6991145610809326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 61/86 [D loss: 0.6936643719673157, acc.: 49.95%] [G loss: 0.6997276544570923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 62/86 [D loss: 0.695325642824173, acc.: 46.83%] [G loss: 0.6960000395774841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 63/86 [D loss: 0.6942563354969025, acc.: 49.07%] [G loss: 0.698073148727417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 64/86 [D loss: 0.6938915848731995, acc.: 48.29%] [G loss: 0.7007024884223938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 65/86 [D loss: 0.6939613223075867, acc.: 48.34%] [G loss: 0.7001588940620422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 66/86 [D loss: 0.6939820051193237, acc.: 49.02%] [G loss: 0.6982126235961914]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 67/86 [D loss: 0.6932086050510406, acc.: 50.39%] [G loss: 0.6991326212882996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 68/86 [D loss: 0.6941955983638763, acc.: 48.24%] [G loss: 0.6989284157752991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 69/86 [D loss: 0.6945130825042725, acc.: 47.56%] [G loss: 0.6995705366134644]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 70/86 [D loss: 0.693289041519165, acc.: 48.00%] [G loss: 0.7018477916717529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 71/86 [D loss: 0.6935363113880157, acc.: 49.71%] [G loss: 0.7008697986602783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 72/86 [D loss: 0.693368524312973, acc.: 49.17%] [G loss: 0.6996631026268005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 73/86 [D loss: 0.6938289403915405, acc.: 49.17%] [G loss: 0.699279248714447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 74/86 [D loss: 0.6942354440689087, acc.: 47.66%] [G loss: 0.6971738934516907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 75/86 [D loss: 0.695574164390564, acc.: 46.44%] [G loss: 0.6991279125213623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 76/86 [D loss: 0.6927920281887054, acc.: 51.17%] [G loss: 0.7013272047042847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 77/86 [D loss: 0.693644642829895, acc.: 49.51%] [G loss: 0.6989152431488037]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 78/86 [D loss: 0.6944543719291687, acc.: 48.54%] [G loss: 0.6998169422149658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 79/86 [D loss: 0.6938726603984833, acc.: 49.41%] [G loss: 0.6998389959335327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 80/86 [D loss: 0.695138692855835, acc.: 47.07%] [G loss: 0.6974252462387085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 81/86 [D loss: 0.6949900090694427, acc.: 46.04%] [G loss: 0.7003637552261353]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 82/86 [D loss: 0.6928034424781799, acc.: 52.00%] [G loss: 0.7027051448822021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 83/86 [D loss: 0.6942954361438751, acc.: 48.05%] [G loss: 0.7005849480628967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 84/86 [D loss: 0.6939398050308228, acc.: 49.41%] [G loss: 0.7000098824501038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 85/86 [D loss: 0.6939273774623871, acc.: 48.88%] [G loss: 0.6985976099967957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 86/86 [D loss: 0.6942913234233856, acc.: 47.90%] [G loss: 0.6952511668205261]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 1/86 [D loss: 0.6957031786441803, acc.: 47.02%] [G loss: 0.7010976076126099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 2/86 [D loss: 0.6935106813907623, acc.: 49.51%] [G loss: 0.7002047896385193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 3/86 [D loss: 0.6942805051803589, acc.: 48.39%] [G loss: 0.6999252438545227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 4/86 [D loss: 0.6940235197544098, acc.: 48.44%] [G loss: 0.700313150882721]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 5/86 [D loss: 0.6932289302349091, acc.: 49.90%] [G loss: 0.6987144947052002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 6/86 [D loss: 0.6948270797729492, acc.: 48.49%] [G loss: 0.6970862150192261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 7/86 [D loss: 0.6940699815750122, acc.: 48.58%] [G loss: 0.7004843950271606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 8/86 [D loss: 0.6928784549236298, acc.: 50.54%] [G loss: 0.7025554776191711]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 9/86 [D loss: 0.6942307949066162, acc.: 48.34%] [G loss: 0.6996665596961975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 10/86 [D loss: 0.6941859126091003, acc.: 47.31%] [G loss: 0.7007930874824524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 11/86 [D loss: 0.69333216547966, acc.: 49.22%] [G loss: 0.6992276310920715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 12/86 [D loss: 0.6945046484470367, acc.: 47.75%] [G loss: 0.6963878870010376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 13/86 [D loss: 0.6943525075912476, acc.: 48.05%] [G loss: 0.7020301222801208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 14/86 [D loss: 0.6932159066200256, acc.: 49.85%] [G loss: 0.7008135914802551]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 15/86 [D loss: 0.6944110691547394, acc.: 48.29%] [G loss: 0.6990411281585693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 16/86 [D loss: 0.694146990776062, acc.: 49.56%] [G loss: 0.6999304890632629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 17/86 [D loss: 0.6941304802894592, acc.: 47.71%] [G loss: 0.6968147158622742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 18/86 [D loss: 0.6956291496753693, acc.: 46.63%] [G loss: 0.6980935335159302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 19/86 [D loss: 0.6938558518886566, acc.: 48.00%] [G loss: 0.7017154097557068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 20/86 [D loss: 0.6925807297229767, acc.: 52.05%] [G loss: 0.7010595798492432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 21/86 [D loss: 0.6943337321281433, acc.: 48.00%] [G loss: 0.7002784609794617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 22/86 [D loss: 0.6948112845420837, acc.: 48.49%] [G loss: 0.6994821429252625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 23/86 [D loss: 0.6931337714195251, acc.: 49.95%] [G loss: 0.6972149610519409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 24/86 [D loss: 0.694745808839798, acc.: 46.83%] [G loss: 0.6975825428962708]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 25/86 [D loss: 0.6930857300758362, acc.: 50.44%] [G loss: 0.7014016509056091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 26/86 [D loss: 0.6934858560562134, acc.: 50.20%] [G loss: 0.7004187107086182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 27/86 [D loss: 0.6943103671073914, acc.: 47.90%] [G loss: 0.7009553909301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 28/86 [D loss: 0.6936264634132385, acc.: 48.97%] [G loss: 0.7000184059143066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 29/86 [D loss: 0.6936336755752563, acc.: 50.29%] [G loss: 0.6946755051612854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 30/86 [D loss: 0.6973867416381836, acc.: 42.72%] [G loss: 0.6975783705711365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 31/86 [D loss: 0.6943891048431396, acc.: 47.66%] [G loss: 0.7035225629806519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 32/86 [D loss: 0.6933287680149078, acc.: 49.85%] [G loss: 0.6984684467315674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 33/86 [D loss: 0.6956265568733215, acc.: 46.68%] [G loss: 0.70058274269104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 34/86 [D loss: 0.6937403380870819, acc.: 49.66%] [G loss: 0.6993587613105774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 35/86 [D loss: 0.6934161186218262, acc.: 50.73%] [G loss: 0.6964249610900879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 36/86 [D loss: 0.6959706544876099, acc.: 45.61%] [G loss: 0.6969015598297119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 37/86 [D loss: 0.6949880421161652, acc.: 46.14%] [G loss: 0.7025559544563293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 38/86 [D loss: 0.6922095119953156, acc.: 52.54%] [G loss: 0.7002103328704834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 39/86 [D loss: 0.6946129500865936, acc.: 47.22%] [G loss: 0.698455274105072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 40/86 [D loss: 0.6943767666816711, acc.: 48.88%] [G loss: 0.7003744840621948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 41/86 [D loss: 0.6938119232654572, acc.: 48.73%] [G loss: 0.6969649195671082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 42/86 [D loss: 0.6947834193706512, acc.: 48.24%] [G loss: 0.6967788934707642]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 43/86 [D loss: 0.6945182383060455, acc.: 48.54%] [G loss: 0.7020387649536133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 44/86 [D loss: 0.6926329135894775, acc.: 50.98%] [G loss: 0.7015533447265625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 45/86 [D loss: 0.6945676505565643, acc.: 48.34%] [G loss: 0.7002602815628052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 46/86 [D loss: 0.6936840116977692, acc.: 49.51%] [G loss: 0.7002304792404175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 47/86 [D loss: 0.6937305927276611, acc.: 48.14%] [G loss: 0.6989681720733643]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 48/86 [D loss: 0.695294976234436, acc.: 47.07%] [G loss: 0.6956987380981445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 49/86 [D loss: 0.6953321993350983, acc.: 47.31%] [G loss: 0.7008407711982727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 50/86 [D loss: 0.6926749646663666, acc.: 51.46%] [G loss: 0.7011674642562866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 51/86 [D loss: 0.6941237151622772, acc.: 49.32%] [G loss: 0.6996392607688904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 52/86 [D loss: 0.6939952373504639, acc.: 49.22%] [G loss: 0.7015038728713989]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 53/86 [D loss: 0.693869948387146, acc.: 48.68%] [G loss: 0.699233889579773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 54/86 [D loss: 0.6948542594909668, acc.: 48.19%] [G loss: 0.6977308988571167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 55/86 [D loss: 0.6950890421867371, acc.: 47.36%] [G loss: 0.7013753652572632]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 56/86 [D loss: 0.6923636794090271, acc.: 52.54%] [G loss: 0.7022644281387329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 57/86 [D loss: 0.6938065588474274, acc.: 50.20%] [G loss: 0.6993277668952942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 58/86 [D loss: 0.6943149268627167, acc.: 49.32%] [G loss: 0.7005932927131653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 59/86 [D loss: 0.6936802268028259, acc.: 49.80%] [G loss: 0.6980253458023071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 60/86 [D loss: 0.6948001682758331, acc.: 48.19%] [G loss: 0.6976490616798401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 61/86 [D loss: 0.6952775418758392, acc.: 45.36%] [G loss: 0.7006574273109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 62/86 [D loss: 0.6929410696029663, acc.: 51.32%] [G loss: 0.7026336193084717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 63/86 [D loss: 0.6933421492576599, acc.: 50.05%] [G loss: 0.699183464050293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 64/86 [D loss: 0.6947534084320068, acc.: 47.61%] [G loss: 0.7004035115242004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 65/86 [D loss: 0.6930850744247437, acc.: 50.49%] [G loss: 0.6985389590263367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 66/86 [D loss: 0.6932428777217865, acc.: 49.02%] [G loss: 0.6975688338279724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 67/86 [D loss: 0.6936256289482117, acc.: 49.51%] [G loss: 0.7005658149719238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 68/86 [D loss: 0.6935530006885529, acc.: 50.10%] [G loss: 0.7008836269378662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 69/86 [D loss: 0.6938267648220062, acc.: 47.56%] [G loss: 0.7007722854614258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 70/86 [D loss: 0.6935803592205048, acc.: 49.22%] [G loss: 0.7005969285964966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 71/86 [D loss: 0.6934303343296051, acc.: 50.78%] [G loss: 0.6996254920959473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 72/86 [D loss: 0.6945140361785889, acc.: 47.56%] [G loss: 0.6998140811920166]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 73/86 [D loss: 0.6932936906814575, acc.: 49.41%] [G loss: 0.7000555396080017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 74/86 [D loss: 0.6931919753551483, acc.: 50.39%] [G loss: 0.700518012046814]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 75/86 [D loss: 0.6944547891616821, acc.: 47.90%] [G loss: 0.6999757885932922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 76/86 [D loss: 0.6936959326267242, acc.: 48.44%] [G loss: 0.7008497714996338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 77/86 [D loss: 0.6946936249732971, acc.: 47.27%] [G loss: 0.6984236836433411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 78/86 [D loss: 0.6942839026451111, acc.: 49.07%] [G loss: 0.6995562314987183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 79/86 [D loss: 0.694306343793869, acc.: 48.10%] [G loss: 0.70039963722229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 80/86 [D loss: 0.6938871741294861, acc.: 48.34%] [G loss: 0.699126660823822]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 81/86 [D loss: 0.6943907141685486, acc.: 47.31%] [G loss: 0.7000001668930054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 82/86 [D loss: 0.6940307915210724, acc.: 49.12%] [G loss: 0.7008964419364929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 83/86 [D loss: 0.6933861374855042, acc.: 50.49%] [G loss: 0.6973345875740051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 84/86 [D loss: 0.6944136619567871, acc.: 48.29%] [G loss: 0.6985179781913757]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 85/86 [D loss: 0.6933362483978271, acc.: 49.66%] [G loss: 0.7011126279830933]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 86/86 [D loss: 0.6928869485855103, acc.: 49.80%] [G loss: 0.7001718878746033]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 1/86 [D loss: 0.6946402490139008, acc.: 48.34%] [G loss: 0.699574887752533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 2/86 [D loss: 0.6931957602500916, acc.: 50.68%] [G loss: 0.6976447105407715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 3/86 [D loss: 0.6939257979393005, acc.: 47.56%] [G loss: 0.6955102682113647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 4/86 [D loss: 0.6960088014602661, acc.: 46.00%] [G loss: 0.7003664374351501]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 5/86 [D loss: 0.69292351603508, acc.: 50.34%] [G loss: 0.7012516856193542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 6/86 [D loss: 0.6940528452396393, acc.: 48.83%] [G loss: 0.7003867030143738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 7/86 [D loss: 0.6939162015914917, acc.: 49.41%] [G loss: 0.7007285952568054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 8/86 [D loss: 0.6937999129295349, acc.: 50.44%] [G loss: 0.6977704763412476]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 9/86 [D loss: 0.6949519217014313, acc.: 47.31%] [G loss: 0.6979416012763977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 10/86 [D loss: 0.694074809551239, acc.: 47.66%] [G loss: 0.702126145362854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 11/86 [D loss: 0.692723959684372, acc.: 51.61%] [G loss: 0.7005844116210938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 12/86 [D loss: 0.6948730945587158, acc.: 46.48%] [G loss: 0.7001029849052429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 13/86 [D loss: 0.6932371556758881, acc.: 50.73%] [G loss: 0.6987765431404114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 14/86 [D loss: 0.6941246688365936, acc.: 47.90%] [G loss: 0.6955370306968689]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 15/86 [D loss: 0.6954148709774017, acc.: 46.00%] [G loss: 0.696631908416748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 16/86 [D loss: 0.6949067115783691, acc.: 47.22%] [G loss: 0.7026000618934631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 17/86 [D loss: 0.693516343832016, acc.: 49.66%] [G loss: 0.6994377374649048]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 18/86 [D loss: 0.6949307322502136, acc.: 48.63%] [G loss: 0.6994727849960327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 19/86 [D loss: 0.6943354904651642, acc.: 47.85%] [G loss: 0.6992244124412537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 20/86 [D loss: 0.6942256689071655, acc.: 49.37%] [G loss: 0.6962407231330872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 21/86 [D loss: 0.6965906322002411, acc.: 45.41%] [G loss: 0.6987023949623108]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 22/86 [D loss: 0.6934513449668884, acc.: 50.10%] [G loss: 0.7030704021453857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 23/86 [D loss: 0.693244606256485, acc.: 50.15%] [G loss: 0.6996446847915649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 24/86 [D loss: 0.6943130493164062, acc.: 47.66%] [G loss: 0.7000799179077148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 25/86 [D loss: 0.6930089592933655, acc.: 50.49%] [G loss: 0.6993117928504944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 26/86 [D loss: 0.6936082541942596, acc.: 48.29%] [G loss: 0.6952199339866638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 27/86 [D loss: 0.6975953578948975, acc.: 43.41%] [G loss: 0.6970593333244324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 28/86 [D loss: 0.6941633224487305, acc.: 48.88%] [G loss: 0.7021856904029846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 29/86 [D loss: 0.6937114894390106, acc.: 49.71%] [G loss: 0.699556291103363]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 30/86 [D loss: 0.6952532231807709, acc.: 46.88%] [G loss: 0.7010911703109741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 31/86 [D loss: 0.6940288841724396, acc.: 48.44%] [G loss: 0.6996235847473145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 32/86 [D loss: 0.6931676268577576, acc.: 50.93%] [G loss: 0.6966376304626465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 33/86 [D loss: 0.6962213218212128, acc.: 45.36%] [G loss: 0.6992590427398682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 34/86 [D loss: 0.6934644877910614, acc.: 49.85%] [G loss: 0.7028061747550964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 35/86 [D loss: 0.6935751736164093, acc.: 50.34%] [G loss: 0.7011474370956421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 36/86 [D loss: 0.694505363702774, acc.: 47.90%] [G loss: 0.7011831998825073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 37/86 [D loss: 0.6938621699810028, acc.: 49.17%] [G loss: 0.7008446455001831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 38/86 [D loss: 0.6945223212242126, acc.: 47.75%] [G loss: 0.6985304951667786]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 39/86 [D loss: 0.6946226954460144, acc.: 47.02%] [G loss: 0.7001644968986511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 40/86 [D loss: 0.693876713514328, acc.: 49.27%] [G loss: 0.7035276889801025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 41/86 [D loss: 0.6930315494537354, acc.: 50.88%] [G loss: 0.701782763004303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 42/86 [D loss: 0.6938514411449432, acc.: 48.24%] [G loss: 0.7013410329818726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 43/86 [D loss: 0.6934655904769897, acc.: 49.37%] [G loss: 0.7009558081626892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 44/86 [D loss: 0.6950371861457825, acc.: 46.78%] [G loss: 0.698971688747406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 45/86 [D loss: 0.6940463185310364, acc.: 48.93%] [G loss: 0.7007709741592407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 46/86 [D loss: 0.6927710473537445, acc.: 51.03%] [G loss: 0.7022954821586609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 47/86 [D loss: 0.6940581202507019, acc.: 47.56%] [G loss: 0.6992815732955933]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 48/86 [D loss: 0.6939005255699158, acc.: 49.02%] [G loss: 0.7008413076400757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 49/86 [D loss: 0.6932356357574463, acc.: 50.83%] [G loss: 0.6973708868026733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 50/86 [D loss: 0.6947521567344666, acc.: 47.41%] [G loss: 0.6976264119148254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 51/86 [D loss: 0.6951158046722412, acc.: 47.02%] [G loss: 0.7021335959434509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 52/86 [D loss: 0.6925648748874664, acc.: 51.71%] [G loss: 0.6999198794364929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 53/86 [D loss: 0.6949309408664703, acc.: 46.29%] [G loss: 0.6997179985046387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 54/86 [D loss: 0.6939718127250671, acc.: 48.44%] [G loss: 0.6998511552810669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 55/86 [D loss: 0.6931935846805573, acc.: 50.63%] [G loss: 0.6970318555831909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 56/86 [D loss: 0.6961551606655121, acc.: 44.82%] [G loss: 0.697434663772583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 57/86 [D loss: 0.6936832666397095, acc.: 48.93%] [G loss: 0.701261579990387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 58/86 [D loss: 0.6943126022815704, acc.: 47.80%] [G loss: 0.7000564336776733]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 59/86 [D loss: 0.6952262222766876, acc.: 47.27%] [G loss: 0.6991807818412781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 60/86 [D loss: 0.6943152546882629, acc.: 48.63%] [G loss: 0.7003641128540039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 61/86 [D loss: 0.6932858228683472, acc.: 48.68%] [G loss: 0.6952937841415405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 62/86 [D loss: 0.6967732906341553, acc.: 45.41%] [G loss: 0.697027325630188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 63/86 [D loss: 0.6947080492973328, acc.: 46.73%] [G loss: 0.7018420100212097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 64/86 [D loss: 0.6937477290630341, acc.: 49.02%] [G loss: 0.6995513439178467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 65/86 [D loss: 0.6946625709533691, acc.: 47.51%] [G loss: 0.7005621194839478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 66/86 [D loss: 0.692962646484375, acc.: 50.63%] [G loss: 0.7012598514556885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 67/86 [D loss: 0.6933822929859161, acc.: 49.76%] [G loss: 0.695500910282135]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 68/86 [D loss: 0.6958296000957489, acc.: 46.00%] [G loss: 0.6965558528900146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 69/86 [D loss: 0.693967878818512, acc.: 48.44%] [G loss: 0.7023192644119263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 70/86 [D loss: 0.6939253509044647, acc.: 49.85%] [G loss: 0.6991856098175049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 71/86 [D loss: 0.6940974593162537, acc.: 48.10%] [G loss: 0.6995255351066589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 72/86 [D loss: 0.6938690543174744, acc.: 49.37%] [G loss: 0.7004087567329407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 73/86 [D loss: 0.6938444972038269, acc.: 49.56%] [G loss: 0.6999106407165527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 74/86 [D loss: 0.6947218775749207, acc.: 47.80%] [G loss: 0.699287474155426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 75/86 [D loss: 0.6944258511066437, acc.: 47.46%] [G loss: 0.7026959657669067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 76/86 [D loss: 0.6943382918834686, acc.: 48.05%] [G loss: 0.7001687288284302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 77/86 [D loss: 0.6946238577365875, acc.: 47.90%] [G loss: 0.7017716765403748]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 78/86 [D loss: 0.6934689879417419, acc.: 50.49%] [G loss: 0.7000346183776855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 79/86 [D loss: 0.6941218376159668, acc.: 49.71%] [G loss: 0.6980316042900085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 80/86 [D loss: 0.6946002244949341, acc.: 47.66%] [G loss: 0.6979619860649109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 81/86 [D loss: 0.6949160993099213, acc.: 47.41%] [G loss: 0.702534019947052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 82/86 [D loss: 0.6943936944007874, acc.: 47.66%] [G loss: 0.7000652551651001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 83/86 [D loss: 0.6938109993934631, acc.: 48.97%] [G loss: 0.6997238993644714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 84/86 [D loss: 0.6937507390975952, acc.: 49.85%] [G loss: 0.699273943901062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 85/86 [D loss: 0.6940293312072754, acc.: 47.85%] [G loss: 0.6971226334571838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 86/86 [D loss: 0.6948604583740234, acc.: 46.83%] [G loss: 0.6993535757064819]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 1/86 [D loss: 0.6945880353450775, acc.: 47.27%] [G loss: 0.7024877667427063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 2/86 [D loss: 0.6930876970291138, acc.: 51.37%] [G loss: 0.7015479207038879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 3/86 [D loss: 0.6949009895324707, acc.: 48.14%] [G loss: 0.7002807855606079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 4/86 [D loss: 0.6943180561065674, acc.: 46.83%] [G loss: 0.6990014314651489]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 5/86 [D loss: 0.6931195557117462, acc.: 50.98%] [G loss: 0.697667121887207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 6/86 [D loss: 0.6944298446178436, acc.: 48.19%] [G loss: 0.698401689529419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 7/86 [D loss: 0.69443279504776, acc.: 47.46%] [G loss: 0.702077329158783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 8/86 [D loss: 0.6934382617473602, acc.: 49.17%] [G loss: 0.7001953721046448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 9/86 [D loss: 0.694783478975296, acc.: 47.17%] [G loss: 0.6997898817062378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 10/86 [D loss: 0.6944699287414551, acc.: 47.27%] [G loss: 0.7000961303710938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 11/86 [D loss: 0.6944045722484589, acc.: 48.14%] [G loss: 0.6976368427276611]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 12/86 [D loss: 0.6946784555912018, acc.: 47.61%] [G loss: 0.6992515325546265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 13/86 [D loss: 0.6941340267658234, acc.: 49.27%] [G loss: 0.7028105854988098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 14/86 [D loss: 0.6929078698158264, acc.: 51.07%] [G loss: 0.6995519995689392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 15/86 [D loss: 0.6947861909866333, acc.: 46.04%] [G loss: 0.7018927335739136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 16/86 [D loss: 0.6933350265026093, acc.: 49.07%] [G loss: 0.7000979781150818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 17/86 [D loss: 0.6931545734405518, acc.: 50.83%] [G loss: 0.6966532468795776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 18/86 [D loss: 0.6956377625465393, acc.: 46.04%] [G loss: 0.6989700794219971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 19/86 [D loss: 0.6932496428489685, acc.: 50.20%] [G loss: 0.7028506994247437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 20/86 [D loss: 0.6934152841567993, acc.: 50.29%] [G loss: 0.700679361820221]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 21/86 [D loss: 0.6944676339626312, acc.: 47.41%] [G loss: 0.7009091377258301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 22/86 [D loss: 0.6935731768608093, acc.: 49.90%] [G loss: 0.6996899843215942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 23/86 [D loss: 0.6937122642993927, acc.: 50.49%] [G loss: 0.6965498328208923]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 24/86 [D loss: 0.6954165101051331, acc.: 45.51%] [G loss: 0.7004784345626831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 25/86 [D loss: 0.6923168301582336, acc.: 51.37%] [G loss: 0.7012385725975037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 26/86 [D loss: 0.6941995620727539, acc.: 48.10%] [G loss: 0.6998389959335327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 27/86 [D loss: 0.6946702301502228, acc.: 47.56%] [G loss: 0.7007203102111816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 28/86 [D loss: 0.6929472386837006, acc.: 50.15%] [G loss: 0.6975477933883667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 29/86 [D loss: 0.6943469941616058, acc.: 48.49%] [G loss: 0.6952379941940308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 30/86 [D loss: 0.6958061158657074, acc.: 45.70%] [G loss: 0.7018623352050781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 31/86 [D loss: 0.6928707957267761, acc.: 50.00%] [G loss: 0.7014121413230896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 32/86 [D loss: 0.6945459842681885, acc.: 46.97%] [G loss: 0.6983542442321777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 33/86 [D loss: 0.6941699683666229, acc.: 47.66%] [G loss: 0.6995350122451782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 34/86 [D loss: 0.6933057606220245, acc.: 49.12%] [G loss: 0.6973230838775635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 35/86 [D loss: 0.6938615441322327, acc.: 50.20%] [G loss: 0.6952345371246338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 36/86 [D loss: 0.6963483393192291, acc.: 46.63%] [G loss: 0.6994889974594116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 37/86 [D loss: 0.6926514804363251, acc.: 50.78%] [G loss: 0.7013245820999146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 38/86 [D loss: 0.6933798789978027, acc.: 49.85%] [G loss: 0.6980712413787842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 39/86 [D loss: 0.6935150325298309, acc.: 49.71%] [G loss: 0.6996716260910034]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 40/86 [D loss: 0.6920183897018433, acc.: 52.29%] [G loss: 0.6978089213371277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 41/86 [D loss: 0.6944399476051331, acc.: 47.51%] [G loss: 0.6950466632843018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 42/86 [D loss: 0.6954973340034485, acc.: 47.27%] [G loss: 0.6993318796157837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 43/86 [D loss: 0.6930895447731018, acc.: 49.95%] [G loss: 0.6992218494415283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 44/86 [D loss: 0.6947988867759705, acc.: 47.80%] [G loss: 0.6982653737068176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 45/86 [D loss: 0.6953124105930328, acc.: 46.14%] [G loss: 0.6985453963279724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 46/86 [D loss: 0.6930394768714905, acc.: 50.34%] [G loss: 0.6973839402198792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 47/86 [D loss: 0.6952795386314392, acc.: 47.12%] [G loss: 0.6956638693809509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 48/86 [D loss: 0.6945851743221283, acc.: 48.10%] [G loss: 0.7003306150436401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 49/86 [D loss: 0.6926905810832977, acc.: 50.83%] [G loss: 0.7009656429290771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 50/86 [D loss: 0.6946952939033508, acc.: 47.27%] [G loss: 0.7001731395721436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 51/86 [D loss: 0.6936300098896027, acc.: 49.07%] [G loss: 0.698440432548523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 52/86 [D loss: 0.693628191947937, acc.: 49.51%] [G loss: 0.698546826839447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 53/86 [D loss: 0.6944979727268219, acc.: 46.68%] [G loss: 0.6973206400871277]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 54/86 [D loss: 0.6948930025100708, acc.: 46.58%] [G loss: 0.6999248266220093]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 55/86 [D loss: 0.6930083632469177, acc.: 50.63%] [G loss: 0.7021955847740173]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 56/86 [D loss: 0.6941533088684082, acc.: 48.29%] [G loss: 0.7013857364654541]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 57/86 [D loss: 0.6934714913368225, acc.: 49.51%] [G loss: 0.7004452347755432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 58/86 [D loss: 0.6942921578884125, acc.: 49.12%] [G loss: 0.7000830173492432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 59/86 [D loss: 0.6937683820724487, acc.: 50.05%] [G loss: 0.6983669400215149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 60/86 [D loss: 0.6942014992237091, acc.: 49.61%] [G loss: 0.7002567052841187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 61/86 [D loss: 0.6934425830841064, acc.: 49.51%] [G loss: 0.701242983341217]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 62/86 [D loss: 0.6938571929931641, acc.: 49.76%] [G loss: 0.7004859447479248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 63/86 [D loss: 0.6942247450351715, acc.: 47.31%] [G loss: 0.70076584815979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 64/86 [D loss: 0.6936580240726471, acc.: 49.22%] [G loss: 0.6994138956069946]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 65/86 [D loss: 0.69463911652565, acc.: 48.05%] [G loss: 0.6995331645011902]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 66/86 [D loss: 0.6935549378395081, acc.: 49.61%] [G loss: 0.7019526958465576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 67/86 [D loss: 0.6931446194648743, acc.: 51.95%] [G loss: 0.7006280422210693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 68/86 [D loss: 0.6931229829788208, acc.: 50.34%] [G loss: 0.7000678777694702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 69/86 [D loss: 0.6945131421089172, acc.: 48.49%] [G loss: 0.7008406519889832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 70/86 [D loss: 0.6937345862388611, acc.: 50.88%] [G loss: 0.6989122629165649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 71/86 [D loss: 0.6938654482364655, acc.: 49.02%] [G loss: 0.6986296772956848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 72/86 [D loss: 0.6952951550483704, acc.: 46.48%] [G loss: 0.6998234987258911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 73/86 [D loss: 0.6939119398593903, acc.: 47.90%] [G loss: 0.7013617753982544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 74/86 [D loss: 0.6935904920101166, acc.: 49.80%] [G loss: 0.7011711597442627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 75/86 [D loss: 0.693576842546463, acc.: 48.63%] [G loss: 0.7003732919692993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 76/86 [D loss: 0.6931297481060028, acc.: 50.78%] [G loss: 0.6984412670135498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 77/86 [D loss: 0.6945489645004272, acc.: 47.12%] [G loss: 0.6977499723434448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 78/86 [D loss: 0.6942669451236725, acc.: 48.63%] [G loss: 0.7010995745658875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 79/86 [D loss: 0.6924535036087036, acc.: 52.25%] [G loss: 0.7014027833938599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 80/86 [D loss: 0.693783164024353, acc.: 48.54%] [G loss: 0.6999008655548096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 81/86 [D loss: 0.693680077791214, acc.: 49.37%] [G loss: 0.699892520904541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 82/86 [D loss: 0.6943285465240479, acc.: 46.73%] [G loss: 0.6970826387405396]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 83/86 [D loss: 0.6945748627185822, acc.: 48.44%] [G loss: 0.69816654920578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 84/86 [D loss: 0.6945578157901764, acc.: 48.39%] [G loss: 0.7012699842453003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 85/86 [D loss: 0.6925076246261597, acc.: 51.22%] [G loss: 0.6999175548553467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 86/86 [D loss: 0.6938014030456543, acc.: 49.90%] [G loss: 0.6989021301269531]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 1/86 [D loss: 0.6939831078052521, acc.: 49.76%] [G loss: 0.7004899978637695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 2/86 [D loss: 0.6928232312202454, acc.: 51.27%] [G loss: 0.6959904432296753]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 3/86 [D loss: 0.6947528421878815, acc.: 48.00%] [G loss: 0.6971234679222107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 4/86 [D loss: 0.6934642195701599, acc.: 49.76%] [G loss: 0.7021524906158447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 5/86 [D loss: 0.6927036345005035, acc.: 51.37%] [G loss: 0.6988449692726135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 6/86 [D loss: 0.694675624370575, acc.: 46.53%] [G loss: 0.6995728015899658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 7/86 [D loss: 0.6936909854412079, acc.: 48.93%] [G loss: 0.7002345323562622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 8/86 [D loss: 0.6935996115207672, acc.: 49.61%] [G loss: 0.6954526305198669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 9/86 [D loss: 0.6957466304302216, acc.: 46.00%] [G loss: 0.6967917680740356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 10/86 [D loss: 0.6945973336696625, acc.: 46.68%] [G loss: 0.7016271352767944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 11/86 [D loss: 0.6931841373443604, acc.: 49.66%] [G loss: 0.6978710293769836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 12/86 [D loss: 0.6942015290260315, acc.: 45.85%] [G loss: 0.6989811658859253]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 13/86 [D loss: 0.6930761635303497, acc.: 50.88%] [G loss: 0.6991664171218872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 14/86 [D loss: 0.6940675377845764, acc.: 49.27%] [G loss: 0.6974567770957947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 15/86 [D loss: 0.6950972676277161, acc.: 47.56%] [G loss: 0.6967934370040894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 16/86 [D loss: 0.6945346891880035, acc.: 46.58%] [G loss: 0.701949954032898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 17/86 [D loss: 0.6937979161739349, acc.: 49.66%] [G loss: 0.6997392177581787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 18/86 [D loss: 0.6947533190250397, acc.: 46.39%] [G loss: 0.6988329887390137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 19/86 [D loss: 0.6935392916202545, acc.: 49.56%] [G loss: 0.6999708414077759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 20/86 [D loss: 0.6927542686462402, acc.: 50.39%] [G loss: 0.6968954801559448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 21/86 [D loss: 0.6957333087921143, acc.: 46.24%] [G loss: 0.6971489191055298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 22/86 [D loss: 0.6940816938877106, acc.: 48.88%] [G loss: 0.702495813369751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 23/86 [D loss: 0.6932964622974396, acc.: 51.37%] [G loss: 0.6989499926567078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 24/86 [D loss: 0.6961471736431122, acc.: 44.63%] [G loss: 0.6983760595321655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 25/86 [D loss: 0.6931051909923553, acc.: 50.05%] [G loss: 0.6997178792953491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 26/86 [D loss: 0.6940064430236816, acc.: 48.34%] [G loss: 0.6959188580513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 27/86 [D loss: 0.6956667602062225, acc.: 46.88%] [G loss: 0.6968616247177124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 28/86 [D loss: 0.6939667761325836, acc.: 49.12%] [G loss: 0.701016902923584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 29/86 [D loss: 0.6933644711971283, acc.: 49.07%] [G loss: 0.6990761160850525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 30/86 [D loss: 0.6939705610275269, acc.: 48.58%] [G loss: 0.6993207931518555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 31/86 [D loss: 0.6936300992965698, acc.: 49.95%] [G loss: 0.6988609433174133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 32/86 [D loss: 0.6935807466506958, acc.: 49.07%] [G loss: 0.6986055374145508]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 33/86 [D loss: 0.6948411762714386, acc.: 46.29%] [G loss: 0.6980515718460083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 34/86 [D loss: 0.6938787996768951, acc.: 49.07%] [G loss: 0.7015618681907654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 35/86 [D loss: 0.6934868395328522, acc.: 48.49%] [G loss: 0.7001574635505676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 36/86 [D loss: 0.6943149864673615, acc.: 47.90%] [G loss: 0.7004483938217163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 37/86 [D loss: 0.6932470202445984, acc.: 49.56%] [G loss: 0.6986825466156006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 38/86 [D loss: 0.6944977641105652, acc.: 47.85%] [G loss: 0.6967326998710632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 39/86 [D loss: 0.6943051517009735, acc.: 48.29%] [G loss: 0.7011884450912476]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 40/86 [D loss: 0.6927299499511719, acc.: 51.27%] [G loss: 0.7001312375068665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 41/86 [D loss: 0.6943047940731049, acc.: 47.41%] [G loss: 0.6988353729248047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 42/86 [D loss: 0.6936752796173096, acc.: 48.14%] [G loss: 0.6994410157203674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 43/86 [D loss: 0.6937004625797272, acc.: 49.22%] [G loss: 0.6989117860794067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 44/86 [D loss: 0.6938593983650208, acc.: 48.44%] [G loss: 0.696550726890564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 45/86 [D loss: 0.6942458748817444, acc.: 48.14%] [G loss: 0.7000605463981628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 46/86 [D loss: 0.6926573812961578, acc.: 50.93%] [G loss: 0.7016595602035522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 47/86 [D loss: 0.6934790313243866, acc.: 49.02%] [G loss: 0.700797975063324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 48/86 [D loss: 0.6931446194648743, acc.: 51.07%] [G loss: 0.7000442743301392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 49/86 [D loss: 0.6930290460586548, acc.: 51.12%] [G loss: 0.699209988117218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 50/86 [D loss: 0.6948866546154022, acc.: 47.41%] [G loss: 0.6991167068481445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 51/86 [D loss: 0.6937357485294342, acc.: 49.46%] [G loss: 0.7004637122154236]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 52/86 [D loss: 0.69259974360466, acc.: 52.69%] [G loss: 0.7004094123840332]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 53/86 [D loss: 0.693833738565445, acc.: 48.83%] [G loss: 0.6994103789329529]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 54/86 [D loss: 0.6938194632530212, acc.: 48.83%] [G loss: 0.7001069784164429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 55/86 [D loss: 0.6932663321495056, acc.: 50.68%] [G loss: 0.6980484127998352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 56/86 [D loss: 0.6941751837730408, acc.: 48.39%] [G loss: 0.6981488466262817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 57/86 [D loss: 0.6941999793052673, acc.: 49.37%] [G loss: 0.7006773948669434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 58/86 [D loss: 0.6931447982788086, acc.: 50.59%] [G loss: 0.6998254060745239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 59/86 [D loss: 0.693710595369339, acc.: 49.27%] [G loss: 0.6982815265655518]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 60/86 [D loss: 0.6940750777721405, acc.: 49.51%] [G loss: 0.6998529434204102]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 61/86 [D loss: 0.6935853362083435, acc.: 48.63%] [G loss: 0.696047306060791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 62/86 [D loss: 0.693932294845581, acc.: 48.54%] [G loss: 0.6976335048675537]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 63/86 [D loss: 0.6938095092773438, acc.: 49.90%] [G loss: 0.7014099359512329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 64/86 [D loss: 0.6925510466098785, acc.: 52.29%] [G loss: 0.6995579600334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 65/86 [D loss: 0.6949580311775208, acc.: 46.92%] [G loss: 0.6990090608596802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 66/86 [D loss: 0.6935190856456757, acc.: 49.41%] [G loss: 0.6985217332839966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 67/86 [D loss: 0.6939394772052765, acc.: 48.49%] [G loss: 0.6981064081192017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 68/86 [D loss: 0.6942468881607056, acc.: 49.71%] [G loss: 0.6985094547271729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 69/86 [D loss: 0.6930770874023438, acc.: 49.27%] [G loss: 0.7011091709136963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 70/86 [D loss: 0.6935677528381348, acc.: 48.10%] [G loss: 0.6994602084159851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 71/86 [D loss: 0.6939032971858978, acc.: 48.49%] [G loss: 0.7000369429588318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 72/86 [D loss: 0.6933408081531525, acc.: 49.27%] [G loss: 0.6982588171958923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 73/86 [D loss: 0.6935429573059082, acc.: 48.58%] [G loss: 0.6964989304542542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 74/86 [D loss: 0.6949440836906433, acc.: 47.61%] [G loss: 0.6984133720397949]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 75/86 [D loss: 0.6934071481227875, acc.: 50.63%] [G loss: 0.7002562284469604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 76/86 [D loss: 0.6930599212646484, acc.: 49.95%] [G loss: 0.7001996636390686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 77/86 [D loss: 0.6940273344516754, acc.: 48.78%] [G loss: 0.700341522693634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 78/86 [D loss: 0.6934380531311035, acc.: 48.97%] [G loss: 0.6997787952423096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 79/86 [D loss: 0.6948925256729126, acc.: 47.31%] [G loss: 0.696193277835846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 80/86 [D loss: 0.6962246596813202, acc.: 45.31%] [G loss: 0.6991942524909973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 81/86 [D loss: 0.6932685971260071, acc.: 48.97%] [G loss: 0.7019892930984497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 82/86 [D loss: 0.6932147741317749, acc.: 51.51%] [G loss: 0.6990994215011597]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 83/86 [D loss: 0.6941916942596436, acc.: 48.05%] [G loss: 0.7004774212837219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 84/86 [D loss: 0.6934337615966797, acc.: 50.59%] [G loss: 0.6998299360275269]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 85/86 [D loss: 0.6942512392997742, acc.: 48.58%] [G loss: 0.6951122879981995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 86/86 [D loss: 0.6961438059806824, acc.: 45.56%] [G loss: 0.7014111280441284]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 1/86 [D loss: 0.6918633282184601, acc.: 52.93%] [G loss: 0.70125812292099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 2/86 [D loss: 0.6942175030708313, acc.: 48.49%] [G loss: 0.6982740759849548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 3/86 [D loss: 0.6940414309501648, acc.: 49.02%] [G loss: 0.7004714012145996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 4/86 [D loss: 0.692306786775589, acc.: 51.95%] [G loss: 0.7001073360443115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 5/86 [D loss: 0.6942894458770752, acc.: 49.02%] [G loss: 0.6942895650863647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 6/86 [D loss: 0.6967139542102814, acc.: 45.56%] [G loss: 0.7011629343032837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 7/86 [D loss: 0.6920972168445587, acc.: 51.61%] [G loss: 0.7003589868545532]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 8/86 [D loss: 0.6945307552814484, acc.: 47.17%] [G loss: 0.6977171897888184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 9/86 [D loss: 0.6948341429233551, acc.: 47.66%] [G loss: 0.6999631524085999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 10/86 [D loss: 0.6929516494274139, acc.: 50.15%] [G loss: 0.6983909010887146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 11/86 [D loss: 0.6946271061897278, acc.: 48.44%] [G loss: 0.6948977708816528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 12/86 [D loss: 0.6954054832458496, acc.: 46.78%] [G loss: 0.6984173655509949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 13/86 [D loss: 0.6918637454509735, acc.: 52.44%] [G loss: 0.7009503245353699]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 14/86 [D loss: 0.6933896541595459, acc.: 49.76%] [G loss: 0.698626697063446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 15/86 [D loss: 0.694848358631134, acc.: 47.12%] [G loss: 0.6992453336715698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 16/86 [D loss: 0.6938863396644592, acc.: 50.68%] [G loss: 0.6996808648109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 17/86 [D loss: 0.6925274431705475, acc.: 52.93%] [G loss: 0.6974031925201416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 18/86 [D loss: 0.6945848762989044, acc.: 48.44%] [G loss: 0.6961644291877747]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 19/86 [D loss: 0.6933994293212891, acc.: 50.00%] [G loss: 0.7032694816589355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 20/86 [D loss: 0.6931403875350952, acc.: 50.10%] [G loss: 0.7001317739486694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 21/86 [D loss: 0.6940309405326843, acc.: 48.68%] [G loss: 0.7001980543136597]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 22/86 [D loss: 0.6933882832527161, acc.: 49.32%] [G loss: 0.7002851963043213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 23/86 [D loss: 0.6928504109382629, acc.: 51.12%] [G loss: 0.697944164276123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 24/86 [D loss: 0.6945893168449402, acc.: 47.95%] [G loss: 0.6973005533218384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 25/86 [D loss: 0.692653626203537, acc.: 52.20%] [G loss: 0.701744794845581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 26/86 [D loss: 0.6930583417415619, acc.: 49.61%] [G loss: 0.7004858255386353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 27/86 [D loss: 0.6945143938064575, acc.: 49.27%] [G loss: 0.7006953358650208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 28/86 [D loss: 0.6930617094039917, acc.: 51.17%] [G loss: 0.6989873647689819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 29/86 [D loss: 0.6936146020889282, acc.: 49.61%] [G loss: 0.6985255479812622]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 30/86 [D loss: 0.6935519576072693, acc.: 49.46%] [G loss: 0.7003925442695618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 31/86 [D loss: 0.6930830776691437, acc.: 50.83%] [G loss: 0.7006517648696899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 32/86 [D loss: 0.6926813721656799, acc.: 50.29%] [G loss: 0.699163556098938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 33/86 [D loss: 0.6940109431743622, acc.: 48.63%] [G loss: 0.700721025466919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 34/86 [D loss: 0.6928472816944122, acc.: 51.86%] [G loss: 0.6990329027175903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 35/86 [D loss: 0.6938095986843109, acc.: 48.93%] [G loss: 0.6977006793022156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 36/86 [D loss: 0.6943365335464478, acc.: 48.83%] [G loss: 0.7001539468765259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 37/86 [D loss: 0.6925519108772278, acc.: 49.90%] [G loss: 0.7006932497024536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 38/86 [D loss: 0.6933167278766632, acc.: 49.71%] [G loss: 0.7007846236228943]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 39/86 [D loss: 0.6939736604690552, acc.: 48.24%] [G loss: 0.6998428702354431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 40/86 [D loss: 0.6926978826522827, acc.: 51.61%] [G loss: 0.6983545422554016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 41/86 [D loss: 0.6942093670368195, acc.: 47.85%] [G loss: 0.6974412798881531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 42/86 [D loss: 0.6946148872375488, acc.: 47.95%] [G loss: 0.7005534768104553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 43/86 [D loss: 0.6924454867839813, acc.: 51.66%] [G loss: 0.7017132043838501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 44/86 [D loss: 0.6935098171234131, acc.: 49.51%] [G loss: 0.7006336450576782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 45/86 [D loss: 0.6932409703731537, acc.: 50.20%] [G loss: 0.7007904052734375]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 46/86 [D loss: 0.6929329633712769, acc.: 51.12%] [G loss: 0.6991451382637024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 47/86 [D loss: 0.6949054002761841, acc.: 46.53%] [G loss: 0.697548508644104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 48/86 [D loss: 0.6944288909435272, acc.: 49.85%] [G loss: 0.7014281153678894]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 49/86 [D loss: 0.6927315294742584, acc.: 51.51%] [G loss: 0.7005009055137634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 50/86 [D loss: 0.6933300793170929, acc.: 49.51%] [G loss: 0.6995458006858826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 51/86 [D loss: 0.6937532424926758, acc.: 49.61%] [G loss: 0.7005759477615356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 52/86 [D loss: 0.6929667294025421, acc.: 49.85%] [G loss: 0.6996912956237793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 53/86 [D loss: 0.6939443051815033, acc.: 48.58%] [G loss: 0.6963989734649658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 54/86 [D loss: 0.6944617629051208, acc.: 48.39%] [G loss: 0.7021273374557495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 55/86 [D loss: 0.6927689909934998, acc.: 50.39%] [G loss: 0.700236439704895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 56/86 [D loss: 0.6940067410469055, acc.: 48.19%] [G loss: 0.6994428634643555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 57/86 [D loss: 0.69258251786232, acc.: 51.03%] [G loss: 0.699988842010498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 58/86 [D loss: 0.6922232806682587, acc.: 54.05%] [G loss: 0.6962981820106506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 59/86 [D loss: 0.6951062083244324, acc.: 46.83%] [G loss: 0.695749819278717]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 60/86 [D loss: 0.6926746070384979, acc.: 50.98%] [G loss: 0.7019504308700562]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 61/86 [D loss: 0.6924781799316406, acc.: 50.98%] [G loss: 0.6995625495910645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 62/86 [D loss: 0.6955662071704865, acc.: 43.55%] [G loss: 0.696461021900177]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 63/86 [D loss: 0.6932981908321381, acc.: 50.15%] [G loss: 0.6999669671058655]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 64/86 [D loss: 0.6922913491725922, acc.: 51.22%] [G loss: 0.6964256763458252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 65/86 [D loss: 0.6954775452613831, acc.: 46.73%] [G loss: 0.6961289048194885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 66/86 [D loss: 0.6939746141433716, acc.: 49.61%] [G loss: 0.7015680074691772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 67/86 [D loss: 0.6921435296535492, acc.: 52.98%] [G loss: 0.6999558210372925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 68/86 [D loss: 0.694685310125351, acc.: 48.19%] [G loss: 0.6994878649711609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 69/86 [D loss: 0.6933832168579102, acc.: 47.95%] [G loss: 0.7007448673248291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 70/86 [D loss: 0.6943087875843048, acc.: 48.44%] [G loss: 0.6977617740631104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 71/86 [D loss: 0.6944732069969177, acc.: 48.68%] [G loss: 0.696082592010498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 72/86 [D loss: 0.6945946216583252, acc.: 47.71%] [G loss: 0.7013388276100159]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 73/86 [D loss: 0.6923063099384308, acc.: 51.03%] [G loss: 0.7003530263900757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 74/86 [D loss: 0.6942364871501923, acc.: 48.97%] [G loss: 0.6983428597450256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 75/86 [D loss: 0.6939870417118073, acc.: 48.39%] [G loss: 0.6990849375724792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 76/86 [D loss: 0.6925045847892761, acc.: 52.00%] [G loss: 0.6990587115287781]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 77/86 [D loss: 0.6947710812091827, acc.: 46.58%] [G loss: 0.6957780718803406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 78/86 [D loss: 0.6939106583595276, acc.: 49.02%] [G loss: 0.7015661597251892]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 79/86 [D loss: 0.6927290558815002, acc.: 51.66%] [G loss: 0.7002002000808716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 80/86 [D loss: 0.6933163702487946, acc.: 50.15%] [G loss: 0.6998057961463928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 81/86 [D loss: 0.6939082145690918, acc.: 49.56%] [G loss: 0.7002089023590088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 82/86 [D loss: 0.692867785692215, acc.: 52.25%] [G loss: 0.6970922350883484]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 83/86 [D loss: 0.6938716769218445, acc.: 49.27%] [G loss: 0.6961144208908081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 84/86 [D loss: 0.6939558386802673, acc.: 49.22%] [G loss: 0.7006703615188599]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 85/86 [D loss: 0.6918719112873077, acc.: 53.03%] [G loss: 0.7006824016571045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 86/86 [D loss: 0.6945773959159851, acc.: 47.66%] [G loss: 0.6989965438842773]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 1/86 [D loss: 0.692572683095932, acc.: 50.59%] [G loss: 0.7004453539848328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 2/86 [D loss: 0.6923549175262451, acc.: 50.98%] [G loss: 0.6993038654327393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 3/86 [D loss: 0.6949166357517242, acc.: 47.95%] [G loss: 0.698974072933197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 4/86 [D loss: 0.694430410861969, acc.: 48.19%] [G loss: 0.700961709022522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 5/86 [D loss: 0.6925372183322906, acc.: 51.95%] [G loss: 0.6990006566047668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 6/86 [D loss: 0.6934220492839813, acc.: 49.41%] [G loss: 0.6999004483222961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 7/86 [D loss: 0.693210631608963, acc.: 51.66%] [G loss: 0.7005818486213684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 8/86 [D loss: 0.6936073005199432, acc.: 49.27%] [G loss: 0.700958788394928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 9/86 [D loss: 0.6933799982070923, acc.: 48.78%] [G loss: 0.69862961769104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 10/86 [D loss: 0.6941631436347961, acc.: 48.19%] [G loss: 0.7010816931724548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 11/86 [D loss: 0.6929018497467041, acc.: 50.73%] [G loss: 0.7017635107040405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 12/86 [D loss: 0.6926158964633942, acc.: 51.71%] [G loss: 0.6995854377746582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 13/86 [D loss: 0.6938576698303223, acc.: 48.44%] [G loss: 0.700080156326294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 14/86 [D loss: 0.6931058466434479, acc.: 51.07%] [G loss: 0.6991840600967407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 15/86 [D loss: 0.6937406957149506, acc.: 49.95%] [G loss: 0.697551965713501]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 16/86 [D loss: 0.6940982639789581, acc.: 48.44%] [G loss: 0.6988773345947266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 17/86 [D loss: 0.6930035352706909, acc.: 50.83%] [G loss: 0.702531099319458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 18/86 [D loss: 0.6929495334625244, acc.: 50.00%] [G loss: 0.6999465823173523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 19/86 [D loss: 0.6931213438510895, acc.: 50.24%] [G loss: 0.6998857259750366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 20/86 [D loss: 0.6932108998298645, acc.: 50.24%] [G loss: 0.6993860006332397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 21/86 [D loss: 0.6928508877754211, acc.: 51.17%] [G loss: 0.6977124810218811]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 22/86 [D loss: 0.6948306858539581, acc.: 47.27%] [G loss: 0.69951331615448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 23/86 [D loss: 0.6932033002376556, acc.: 49.95%] [G loss: 0.7002259492874146]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 24/86 [D loss: 0.6934627890586853, acc.: 50.29%] [G loss: 0.6993004083633423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 25/86 [D loss: 0.6925229430198669, acc.: 50.73%] [G loss: 0.698211133480072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 26/86 [D loss: 0.693194180727005, acc.: 50.24%] [G loss: 0.69954514503479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 27/86 [D loss: 0.6937752664089203, acc.: 50.98%] [G loss: 0.6985833644866943]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 28/86 [D loss: 0.6948967576026917, acc.: 46.34%] [G loss: 0.700258731842041]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 29/86 [D loss: 0.6938614249229431, acc.: 49.02%] [G loss: 0.7024790644645691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 30/86 [D loss: 0.693072497844696, acc.: 50.88%] [G loss: 0.700179398059845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 31/86 [D loss: 0.6934333145618439, acc.: 50.54%] [G loss: 0.7011630535125732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 32/86 [D loss: 0.6937244534492493, acc.: 48.83%] [G loss: 0.6991345286369324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 33/86 [D loss: 0.694768100976944, acc.: 47.31%] [G loss: 0.698717474937439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 34/86 [D loss: 0.6936220526695251, acc.: 49.90%] [G loss: 0.7002578377723694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 35/86 [D loss: 0.6926920711994171, acc.: 50.78%] [G loss: 0.7009980082511902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 36/86 [D loss: 0.693405270576477, acc.: 49.80%] [G loss: 0.6999902725219727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 37/86 [D loss: 0.6939120888710022, acc.: 50.10%] [G loss: 0.7004823684692383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 38/86 [D loss: 0.6928795874118805, acc.: 50.93%] [G loss: 0.6987183690071106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 39/86 [D loss: 0.6941118240356445, acc.: 48.93%] [G loss: 0.6975893378257751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 40/86 [D loss: 0.6935155689716339, acc.: 49.41%] [G loss: 0.7007755637168884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 41/86 [D loss: 0.6933972537517548, acc.: 50.59%] [G loss: 0.7013581991195679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 42/86 [D loss: 0.6937785148620605, acc.: 48.88%] [G loss: 0.7001188397407532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 43/86 [D loss: 0.6931240856647491, acc.: 49.80%] [G loss: 0.7014889717102051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 44/86 [D loss: 0.6934953927993774, acc.: 50.63%] [G loss: 0.6984213590621948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 45/86 [D loss: 0.6950041055679321, acc.: 48.68%] [G loss: 0.6984532475471497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 46/86 [D loss: 0.6934887766838074, acc.: 50.34%] [G loss: 0.7012573480606079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 47/86 [D loss: 0.6918984949588776, acc.: 52.73%] [G loss: 0.6997183561325073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 48/86 [D loss: 0.6944607496261597, acc.: 48.63%] [G loss: 0.7001574039459229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 49/86 [D loss: 0.6932675838470459, acc.: 50.54%] [G loss: 0.7007180452346802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 50/86 [D loss: 0.6939446032047272, acc.: 48.29%] [G loss: 0.6995049715042114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 51/86 [D loss: 0.6945580840110779, acc.: 47.61%] [G loss: 0.6983349919319153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 52/86 [D loss: 0.6935270726680756, acc.: 50.73%] [G loss: 0.70115065574646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 53/86 [D loss: 0.6942538321018219, acc.: 47.07%] [G loss: 0.7001460790634155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 54/86 [D loss: 0.694340705871582, acc.: 48.54%] [G loss: 0.7000094652175903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 55/86 [D loss: 0.6936188042163849, acc.: 49.37%] [G loss: 0.6996983885765076]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 56/86 [D loss: 0.6938541829586029, acc.: 49.41%] [G loss: 0.6990697979927063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 57/86 [D loss: 0.6937984526157379, acc.: 49.27%] [G loss: 0.700094997882843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 58/86 [D loss: 0.6929876506328583, acc.: 50.98%] [G loss: 0.6996082067489624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 59/86 [D loss: 0.6936309635639191, acc.: 49.46%] [G loss: 0.7001086473464966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 60/86 [D loss: 0.6942948400974274, acc.: 47.90%] [G loss: 0.7014274597167969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 61/86 [D loss: 0.693015068769455, acc.: 51.51%] [G loss: 0.6998173594474792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 62/86 [D loss: 0.6933439075946808, acc.: 50.49%] [G loss: 0.7002631425857544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 63/86 [D loss: 0.6929089426994324, acc.: 50.59%] [G loss: 0.7018989324569702]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 64/86 [D loss: 0.6930201947689056, acc.: 50.54%] [G loss: 0.7011662721633911]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 65/86 [D loss: 0.6935253739356995, acc.: 49.32%] [G loss: 0.7008669972419739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 66/86 [D loss: 0.6934401988983154, acc.: 49.07%] [G loss: 0.6986318230628967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 67/86 [D loss: 0.6927898526191711, acc.: 51.07%] [G loss: 0.6991626620292664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 68/86 [D loss: 0.6940439939498901, acc.: 49.22%] [G loss: 0.6995442509651184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 69/86 [D loss: 0.6938057541847229, acc.: 48.39%] [G loss: 0.7011746168136597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 70/86 [D loss: 0.692315012216568, acc.: 51.81%] [G loss: 0.6992039084434509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 71/86 [D loss: 0.693583071231842, acc.: 50.39%] [G loss: 0.6991815567016602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 72/86 [D loss: 0.6938228905200958, acc.: 48.54%] [G loss: 0.7002159357070923]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 73/86 [D loss: 0.6932355165481567, acc.: 50.73%] [G loss: 0.6977193355560303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 74/86 [D loss: 0.6938602924346924, acc.: 48.34%] [G loss: 0.7000218033790588]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 75/86 [D loss: 0.6926735043525696, acc.: 52.00%] [G loss: 0.7008225917816162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 76/86 [D loss: 0.6934807002544403, acc.: 49.27%] [G loss: 0.6995761394500732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 77/86 [D loss: 0.6940988004207611, acc.: 48.63%] [G loss: 0.7004823088645935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 78/86 [D loss: 0.6932488977909088, acc.: 50.34%] [G loss: 0.6988303661346436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 79/86 [D loss: 0.6937786936759949, acc.: 49.90%] [G loss: 0.6987365484237671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 80/86 [D loss: 0.694118082523346, acc.: 49.02%] [G loss: 0.6999809741973877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 81/86 [D loss: 0.6928587853908539, acc.: 51.17%] [G loss: 0.7012308835983276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 82/86 [D loss: 0.6931217908859253, acc.: 49.95%] [G loss: 0.6998748779296875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 83/86 [D loss: 0.6933285593986511, acc.: 48.97%] [G loss: 0.699813187122345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 84/86 [D loss: 0.69336798787117, acc.: 50.59%] [G loss: 0.6991842985153198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 85/86 [D loss: 0.6934807300567627, acc.: 50.05%] [G loss: 0.6983991861343384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 86/86 [D loss: 0.6937640905380249, acc.: 49.41%] [G loss: 0.6989758610725403]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 44/200, Batch 1/86 [D loss: 0.6932885944843292, acc.: 50.29%] [G loss: 0.700806200504303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 2/86 [D loss: 0.6927591860294342, acc.: 51.51%] [G loss: 0.7004295587539673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 3/86 [D loss: 0.6928781867027283, acc.: 49.56%] [G loss: 0.6995916366577148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 4/86 [D loss: 0.6930641829967499, acc.: 51.17%] [G loss: 0.6991168260574341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 5/86 [D loss: 0.6940015554428101, acc.: 49.41%] [G loss: 0.7002413272857666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 6/86 [D loss: 0.6926739811897278, acc.: 51.51%] [G loss: 0.7016519904136658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 7/86 [D loss: 0.693034291267395, acc.: 51.71%] [G loss: 0.7006287574768066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 8/86 [D loss: 0.6928355991840363, acc.: 51.27%] [G loss: 0.7001852989196777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 9/86 [D loss: 0.6928122043609619, acc.: 50.49%] [G loss: 0.6995383501052856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 10/86 [D loss: 0.6936671435832977, acc.: 48.24%] [G loss: 0.6984758377075195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 11/86 [D loss: 0.6935745179653168, acc.: 50.29%] [G loss: 0.7008392214775085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 12/86 [D loss: 0.6929708123207092, acc.: 49.95%] [G loss: 0.7012226581573486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 13/86 [D loss: 0.6937722861766815, acc.: 49.02%] [G loss: 0.7008012533187866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 14/86 [D loss: 0.6932583749294281, acc.: 50.15%] [G loss: 0.7018329501152039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 15/86 [D loss: 0.6924259066581726, acc.: 52.15%] [G loss: 0.6996685862541199]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 16/86 [D loss: 0.69320148229599, acc.: 49.66%] [G loss: 0.7006303668022156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 17/86 [D loss: 0.6936183869838715, acc.: 49.61%] [G loss: 0.7015531063079834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 18/86 [D loss: 0.6935631930828094, acc.: 49.37%] [G loss: 0.701145350933075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 19/86 [D loss: 0.6931371092796326, acc.: 50.54%] [G loss: 0.7011069655418396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 20/86 [D loss: 0.6930796504020691, acc.: 50.93%] [G loss: 0.6997907757759094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 21/86 [D loss: 0.6937626898288727, acc.: 49.51%] [G loss: 0.698948085308075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 22/86 [D loss: 0.6939977407455444, acc.: 49.76%] [G loss: 0.6996534466743469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 23/86 [D loss: 0.6929676532745361, acc.: 49.71%] [G loss: 0.6998041868209839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 24/86 [D loss: 0.6924405992031097, acc.: 52.10%] [G loss: 0.7006564140319824]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 25/86 [D loss: 0.6931246519088745, acc.: 49.12%] [G loss: 0.7019135355949402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 26/86 [D loss: 0.6929800808429718, acc.: 50.83%] [G loss: 0.7017273306846619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 27/86 [D loss: 0.693010538816452, acc.: 49.95%] [G loss: 0.7001562118530273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 28/86 [D loss: 0.6931928992271423, acc.: 50.78%] [G loss: 0.701349139213562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 29/86 [D loss: 0.6918657124042511, acc.: 52.49%] [G loss: 0.7014518976211548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 30/86 [D loss: 0.6931636929512024, acc.: 50.39%] [G loss: 0.7016136646270752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 31/86 [D loss: 0.6928378343582153, acc.: 50.59%] [G loss: 0.7010641098022461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 32/86 [D loss: 0.6930999755859375, acc.: 49.32%] [G loss: 0.7012402415275574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 33/86 [D loss: 0.6939161717891693, acc.: 48.19%] [G loss: 0.7013863921165466]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 34/86 [D loss: 0.6936396658420563, acc.: 50.34%] [G loss: 0.7016427516937256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 35/86 [D loss: 0.6941538751125336, acc.: 48.39%] [G loss: 0.7015668749809265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 36/86 [D loss: 0.6934567093849182, acc.: 48.58%] [G loss: 0.7016541361808777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 37/86 [D loss: 0.6939761638641357, acc.: 48.93%] [G loss: 0.7003927826881409]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 38/86 [D loss: 0.6933075785636902, acc.: 50.20%] [G loss: 0.7000638842582703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 39/86 [D loss: 0.693992555141449, acc.: 47.41%] [G loss: 0.6986491680145264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 40/86 [D loss: 0.6928243637084961, acc.: 50.29%] [G loss: 0.7015116214752197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 41/86 [D loss: 0.6925179958343506, acc.: 50.93%] [G loss: 0.7000903487205505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 42/86 [D loss: 0.6933475434780121, acc.: 50.39%] [G loss: 0.6995447874069214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 43/86 [D loss: 0.6931049227714539, acc.: 50.63%] [G loss: 0.7011232376098633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 44/86 [D loss: 0.6939541101455688, acc.: 49.12%] [G loss: 0.6993928551673889]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 45/86 [D loss: 0.6934719681739807, acc.: 49.80%] [G loss: 0.701185941696167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 46/86 [D loss: 0.6932756006717682, acc.: 50.29%] [G loss: 0.7013919949531555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 47/86 [D loss: 0.6930795609951019, acc.: 51.46%] [G loss: 0.7012275457382202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 48/86 [D loss: 0.6932144463062286, acc.: 49.41%] [G loss: 0.701550304889679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 49/86 [D loss: 0.6933868229389191, acc.: 49.41%] [G loss: 0.7003426551818848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 50/86 [D loss: 0.692648857831955, acc.: 51.42%] [G loss: 0.7013940811157227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 51/86 [D loss: 0.6933817863464355, acc.: 50.44%] [G loss: 0.7011074423789978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 52/86 [D loss: 0.6938866972923279, acc.: 50.20%] [G loss: 0.7008967399597168]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 53/86 [D loss: 0.6935951411724091, acc.: 49.32%] [G loss: 0.7012249231338501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 54/86 [D loss: 0.6932011246681213, acc.: 49.56%] [G loss: 0.699831485748291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 55/86 [D loss: 0.693083643913269, acc.: 50.78%] [G loss: 0.6994174122810364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 56/86 [D loss: 0.6928631365299225, acc.: 51.07%] [G loss: 0.7002502083778381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 57/86 [D loss: 0.6938642859458923, acc.: 48.39%] [G loss: 0.7010494470596313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 58/86 [D loss: 0.6937559843063354, acc.: 48.00%] [G loss: 0.7018677592277527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 59/86 [D loss: 0.6929886341094971, acc.: 49.66%] [G loss: 0.7003015875816345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 60/86 [D loss: 0.6933617293834686, acc.: 49.66%] [G loss: 0.6999114155769348]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 61/86 [D loss: 0.6926104724407196, acc.: 52.25%] [G loss: 0.7010215520858765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 62/86 [D loss: 0.6930933892726898, acc.: 50.34%] [G loss: 0.7000486254692078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 63/86 [D loss: 0.6928446888923645, acc.: 51.90%] [G loss: 0.7006893157958984]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 64/86 [D loss: 0.6950121223926544, acc.: 46.88%] [G loss: 0.7004806995391846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 65/86 [D loss: 0.692770779132843, acc.: 52.73%] [G loss: 0.7016148567199707]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 66/86 [D loss: 0.6927166879177094, acc.: 49.61%] [G loss: 0.7012686729431152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 67/86 [D loss: 0.6924391686916351, acc.: 52.54%] [G loss: 0.7004809379577637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 68/86 [D loss: 0.6928120851516724, acc.: 51.86%] [G loss: 0.6993470191955566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 69/86 [D loss: 0.6935368478298187, acc.: 49.76%] [G loss: 0.700701892375946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 70/86 [D loss: 0.6924619972705841, acc.: 51.76%] [G loss: 0.7001723647117615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 71/86 [D loss: 0.6939891278743744, acc.: 47.61%] [G loss: 0.7004307508468628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 72/86 [D loss: 0.6925829946994781, acc.: 51.56%] [G loss: 0.7008541822433472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 73/86 [D loss: 0.6936821341514587, acc.: 50.10%] [G loss: 0.6999427080154419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 74/86 [D loss: 0.6933812201023102, acc.: 49.32%] [G loss: 0.700592041015625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 75/86 [D loss: 0.6921472251415253, acc.: 51.22%] [G loss: 0.7005853056907654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 76/86 [D loss: 0.6929854452610016, acc.: 51.71%] [G loss: 0.7010980844497681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 77/86 [D loss: 0.6923228800296783, acc.: 51.46%] [G loss: 0.7017823457717896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 78/86 [D loss: 0.6934115886688232, acc.: 48.19%] [G loss: 0.7001120448112488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 79/86 [D loss: 0.6935803890228271, acc.: 49.61%] [G loss: 0.7002590894699097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 80/86 [D loss: 0.6931368410587311, acc.: 49.56%] [G loss: 0.7005729079246521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 81/86 [D loss: 0.6934270560741425, acc.: 48.88%] [G loss: 0.7015753984451294]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 82/86 [D loss: 0.6926822066307068, acc.: 50.59%] [G loss: 0.7020623683929443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 83/86 [D loss: 0.6927199959754944, acc.: 51.37%] [G loss: 0.7019317746162415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 84/86 [D loss: 0.6921487152576447, acc.: 52.10%] [G loss: 0.702204704284668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 85/86 [D loss: 0.6930650770664215, acc.: 51.17%] [G loss: 0.7012318968772888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 86/86 [D loss: 0.6934637725353241, acc.: 50.73%] [G loss: 0.6998810768127441]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 1/86 [D loss: 0.692067414522171, acc.: 52.05%] [G loss: 0.7011322975158691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 2/86 [D loss: 0.6929269731044769, acc.: 50.88%] [G loss: 0.7015372514724731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 3/86 [D loss: 0.693457841873169, acc.: 50.73%] [G loss: 0.7010176181793213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 4/86 [D loss: 0.6925654709339142, acc.: 52.25%] [G loss: 0.7004021406173706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 5/86 [D loss: 0.6935323178768158, acc.: 49.12%] [G loss: 0.700390100479126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 6/86 [D loss: 0.693502813577652, acc.: 50.54%] [G loss: 0.7006959915161133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 7/86 [D loss: 0.6925651133060455, acc.: 51.07%] [G loss: 0.699799656867981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 8/86 [D loss: 0.6929572820663452, acc.: 50.78%] [G loss: 0.7013302445411682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 9/86 [D loss: 0.6925182342529297, acc.: 50.93%] [G loss: 0.7006683349609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 10/86 [D loss: 0.6929061412811279, acc.: 50.88%] [G loss: 0.701583981513977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 11/86 [D loss: 0.6938271522521973, acc.: 49.07%] [G loss: 0.7008888125419617]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 12/86 [D loss: 0.6921256482601166, acc.: 53.08%] [G loss: 0.7017755508422852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 13/86 [D loss: 0.6926854848861694, acc.: 51.66%] [G loss: 0.701287567615509]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 14/86 [D loss: 0.6922870874404907, acc.: 52.73%] [G loss: 0.7001062035560608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 15/86 [D loss: 0.6934729218482971, acc.: 50.24%] [G loss: 0.6991531252861023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 16/86 [D loss: 0.6930570602416992, acc.: 51.17%] [G loss: 0.7013128995895386]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 17/86 [D loss: 0.6929270923137665, acc.: 51.27%] [G loss: 0.7020330429077148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 18/86 [D loss: 0.6937671899795532, acc.: 49.51%] [G loss: 0.7012722492218018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 19/86 [D loss: 0.6929918527603149, acc.: 50.73%] [G loss: 0.7005425095558167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 20/86 [D loss: 0.6929164528846741, acc.: 50.78%] [G loss: 0.7014076709747314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 21/86 [D loss: 0.692037969827652, acc.: 52.83%] [G loss: 0.7009475231170654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 22/86 [D loss: 0.6932621598243713, acc.: 49.56%] [G loss: 0.7003949880599976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 23/86 [D loss: 0.6925207078456879, acc.: 50.54%] [G loss: 0.7012288570404053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 24/86 [D loss: 0.6930999457836151, acc.: 50.24%] [G loss: 0.7017695903778076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 25/86 [D loss: 0.693336695432663, acc.: 50.10%] [G loss: 0.701008677482605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 26/86 [D loss: 0.6929002702236176, acc.: 51.46%] [G loss: 0.701739490032196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 27/86 [D loss: 0.6929735541343689, acc.: 51.95%] [G loss: 0.7010870575904846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 28/86 [D loss: 0.6918958127498627, acc.: 53.27%] [G loss: 0.7021315097808838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 29/86 [D loss: 0.6930740773677826, acc.: 51.32%] [G loss: 0.7018332481384277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 30/86 [D loss: 0.6935370564460754, acc.: 50.05%] [G loss: 0.701223611831665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 31/86 [D loss: 0.6930312514305115, acc.: 50.73%] [G loss: 0.7014905214309692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 32/86 [D loss: 0.6929529309272766, acc.: 50.83%] [G loss: 0.7012113928794861]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 33/86 [D loss: 0.6933520436286926, acc.: 49.90%] [G loss: 0.7005082964897156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 34/86 [D loss: 0.6934005618095398, acc.: 50.20%] [G loss: 0.7010849118232727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 35/86 [D loss: 0.6926409006118774, acc.: 51.42%] [G loss: 0.701219379901886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 36/86 [D loss: 0.692802369594574, acc.: 51.37%] [G loss: 0.7004047632217407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 37/86 [D loss: 0.6937576532363892, acc.: 48.78%] [G loss: 0.7003253102302551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 38/86 [D loss: 0.6934754252433777, acc.: 49.32%] [G loss: 0.700223982334137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 39/86 [D loss: 0.6936059892177582, acc.: 48.78%] [G loss: 0.6992948055267334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 40/86 [D loss: 0.6935263574123383, acc.: 49.80%] [G loss: 0.7012631893157959]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 41/86 [D loss: 0.6930682063102722, acc.: 51.03%] [G loss: 0.7012455463409424]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 42/86 [D loss: 0.6932457089424133, acc.: 50.00%] [G loss: 0.6997655630111694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 43/86 [D loss: 0.693547785282135, acc.: 48.78%] [G loss: 0.6995592713356018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 44/86 [D loss: 0.6936221122741699, acc.: 49.12%] [G loss: 0.7018654942512512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 45/86 [D loss: 0.6928826570510864, acc.: 51.03%] [G loss: 0.7012995481491089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 46/86 [D loss: 0.6929181218147278, acc.: 50.83%] [G loss: 0.7013380527496338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 47/86 [D loss: 0.6926537752151489, acc.: 50.20%] [G loss: 0.7009222507476807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 48/86 [D loss: 0.6936722695827484, acc.: 49.66%] [G loss: 0.7003162503242493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 49/86 [D loss: 0.693837970495224, acc.: 49.41%] [G loss: 0.6999040842056274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 50/86 [D loss: 0.6931926906108856, acc.: 50.93%] [G loss: 0.7015594840049744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 51/86 [D loss: 0.6929613947868347, acc.: 50.34%] [G loss: 0.7023259997367859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 52/86 [D loss: 0.6924709677696228, acc.: 50.98%] [G loss: 0.7005273103713989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 53/86 [D loss: 0.6931193768978119, acc.: 49.66%] [G loss: 0.70135098695755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 54/86 [D loss: 0.6942022144794464, acc.: 48.10%] [G loss: 0.7007384300231934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 55/86 [D loss: 0.6931184828281403, acc.: 49.07%] [G loss: 0.7001661062240601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 56/86 [D loss: 0.6929247081279755, acc.: 50.15%] [G loss: 0.7015324831008911]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 57/86 [D loss: 0.6930265724658966, acc.: 51.17%] [G loss: 0.7022873759269714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 58/86 [D loss: 0.6923505365848541, acc.: 52.15%] [G loss: 0.7001137733459473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 59/86 [D loss: 0.6928491294384003, acc.: 51.66%] [G loss: 0.6995475888252258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 60/86 [D loss: 0.6924859583377838, acc.: 50.73%] [G loss: 0.7026952505111694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 61/86 [D loss: 0.6922257840633392, acc.: 52.39%] [G loss: 0.7008157968521118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 62/86 [D loss: 0.6928395330905914, acc.: 50.49%] [G loss: 0.7014384269714355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 63/86 [D loss: 0.6928003430366516, acc.: 51.51%] [G loss: 0.7011350989341736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 64/86 [D loss: 0.6925082504749298, acc.: 52.05%] [G loss: 0.6996840238571167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 65/86 [D loss: 0.692975252866745, acc.: 51.42%] [G loss: 0.6999849081039429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 66/86 [D loss: 0.6930857598781586, acc.: 49.90%] [G loss: 0.6997990608215332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 67/86 [D loss: 0.6927708387374878, acc.: 51.71%] [G loss: 0.7007644176483154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 68/86 [D loss: 0.6929101347923279, acc.: 51.03%] [G loss: 0.7013086080551147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 69/86 [D loss: 0.6931528747081757, acc.: 49.76%] [G loss: 0.700903594493866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 70/86 [D loss: 0.6932799518108368, acc.: 49.71%] [G loss: 0.7013760209083557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 71/86 [D loss: 0.6938065886497498, acc.: 49.12%] [G loss: 0.7008296251296997]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 72/86 [D loss: 0.6922711133956909, acc.: 51.95%] [G loss: 0.7015809416770935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 73/86 [D loss: 0.6927444040775299, acc.: 51.66%] [G loss: 0.7018275260925293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 74/86 [D loss: 0.6924908757209778, acc.: 51.86%] [G loss: 0.7005407214164734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 75/86 [D loss: 0.6927123963832855, acc.: 51.90%] [G loss: 0.7018899321556091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 76/86 [D loss: 0.6929022371768951, acc.: 50.83%] [G loss: 0.7019943594932556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 77/86 [D loss: 0.692367672920227, acc.: 51.61%] [G loss: 0.7028183937072754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 78/86 [D loss: 0.6926105320453644, acc.: 51.56%] [G loss: 0.7004638910293579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 79/86 [D loss: 0.6936571598052979, acc.: 48.88%] [G loss: 0.7021583318710327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 80/86 [D loss: 0.692606121301651, acc.: 52.05%] [G loss: 0.7015189528465271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 81/86 [D loss: 0.6939773559570312, acc.: 48.44%] [G loss: 0.7001051902770996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 82/86 [D loss: 0.6933577358722687, acc.: 49.66%] [G loss: 0.7004556059837341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 83/86 [D loss: 0.6932390034198761, acc.: 50.39%] [G loss: 0.7007504105567932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 84/86 [D loss: 0.691995233297348, acc.: 52.73%] [G loss: 0.7014604210853577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 85/86 [D loss: 0.692575603723526, acc.: 52.59%] [G loss: 0.7011030316352844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 86/86 [D loss: 0.6931126415729523, acc.: 49.71%] [G loss: 0.7006763815879822]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 1/86 [D loss: 0.6932684183120728, acc.: 50.54%] [G loss: 0.700961172580719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 2/86 [D loss: 0.6932277381420135, acc.: 49.41%] [G loss: 0.7017143964767456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 3/86 [D loss: 0.6930252909660339, acc.: 50.93%] [G loss: 0.7025426030158997]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 4/86 [D loss: 0.6929860413074493, acc.: 49.17%] [G loss: 0.7000629901885986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 5/86 [D loss: 0.692750483751297, acc.: 50.15%] [G loss: 0.7009716629981995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 6/86 [D loss: 0.6933575868606567, acc.: 49.76%] [G loss: 0.7029328346252441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 7/86 [D loss: 0.693710446357727, acc.: 48.78%] [G loss: 0.7013280391693115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 8/86 [D loss: 0.6925421059131622, acc.: 51.27%] [G loss: 0.7001055479049683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 9/86 [D loss: 0.6926154792308807, acc.: 52.29%] [G loss: 0.7012068033218384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 10/86 [D loss: 0.6932789385318756, acc.: 49.90%] [G loss: 0.7009768486022949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 11/86 [D loss: 0.6931430399417877, acc.: 50.10%] [G loss: 0.7000373601913452]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 12/86 [D loss: 0.6930819749832153, acc.: 50.29%] [G loss: 0.7000592947006226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 13/86 [D loss: 0.6925474405288696, acc.: 51.27%] [G loss: 0.7014838457107544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 14/86 [D loss: 0.6925911605358124, acc.: 52.59%] [G loss: 0.7015300989151001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 15/86 [D loss: 0.6934354603290558, acc.: 49.46%] [G loss: 0.7003506422042847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 16/86 [D loss: 0.6925867795944214, acc.: 50.24%] [G loss: 0.7014281749725342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 17/86 [D loss: 0.6924393475055695, acc.: 51.22%] [G loss: 0.7001622319221497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 18/86 [D loss: 0.6927512288093567, acc.: 51.07%] [G loss: 0.6996235847473145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 19/86 [D loss: 0.6926222741603851, acc.: 50.34%] [G loss: 0.7009027600288391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 20/86 [D loss: 0.6933328211307526, acc.: 49.80%] [G loss: 0.7015401721000671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 21/86 [D loss: 0.692559152841568, acc.: 51.46%] [G loss: 0.7007949352264404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 22/86 [D loss: 0.6933764815330505, acc.: 49.46%] [G loss: 0.7014451026916504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 23/86 [D loss: 0.6928808093070984, acc.: 49.61%] [G loss: 0.7009295225143433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 24/86 [D loss: 0.692785233259201, acc.: 51.12%] [G loss: 0.7008836269378662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 25/86 [D loss: 0.6920934617519379, acc.: 51.61%] [G loss: 0.7001241445541382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 26/86 [D loss: 0.6933327317237854, acc.: 49.12%] [G loss: 0.7006803750991821]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 27/86 [D loss: 0.6924678087234497, acc.: 50.39%] [G loss: 0.7006368041038513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 28/86 [D loss: 0.693848192691803, acc.: 48.78%] [G loss: 0.7017526030540466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 29/86 [D loss: 0.69259312748909, acc.: 50.93%] [G loss: 0.7029866576194763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 30/86 [D loss: 0.693374902009964, acc.: 49.71%] [G loss: 0.7016854882240295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 31/86 [D loss: 0.6922094225883484, acc.: 52.25%] [G loss: 0.700843095779419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 32/86 [D loss: 0.6931069493293762, acc.: 49.80%] [G loss: 0.7007611989974976]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 33/86 [D loss: 0.6925190687179565, acc.: 52.44%] [G loss: 0.7019065618515015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 34/86 [D loss: 0.6927869319915771, acc.: 52.00%] [G loss: 0.7008098363876343]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 35/86 [D loss: 0.6927400231361389, acc.: 50.63%] [G loss: 0.7013569474220276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 36/86 [D loss: 0.6933691203594208, acc.: 49.56%] [G loss: 0.6994772553443909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 37/86 [D loss: 0.6951159238815308, acc.: 46.63%] [G loss: 0.7007489204406738]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 38/86 [D loss: 0.6922166347503662, acc.: 52.20%] [G loss: 0.7004384994506836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 39/86 [D loss: 0.6932865977287292, acc.: 50.34%] [G loss: 0.7014427781105042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 40/86 [D loss: 0.6934686303138733, acc.: 51.12%] [G loss: 0.7001046538352966]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 41/86 [D loss: 0.6938095986843109, acc.: 47.61%] [G loss: 0.699364185333252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 42/86 [D loss: 0.693004846572876, acc.: 49.41%] [G loss: 0.7006999254226685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 43/86 [D loss: 0.6926355361938477, acc.: 51.86%] [G loss: 0.7015400528907776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 44/86 [D loss: 0.6924577355384827, acc.: 50.15%] [G loss: 0.7015221118927002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 45/86 [D loss: 0.6923017501831055, acc.: 52.83%] [G loss: 0.6996564865112305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 46/86 [D loss: 0.6939065754413605, acc.: 48.14%] [G loss: 0.7003179788589478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 47/86 [D loss: 0.6933567225933075, acc.: 50.68%] [G loss: 0.7009698152542114]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 48/86 [D loss: 0.6923053860664368, acc.: 51.51%] [G loss: 0.7016400694847107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 49/86 [D loss: 0.6927684247493744, acc.: 51.56%] [G loss: 0.7007614374160767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 50/86 [D loss: 0.6941038072109222, acc.: 49.17%] [G loss: 0.7005043625831604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 51/86 [D loss: 0.692868173122406, acc.: 51.07%] [G loss: 0.6990796327590942]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 52/86 [D loss: 0.6931782960891724, acc.: 50.73%] [G loss: 0.6999324560165405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 53/86 [D loss: 0.6917555630207062, acc.: 52.20%] [G loss: 0.7002017498016357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 54/86 [D loss: 0.6931666731834412, acc.: 50.59%] [G loss: 0.7003410458564758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 55/86 [D loss: 0.6920107305049896, acc.: 52.83%] [G loss: 0.700575053691864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 56/86 [D loss: 0.6938325762748718, acc.: 49.27%] [G loss: 0.7006686925888062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 57/86 [D loss: 0.6931338310241699, acc.: 50.20%] [G loss: 0.7009315490722656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 58/86 [D loss: 0.6923589408397675, acc.: 50.83%] [G loss: 0.7012544870376587]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 59/86 [D loss: 0.6930084228515625, acc.: 50.73%] [G loss: 0.7005563378334045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 60/86 [D loss: 0.692432165145874, acc.: 51.86%] [G loss: 0.7004303336143494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 61/86 [D loss: 0.6928498148918152, acc.: 50.44%] [G loss: 0.7012248635292053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 62/86 [D loss: 0.6930518746376038, acc.: 50.93%] [G loss: 0.7019450664520264]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 63/86 [D loss: 0.6926709711551666, acc.: 50.68%] [G loss: 0.7017014622688293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 64/86 [D loss: 0.6922572255134583, acc.: 51.27%] [G loss: 0.7002516388893127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 65/86 [D loss: 0.6928406655788422, acc.: 50.93%] [G loss: 0.7002198100090027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 66/86 [D loss: 0.6931585669517517, acc.: 51.12%] [G loss: 0.7006885409355164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 67/86 [D loss: 0.693062961101532, acc.: 51.07%] [G loss: 0.7005802989006042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 68/86 [D loss: 0.6931271255016327, acc.: 50.34%] [G loss: 0.7012280225753784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 69/86 [D loss: 0.6928240060806274, acc.: 51.51%] [G loss: 0.7002130746841431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 70/86 [D loss: 0.6924051642417908, acc.: 51.37%] [G loss: 0.7005679607391357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 71/86 [D loss: 0.6923632323741913, acc.: 52.00%] [G loss: 0.6998022198677063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 72/86 [D loss: 0.6924267411231995, acc.: 52.10%] [G loss: 0.7002162933349609]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 73/86 [D loss: 0.6924527883529663, acc.: 51.22%] [G loss: 0.7016260623931885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 74/86 [D loss: 0.6935260593891144, acc.: 49.95%] [G loss: 0.7013500928878784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 75/86 [D loss: 0.6922931671142578, acc.: 52.49%] [G loss: 0.7002972364425659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 76/86 [D loss: 0.6925459504127502, acc.: 51.86%] [G loss: 0.6997997760772705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 77/86 [D loss: 0.6929206550121307, acc.: 50.20%] [G loss: 0.7003482580184937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 78/86 [D loss: 0.6930396854877472, acc.: 51.37%] [G loss: 0.7011610865592957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 79/86 [D loss: 0.6931892037391663, acc.: 50.10%] [G loss: 0.7014121413230896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 80/86 [D loss: 0.6931964159011841, acc.: 49.66%] [G loss: 0.7012187242507935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 81/86 [D loss: 0.693461149930954, acc.: 50.00%] [G loss: 0.6998546719551086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 82/86 [D loss: 0.6921610236167908, acc.: 52.69%] [G loss: 0.7005624771118164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 83/86 [D loss: 0.6927078664302826, acc.: 50.54%] [G loss: 0.7013981342315674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 84/86 [D loss: 0.6921779811382294, acc.: 52.83%] [G loss: 0.7011862993240356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 85/86 [D loss: 0.6934025585651398, acc.: 50.10%] [G loss: 0.7012857794761658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 86/86 [D loss: 0.6930955052375793, acc.: 50.54%] [G loss: 0.7006574869155884]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 1/86 [D loss: 0.6926400661468506, acc.: 50.54%] [G loss: 0.7001978158950806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 2/86 [D loss: 0.6934740245342255, acc.: 49.95%] [G loss: 0.7008942365646362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 3/86 [D loss: 0.693475067615509, acc.: 50.05%] [G loss: 0.7000608444213867]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 4/86 [D loss: 0.6938076019287109, acc.: 48.49%] [G loss: 0.7008272409439087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 5/86 [D loss: 0.6927725970745087, acc.: 51.07%] [G loss: 0.6994967460632324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 6/86 [D loss: 0.6920760273933411, acc.: 51.71%] [G loss: 0.700950026512146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 7/86 [D loss: 0.6929966509342194, acc.: 51.22%] [G loss: 0.6995771527290344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 8/86 [D loss: 0.6923204660415649, acc.: 51.46%] [G loss: 0.7000159025192261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 9/86 [D loss: 0.6943753659725189, acc.: 48.58%] [G loss: 0.7008604407310486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 10/86 [D loss: 0.6932848691940308, acc.: 49.71%] [G loss: 0.700914204120636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 11/86 [D loss: 0.6930547952651978, acc.: 50.88%] [G loss: 0.6999818682670593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 12/86 [D loss: 0.692654013633728, acc.: 51.12%] [G loss: 0.7011280059814453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 13/86 [D loss: 0.693025529384613, acc.: 50.83%] [G loss: 0.6992126703262329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 14/86 [D loss: 0.693176656961441, acc.: 50.20%] [G loss: 0.7000691890716553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 15/86 [D loss: 0.6938782632350922, acc.: 47.31%] [G loss: 0.7017062902450562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 16/86 [D loss: 0.6936788558959961, acc.: 49.90%] [G loss: 0.7014015316963196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 17/86 [D loss: 0.6922988593578339, acc.: 51.56%] [G loss: 0.701606035232544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 18/86 [D loss: 0.6940375566482544, acc.: 48.00%] [G loss: 0.7013184428215027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 19/86 [D loss: 0.6943993866443634, acc.: 48.14%] [G loss: 0.7007066011428833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 20/86 [D loss: 0.6926174461841583, acc.: 51.07%] [G loss: 0.7016860842704773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 21/86 [D loss: 0.6922295987606049, acc.: 52.44%] [G loss: 0.7005001306533813]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 22/86 [D loss: 0.6927033364772797, acc.: 50.63%] [G loss: 0.7011517882347107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 23/86 [D loss: 0.6929596662521362, acc.: 49.76%] [G loss: 0.6995506882667542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 24/86 [D loss: 0.6942543089389801, acc.: 48.93%] [G loss: 0.7006494998931885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 25/86 [D loss: 0.6923740208148956, acc.: 52.00%] [G loss: 0.70059734582901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 26/86 [D loss: 0.6921215951442719, acc.: 52.25%] [G loss: 0.7007498145103455]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 27/86 [D loss: 0.692448079586029, acc.: 51.17%] [G loss: 0.7016170620918274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 28/86 [D loss: 0.6928762793540955, acc.: 49.95%] [G loss: 0.6980642676353455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 29/86 [D loss: 0.6939652264118195, acc.: 49.32%] [G loss: 0.6997394561767578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 30/86 [D loss: 0.692660003900528, acc.: 51.27%] [G loss: 0.7006085515022278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 31/86 [D loss: 0.6931340396404266, acc.: 49.85%] [G loss: 0.7014545202255249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 32/86 [D loss: 0.6928463578224182, acc.: 51.37%] [G loss: 0.7008786201477051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 33/86 [D loss: 0.6936270296573639, acc.: 48.63%] [G loss: 0.6995802521705627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 34/86 [D loss: 0.6926925778388977, acc.: 52.93%] [G loss: 0.7001739144325256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 35/86 [D loss: 0.6929257214069366, acc.: 50.63%] [G loss: 0.7005428671836853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 36/86 [D loss: 0.6927048861980438, acc.: 51.46%] [G loss: 0.6995552778244019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 37/86 [D loss: 0.6932012736797333, acc.: 49.56%] [G loss: 0.699042558670044]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 38/86 [D loss: 0.6935780942440033, acc.: 48.83%] [G loss: 0.6995397806167603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 39/86 [D loss: 0.6925942003726959, acc.: 52.64%] [G loss: 0.7005227208137512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 40/86 [D loss: 0.6929700374603271, acc.: 50.39%] [G loss: 0.7013893127441406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 41/86 [D loss: 0.692141979932785, acc.: 51.07%] [G loss: 0.6998302936553955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 42/86 [D loss: 0.693105936050415, acc.: 48.68%] [G loss: 0.7009192705154419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 43/86 [D loss: 0.6925123929977417, acc.: 51.42%] [G loss: 0.7007337212562561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 44/86 [D loss: 0.6918377578258514, acc.: 53.47%] [G loss: 0.7008645534515381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 45/86 [D loss: 0.6924046277999878, acc.: 51.81%] [G loss: 0.6998186111450195]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 47/200, Batch 46/86 [D loss: 0.6926396787166595, acc.: 51.32%] [G loss: 0.7009227275848389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 47/86 [D loss: 0.6934468150138855, acc.: 49.07%] [G loss: 0.6998792290687561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 48/86 [D loss: 0.6925539672374725, acc.: 52.00%] [G loss: 0.7008086442947388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 49/86 [D loss: 0.6927632689476013, acc.: 49.95%] [G loss: 0.699675440788269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 50/86 [D loss: 0.6935029625892639, acc.: 49.32%] [G loss: 0.7002466917037964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 51/86 [D loss: 0.6924205124378204, acc.: 51.07%] [G loss: 0.7006142735481262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 52/86 [D loss: 0.6928753554821014, acc.: 51.66%] [G loss: 0.7012448310852051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 53/86 [D loss: 0.6929469704627991, acc.: 50.00%] [G loss: 0.7001540660858154]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 54/86 [D loss: 0.6929931938648224, acc.: 50.54%] [G loss: 0.6986228227615356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 55/86 [D loss: 0.6931922435760498, acc.: 50.83%] [G loss: 0.6999450922012329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 56/86 [D loss: 0.6923627853393555, acc.: 50.98%] [G loss: 0.7004233598709106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 57/86 [D loss: 0.6932684183120728, acc.: 48.54%] [G loss: 0.701985239982605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 58/86 [D loss: 0.6927740275859833, acc.: 50.39%] [G loss: 0.7007548809051514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 59/86 [D loss: 0.693335771560669, acc.: 49.66%] [G loss: 0.7006728649139404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 60/86 [D loss: 0.6927594542503357, acc.: 51.27%] [G loss: 0.7002363801002502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 61/86 [D loss: 0.6929743587970734, acc.: 51.51%] [G loss: 0.7006431818008423]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 62/86 [D loss: 0.6931897401809692, acc.: 50.15%] [G loss: 0.7004716396331787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 63/86 [D loss: 0.6927123665809631, acc.: 50.59%] [G loss: 0.6999000310897827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 64/86 [D loss: 0.6927380561828613, acc.: 51.86%] [G loss: 0.7013806104660034]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 65/86 [D loss: 0.6929187476634979, acc.: 51.61%] [G loss: 0.7011528611183167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 66/86 [D loss: 0.692943662405014, acc.: 50.93%] [G loss: 0.7005873918533325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 67/86 [D loss: 0.6924494206905365, acc.: 51.95%] [G loss: 0.7014079093933105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 68/86 [D loss: 0.6934698820114136, acc.: 48.97%] [G loss: 0.6997695565223694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 69/86 [D loss: 0.6918407380580902, acc.: 53.22%] [G loss: 0.7008075714111328]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 70/86 [D loss: 0.6940672099590302, acc.: 49.76%] [G loss: 0.7005188465118408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 71/86 [D loss: 0.6937685310840607, acc.: 49.17%] [G loss: 0.7008535861968994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 72/86 [D loss: 0.6928934454917908, acc.: 51.46%] [G loss: 0.7005842924118042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 73/86 [D loss: 0.6933839619159698, acc.: 49.61%] [G loss: 0.7013252973556519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 74/86 [D loss: 0.693080723285675, acc.: 50.63%] [G loss: 0.6997172236442566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 75/86 [D loss: 0.6931026577949524, acc.: 49.85%] [G loss: 0.7005234360694885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 76/86 [D loss: 0.6936567425727844, acc.: 50.05%] [G loss: 0.7017120122909546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 77/86 [D loss: 0.6917544603347778, acc.: 54.15%] [G loss: 0.7003644704818726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 78/86 [D loss: 0.6932834982872009, acc.: 49.17%] [G loss: 0.7006529569625854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 79/86 [D loss: 0.6917562186717987, acc.: 53.37%] [G loss: 0.7004193067550659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 80/86 [D loss: 0.6920953691005707, acc.: 52.10%] [G loss: 0.7011282444000244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 81/86 [D loss: 0.692352682352066, acc.: 51.71%] [G loss: 0.7016807794570923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 82/86 [D loss: 0.6925143301486969, acc.: 51.27%] [G loss: 0.700745701789856]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 83/86 [D loss: 0.6932300627231598, acc.: 49.76%] [G loss: 0.7004963159561157]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 84/86 [D loss: 0.6930336356163025, acc.: 51.27%] [G loss: 0.7002829313278198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 85/86 [D loss: 0.6931354701519012, acc.: 49.41%] [G loss: 0.7010799646377563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 86/86 [D loss: 0.6928291320800781, acc.: 50.34%] [G loss: 0.6998215317726135]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 1/86 [D loss: 0.6930414140224457, acc.: 50.63%] [G loss: 0.700537919998169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 2/86 [D loss: 0.6931747794151306, acc.: 51.12%] [G loss: 0.7000104188919067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 3/86 [D loss: 0.6936355233192444, acc.: 49.02%] [G loss: 0.6998094320297241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 4/86 [D loss: 0.6925697922706604, acc.: 51.90%] [G loss: 0.7006255984306335]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 5/86 [D loss: 0.6917842030525208, acc.: 53.12%] [G loss: 0.69941645860672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 6/86 [D loss: 0.6927409768104553, acc.: 50.49%] [G loss: 0.6989398002624512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 7/86 [D loss: 0.6933155059814453, acc.: 50.29%] [G loss: 0.7007185220718384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 8/86 [D loss: 0.6917016506195068, acc.: 53.37%] [G loss: 0.7006657719612122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 9/86 [D loss: 0.693377286195755, acc.: 49.90%] [G loss: 0.6992884278297424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 10/86 [D loss: 0.6936192512512207, acc.: 50.00%] [G loss: 0.697935163974762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 11/86 [D loss: 0.6929222941398621, acc.: 50.54%] [G loss: 0.7011967897415161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 12/86 [D loss: 0.6918106377124786, acc.: 52.34%] [G loss: 0.7011873722076416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 13/86 [D loss: 0.6927988529205322, acc.: 52.29%] [G loss: 0.7006858587265015]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 14/86 [D loss: 0.6930614113807678, acc.: 51.03%] [G loss: 0.6992565989494324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 15/86 [D loss: 0.6941823959350586, acc.: 47.66%] [G loss: 0.6991702318191528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 16/86 [D loss: 0.692175030708313, acc.: 53.03%] [G loss: 0.699504017829895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 17/86 [D loss: 0.692552387714386, acc.: 52.34%] [G loss: 0.6996051073074341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 18/86 [D loss: 0.6928233504295349, acc.: 49.22%] [G loss: 0.700428307056427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 19/86 [D loss: 0.6934929192066193, acc.: 50.49%] [G loss: 0.6989070177078247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 20/86 [D loss: 0.6934851109981537, acc.: 49.51%] [G loss: 0.7004615664482117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 21/86 [D loss: 0.6930215954780579, acc.: 49.66%] [G loss: 0.699738085269928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 22/86 [D loss: 0.6933419704437256, acc.: 49.80%] [G loss: 0.7008417844772339]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 23/86 [D loss: 0.6930716037750244, acc.: 48.83%] [G loss: 0.6988587975502014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 24/86 [D loss: 0.6933740377426147, acc.: 49.51%] [G loss: 0.6991565823554993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 25/86 [D loss: 0.6940256059169769, acc.: 48.68%] [G loss: 0.7003973722457886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 26/86 [D loss: 0.6929349899291992, acc.: 50.93%] [G loss: 0.7003206610679626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 27/86 [D loss: 0.6929912865161896, acc.: 50.20%] [G loss: 0.7003137469291687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 28/86 [D loss: 0.6926477551460266, acc.: 52.29%] [G loss: 0.6994691491127014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 29/86 [D loss: 0.6936880350112915, acc.: 48.88%] [G loss: 0.6999347805976868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 30/86 [D loss: 0.6928934454917908, acc.: 50.34%] [G loss: 0.6999796628952026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 31/86 [D loss: 0.6925750076770782, acc.: 52.44%] [G loss: 0.7010706663131714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 32/86 [D loss: 0.6932676136493683, acc.: 49.51%] [G loss: 0.7010818719863892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 33/86 [D loss: 0.693442165851593, acc.: 48.83%] [G loss: 0.700777530670166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 34/86 [D loss: 0.6927254498004913, acc.: 51.07%] [G loss: 0.7006253004074097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 35/86 [D loss: 0.6929244995117188, acc.: 51.27%] [G loss: 0.6997121572494507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 36/86 [D loss: 0.6927862465381622, acc.: 51.07%] [G loss: 0.7000913023948669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 37/86 [D loss: 0.6928625404834747, acc.: 50.88%] [G loss: 0.700033962726593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 38/86 [D loss: 0.6929474174976349, acc.: 51.42%] [G loss: 0.6996811628341675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 39/86 [D loss: 0.6938027739524841, acc.: 48.83%] [G loss: 0.6995241641998291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 40/86 [D loss: 0.6930955946445465, acc.: 51.37%] [G loss: 0.7015447020530701]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 41/86 [D loss: 0.6928088366985321, acc.: 50.73%] [G loss: 0.699820339679718]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 42/86 [D loss: 0.6929543614387512, acc.: 50.34%] [G loss: 0.700025737285614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 43/86 [D loss: 0.6928505897521973, acc.: 50.78%] [G loss: 0.7008067965507507]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 44/86 [D loss: 0.6934144198894501, acc.: 50.78%] [G loss: 0.7013726830482483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 45/86 [D loss: 0.6929775774478912, acc.: 49.71%] [G loss: 0.6987585425376892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 46/86 [D loss: 0.6938278079032898, acc.: 47.95%] [G loss: 0.7020437121391296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 47/86 [D loss: 0.6924967765808105, acc.: 51.42%] [G loss: 0.7014449834823608]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 48/86 [D loss: 0.6917375922203064, acc.: 52.34%] [G loss: 0.7012917399406433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 49/86 [D loss: 0.6925328075885773, acc.: 50.59%] [G loss: 0.7006864547729492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 50/86 [D loss: 0.6933921277523041, acc.: 51.12%] [G loss: 0.7000017166137695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 51/86 [D loss: 0.6929797530174255, acc.: 50.15%] [G loss: 0.7004692554473877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 52/86 [D loss: 0.693202018737793, acc.: 49.32%] [G loss: 0.7012295722961426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 53/86 [D loss: 0.6931834518909454, acc.: 50.20%] [G loss: 0.6999611854553223]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 54/86 [D loss: 0.6929866671562195, acc.: 50.05%] [G loss: 0.699042797088623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 55/86 [D loss: 0.6922333836555481, acc.: 52.44%] [G loss: 0.7001706957817078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 56/86 [D loss: 0.6920855641365051, acc.: 52.20%] [G loss: 0.6982454657554626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 57/86 [D loss: 0.6923261284828186, acc.: 50.93%] [G loss: 0.6995387673377991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 58/86 [D loss: 0.6934066712856293, acc.: 49.61%] [G loss: 0.7004745006561279]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 59/86 [D loss: 0.6930038630962372, acc.: 50.63%] [G loss: 0.7003562450408936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 60/86 [D loss: 0.6927375793457031, acc.: 51.07%] [G loss: 0.7014672756195068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 61/86 [D loss: 0.6926765143871307, acc.: 51.07%] [G loss: 0.7001224756240845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 62/86 [D loss: 0.6936038732528687, acc.: 49.22%] [G loss: 0.7003053426742554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 63/86 [D loss: 0.6928752660751343, acc.: 50.54%] [G loss: 0.701348602771759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 64/86 [D loss: 0.6932229697704315, acc.: 50.78%] [G loss: 0.7023403644561768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 65/86 [D loss: 0.6939887702465057, acc.: 49.07%] [G loss: 0.701042890548706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 66/86 [D loss: 0.6922735571861267, acc.: 52.44%] [G loss: 0.7002029418945312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 67/86 [D loss: 0.6930335462093353, acc.: 50.49%] [G loss: 0.6991356611251831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 68/86 [D loss: 0.6938100159168243, acc.: 49.51%] [G loss: 0.7009514570236206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 69/86 [D loss: 0.6922503709793091, acc.: 51.22%] [G loss: 0.6999871730804443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 70/86 [D loss: 0.6933372318744659, acc.: 50.20%] [G loss: 0.6996997594833374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 71/86 [D loss: 0.692187637090683, acc.: 52.00%] [G loss: 0.6996351480484009]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 72/86 [D loss: 0.6933103501796722, acc.: 50.29%] [G loss: 0.6994165778160095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 73/86 [D loss: 0.6923507153987885, acc.: 51.81%] [G loss: 0.6999406218528748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 74/86 [D loss: 0.6932275593280792, acc.: 50.15%] [G loss: 0.7000109553337097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 75/86 [D loss: 0.693111002445221, acc.: 50.78%] [G loss: 0.7005730867385864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 76/86 [D loss: 0.6927435994148254, acc.: 51.37%] [G loss: 0.700276255607605]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 77/86 [D loss: 0.6925199031829834, acc.: 51.61%] [G loss: 0.7005468010902405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 78/86 [D loss: 0.692611426115036, acc.: 50.88%] [G loss: 0.7003010511398315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 79/86 [D loss: 0.6920329928398132, acc.: 53.61%] [G loss: 0.6990069150924683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 80/86 [D loss: 0.6924584805965424, acc.: 51.03%] [G loss: 0.70025235414505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 81/86 [D loss: 0.6929043829441071, acc.: 50.88%] [G loss: 0.7001268267631531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 82/86 [D loss: 0.6916943192481995, acc.: 52.44%] [G loss: 0.7008159756660461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 83/86 [D loss: 0.6929115653038025, acc.: 50.15%] [G loss: 0.7009540796279907]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 84/86 [D loss: 0.6935084462165833, acc.: 49.46%] [G loss: 0.7019895911216736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 85/86 [D loss: 0.6916854679584503, acc.: 52.64%] [G loss: 0.7014203071594238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 86/86 [D loss: 0.6932846903800964, acc.: 49.46%] [G loss: 0.7013679146766663]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 1/86 [D loss: 0.6936623454093933, acc.: 49.90%] [G loss: 0.7009443044662476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 2/86 [D loss: 0.6924978792667389, acc.: 51.37%] [G loss: 0.7007644176483154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 3/86 [D loss: 0.6931275427341461, acc.: 50.39%] [G loss: 0.6996359825134277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 4/86 [D loss: 0.693706750869751, acc.: 49.85%] [G loss: 0.6996932625770569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 5/86 [D loss: 0.6922125220298767, acc.: 51.95%] [G loss: 0.7013112306594849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 6/86 [D loss: 0.6928005516529083, acc.: 51.27%] [G loss: 0.7021238803863525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 7/86 [D loss: 0.6919719874858856, acc.: 53.22%] [G loss: 0.6994714140892029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 8/86 [D loss: 0.6942501366138458, acc.: 47.90%] [G loss: 0.6995947360992432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 9/86 [D loss: 0.6930542290210724, acc.: 50.98%] [G loss: 0.6998483538627625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 10/86 [D loss: 0.6932650804519653, acc.: 49.37%] [G loss: 0.6993104219436646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 11/86 [D loss: 0.6938073337078094, acc.: 48.68%] [G loss: 0.6985917091369629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 12/86 [D loss: 0.6934710144996643, acc.: 49.66%] [G loss: 0.6988639831542969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 13/86 [D loss: 0.6938660442829132, acc.: 48.83%] [G loss: 0.7003672122955322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 14/86 [D loss: 0.6930713653564453, acc.: 50.49%] [G loss: 0.7005888223648071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 15/86 [D loss: 0.6929529011249542, acc.: 51.12%] [G loss: 0.700617790222168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 16/86 [D loss: 0.6923806965351105, acc.: 52.05%] [G loss: 0.6984291672706604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 17/86 [D loss: 0.6940832734107971, acc.: 49.46%] [G loss: 0.7008558511734009]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 18/86 [D loss: 0.69240802526474, acc.: 53.12%] [G loss: 0.7007207870483398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 19/86 [D loss: 0.6920059323310852, acc.: 52.54%] [G loss: 0.6986231803894043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 20/86 [D loss: 0.6923233270645142, acc.: 52.00%] [G loss: 0.6990228295326233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 21/86 [D loss: 0.6934274137020111, acc.: 50.10%] [G loss: 0.6984522342681885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 22/86 [D loss: 0.6930496990680695, acc.: 52.05%] [G loss: 0.6994739174842834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 23/86 [D loss: 0.6930972635746002, acc.: 49.56%] [G loss: 0.6988500952720642]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 24/86 [D loss: 0.6926793456077576, acc.: 50.68%] [G loss: 0.7005452513694763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 25/86 [D loss: 0.6925943493843079, acc.: 51.76%] [G loss: 0.6984919905662537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 26/86 [D loss: 0.6946832835674286, acc.: 47.85%] [G loss: 0.7010483145713806]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 27/86 [D loss: 0.6927486062049866, acc.: 52.10%] [G loss: 0.6986405253410339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 28/86 [D loss: 0.6926157772541046, acc.: 51.66%] [G loss: 0.7006694674491882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 29/86 [D loss: 0.6922660768032074, acc.: 51.17%] [G loss: 0.6976525783538818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 30/86 [D loss: 0.6950893402099609, acc.: 47.22%] [G loss: 0.6990346908569336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 31/86 [D loss: 0.6922254264354706, acc.: 53.12%] [G loss: 0.6981980204582214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 32/86 [D loss: 0.6946562230587006, acc.: 47.31%] [G loss: 0.6993247270584106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 33/86 [D loss: 0.6914431750774384, acc.: 53.66%] [G loss: 0.6983428001403809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 34/86 [D loss: 0.693545013666153, acc.: 48.78%] [G loss: 0.698432445526123]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 35/86 [D loss: 0.6923443377017975, acc.: 50.68%] [G loss: 0.6987345218658447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 36/86 [D loss: 0.6927786767482758, acc.: 50.93%] [G loss: 0.698185920715332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 37/86 [D loss: 0.6921828389167786, acc.: 52.05%] [G loss: 0.6982181668281555]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 38/86 [D loss: 0.6926040947437286, acc.: 51.37%] [G loss: 0.6982553601264954]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 39/86 [D loss: 0.6931626796722412, acc.: 50.29%] [G loss: 0.6987253427505493]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 40/86 [D loss: 0.6920126378536224, acc.: 52.83%] [G loss: 0.6998567581176758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 41/86 [D loss: 0.6928696036338806, acc.: 50.59%] [G loss: 0.6988554000854492]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 42/86 [D loss: 0.69341841340065, acc.: 49.95%] [G loss: 0.6985813975334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 43/86 [D loss: 0.6918854415416718, acc.: 52.34%] [G loss: 0.7010130286216736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 44/86 [D loss: 0.692449301481247, acc.: 51.12%] [G loss: 0.6992089152336121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 45/86 [D loss: 0.6927871108055115, acc.: 50.59%] [G loss: 0.6998535394668579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 46/86 [D loss: 0.6920813918113708, acc.: 51.95%] [G loss: 0.6990583539009094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 47/86 [D loss: 0.6916983425617218, acc.: 52.00%] [G loss: 0.698706865310669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 48/86 [D loss: 0.6929894089698792, acc.: 50.00%] [G loss: 0.7000248432159424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 49/86 [D loss: 0.6921818554401398, acc.: 52.73%] [G loss: 0.6990736722946167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 50/86 [D loss: 0.6927578151226044, acc.: 50.39%] [G loss: 0.6985225081443787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 51/86 [D loss: 0.6924135982990265, acc.: 51.17%] [G loss: 0.6998680233955383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 52/86 [D loss: 0.6926106810569763, acc.: 50.88%] [G loss: 0.6999527812004089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 53/86 [D loss: 0.6921747624874115, acc.: 51.61%] [G loss: 0.7004947662353516]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 49/200, Batch 54/86 [D loss: 0.6923756003379822, acc.: 52.00%] [G loss: 0.7000706195831299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 55/86 [D loss: 0.6922680139541626, acc.: 51.51%] [G loss: 0.700324296951294]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 56/86 [D loss: 0.692484587430954, acc.: 51.86%] [G loss: 0.7001025080680847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 57/86 [D loss: 0.6922231912612915, acc.: 52.10%] [G loss: 0.7001219987869263]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 58/86 [D loss: 0.6915014088153839, acc.: 54.35%] [G loss: 0.6983129978179932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 59/86 [D loss: 0.6944700181484222, acc.: 47.46%] [G loss: 0.7004583477973938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 60/86 [D loss: 0.6920212507247925, acc.: 52.69%] [G loss: 0.699720561504364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 61/86 [D loss: 0.6927483379840851, acc.: 50.78%] [G loss: 0.6991546750068665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 62/86 [D loss: 0.6925637722015381, acc.: 51.12%] [G loss: 0.6990975141525269]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 63/86 [D loss: 0.6932356953620911, acc.: 50.34%] [G loss: 0.6991096138954163]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 64/86 [D loss: 0.6916687488555908, acc.: 52.98%] [G loss: 0.7011314630508423]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 65/86 [D loss: 0.6919236183166504, acc.: 51.86%] [G loss: 0.6998588442802429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 66/86 [D loss: 0.6927087903022766, acc.: 51.32%] [G loss: 0.6995460987091064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 67/86 [D loss: 0.6928384602069855, acc.: 51.27%] [G loss: 0.6998642086982727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 68/86 [D loss: 0.6935686469078064, acc.: 49.95%] [G loss: 0.6991804242134094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 69/86 [D loss: 0.6927940249443054, acc.: 51.42%] [G loss: 0.6992959380149841]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 70/86 [D loss: 0.6925618648529053, acc.: 51.66%] [G loss: 0.700279951095581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 71/86 [D loss: 0.6913181245326996, acc.: 54.83%] [G loss: 0.6994136571884155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 72/86 [D loss: 0.6933943033218384, acc.: 48.88%] [G loss: 0.7006256580352783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 73/86 [D loss: 0.6934551298618317, acc.: 50.83%] [G loss: 0.6999974846839905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 74/86 [D loss: 0.6927179992198944, acc.: 50.63%] [G loss: 0.6997122764587402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 75/86 [D loss: 0.6929236054420471, acc.: 49.41%] [G loss: 0.6990312933921814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 76/86 [D loss: 0.6928649544715881, acc.: 50.98%] [G loss: 0.6976733803749084]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 77/86 [D loss: 0.6927427351474762, acc.: 51.56%] [G loss: 0.6999558806419373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 78/86 [D loss: 0.6930654644966125, acc.: 50.59%] [G loss: 0.697982668876648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 79/86 [D loss: 0.6923503875732422, acc.: 52.44%] [G loss: 0.6996672749519348]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 80/86 [D loss: 0.6932195723056793, acc.: 50.49%] [G loss: 0.697373628616333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 81/86 [D loss: 0.6927507519721985, acc.: 49.85%] [G loss: 0.700279951095581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 82/86 [D loss: 0.6919115483760834, acc.: 52.64%] [G loss: 0.6995766162872314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 83/86 [D loss: 0.692287027835846, acc.: 51.76%] [G loss: 0.699489414691925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 84/86 [D loss: 0.691437155008316, acc.: 54.93%] [G loss: 0.6983214020729065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 85/86 [D loss: 0.6933064758777618, acc.: 50.39%] [G loss: 0.6982898116111755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 86/86 [D loss: 0.6921687722206116, acc.: 52.88%] [G loss: 0.6981871128082275]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 1/86 [D loss: 0.6922928392887115, acc.: 51.66%] [G loss: 0.6976416110992432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 2/86 [D loss: 0.6920266151428223, acc.: 53.12%] [G loss: 0.6987546682357788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 3/86 [D loss: 0.6926791667938232, acc.: 50.44%] [G loss: 0.6982428431510925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 4/86 [D loss: 0.6923578083515167, acc.: 51.27%] [G loss: 0.7006113529205322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 5/86 [D loss: 0.6927831470966339, acc.: 50.29%] [G loss: 0.69864821434021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 6/86 [D loss: 0.692469596862793, acc.: 52.05%] [G loss: 0.6988381147384644]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 7/86 [D loss: 0.6923618018627167, acc.: 51.61%] [G loss: 0.6977053284645081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 8/86 [D loss: 0.6935644149780273, acc.: 50.49%] [G loss: 0.699120283126831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 9/86 [D loss: 0.6920467615127563, acc.: 51.76%] [G loss: 0.6987380981445312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 10/86 [D loss: 0.6918491423130035, acc.: 51.42%] [G loss: 0.6998112201690674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 11/86 [D loss: 0.6918492317199707, acc.: 53.22%] [G loss: 0.698415219783783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 12/86 [D loss: 0.6929706931114197, acc.: 51.27%] [G loss: 0.7008944749832153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 13/86 [D loss: 0.6925052106380463, acc.: 51.27%] [G loss: 0.7008731365203857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 14/86 [D loss: 0.6932143867015839, acc.: 48.88%] [G loss: 0.6989349722862244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 15/86 [D loss: 0.6929475963115692, acc.: 50.88%] [G loss: 0.69923996925354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 16/86 [D loss: 0.6929997801780701, acc.: 51.27%] [G loss: 0.6991389989852905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 17/86 [D loss: 0.6920700669288635, acc.: 52.00%] [G loss: 0.6994086503982544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 18/86 [D loss: 0.6918420791625977, acc.: 52.10%] [G loss: 0.6989474892616272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 19/86 [D loss: 0.6918196678161621, acc.: 52.49%] [G loss: 0.6994884014129639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 20/86 [D loss: 0.6928756535053253, acc.: 50.05%] [G loss: 0.6992145776748657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 21/86 [D loss: 0.6923083961009979, acc.: 51.42%] [G loss: 0.7001025676727295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 22/86 [D loss: 0.6927797496318817, acc.: 52.15%] [G loss: 0.6981630325317383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 23/86 [D loss: 0.6925361454486847, acc.: 50.54%] [G loss: 0.6993162035942078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 24/86 [D loss: 0.6920480728149414, acc.: 52.00%] [G loss: 0.698890745639801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 25/86 [D loss: 0.6924170851707458, acc.: 49.51%] [G loss: 0.6999484300613403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 26/86 [D loss: 0.6921999454498291, acc.: 51.81%] [G loss: 0.7004346251487732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 27/86 [D loss: 0.6921201646327972, acc.: 52.44%] [G loss: 0.6989734172821045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 28/86 [D loss: 0.6918013095855713, acc.: 52.69%] [G loss: 0.7003121972084045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 29/86 [D loss: 0.6917347311973572, acc.: 52.64%] [G loss: 0.6977787017822266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 30/86 [D loss: 0.6918773353099823, acc.: 52.10%] [G loss: 0.6995789408683777]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 31/86 [D loss: 0.6919289827346802, acc.: 53.32%] [G loss: 0.698334813117981]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 32/86 [D loss: 0.6913380026817322, acc.: 54.15%] [G loss: 0.7002962827682495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 33/86 [D loss: 0.6923891603946686, acc.: 51.81%] [G loss: 0.698837161064148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 34/86 [D loss: 0.691697746515274, acc.: 52.34%] [G loss: 0.6981990933418274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 35/86 [D loss: 0.6921693980693817, acc.: 51.27%] [G loss: 0.6977294087409973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 36/86 [D loss: 0.69181689620018, acc.: 51.27%] [G loss: 0.698720395565033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 37/86 [D loss: 0.6922799944877625, acc.: 52.29%] [G loss: 0.6990482211112976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 38/86 [D loss: 0.6926194429397583, acc.: 52.05%] [G loss: 0.698600172996521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 39/86 [D loss: 0.693049430847168, acc.: 50.54%] [G loss: 0.7001739740371704]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 40/86 [D loss: 0.691765546798706, acc.: 53.61%] [G loss: 0.698328971862793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 41/86 [D loss: 0.6911870539188385, acc.: 54.44%] [G loss: 0.6974273920059204]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 42/86 [D loss: 0.6913505792617798, acc.: 53.47%] [G loss: 0.6978405117988586]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 43/86 [D loss: 0.6932405233383179, acc.: 50.05%] [G loss: 0.6989853978157043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 44/86 [D loss: 0.6918774247169495, acc.: 53.37%] [G loss: 0.6986376643180847]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 45/86 [D loss: 0.6917305886745453, acc.: 52.69%] [G loss: 0.6995410919189453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 46/86 [D loss: 0.6927923858165741, acc.: 50.34%] [G loss: 0.6986990571022034]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 47/86 [D loss: 0.6923094093799591, acc.: 51.22%] [G loss: 0.6985890865325928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 48/86 [D loss: 0.6925333738327026, acc.: 51.12%] [G loss: 0.6993890404701233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 49/86 [D loss: 0.6912560164928436, acc.: 53.03%] [G loss: 0.6998463869094849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 50/86 [D loss: 0.6914330422878265, acc.: 50.73%] [G loss: 0.6972768902778625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 51/86 [D loss: 0.692450761795044, acc.: 51.81%] [G loss: 0.6991572976112366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 52/86 [D loss: 0.6921823620796204, acc.: 52.15%] [G loss: 0.6967836618423462]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 53/86 [D loss: 0.6930809617042542, acc.: 50.20%] [G loss: 0.6990782618522644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 54/86 [D loss: 0.6920655071735382, acc.: 51.51%] [G loss: 0.6985587477684021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 55/86 [D loss: 0.6927917897701263, acc.: 51.76%] [G loss: 0.6976503133773804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 56/86 [D loss: 0.6923954784870148, acc.: 50.93%] [G loss: 0.6997512578964233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 57/86 [D loss: 0.6928896009922028, acc.: 50.49%] [G loss: 0.6974777579307556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 58/86 [D loss: 0.6923732459545135, acc.: 51.27%] [G loss: 0.6969117522239685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 59/86 [D loss: 0.6939236223697662, acc.: 49.76%] [G loss: 0.6972601413726807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 60/86 [D loss: 0.6914830505847931, acc.: 52.88%] [G loss: 0.6991725564002991]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 61/86 [D loss: 0.6926683783531189, acc.: 51.71%] [G loss: 0.6992194056510925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 62/86 [D loss: 0.6925292015075684, acc.: 50.20%] [G loss: 0.6974310278892517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 63/86 [D loss: 0.6930352449417114, acc.: 51.42%] [G loss: 0.6972055435180664]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 64/86 [D loss: 0.6924875676631927, acc.: 50.39%] [G loss: 0.6985281705856323]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 65/86 [D loss: 0.692591518163681, acc.: 51.27%] [G loss: 0.6996789574623108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 66/86 [D loss: 0.6920152902603149, acc.: 51.32%] [G loss: 0.6988657116889954]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 67/86 [D loss: 0.6932722926139832, acc.: 49.95%] [G loss: 0.698151707649231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 68/86 [D loss: 0.6937121748924255, acc.: 49.12%] [G loss: 0.6998523473739624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 69/86 [D loss: 0.6921277642250061, acc.: 52.20%] [G loss: 0.697727382183075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 70/86 [D loss: 0.6913806796073914, acc.: 52.25%] [G loss: 0.6993551254272461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 71/86 [D loss: 0.6926851570606232, acc.: 52.05%] [G loss: 0.6968376040458679]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 72/86 [D loss: 0.6934592723846436, acc.: 50.29%] [G loss: 0.6997831463813782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 73/86 [D loss: 0.6919463574886322, acc.: 52.00%] [G loss: 0.6978157162666321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 74/86 [D loss: 0.6936032474040985, acc.: 50.00%] [G loss: 0.7001256346702576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 75/86 [D loss: 0.690724641084671, acc.: 55.13%] [G loss: 0.7002580165863037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 76/86 [D loss: 0.6935367584228516, acc.: 49.17%] [G loss: 0.6990647912025452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 77/86 [D loss: 0.6910288035869598, acc.: 54.00%] [G loss: 0.6995831727981567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 78/86 [D loss: 0.6922935843467712, acc.: 51.22%] [G loss: 0.6993378400802612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 79/86 [D loss: 0.6925651431083679, acc.: 50.98%] [G loss: 0.6986565589904785]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 80/86 [D loss: 0.6926607191562653, acc.: 50.05%] [G loss: 0.6984193921089172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 81/86 [D loss: 0.6928614377975464, acc.: 50.83%] [G loss: 0.6998869776725769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 82/86 [D loss: 0.6925807595252991, acc.: 50.59%] [G loss: 0.6995415091514587]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 83/86 [D loss: 0.6927196383476257, acc.: 52.15%] [G loss: 0.698448896408081]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 84/86 [D loss: 0.6918337047100067, acc.: 52.73%] [G loss: 0.6981383562088013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 85/86 [D loss: 0.6924519836902618, acc.: 51.32%] [G loss: 0.6990920305252075]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 86/86 [D loss: 0.6918478310108185, acc.: 53.52%] [G loss: 0.7000569701194763]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 1/86 [D loss: 0.6925837695598602, acc.: 51.71%] [G loss: 0.699824333190918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 2/86 [D loss: 0.693169355392456, acc.: 50.59%] [G loss: 0.6988158226013184]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 3/86 [D loss: 0.6921646595001221, acc.: 51.90%] [G loss: 0.6997889280319214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 4/86 [D loss: 0.6919941604137421, acc.: 50.88%] [G loss: 0.6992238759994507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 5/86 [D loss: 0.6927235126495361, acc.: 50.88%] [G loss: 0.6997112035751343]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 6/86 [D loss: 0.69267338514328, acc.: 51.66%] [G loss: 0.7000659108161926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 7/86 [D loss: 0.6925966143608093, acc.: 50.68%] [G loss: 0.6994691491127014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 8/86 [D loss: 0.6920638084411621, acc.: 52.54%] [G loss: 0.7000228762626648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 9/86 [D loss: 0.6922708749771118, acc.: 50.73%] [G loss: 0.6995869874954224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 10/86 [D loss: 0.6918789744377136, acc.: 52.20%] [G loss: 0.6988822817802429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 11/86 [D loss: 0.6921059787273407, acc.: 53.08%] [G loss: 0.6985018849372864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 12/86 [D loss: 0.6918568015098572, acc.: 52.49%] [G loss: 0.6984891295433044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 13/86 [D loss: 0.6926793158054352, acc.: 51.56%] [G loss: 0.6992782354354858]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 14/86 [D loss: 0.6919649839401245, acc.: 53.37%] [G loss: 0.6992515921592712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 15/86 [D loss: 0.6922040581703186, acc.: 50.98%] [G loss: 0.698779284954071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 16/86 [D loss: 0.6921176910400391, acc.: 52.78%] [G loss: 0.7000125646591187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 17/86 [D loss: 0.6922504603862762, acc.: 51.22%] [G loss: 0.6997424364089966]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 18/86 [D loss: 0.6923863887786865, acc.: 51.56%] [G loss: 0.7005940675735474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 19/86 [D loss: 0.6919573247432709, acc.: 52.10%] [G loss: 0.6995749473571777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 20/86 [D loss: 0.693056583404541, acc.: 50.73%] [G loss: 0.7002468705177307]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 21/86 [D loss: 0.6919284164905548, acc.: 52.10%] [G loss: 0.6986126899719238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 22/86 [D loss: 0.6916281282901764, acc.: 52.54%] [G loss: 0.6997801661491394]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 23/86 [D loss: 0.6925246119499207, acc.: 50.93%] [G loss: 0.7000622749328613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 24/86 [D loss: 0.6928361058235168, acc.: 50.93%] [G loss: 0.6991545557975769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 25/86 [D loss: 0.6921877264976501, acc.: 52.00%] [G loss: 0.7000382542610168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 26/86 [D loss: 0.6928734481334686, acc.: 49.90%] [G loss: 0.6989330649375916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 27/86 [D loss: 0.6924415826797485, acc.: 52.44%] [G loss: 0.6988189816474915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 28/86 [D loss: 0.6926842927932739, acc.: 50.54%] [G loss: 0.6986024975776672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 29/86 [D loss: 0.6924338340759277, acc.: 50.59%] [G loss: 0.7010536193847656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 30/86 [D loss: 0.6925764083862305, acc.: 51.32%] [G loss: 0.6995973587036133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 31/86 [D loss: 0.691604733467102, acc.: 52.39%] [G loss: 0.6995140910148621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 32/86 [D loss: 0.6925636827945709, acc.: 52.34%] [G loss: 0.6989434957504272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 33/86 [D loss: 0.6919447183609009, acc.: 52.15%] [G loss: 0.7001758813858032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 34/86 [D loss: 0.6922213137149811, acc.: 51.90%] [G loss: 0.698617160320282]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 35/86 [D loss: 0.692949116230011, acc.: 50.20%] [G loss: 0.6992778182029724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 36/86 [D loss: 0.6924435198307037, acc.: 53.22%] [G loss: 0.7004567980766296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 37/86 [D loss: 0.691796600818634, acc.: 52.49%] [G loss: 0.6996244788169861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 38/86 [D loss: 0.6930306553840637, acc.: 50.05%] [G loss: 0.6993398666381836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 39/86 [D loss: 0.6921308040618896, acc.: 50.93%] [G loss: 0.6994789242744446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 40/86 [D loss: 0.6928179562091827, acc.: 51.12%] [G loss: 0.6973808407783508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 41/86 [D loss: 0.6916515231132507, acc.: 51.71%] [G loss: 0.6987553834915161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 42/86 [D loss: 0.6918525695800781, acc.: 51.76%] [G loss: 0.6996817588806152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 43/86 [D loss: 0.6923891603946686, acc.: 52.05%] [G loss: 0.6997112035751343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 44/86 [D loss: 0.6928724646568298, acc.: 50.59%] [G loss: 0.6977210640907288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 45/86 [D loss: 0.6933117210865021, acc.: 50.44%] [G loss: 0.6998085975646973]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 46/86 [D loss: 0.6925894618034363, acc.: 50.98%] [G loss: 0.6997134685516357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 47/86 [D loss: 0.6923500299453735, acc.: 51.51%] [G loss: 0.6971092820167542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 48/86 [D loss: 0.6925097107887268, acc.: 52.00%] [G loss: 0.6989939212799072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 49/86 [D loss: 0.6930795013904572, acc.: 50.05%] [G loss: 0.6984698176383972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 50/86 [D loss: 0.6921618580818176, acc.: 51.95%] [G loss: 0.6984097957611084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 51/86 [D loss: 0.6918809115886688, acc.: 53.52%] [G loss: 0.6987770199775696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 52/86 [D loss: 0.69227334856987, acc.: 51.07%] [G loss: 0.6979689598083496]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 53/86 [D loss: 0.6926975846290588, acc.: 50.78%] [G loss: 0.6996303796768188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 54/86 [D loss: 0.6921041309833527, acc.: 50.98%] [G loss: 0.6987000107765198]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 55/86 [D loss: 0.6920012831687927, acc.: 52.34%] [G loss: 0.6985264420509338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 56/86 [D loss: 0.6926077306270599, acc.: 50.83%] [G loss: 0.698063850402832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 57/86 [D loss: 0.6920400857925415, acc.: 52.64%] [G loss: 0.6982138752937317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 58/86 [D loss: 0.6926301717758179, acc.: 50.98%] [G loss: 0.699213445186615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 59/86 [D loss: 0.6919599175453186, acc.: 51.32%] [G loss: 0.6971392631530762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 60/86 [D loss: 0.6923955082893372, acc.: 51.71%] [G loss: 0.6993385553359985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 61/86 [D loss: 0.6923824548721313, acc.: 51.90%] [G loss: 0.6984844207763672]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 62/86 [D loss: 0.6927784085273743, acc.: 50.59%] [G loss: 0.6974360942840576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 63/86 [D loss: 0.6922717988491058, acc.: 50.83%] [G loss: 0.6983017325401306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 64/86 [D loss: 0.6922509372234344, acc.: 51.37%] [G loss: 0.6988219022750854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 65/86 [D loss: 0.6921475529670715, acc.: 53.56%] [G loss: 0.6991288065910339]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 66/86 [D loss: 0.6913510262966156, acc.: 53.96%] [G loss: 0.6986919641494751]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 67/86 [D loss: 0.6920206546783447, acc.: 52.64%] [G loss: 0.6985820531845093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 68/86 [D loss: 0.6927560269832611, acc.: 50.68%] [G loss: 0.6976677179336548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 69/86 [D loss: 0.692882239818573, acc.: 50.54%] [G loss: 0.6977270245552063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 70/86 [D loss: 0.6925191581249237, acc.: 51.46%] [G loss: 0.6982882618904114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 71/86 [D loss: 0.6920549869537354, acc.: 51.32%] [G loss: 0.699444591999054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 72/86 [D loss: 0.6923239827156067, acc.: 52.34%] [G loss: 0.6987650990486145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 73/86 [D loss: 0.6932661831378937, acc.: 50.15%] [G loss: 0.6984654068946838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 74/86 [D loss: 0.691782683134079, acc.: 52.29%] [G loss: 0.6989546418190002]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 75/86 [D loss: 0.6931648254394531, acc.: 50.63%] [G loss: 0.6989690661430359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 76/86 [D loss: 0.6920237839221954, acc.: 53.27%] [G loss: 0.6988323330879211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 77/86 [D loss: 0.6922761499881744, acc.: 51.61%] [G loss: 0.6984202861785889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 78/86 [D loss: 0.691391110420227, acc.: 53.52%] [G loss: 0.6989254951477051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 79/86 [D loss: 0.6929077208042145, acc.: 50.05%] [G loss: 0.6986881494522095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 80/86 [D loss: 0.6921696364879608, acc.: 52.69%] [G loss: 0.6993629932403564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 81/86 [D loss: 0.6924129128456116, acc.: 50.00%] [G loss: 0.6978773474693298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 82/86 [D loss: 0.6924936473369598, acc.: 51.86%] [G loss: 0.7004443407058716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 83/86 [D loss: 0.6921464502811432, acc.: 51.46%] [G loss: 0.6997727155685425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 84/86 [D loss: 0.6923057734966278, acc.: 51.42%] [G loss: 0.6997678875923157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 85/86 [D loss: 0.6918357312679291, acc.: 52.29%] [G loss: 0.6984630823135376]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 86/86 [D loss: 0.6926498115062714, acc.: 52.34%] [G loss: 0.7005060315132141]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 1/86 [D loss: 0.6925047636032104, acc.: 51.17%] [G loss: 0.6975874304771423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 2/86 [D loss: 0.6927452385425568, acc.: 50.05%] [G loss: 0.7000336050987244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 3/86 [D loss: 0.6934276223182678, acc.: 48.00%] [G loss: 0.7017672061920166]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 4/86 [D loss: 0.692257434129715, acc.: 52.10%] [G loss: 0.6989095211029053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 5/86 [D loss: 0.6929911375045776, acc.: 50.49%] [G loss: 0.6992453336715698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 6/86 [D loss: 0.6928427517414093, acc.: 49.85%] [G loss: 0.6995851993560791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 7/86 [D loss: 0.6924209296703339, acc.: 51.66%] [G loss: 0.699643611907959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 8/86 [D loss: 0.692330539226532, acc.: 52.10%] [G loss: 0.6988138556480408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 9/86 [D loss: 0.6923864781856537, acc.: 51.71%] [G loss: 0.6974598169326782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 10/86 [D loss: 0.6935025751590729, acc.: 48.88%] [G loss: 0.6985189914703369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 11/86 [D loss: 0.6922161877155304, acc.: 52.83%] [G loss: 0.6987785696983337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 12/86 [D loss: 0.6926684081554413, acc.: 50.49%] [G loss: 0.6996177434921265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 13/86 [D loss: 0.6925571858882904, acc.: 50.98%] [G loss: 0.6990784406661987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 14/86 [D loss: 0.6923565566539764, acc.: 51.86%] [G loss: 0.6994596123695374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 15/86 [D loss: 0.6925965845584869, acc.: 50.54%] [G loss: 0.6987800002098083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 16/86 [D loss: 0.6916996240615845, acc.: 52.54%] [G loss: 0.6982117295265198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 17/86 [D loss: 0.6935487687587738, acc.: 50.34%] [G loss: 0.7002419233322144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 18/86 [D loss: 0.6919973194599152, acc.: 53.42%] [G loss: 0.6998393535614014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 19/86 [D loss: 0.6925820708274841, acc.: 51.90%] [G loss: 0.6999590396881104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 20/86 [D loss: 0.6913948059082031, acc.: 53.96%] [G loss: 0.6986951231956482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 21/86 [D loss: 0.6927962005138397, acc.: 52.05%] [G loss: 0.698158860206604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 22/86 [D loss: 0.6920891106128693, acc.: 53.17%] [G loss: 0.6990304589271545]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 23/86 [D loss: 0.6927167773246765, acc.: 50.49%] [G loss: 0.6983462572097778]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 24/86 [D loss: 0.691980630159378, acc.: 52.39%] [G loss: 0.6993889808654785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 25/86 [D loss: 0.6931090354919434, acc.: 51.42%] [G loss: 0.6974326968193054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 26/86 [D loss: 0.6935204863548279, acc.: 49.46%] [G loss: 0.6987828016281128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 27/86 [D loss: 0.6926179528236389, acc.: 52.10%] [G loss: 0.6983937621116638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 28/86 [D loss: 0.6931541264057159, acc.: 50.39%] [G loss: 0.6990638971328735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 29/86 [D loss: 0.6932215988636017, acc.: 51.07%] [G loss: 0.6979552507400513]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 30/86 [D loss: 0.6940528154373169, acc.: 47.56%] [G loss: 0.7010757923126221]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 31/86 [D loss: 0.6926866769790649, acc.: 51.37%] [G loss: 0.699408233165741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 32/86 [D loss: 0.6918880343437195, acc.: 53.56%] [G loss: 0.7001866698265076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 33/86 [D loss: 0.6934662759304047, acc.: 49.56%] [G loss: 0.6982903480529785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 34/86 [D loss: 0.6927367448806763, acc.: 49.51%] [G loss: 0.6989639401435852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 35/86 [D loss: 0.6924371123313904, acc.: 51.86%] [G loss: 0.6991080045700073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 36/86 [D loss: 0.6928220391273499, acc.: 51.42%] [G loss: 0.7009127140045166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 37/86 [D loss: 0.6924086809158325, acc.: 51.90%] [G loss: 0.7000486254692078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 38/86 [D loss: 0.6916028261184692, acc.: 52.54%] [G loss: 0.7000668048858643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 39/86 [D loss: 0.6932552456855774, acc.: 51.03%] [G loss: 0.6996529698371887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 40/86 [D loss: 0.6929292380809784, acc.: 51.07%] [G loss: 0.6993808746337891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 41/86 [D loss: 0.692278653383255, acc.: 52.54%] [G loss: 0.6989269852638245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 42/86 [D loss: 0.6931929886341095, acc.: 49.90%] [G loss: 0.7000707387924194]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 43/86 [D loss: 0.6925172209739685, acc.: 50.88%] [G loss: 0.7010206580162048]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 44/86 [D loss: 0.6918898820877075, acc.: 51.86%] [G loss: 0.6993522644042969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 45/86 [D loss: 0.6937838494777679, acc.: 48.83%] [G loss: 0.6996451020240784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 46/86 [D loss: 0.692531943321228, acc.: 51.81%] [G loss: 0.7002274394035339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 47/86 [D loss: 0.6919056475162506, acc.: 52.78%] [G loss: 0.7002261281013489]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 48/86 [D loss: 0.6922385096549988, acc.: 52.44%] [G loss: 0.7001044750213623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 49/86 [D loss: 0.6927176415920258, acc.: 51.03%] [G loss: 0.6992099285125732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 50/86 [D loss: 0.6927165687084198, acc.: 50.98%] [G loss: 0.6997785568237305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 51/86 [D loss: 0.6921525299549103, acc.: 52.29%] [G loss: 0.699856162071228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 52/86 [D loss: 0.6925802528858185, acc.: 51.86%] [G loss: 0.7017935514450073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 53/86 [D loss: 0.6932090818881989, acc.: 50.73%] [G loss: 0.6986632347106934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 54/86 [D loss: 0.6923370063304901, acc.: 52.15%] [G loss: 0.7005492448806763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 55/86 [D loss: 0.6922386884689331, acc.: 51.81%] [G loss: 0.6997619867324829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 56/86 [D loss: 0.6926749348640442, acc.: 51.56%] [G loss: 0.7011712193489075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 57/86 [D loss: 0.6921302974224091, acc.: 53.08%] [G loss: 0.7000207901000977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 58/86 [D loss: 0.6932057738304138, acc.: 49.71%] [G loss: 0.701492965221405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 59/86 [D loss: 0.6925856173038483, acc.: 51.32%] [G loss: 0.6995137929916382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 60/86 [D loss: 0.6930184960365295, acc.: 49.66%] [G loss: 0.7002584338188171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 61/86 [D loss: 0.6927303969860077, acc.: 51.32%] [G loss: 0.698898434638977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 62/86 [D loss: 0.6934897005558014, acc.: 49.12%] [G loss: 0.699967622756958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 63/86 [D loss: 0.6923122704029083, acc.: 51.81%] [G loss: 0.6992865204811096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 64/86 [D loss: 0.6928563416004181, acc.: 51.22%] [G loss: 0.7000658512115479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 65/86 [D loss: 0.6922218799591064, acc.: 51.76%] [G loss: 0.6993294358253479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 66/86 [D loss: 0.6921028196811676, acc.: 52.00%] [G loss: 0.699763298034668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 67/86 [D loss: 0.6929557919502258, acc.: 50.68%] [G loss: 0.6999136209487915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 68/86 [D loss: 0.6920354664325714, acc.: 52.64%] [G loss: 0.6997974514961243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 69/86 [D loss: 0.693324863910675, acc.: 50.44%] [G loss: 0.6990022659301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 70/86 [D loss: 0.6927899420261383, acc.: 50.34%] [G loss: 0.6974726319313049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 71/86 [D loss: 0.6929438710212708, acc.: 51.22%] [G loss: 0.7004082798957825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 72/86 [D loss: 0.6941621601581573, acc.: 47.75%] [G loss: 0.6988232731819153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 73/86 [D loss: 0.6920726895332336, acc.: 52.59%] [G loss: 0.6990126371383667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 74/86 [D loss: 0.6922597587108612, acc.: 52.64%] [G loss: 0.697243869304657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 75/86 [D loss: 0.6932031512260437, acc.: 51.17%] [G loss: 0.7011502981185913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 76/86 [D loss: 0.6924102902412415, acc.: 51.81%] [G loss: 0.6974822878837585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 77/86 [D loss: 0.6933388411998749, acc.: 50.05%] [G loss: 0.6997562050819397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 78/86 [D loss: 0.6934017539024353, acc.: 50.78%] [G loss: 0.697871744632721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 79/86 [D loss: 0.6931853592395782, acc.: 50.10%] [G loss: 0.6992751955986023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 80/86 [D loss: 0.6929938793182373, acc.: 50.15%] [G loss: 0.6981035470962524]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 81/86 [D loss: 0.693479597568512, acc.: 49.37%] [G loss: 0.6992779970169067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 82/86 [D loss: 0.6926619410514832, acc.: 50.49%] [G loss: 0.6992237567901611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 83/86 [D loss: 0.6937776803970337, acc.: 50.00%] [G loss: 0.6992501020431519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 84/86 [D loss: 0.6927165687084198, acc.: 52.64%] [G loss: 0.7004624009132385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 85/86 [D loss: 0.692415714263916, acc.: 51.71%] [G loss: 0.7006077766418457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 86/86 [D loss: 0.6924125552177429, acc.: 50.83%] [G loss: 0.700391948223114]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 1/86 [D loss: 0.6935329437255859, acc.: 49.22%] [G loss: 0.7006811499595642]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 2/86 [D loss: 0.692501574754715, acc.: 51.66%] [G loss: 0.7008798122406006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 3/86 [D loss: 0.6926737427711487, acc.: 51.12%] [G loss: 0.7010855078697205]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 4/86 [D loss: 0.6919934153556824, acc.: 52.25%] [G loss: 0.700645923614502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 5/86 [D loss: 0.6933112442493439, acc.: 50.24%] [G loss: 0.7003424167633057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 6/86 [D loss: 0.6928142011165619, acc.: 52.05%] [G loss: 0.7004991769790649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 7/86 [D loss: 0.6936337649822235, acc.: 49.37%] [G loss: 0.7004847526550293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 8/86 [D loss: 0.6927892565727234, acc.: 51.17%] [G loss: 0.7008920311927795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 9/86 [D loss: 0.6931424140930176, acc.: 50.10%] [G loss: 0.7001551985740662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 10/86 [D loss: 0.6930932700634003, acc.: 51.17%] [G loss: 0.7005316019058228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 11/86 [D loss: 0.6932789981365204, acc.: 49.90%] [G loss: 0.7012656927108765]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 12/86 [D loss: 0.6935224235057831, acc.: 49.41%] [G loss: 0.7001534104347229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 13/86 [D loss: 0.6923553645610809, acc.: 51.56%] [G loss: 0.7004879117012024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 14/86 [D loss: 0.6935624182224274, acc.: 49.80%] [G loss: 0.6992937922477722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 15/86 [D loss: 0.6932086944580078, acc.: 50.24%] [G loss: 0.7013550400733948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 16/86 [D loss: 0.6928068697452545, acc.: 51.07%] [G loss: 0.7015705704689026]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 17/86 [D loss: 0.6931293308734894, acc.: 50.59%] [G loss: 0.6992209553718567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 18/86 [D loss: 0.6930555999279022, acc.: 50.44%] [G loss: 0.7010194659233093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 19/86 [D loss: 0.6927500367164612, acc.: 50.44%] [G loss: 0.7002438902854919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 20/86 [D loss: 0.6922547221183777, acc.: 52.69%] [G loss: 0.7013305425643921]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 21/86 [D loss: 0.6933891773223877, acc.: 49.22%] [G loss: 0.6989818215370178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 22/86 [D loss: 0.6935169100761414, acc.: 48.58%] [G loss: 0.7009662985801697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 23/86 [D loss: 0.6927918195724487, acc.: 51.07%] [G loss: 0.7001306414604187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 24/86 [D loss: 0.6925445199012756, acc.: 50.20%] [G loss: 0.7009224891662598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 25/86 [D loss: 0.6923706531524658, acc.: 51.66%] [G loss: 0.697828471660614]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 26/86 [D loss: 0.694097101688385, acc.: 47.95%] [G loss: 0.6995527148246765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 27/86 [D loss: 0.6921322047710419, acc.: 52.20%] [G loss: 0.7000271081924438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 28/86 [D loss: 0.6932758390903473, acc.: 48.97%] [G loss: 0.7002312541007996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 29/86 [D loss: 0.6922461092472076, acc.: 51.71%] [G loss: 0.7003124356269836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 30/86 [D loss: 0.6936589479446411, acc.: 50.44%] [G loss: 0.700356662273407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 31/86 [D loss: 0.6921730041503906, acc.: 52.73%] [G loss: 0.6987724304199219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 32/86 [D loss: 0.6933445036411285, acc.: 49.07%] [G loss: 0.7004321813583374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 33/86 [D loss: 0.6925094425678253, acc.: 51.71%] [G loss: 0.6994234919548035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 34/86 [D loss: 0.693803071975708, acc.: 51.07%] [G loss: 0.6996184587478638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 35/86 [D loss: 0.6931990087032318, acc.: 49.85%] [G loss: 0.6990280151367188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 36/86 [D loss: 0.6920494735240936, acc.: 52.39%] [G loss: 0.6994168758392334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 37/86 [D loss: 0.6930745542049408, acc.: 51.17%] [G loss: 0.6997534036636353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 38/86 [D loss: 0.6926852464675903, acc.: 52.69%] [G loss: 0.6998307108879089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 39/86 [D loss: 0.6936482191085815, acc.: 49.95%] [G loss: 0.7002660036087036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 40/86 [D loss: 0.6919145882129669, acc.: 51.66%] [G loss: 0.6985338926315308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 41/86 [D loss: 0.6934153735637665, acc.: 50.39%] [G loss: 0.698870837688446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 42/86 [D loss: 0.6932027637958527, acc.: 49.80%] [G loss: 0.6988515853881836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 43/86 [D loss: 0.692942351102829, acc.: 50.49%] [G loss: 0.7012064456939697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 44/86 [D loss: 0.6930108368396759, acc.: 50.29%] [G loss: 0.6981351375579834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 45/86 [D loss: 0.6925703287124634, acc.: 52.00%] [G loss: 0.6987560391426086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 46/86 [D loss: 0.6922187805175781, acc.: 52.29%] [G loss: 0.697571873664856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 47/86 [D loss: 0.6923380494117737, acc.: 51.51%] [G loss: 0.6990852952003479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 48/86 [D loss: 0.6931311786174774, acc.: 50.34%] [G loss: 0.6990219354629517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 49/86 [D loss: 0.692740797996521, acc.: 51.42%] [G loss: 0.6990664005279541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 50/86 [D loss: 0.6927943825721741, acc.: 50.24%] [G loss: 0.6964412927627563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 51/86 [D loss: 0.6936879456043243, acc.: 49.90%] [G loss: 0.7002683877944946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 52/86 [D loss: 0.6934912204742432, acc.: 49.66%] [G loss: 0.6990938782691956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 53/86 [D loss: 0.6926432549953461, acc.: 50.54%] [G loss: 0.7008243203163147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 54/86 [D loss: 0.6928805410861969, acc.: 52.15%] [G loss: 0.6984135508537292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 55/86 [D loss: 0.6936217546463013, acc.: 50.34%] [G loss: 0.7001120448112488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 56/86 [D loss: 0.6920818388462067, acc.: 51.90%] [G loss: 0.6988712549209595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 57/86 [D loss: 0.6930589973926544, acc.: 49.71%] [G loss: 0.6994369029998779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 58/86 [D loss: 0.6931347250938416, acc.: 51.71%] [G loss: 0.6991700530052185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 59/86 [D loss: 0.693618655204773, acc.: 48.24%] [G loss: 0.7003081440925598]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 60/86 [D loss: 0.6921077370643616, acc.: 52.88%] [G loss: 0.6996484994888306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 61/86 [D loss: 0.6936922371387482, acc.: 48.63%] [G loss: 0.7011075019836426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 62/86 [D loss: 0.6928815543651581, acc.: 51.61%] [G loss: 0.6989604830741882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 63/86 [D loss: 0.6936885714530945, acc.: 49.22%] [G loss: 0.7018682956695557]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 64/86 [D loss: 0.6925318539142609, acc.: 51.90%] [G loss: 0.6999431848526001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 65/86 [D loss: 0.6932068467140198, acc.: 50.10%] [G loss: 0.7003108263015747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 66/86 [D loss: 0.6922954320907593, acc.: 51.51%] [G loss: 0.6995024681091309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 67/86 [D loss: 0.694182276725769, acc.: 47.80%] [G loss: 0.699676513671875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 68/86 [D loss: 0.6913129389286041, acc.: 54.69%] [G loss: 0.6990637183189392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 69/86 [D loss: 0.6936158239841461, acc.: 49.41%] [G loss: 0.6995615363121033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 70/86 [D loss: 0.6925112307071686, acc.: 52.15%] [G loss: 0.6987416744232178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 71/86 [D loss: 0.6937284171581268, acc.: 49.37%] [G loss: 0.6972178816795349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 72/86 [D loss: 0.6925093531608582, acc.: 51.56%] [G loss: 0.7003262042999268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 73/86 [D loss: 0.6934053003787994, acc.: 50.00%] [G loss: 0.6981464624404907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 74/86 [D loss: 0.6924299597740173, acc.: 51.17%] [G loss: 0.6993123888969421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 75/86 [D loss: 0.6943680644035339, acc.: 48.88%] [G loss: 0.6966513395309448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 76/86 [D loss: 0.6928597986698151, acc.: 52.15%] [G loss: 0.6984474062919617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 77/86 [D loss: 0.6934376657009125, acc.: 49.56%] [G loss: 0.6980230212211609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 78/86 [D loss: 0.6940799653530121, acc.: 48.19%] [G loss: 0.6993392705917358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 79/86 [D loss: 0.6933759152889252, acc.: 49.02%] [G loss: 0.6985833048820496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 80/86 [D loss: 0.6937054991722107, acc.: 50.29%] [G loss: 0.699424684047699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 81/86 [D loss: 0.6919145584106445, acc.: 53.12%] [G loss: 0.6998895406723022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 82/86 [D loss: 0.6929024457931519, acc.: 50.15%] [G loss: 0.6996256113052368]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 83/86 [D loss: 0.6928267776966095, acc.: 51.61%] [G loss: 0.6978046894073486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 84/86 [D loss: 0.6940879225730896, acc.: 48.97%] [G loss: 0.7002971768379211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 85/86 [D loss: 0.6920051872730255, acc.: 52.39%] [G loss: 0.6986958980560303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 86/86 [D loss: 0.6933656632900238, acc.: 49.46%] [G loss: 0.6998815536499023]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 1/86 [D loss: 0.6929166913032532, acc.: 50.78%] [G loss: 0.6989587545394897]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 2/86 [D loss: 0.6932729780673981, acc.: 50.05%] [G loss: 0.700525164604187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 3/86 [D loss: 0.6930585205554962, acc.: 49.37%] [G loss: 0.7011367678642273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 4/86 [D loss: 0.691843718290329, acc.: 53.61%] [G loss: 0.7008634209632874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 5/86 [D loss: 0.6921502649784088, acc.: 51.90%] [G loss: 0.6994463801383972]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 6/86 [D loss: 0.6934620440006256, acc.: 50.54%] [G loss: 0.7010546922683716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 7/86 [D loss: 0.692084014415741, acc.: 52.49%] [G loss: 0.7001655101776123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 8/86 [D loss: 0.6919479370117188, acc.: 53.17%] [G loss: 0.7006127834320068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 9/86 [D loss: 0.692936897277832, acc.: 51.90%] [G loss: 0.7008972764015198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 10/86 [D loss: 0.6925385892391205, acc.: 51.12%] [G loss: 0.7002168297767639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 11/86 [D loss: 0.6922780275344849, acc.: 52.78%] [G loss: 0.700232207775116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 12/86 [D loss: 0.6924380660057068, acc.: 51.86%] [G loss: 0.7005410194396973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 13/86 [D loss: 0.6925686299800873, acc.: 51.32%] [G loss: 0.7010313868522644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 14/86 [D loss: 0.6928043365478516, acc.: 51.61%] [G loss: 0.7008242607116699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 15/86 [D loss: 0.6929133236408234, acc.: 50.59%] [G loss: 0.701134979724884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 16/86 [D loss: 0.6919367015361786, acc.: 53.08%] [G loss: 0.7004978656768799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 17/86 [D loss: 0.6915186047554016, acc.: 53.71%] [G loss: 0.7014979720115662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 18/86 [D loss: 0.6930822432041168, acc.: 48.68%] [G loss: 0.7002647519111633]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 19/86 [D loss: 0.693554699420929, acc.: 49.80%] [G loss: 0.7005383968353271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 20/86 [D loss: 0.6916109025478363, acc.: 52.59%] [G loss: 0.7009686231613159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 21/86 [D loss: 0.6928987205028534, acc.: 50.24%] [G loss: 0.7003529667854309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 22/86 [D loss: 0.6924600303173065, acc.: 51.76%] [G loss: 0.7000802159309387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 23/86 [D loss: 0.6934871673583984, acc.: 49.71%] [G loss: 0.7004899978637695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 24/86 [D loss: 0.6927695572376251, acc.: 51.07%] [G loss: 0.7017480731010437]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 25/86 [D loss: 0.6926904022693634, acc.: 51.56%] [G loss: 0.6997173428535461]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 26/86 [D loss: 0.6924349367618561, acc.: 51.42%] [G loss: 0.6985575556755066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 27/86 [D loss: 0.6930901408195496, acc.: 50.10%] [G loss: 0.6994072794914246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 28/86 [D loss: 0.6926034390926361, acc.: 50.59%] [G loss: 0.7002221941947937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 29/86 [D loss: 0.6930536925792694, acc.: 52.00%] [G loss: 0.6991026401519775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 30/86 [D loss: 0.6927002966403961, acc.: 50.98%] [G loss: 0.7014551162719727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 31/86 [D loss: 0.6925611197948456, acc.: 51.37%] [G loss: 0.6979103088378906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 32/86 [D loss: 0.6932713389396667, acc.: 51.32%] [G loss: 0.7002320289611816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 33/86 [D loss: 0.693175196647644, acc.: 50.29%] [G loss: 0.7009447813034058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 34/86 [D loss: 0.6930859684944153, acc.: 50.39%] [G loss: 0.7011134624481201]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 35/86 [D loss: 0.693163275718689, acc.: 50.10%] [G loss: 0.6997489333152771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 36/86 [D loss: 0.6933734118938446, acc.: 51.46%] [G loss: 0.701688289642334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 37/86 [D loss: 0.6917600929737091, acc.: 53.08%] [G loss: 0.6995074152946472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 38/86 [D loss: 0.6932626962661743, acc.: 50.98%] [G loss: 0.6996774077415466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 39/86 [D loss: 0.6933168768882751, acc.: 50.00%] [G loss: 0.6993798017501831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 40/86 [D loss: 0.6928104758262634, acc.: 50.59%] [G loss: 0.7009680271148682]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 41/86 [D loss: 0.6919627487659454, acc.: 52.88%] [G loss: 0.7003413438796997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 42/86 [D loss: 0.693089097738266, acc.: 48.63%] [G loss: 0.700616717338562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 43/86 [D loss: 0.6921132206916809, acc.: 52.64%] [G loss: 0.6996475458145142]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 44/86 [D loss: 0.693066418170929, acc.: 50.49%] [G loss: 0.7008437514305115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 45/86 [D loss: 0.692251056432724, acc.: 52.05%] [G loss: 0.7013390064239502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 46/86 [D loss: 0.6924327313899994, acc.: 51.42%] [G loss: 0.700332760810852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 47/86 [D loss: 0.6921131014823914, acc.: 51.86%] [G loss: 0.700136125087738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 48/86 [D loss: 0.6928500235080719, acc.: 50.78%] [G loss: 0.7008898854255676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 49/86 [D loss: 0.6925010681152344, acc.: 52.10%] [G loss: 0.6994779706001282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 50/86 [D loss: 0.6918641328811646, acc.: 52.93%] [G loss: 0.6997732520103455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 51/86 [D loss: 0.6920978426933289, acc.: 52.10%] [G loss: 0.7009379267692566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 52/86 [D loss: 0.6934297382831573, acc.: 49.27%] [G loss: 0.7016201019287109]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 53/86 [D loss: 0.6923035383224487, acc.: 52.83%] [G loss: 0.7010191679000854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 54/86 [D loss: 0.6925793886184692, acc.: 50.39%] [G loss: 0.7006429433822632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 55/86 [D loss: 0.693309873342514, acc.: 49.51%] [G loss: 0.7005605697631836]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 56/86 [D loss: 0.6927648186683655, acc.: 51.03%] [G loss: 0.7004968523979187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 57/86 [D loss: 0.6927685141563416, acc.: 50.20%] [G loss: 0.7015584707260132]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 58/86 [D loss: 0.6927656531333923, acc.: 50.59%] [G loss: 0.7017171382904053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 59/86 [D loss: 0.6919872164726257, acc.: 52.39%] [G loss: 0.70063716173172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 60/86 [D loss: 0.6918831467628479, acc.: 53.17%] [G loss: 0.7002602219581604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 61/86 [D loss: 0.6925236284732819, acc.: 52.25%] [G loss: 0.7011584043502808]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 62/86 [D loss: 0.6920079290866852, acc.: 52.44%] [G loss: 0.7018227577209473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 63/86 [D loss: 0.6924267411231995, acc.: 51.51%] [G loss: 0.7004679441452026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 64/86 [D loss: 0.6925704777240753, acc.: 51.32%] [G loss: 0.7009658217430115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 65/86 [D loss: 0.6927060186862946, acc.: 51.27%] [G loss: 0.7013742923736572]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 66/86 [D loss: 0.6913374960422516, acc.: 54.20%] [G loss: 0.7004848122596741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 67/86 [D loss: 0.6927679479122162, acc.: 51.86%] [G loss: 0.7026365399360657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 68/86 [D loss: 0.6922658383846283, acc.: 50.34%] [G loss: 0.7008616328239441]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 69/86 [D loss: 0.6940194368362427, acc.: 49.71%] [G loss: 0.7012182474136353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 70/86 [D loss: 0.691727340221405, acc.: 54.15%] [G loss: 0.7021042108535767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 71/86 [D loss: 0.6924665868282318, acc.: 52.05%] [G loss: 0.7000454068183899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 72/86 [D loss: 0.6920031011104584, acc.: 53.32%] [G loss: 0.7016128301620483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 73/86 [D loss: 0.6930577158927917, acc.: 50.24%] [G loss: 0.7014400362968445]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 74/86 [D loss: 0.69213005900383, acc.: 52.93%] [G loss: 0.6998123526573181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 75/86 [D loss: 0.6921269595623016, acc.: 52.05%] [G loss: 0.700124979019165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 76/86 [D loss: 0.6917265057563782, acc.: 53.12%] [G loss: 0.702021598815918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 77/86 [D loss: 0.6928680837154388, acc.: 51.03%] [G loss: 0.7014158964157104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 78/86 [D loss: 0.6921467781066895, acc.: 51.46%] [G loss: 0.7015726566314697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 79/86 [D loss: 0.6923923790454865, acc.: 51.95%] [G loss: 0.7017148733139038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 80/86 [D loss: 0.6929031014442444, acc.: 49.85%] [G loss: 0.7015518546104431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 81/86 [D loss: 0.6923378705978394, acc.: 52.39%] [G loss: 0.7016924619674683]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 82/86 [D loss: 0.6924735009670258, acc.: 51.22%] [G loss: 0.7005617022514343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 83/86 [D loss: 0.6920993030071259, acc.: 52.10%] [G loss: 0.7007440328598022]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 84/86 [D loss: 0.691402792930603, acc.: 54.20%] [G loss: 0.7009616494178772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 85/86 [D loss: 0.691524475812912, acc.: 54.05%] [G loss: 0.7008150219917297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 86/86 [D loss: 0.6913870871067047, acc.: 53.47%] [G loss: 0.7012637853622437]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 55/200, Batch 1/86 [D loss: 0.6924899220466614, acc.: 52.15%] [G loss: 0.7012740969657898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 2/86 [D loss: 0.6930674016475677, acc.: 51.22%] [G loss: 0.7006059288978577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 3/86 [D loss: 0.6915597915649414, acc.: 52.54%] [G loss: 0.7012208700180054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 4/86 [D loss: 0.6920791864395142, acc.: 51.81%] [G loss: 0.70075523853302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 5/86 [D loss: 0.6923688352108002, acc.: 50.10%] [G loss: 0.7020691633224487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 6/86 [D loss: 0.6928090751171112, acc.: 50.63%] [G loss: 0.7011038661003113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 7/86 [D loss: 0.6917701959609985, acc.: 52.15%] [G loss: 0.7010602355003357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 8/86 [D loss: 0.69268798828125, acc.: 49.66%] [G loss: 0.7016560435295105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 9/86 [D loss: 0.6929068565368652, acc.: 50.93%] [G loss: 0.7021782398223877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 10/86 [D loss: 0.6925938129425049, acc.: 50.44%] [G loss: 0.7014814615249634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 11/86 [D loss: 0.6921758353710175, acc.: 52.20%] [G loss: 0.7017509937286377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 12/86 [D loss: 0.6922253966331482, acc.: 52.49%] [G loss: 0.7024573683738708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 13/86 [D loss: 0.6923860311508179, acc.: 51.03%] [G loss: 0.7013857364654541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 14/86 [D loss: 0.6925541758537292, acc.: 51.07%] [G loss: 0.7020494937896729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 15/86 [D loss: 0.6928462982177734, acc.: 50.98%] [G loss: 0.7017670273780823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 16/86 [D loss: 0.6922658383846283, acc.: 51.12%] [G loss: 0.7014434337615967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 17/86 [D loss: 0.6922910213470459, acc.: 52.49%] [G loss: 0.7011785507202148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 18/86 [D loss: 0.6918813586235046, acc.: 53.61%] [G loss: 0.7023782730102539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 19/86 [D loss: 0.6922766864299774, acc.: 52.25%] [G loss: 0.700335681438446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 20/86 [D loss: 0.6923230588436127, acc.: 51.61%] [G loss: 0.7019665837287903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 21/86 [D loss: 0.6914554834365845, acc.: 53.56%] [G loss: 0.7019797563552856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 22/86 [D loss: 0.6919669806957245, acc.: 52.25%] [G loss: 0.7018722295761108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 23/86 [D loss: 0.6927694380283356, acc.: 51.66%] [G loss: 0.6994277834892273]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 24/86 [D loss: 0.693405032157898, acc.: 49.61%] [G loss: 0.7015348672866821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 25/86 [D loss: 0.6918267011642456, acc.: 52.39%] [G loss: 0.7036750316619873]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 26/86 [D loss: 0.6928697526454926, acc.: 50.68%] [G loss: 0.7015664577484131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 27/86 [D loss: 0.6933034360408783, acc.: 49.85%] [G loss: 0.7009454965591431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 28/86 [D loss: 0.6927517950534821, acc.: 50.88%] [G loss: 0.702655553817749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 29/86 [D loss: 0.6928742229938507, acc.: 50.98%] [G loss: 0.7018311023712158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 30/86 [D loss: 0.6922557353973389, acc.: 52.39%] [G loss: 0.7022773027420044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 31/86 [D loss: 0.6926806271076202, acc.: 50.88%] [G loss: 0.7022599577903748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 32/86 [D loss: 0.6929180324077606, acc.: 50.59%] [G loss: 0.7025970816612244]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 33/86 [D loss: 0.6916530132293701, acc.: 54.79%] [G loss: 0.7007582187652588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 34/86 [D loss: 0.6920002102851868, acc.: 52.44%] [G loss: 0.7016984224319458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 35/86 [D loss: 0.6936874091625214, acc.: 48.88%] [G loss: 0.7025801539421082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 36/86 [D loss: 0.6913225650787354, acc.: 54.64%] [G loss: 0.701452374458313]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 37/86 [D loss: 0.6930990815162659, acc.: 50.39%] [G loss: 0.7002067565917969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 38/86 [D loss: 0.6923608183860779, acc.: 51.37%] [G loss: 0.7012661695480347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 39/86 [D loss: 0.6916942596435547, acc.: 52.39%] [G loss: 0.7010669708251953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 40/86 [D loss: 0.6916672885417938, acc.: 53.52%] [G loss: 0.7010042667388916]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 41/86 [D loss: 0.6917594373226166, acc.: 51.95%] [G loss: 0.7019127011299133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 42/86 [D loss: 0.6920627355575562, acc.: 52.88%] [G loss: 0.7026206254959106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 43/86 [D loss: 0.6924282014369965, acc.: 50.49%] [G loss: 0.7013112306594849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 44/86 [D loss: 0.6931750178337097, acc.: 49.90%] [G loss: 0.7019222378730774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 45/86 [D loss: 0.6921794712543488, acc.: 52.64%] [G loss: 0.7000565528869629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 46/86 [D loss: 0.691785991191864, acc.: 52.93%] [G loss: 0.7020708918571472]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 47/86 [D loss: 0.6928527057170868, acc.: 50.34%] [G loss: 0.6996399760246277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 48/86 [D loss: 0.6929464042186737, acc.: 50.00%] [G loss: 0.7033425569534302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 49/86 [D loss: 0.6922286152839661, acc.: 52.15%] [G loss: 0.70085608959198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 50/86 [D loss: 0.6926196217536926, acc.: 50.63%] [G loss: 0.7025560736656189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 51/86 [D loss: 0.6926891207695007, acc.: 52.64%] [G loss: 0.6994872093200684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 52/86 [D loss: 0.693385899066925, acc.: 49.51%] [G loss: 0.7027502059936523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 53/86 [D loss: 0.6913169026374817, acc.: 53.66%] [G loss: 0.7008013725280762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 54/86 [D loss: 0.6923088729381561, acc.: 52.49%] [G loss: 0.7019928097724915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 55/86 [D loss: 0.6915764510631561, acc.: 52.10%] [G loss: 0.6993189454078674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 56/86 [D loss: 0.6928874850273132, acc.: 50.78%] [G loss: 0.7015359401702881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 57/86 [D loss: 0.6915676891803741, acc.: 52.73%] [G loss: 0.7000445127487183]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 58/86 [D loss: 0.6918050348758698, acc.: 53.08%] [G loss: 0.7014410495758057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 59/86 [D loss: 0.6922764182090759, acc.: 51.07%] [G loss: 0.6995542049407959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 60/86 [D loss: 0.6928896009922028, acc.: 49.46%] [G loss: 0.7011272311210632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 61/86 [D loss: 0.6920467019081116, acc.: 51.66%] [G loss: 0.7007406949996948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 62/86 [D loss: 0.6916880011558533, acc.: 52.39%] [G loss: 0.7014989852905273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 63/86 [D loss: 0.6921243369579315, acc.: 52.54%] [G loss: 0.6996052861213684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 64/86 [D loss: 0.6933071911334991, acc.: 50.68%] [G loss: 0.7029445171356201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 65/86 [D loss: 0.6925840973854065, acc.: 51.61%] [G loss: 0.6998836994171143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 66/86 [D loss: 0.6914339065551758, acc.: 52.25%] [G loss: 0.7028100490570068]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 67/86 [D loss: 0.6927973031997681, acc.: 50.68%] [G loss: 0.7011760473251343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 68/86 [D loss: 0.6932208836078644, acc.: 50.24%] [G loss: 0.7020270228385925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 69/86 [D loss: 0.6921848058700562, acc.: 51.56%] [G loss: 0.7021166086196899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 70/86 [D loss: 0.6914345622062683, acc.: 53.91%] [G loss: 0.7006935477256775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 71/86 [D loss: 0.6926310658454895, acc.: 51.51%] [G loss: 0.701724648475647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 72/86 [D loss: 0.6925039887428284, acc.: 51.22%] [G loss: 0.7013982534408569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 73/86 [D loss: 0.6922876834869385, acc.: 52.49%] [G loss: 0.7015402913093567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 74/86 [D loss: 0.6918005645275116, acc.: 52.83%] [G loss: 0.7023260593414307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 75/86 [D loss: 0.6932622492313385, acc.: 51.27%] [G loss: 0.7005985975265503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 76/86 [D loss: 0.6922855079174042, acc.: 51.66%] [G loss: 0.7018519639968872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 77/86 [D loss: 0.6918257176876068, acc.: 52.44%] [G loss: 0.7014110088348389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 78/86 [D loss: 0.69194296002388, acc.: 52.83%] [G loss: 0.7015737891197205]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 79/86 [D loss: 0.6924146413803101, acc.: 51.03%] [G loss: 0.6999743580818176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 80/86 [D loss: 0.6926109492778778, acc.: 50.73%] [G loss: 0.7022637724876404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 81/86 [D loss: 0.6926760673522949, acc.: 51.12%] [G loss: 0.7024548053741455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 82/86 [D loss: 0.6908472776412964, acc.: 54.10%] [G loss: 0.702759325504303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 83/86 [D loss: 0.6925399303436279, acc.: 51.22%] [G loss: 0.6994779109954834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 84/86 [D loss: 0.6926348805427551, acc.: 50.63%] [G loss: 0.7033591270446777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 85/86 [D loss: 0.691972404718399, acc.: 53.86%] [G loss: 0.7012844085693359]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 86/86 [D loss: 0.691622793674469, acc.: 52.93%] [G loss: 0.7015700936317444]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 1/86 [D loss: 0.6920478641986847, acc.: 52.29%] [G loss: 0.7015909552574158]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 2/86 [D loss: 0.692851185798645, acc.: 50.49%] [G loss: 0.702703595161438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 3/86 [D loss: 0.6917734146118164, acc.: 53.12%] [G loss: 0.7022951245307922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 4/86 [D loss: 0.6918034255504608, acc.: 51.56%] [G loss: 0.7012295126914978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 5/86 [D loss: 0.6920652091503143, acc.: 52.64%] [G loss: 0.702014148235321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 6/86 [D loss: 0.6923338174819946, acc.: 51.66%] [G loss: 0.7025420665740967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 7/86 [D loss: 0.6917739808559418, acc.: 54.00%] [G loss: 0.7018802762031555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 8/86 [D loss: 0.6916446089744568, acc.: 53.42%] [G loss: 0.7025892734527588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 9/86 [D loss: 0.6922513544559479, acc.: 50.98%] [G loss: 0.702406644821167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 10/86 [D loss: 0.6918942630290985, acc.: 52.83%] [G loss: 0.7014582753181458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 11/86 [D loss: 0.6910403072834015, acc.: 53.86%] [G loss: 0.7030915021896362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 12/86 [D loss: 0.6909382343292236, acc.: 53.66%] [G loss: 0.7028315663337708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 13/86 [D loss: 0.6920212507247925, acc.: 52.15%] [G loss: 0.7035303115844727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 14/86 [D loss: 0.6923598051071167, acc.: 50.83%] [G loss: 0.7035738229751587]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 15/86 [D loss: 0.6924328207969666, acc.: 51.07%] [G loss: 0.7024332284927368]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 16/86 [D loss: 0.6917342245578766, acc.: 51.76%] [G loss: 0.7022959589958191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 17/86 [D loss: 0.6919946074485779, acc.: 52.73%] [G loss: 0.7034652829170227]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 18/86 [D loss: 0.6924579739570618, acc.: 52.69%] [G loss: 0.7017467021942139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 19/86 [D loss: 0.6912282705307007, acc.: 53.22%] [G loss: 0.702889621257782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 20/86 [D loss: 0.6921842396259308, acc.: 52.54%] [G loss: 0.7001644968986511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 21/86 [D loss: 0.6932656764984131, acc.: 50.68%] [G loss: 0.7020447850227356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 22/86 [D loss: 0.6917526721954346, acc.: 53.56%] [G loss: 0.7018148303031921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 23/86 [D loss: 0.6923461258411407, acc.: 50.49%] [G loss: 0.702589213848114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 24/86 [D loss: 0.691493034362793, acc.: 53.61%] [G loss: 0.7012843489646912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 25/86 [D loss: 0.6932639181613922, acc.: 49.66%] [G loss: 0.7036682367324829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 26/86 [D loss: 0.6911201775074005, acc.: 54.25%] [G loss: 0.7016770839691162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 27/86 [D loss: 0.6925614774227142, acc.: 50.44%] [G loss: 0.7023645043373108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 28/86 [D loss: 0.6915571987628937, acc.: 53.76%] [G loss: 0.7018229365348816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 29/86 [D loss: 0.6935282349586487, acc.: 49.80%] [G loss: 0.7016485929489136]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 30/86 [D loss: 0.691224604845047, acc.: 52.59%] [G loss: 0.7009615898132324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 31/86 [D loss: 0.6923637986183167, acc.: 51.32%] [G loss: 0.7009571194648743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 32/86 [D loss: 0.6911556124687195, acc.: 53.96%] [G loss: 0.7016199827194214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 33/86 [D loss: 0.6923959851264954, acc.: 51.76%] [G loss: 0.701526403427124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 34/86 [D loss: 0.6919264495372772, acc.: 51.42%] [G loss: 0.7023137211799622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 35/86 [D loss: 0.6918444931507111, acc.: 52.69%] [G loss: 0.7000991702079773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 36/86 [D loss: 0.6916382610797882, acc.: 53.08%] [G loss: 0.7022386789321899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 37/86 [D loss: 0.6928148865699768, acc.: 51.22%] [G loss: 0.6997122168540955]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 38/86 [D loss: 0.6933518648147583, acc.: 50.93%] [G loss: 0.701797366142273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 39/86 [D loss: 0.6925302743911743, acc.: 52.10%] [G loss: 0.6995569467544556]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 40/86 [D loss: 0.6913433969020844, acc.: 52.78%] [G loss: 0.7021758556365967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 41/86 [D loss: 0.6928693950176239, acc.: 52.00%] [G loss: 0.6968371868133545]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 42/86 [D loss: 0.6955572068691254, acc.: 46.63%] [G loss: 0.700993537902832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 43/86 [D loss: 0.6918877065181732, acc.: 53.17%] [G loss: 0.699821412563324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 44/86 [D loss: 0.6924290955066681, acc.: 51.12%] [G loss: 0.7014046907424927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 45/86 [D loss: 0.6919805407524109, acc.: 50.78%] [G loss: 0.6963315606117249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 46/86 [D loss: 0.6950235366821289, acc.: 46.29%] [G loss: 0.7020322680473328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 47/86 [D loss: 0.6924231946468353, acc.: 50.24%] [G loss: 0.7001465559005737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 48/86 [D loss: 0.6926647424697876, acc.: 50.78%] [G loss: 0.7019434571266174]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 49/86 [D loss: 0.6914357244968414, acc.: 53.42%] [G loss: 0.6994973421096802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 50/86 [D loss: 0.6943026781082153, acc.: 47.41%] [G loss: 0.7018726468086243]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 51/86 [D loss: 0.6916733682155609, acc.: 53.42%] [G loss: 0.7009965777397156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 52/86 [D loss: 0.6914932429790497, acc.: 52.98%] [G loss: 0.7003797292709351]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 53/86 [D loss: 0.6918938755989075, acc.: 50.78%] [G loss: 0.7011196613311768]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 54/86 [D loss: 0.6926731765270233, acc.: 50.93%] [G loss: 0.7033873796463013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 55/86 [D loss: 0.6929938793182373, acc.: 49.95%] [G loss: 0.701970100402832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 56/86 [D loss: 0.6918885409832001, acc.: 53.22%] [G loss: 0.7004939317703247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 57/86 [D loss: 0.6913753151893616, acc.: 54.10%] [G loss: 0.7010612487792969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 58/86 [D loss: 0.6930457949638367, acc.: 51.95%] [G loss: 0.7027053833007812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 59/86 [D loss: 0.6916977167129517, acc.: 52.25%] [G loss: 0.7020375728607178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 60/86 [D loss: 0.6914023756980896, acc.: 52.44%] [G loss: 0.7021350860595703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 61/86 [D loss: 0.6914424896240234, acc.: 51.90%] [G loss: 0.7026960253715515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 62/86 [D loss: 0.6924887597560883, acc.: 50.83%] [G loss: 0.7014967203140259]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 63/86 [D loss: 0.6918557584285736, acc.: 52.83%] [G loss: 0.7030168771743774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 64/86 [D loss: 0.6913780868053436, acc.: 53.03%] [G loss: 0.701625645160675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 65/86 [D loss: 0.69193434715271, acc.: 53.91%] [G loss: 0.7013821005821228]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 66/86 [D loss: 0.691862553358078, acc.: 51.71%] [G loss: 0.7022936940193176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 67/86 [D loss: 0.6913987696170807, acc.: 53.08%] [G loss: 0.7019089460372925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 68/86 [D loss: 0.6923448145389557, acc.: 51.32%] [G loss: 0.7011536955833435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 69/86 [D loss: 0.691723108291626, acc.: 52.59%] [G loss: 0.7022091150283813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 70/86 [D loss: 0.6912969946861267, acc.: 54.20%] [G loss: 0.7032404541969299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 71/86 [D loss: 0.6921333074569702, acc.: 52.49%] [G loss: 0.7011083364486694]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 72/86 [D loss: 0.6924176216125488, acc.: 52.00%] [G loss: 0.7023542523384094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 73/86 [D loss: 0.6906060576438904, acc.: 53.52%] [G loss: 0.7014736533164978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 74/86 [D loss: 0.6909647881984711, acc.: 54.00%] [G loss: 0.7029699683189392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 75/86 [D loss: 0.6917279362678528, acc.: 52.83%] [G loss: 0.7017322778701782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 76/86 [D loss: 0.6917241215705872, acc.: 52.98%] [G loss: 0.7008785009384155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 77/86 [D loss: 0.6904865503311157, acc.: 53.71%] [G loss: 0.7017255425453186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 78/86 [D loss: 0.691482663154602, acc.: 52.49%] [G loss: 0.702786386013031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 79/86 [D loss: 0.6924777030944824, acc.: 51.37%] [G loss: 0.7015295028686523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 80/86 [D loss: 0.6911988258361816, acc.: 53.42%] [G loss: 0.7017015814781189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 81/86 [D loss: 0.6910098493099213, acc.: 54.05%] [G loss: 0.7022839188575745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 82/86 [D loss: 0.6923443675041199, acc.: 51.27%] [G loss: 0.7018102407455444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 83/86 [D loss: 0.6915820837020874, acc.: 52.83%] [G loss: 0.7025504112243652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 84/86 [D loss: 0.6916829943656921, acc.: 52.15%] [G loss: 0.7014182209968567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 85/86 [D loss: 0.6914963126182556, acc.: 54.44%] [G loss: 0.7020004391670227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 86/86 [D loss: 0.691487580537796, acc.: 52.44%] [G loss: 0.7021770477294922]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 1/86 [D loss: 0.6925373077392578, acc.: 51.12%] [G loss: 0.7025672793388367]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 2/86 [D loss: 0.6910285353660583, acc.: 53.76%] [G loss: 0.7018938064575195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 3/86 [D loss: 0.6906002461910248, acc.: 55.57%] [G loss: 0.7018328905105591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 4/86 [D loss: 0.6913354098796844, acc.: 55.27%] [G loss: 0.7023959159851074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 5/86 [D loss: 0.6922899186611176, acc.: 51.46%] [G loss: 0.7006328701972961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 6/86 [D loss: 0.6913646161556244, acc.: 52.64%] [G loss: 0.701305091381073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 7/86 [D loss: 0.6918549835681915, acc.: 52.59%] [G loss: 0.7022742629051208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 8/86 [D loss: 0.6897147595882416, acc.: 55.81%] [G loss: 0.7006782293319702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 9/86 [D loss: 0.6924026608467102, acc.: 50.44%] [G loss: 0.7006842494010925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 10/86 [D loss: 0.690457284450531, acc.: 54.44%] [G loss: 0.701063871383667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 11/86 [D loss: 0.6919888257980347, acc.: 53.56%] [G loss: 0.7020391821861267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 12/86 [D loss: 0.6916649043560028, acc.: 53.22%] [G loss: 0.7000912427902222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 13/86 [D loss: 0.6933202743530273, acc.: 50.83%] [G loss: 0.7020956873893738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 14/86 [D loss: 0.6911379992961884, acc.: 53.61%] [G loss: 0.7014765739440918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 15/86 [D loss: 0.6922726631164551, acc.: 52.54%] [G loss: 0.7005754113197327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 16/86 [D loss: 0.6920479834079742, acc.: 51.66%] [G loss: 0.7024918794631958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 17/86 [D loss: 0.6924141049385071, acc.: 52.25%] [G loss: 0.6990821957588196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 18/86 [D loss: 0.6917224228382111, acc.: 53.08%] [G loss: 0.7025138735771179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 19/86 [D loss: 0.6927365064620972, acc.: 49.80%] [G loss: 0.7009639739990234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 20/86 [D loss: 0.6920947134494781, acc.: 51.46%] [G loss: 0.7017250657081604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 21/86 [D loss: 0.6922069191932678, acc.: 52.34%] [G loss: 0.6992166042327881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 22/86 [D loss: 0.6920645833015442, acc.: 51.66%] [G loss: 0.7009131908416748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 23/86 [D loss: 0.6931021511554718, acc.: 50.44%] [G loss: 0.7002130150794983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 24/86 [D loss: 0.6913906633853912, acc.: 53.12%] [G loss: 0.7008887529373169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 25/86 [D loss: 0.6921157538890839, acc.: 50.63%] [G loss: 0.6988215446472168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 26/86 [D loss: 0.6914527416229248, acc.: 53.52%] [G loss: 0.7012031674385071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 27/86 [D loss: 0.6926089227199554, acc.: 51.86%] [G loss: 0.6997197866439819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 28/86 [D loss: 0.6919983327388763, acc.: 52.05%] [G loss: 0.7010630369186401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 29/86 [D loss: 0.6919218897819519, acc.: 51.95%] [G loss: 0.6982479691505432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 30/86 [D loss: 0.6928842961788177, acc.: 49.41%] [G loss: 0.7019307017326355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 31/86 [D loss: 0.6924814581871033, acc.: 50.98%] [G loss: 0.6993201971054077]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 32/86 [D loss: 0.6911492347717285, acc.: 53.56%] [G loss: 0.7005395293235779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 33/86 [D loss: 0.6912490427494049, acc.: 54.10%] [G loss: 0.6978611350059509]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 34/86 [D loss: 0.6938043534755707, acc.: 49.17%] [G loss: 0.7024018168449402]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 35/86 [D loss: 0.6917417049407959, acc.: 53.66%] [G loss: 0.7001458406448364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 36/86 [D loss: 0.6930289268493652, acc.: 49.37%] [G loss: 0.7032961845397949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 37/86 [D loss: 0.6908257007598877, acc.: 54.93%] [G loss: 0.7000192999839783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 38/86 [D loss: 0.6926270127296448, acc.: 50.44%] [G loss: 0.7000257968902588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 39/86 [D loss: 0.6919556558132172, acc.: 52.64%] [G loss: 0.7011391520500183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 40/86 [D loss: 0.6929813027381897, acc.: 50.63%] [G loss: 0.7017906904220581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 41/86 [D loss: 0.6926737129688263, acc.: 51.42%] [G loss: 0.701251208782196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 42/86 [D loss: 0.691769152879715, acc.: 52.20%] [G loss: 0.699028491973877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 43/86 [D loss: 0.692796528339386, acc.: 50.68%] [G loss: 0.7021013498306274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 44/86 [D loss: 0.6912522614002228, acc.: 53.47%] [G loss: 0.7011108994483948]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 45/86 [D loss: 0.6918160319328308, acc.: 52.05%] [G loss: 0.7016383409500122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 46/86 [D loss: 0.6920890212059021, acc.: 53.12%] [G loss: 0.7005577087402344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 47/86 [D loss: 0.6923587918281555, acc.: 51.32%] [G loss: 0.7023492455482483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 48/86 [D loss: 0.6907985508441925, acc.: 54.74%] [G loss: 0.7011094093322754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 49/86 [D loss: 0.6911048889160156, acc.: 53.56%] [G loss: 0.7029390931129456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 50/86 [D loss: 0.6911066770553589, acc.: 54.00%] [G loss: 0.7015771865844727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 51/86 [D loss: 0.6925235986709595, acc.: 51.37%] [G loss: 0.7014589309692383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 52/86 [D loss: 0.6910772621631622, acc.: 55.03%] [G loss: 0.701292872428894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 53/86 [D loss: 0.691880077123642, acc.: 52.29%] [G loss: 0.7001631855964661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 54/86 [D loss: 0.690923422574997, acc.: 53.86%] [G loss: 0.701275110244751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 55/86 [D loss: 0.6920427680015564, acc.: 52.25%] [G loss: 0.7008423805236816]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 56/86 [D loss: 0.6914437413215637, acc.: 53.91%] [G loss: 0.7016568779945374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 57/86 [D loss: 0.691165417432785, acc.: 53.76%] [G loss: 0.7000876665115356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 58/86 [D loss: 0.6924385726451874, acc.: 51.71%] [G loss: 0.701932966709137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 59/86 [D loss: 0.6914272010326385, acc.: 52.88%] [G loss: 0.6989635229110718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 60/86 [D loss: 0.6934956312179565, acc.: 49.12%] [G loss: 0.7027100324630737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 61/86 [D loss: 0.6910755336284637, acc.: 54.15%] [G loss: 0.7007988095283508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 62/86 [D loss: 0.6924599707126617, acc.: 51.51%] [G loss: 0.702124834060669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 63/86 [D loss: 0.6911549866199493, acc.: 53.17%] [G loss: 0.7010226249694824]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 64/86 [D loss: 0.6927501857280731, acc.: 51.42%] [G loss: 0.7009495496749878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 65/86 [D loss: 0.6911595165729523, acc.: 53.37%] [G loss: 0.7024139761924744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 66/86 [D loss: 0.6923049986362457, acc.: 51.37%] [G loss: 0.7001305818557739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 67/86 [D loss: 0.6921561658382416, acc.: 53.37%] [G loss: 0.7016775608062744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 68/86 [D loss: 0.691465824842453, acc.: 53.56%] [G loss: 0.6990820169448853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 69/86 [D loss: 0.6936875283718109, acc.: 49.02%] [G loss: 0.7021897435188293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 70/86 [D loss: 0.6917346119880676, acc.: 53.08%] [G loss: 0.6987475752830505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 71/86 [D loss: 0.6912843585014343, acc.: 52.98%] [G loss: 0.7017021179199219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 72/86 [D loss: 0.6913062930107117, acc.: 54.10%] [G loss: 0.6992020606994629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 73/86 [D loss: 0.6934021413326263, acc.: 49.51%] [G loss: 0.6997779607772827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 74/86 [D loss: 0.6915824711322784, acc.: 53.08%] [G loss: 0.7002545595169067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 75/86 [D loss: 0.6915024816989899, acc.: 53.27%] [G loss: 0.7014833092689514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 76/86 [D loss: 0.6914008855819702, acc.: 52.34%] [G loss: 0.7008799314498901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 77/86 [D loss: 0.6916907727718353, acc.: 51.12%] [G loss: 0.7003626823425293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 78/86 [D loss: 0.6916142404079437, acc.: 52.49%] [G loss: 0.7021335363388062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 79/86 [D loss: 0.6920490860939026, acc.: 52.49%] [G loss: 0.7006624937057495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 80/86 [D loss: 0.6919355094432831, acc.: 50.78%] [G loss: 0.7009825706481934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 81/86 [D loss: 0.6915227770805359, acc.: 52.93%] [G loss: 0.7002221345901489]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 82/86 [D loss: 0.6907936036586761, acc.: 53.37%] [G loss: 0.7017382383346558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 83/86 [D loss: 0.6911800503730774, acc.: 53.96%] [G loss: 0.7002701759338379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 84/86 [D loss: 0.6918919682502747, acc.: 51.27%] [G loss: 0.7001863718032837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 85/86 [D loss: 0.6918277144432068, acc.: 52.54%] [G loss: 0.7006440758705139]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 86/86 [D loss: 0.6924080848693848, acc.: 52.10%] [G loss: 0.7004615068435669]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 1/86 [D loss: 0.6907938420772552, acc.: 54.00%] [G loss: 0.7009673714637756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 2/86 [D loss: 0.6916673481464386, acc.: 52.78%] [G loss: 0.7006925344467163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 3/86 [D loss: 0.6917460560798645, acc.: 52.59%] [G loss: 0.6991907358169556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 4/86 [D loss: 0.6917428076267242, acc.: 52.20%] [G loss: 0.7007782459259033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 5/86 [D loss: 0.6919376850128174, acc.: 52.54%] [G loss: 0.7020748853683472]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 6/86 [D loss: 0.69176384806633, acc.: 52.88%] [G loss: 0.7008236646652222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 7/86 [D loss: 0.6918143630027771, acc.: 52.54%] [G loss: 0.7022572159767151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 8/86 [D loss: 0.6914942860603333, acc.: 52.49%] [G loss: 0.7006354928016663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 9/86 [D loss: 0.6935530006885529, acc.: 48.49%] [G loss: 0.7007006406784058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 10/86 [D loss: 0.6909559369087219, acc.: 53.66%] [G loss: 0.7003841996192932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 11/86 [D loss: 0.6921002268791199, acc.: 51.95%] [G loss: 0.7014603614807129]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 12/86 [D loss: 0.690649539232254, acc.: 53.71%] [G loss: 0.700208842754364]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 13/86 [D loss: 0.6935096681118011, acc.: 50.83%] [G loss: 0.7011474370956421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 14/86 [D loss: 0.6921994984149933, acc.: 52.39%] [G loss: 0.7005784511566162]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 15/86 [D loss: 0.6919979751110077, acc.: 52.00%] [G loss: 0.6995976567268372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 16/86 [D loss: 0.6907210052013397, acc.: 54.54%] [G loss: 0.7001782655715942]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 17/86 [D loss: 0.69182088971138, acc.: 53.03%] [G loss: 0.6997572779655457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 18/86 [D loss: 0.6918711364269257, acc.: 51.81%] [G loss: 0.701886773109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 19/86 [D loss: 0.6928133368492126, acc.: 52.15%] [G loss: 0.701216459274292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 20/86 [D loss: 0.6917587220668793, acc.: 53.22%] [G loss: 0.7016845941543579]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 21/86 [D loss: 0.692464292049408, acc.: 50.83%] [G loss: 0.7000003457069397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 22/86 [D loss: 0.693303644657135, acc.: 48.97%] [G loss: 0.7011458873748779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 23/86 [D loss: 0.6909742653369904, acc.: 54.00%] [G loss: 0.7009345889091492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 24/86 [D loss: 0.6920524537563324, acc.: 52.44%] [G loss: 0.7021238207817078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 25/86 [D loss: 0.6914851665496826, acc.: 52.49%] [G loss: 0.7005073428153992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 26/86 [D loss: 0.6921481490135193, acc.: 51.51%] [G loss: 0.7006826996803284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 27/86 [D loss: 0.6900799870491028, acc.: 55.27%] [G loss: 0.6998524069786072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 28/86 [D loss: 0.6910771727561951, acc.: 53.42%] [G loss: 0.7012538313865662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 29/86 [D loss: 0.6915030181407928, acc.: 52.44%] [G loss: 0.7006609439849854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 30/86 [D loss: 0.6922057569026947, acc.: 50.98%] [G loss: 0.6976500749588013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 31/86 [D loss: 0.6920336186885834, acc.: 52.15%] [G loss: 0.7022005319595337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 32/86 [D loss: 0.6922791302204132, acc.: 50.29%] [G loss: 0.6987093687057495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 33/86 [D loss: 0.6924895644187927, acc.: 50.93%] [G loss: 0.7002721428871155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 34/86 [D loss: 0.6917158365249634, acc.: 51.90%] [G loss: 0.697886049747467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 35/86 [D loss: 0.6938469707965851, acc.: 48.10%] [G loss: 0.7002829909324646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 36/86 [D loss: 0.6897204518318176, acc.: 54.98%] [G loss: 0.7001283168792725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 37/86 [D loss: 0.6923541128635406, acc.: 51.03%] [G loss: 0.7003704309463501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 38/86 [D loss: 0.6914395987987518, acc.: 53.42%] [G loss: 0.7005115747451782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 39/86 [D loss: 0.6931959688663483, acc.: 50.05%] [G loss: 0.6991596221923828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 40/86 [D loss: 0.6914782822132111, acc.: 54.59%] [G loss: 0.7016105055809021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 41/86 [D loss: 0.691726803779602, acc.: 52.29%] [G loss: 0.6992619037628174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 42/86 [D loss: 0.6910321116447449, acc.: 53.37%] [G loss: 0.6996572613716125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 43/86 [D loss: 0.6925847828388214, acc.: 50.10%] [G loss: 0.6970429420471191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 44/86 [D loss: 0.693186342716217, acc.: 50.78%] [G loss: 0.7014349102973938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 45/86 [D loss: 0.6920930445194244, acc.: 51.66%] [G loss: 0.698379635810852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 46/86 [D loss: 0.6927779316902161, acc.: 50.54%] [G loss: 0.7005631923675537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 47/86 [D loss: 0.6909486651420593, acc.: 55.42%] [G loss: 0.6965100765228271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 48/86 [D loss: 0.6928346157073975, acc.: 51.81%] [G loss: 0.6993048191070557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 49/86 [D loss: 0.6916682124137878, acc.: 51.86%] [G loss: 0.6998857259750366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 50/86 [D loss: 0.6916168630123138, acc.: 53.56%] [G loss: 0.7009290456771851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 51/86 [D loss: 0.6908068060874939, acc.: 54.59%] [G loss: 0.6994283199310303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 52/86 [D loss: 0.6935287714004517, acc.: 50.44%] [G loss: 0.6989206671714783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 53/86 [D loss: 0.6913168132305145, acc.: 52.64%] [G loss: 0.7008053064346313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 54/86 [D loss: 0.6929211616516113, acc.: 49.51%] [G loss: 0.6985983848571777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 55/86 [D loss: 0.6919132769107819, acc.: 52.20%] [G loss: 0.6995022296905518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 56/86 [D loss: 0.6927422285079956, acc.: 49.71%] [G loss: 0.698273777961731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 57/86 [D loss: 0.6929768323898315, acc.: 50.44%] [G loss: 0.7024370431900024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 58/86 [D loss: 0.6904122233390808, acc.: 54.64%] [G loss: 0.7012168169021606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 59/86 [D loss: 0.6921307444572449, acc.: 52.10%] [G loss: 0.6998461484909058]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 60/86 [D loss: 0.6919293701648712, acc.: 54.00%] [G loss: 0.698923647403717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 61/86 [D loss: 0.6922942996025085, acc.: 51.46%] [G loss: 0.6997520923614502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 62/86 [D loss: 0.6907301545143127, acc.: 53.22%] [G loss: 0.700899064540863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 63/86 [D loss: 0.6926977932453156, acc.: 50.88%] [G loss: 0.7013260126113892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 64/86 [D loss: 0.6927708983421326, acc.: 49.95%] [G loss: 0.7012331485748291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 65/86 [D loss: 0.6933498382568359, acc.: 50.78%] [G loss: 0.6995043754577637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 66/86 [D loss: 0.6920510828495026, acc.: 51.95%] [G loss: 0.7006211876869202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 67/86 [D loss: 0.6913941204547882, acc.: 52.83%] [G loss: 0.6998946070671082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 68/86 [D loss: 0.6921676099300385, acc.: 52.00%] [G loss: 0.7018110752105713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 69/86 [D loss: 0.6901240646839142, acc.: 55.57%] [G loss: 0.6976180076599121]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 70/86 [D loss: 0.6950162947177887, acc.: 47.85%] [G loss: 0.7009671330451965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 71/86 [D loss: 0.6912684738636017, acc.: 53.52%] [G loss: 0.7005212306976318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 72/86 [D loss: 0.6938019692897797, acc.: 48.73%] [G loss: 0.699447751045227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 73/86 [D loss: 0.6924648582935333, acc.: 51.03%] [G loss: 0.7021064758300781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 74/86 [D loss: 0.6917431056499481, acc.: 52.10%] [G loss: 0.6954810619354248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 75/86 [D loss: 0.6962086260318756, acc.: 45.70%] [G loss: 0.7023561000823975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 76/86 [D loss: 0.6903335452079773, acc.: 53.71%] [G loss: 0.6967751979827881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 77/86 [D loss: 0.6956096887588501, acc.: 47.31%] [G loss: 0.6988485455513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 78/86 [D loss: 0.6913988590240479, acc.: 53.56%] [G loss: 0.7012142539024353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 79/86 [D loss: 0.6920196413993835, acc.: 52.83%] [G loss: 0.6928287148475647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 80/86 [D loss: 0.6962957680225372, acc.: 46.53%] [G loss: 0.7019979357719421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 81/86 [D loss: 0.6905674636363983, acc.: 54.25%] [G loss: 0.697537899017334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 82/86 [D loss: 0.6940395534038544, acc.: 48.54%] [G loss: 0.6983608603477478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 83/86 [D loss: 0.6908266544342041, acc.: 53.27%] [G loss: 0.7000324130058289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 84/86 [D loss: 0.6920885145664215, acc.: 51.37%] [G loss: 0.6955867409706116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 85/86 [D loss: 0.6935043632984161, acc.: 50.00%] [G loss: 0.699815571308136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 86/86 [D loss: 0.691107988357544, acc.: 53.27%] [G loss: 0.7000853419303894]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 1/86 [D loss: 0.6927929222583771, acc.: 49.95%] [G loss: 0.6992015242576599]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 2/86 [D loss: 0.6912829279899597, acc.: 52.59%] [G loss: 0.7007468938827515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 3/86 [D loss: 0.6931628584861755, acc.: 49.32%] [G loss: 0.6985403299331665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 4/86 [D loss: 0.693198025226593, acc.: 50.10%] [G loss: 0.7018523812294006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 5/86 [D loss: 0.6920468509197235, acc.: 52.25%] [G loss: 0.7010560035705566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 6/86 [D loss: 0.6925674974918365, acc.: 51.86%] [G loss: 0.7011971473693848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 7/86 [D loss: 0.6913812756538391, acc.: 52.88%] [G loss: 0.701962947845459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 8/86 [D loss: 0.6923801004886627, acc.: 51.32%] [G loss: 0.6999975442886353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 9/86 [D loss: 0.6924032866954803, acc.: 52.83%] [G loss: 0.7029970288276672]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 10/86 [D loss: 0.6919792890548706, acc.: 52.73%] [G loss: 0.7018967270851135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 11/86 [D loss: 0.6923652291297913, acc.: 52.10%] [G loss: 0.7009295225143433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 12/86 [D loss: 0.6920109689235687, acc.: 52.88%] [G loss: 0.7017381191253662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 13/86 [D loss: 0.6917981803417206, acc.: 52.59%] [G loss: 0.7015358209609985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 14/86 [D loss: 0.6924050152301788, acc.: 50.78%] [G loss: 0.7010648250579834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 15/86 [D loss: 0.6914332211017609, acc.: 52.69%] [G loss: 0.701080322265625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 16/86 [D loss: 0.6913780272006989, acc.: 53.32%] [G loss: 0.7017567157745361]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 17/86 [D loss: 0.6905447244644165, acc.: 55.52%] [G loss: 0.701487123966217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 18/86 [D loss: 0.6921147704124451, acc.: 51.76%] [G loss: 0.7018875479698181]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 19/86 [D loss: 0.6913675963878632, acc.: 54.00%] [G loss: 0.7018348574638367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 20/86 [D loss: 0.6920306384563446, acc.: 51.71%] [G loss: 0.7002818584442139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 21/86 [D loss: 0.6920741498470306, acc.: 51.95%] [G loss: 0.7019041776657104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 22/86 [D loss: 0.6929917335510254, acc.: 49.12%] [G loss: 0.7005403637886047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 23/86 [D loss: 0.6915452182292938, acc.: 53.56%] [G loss: 0.7021000385284424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 24/86 [D loss: 0.691798210144043, acc.: 51.46%] [G loss: 0.7009326815605164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 25/86 [D loss: 0.6905017495155334, acc.: 56.74%] [G loss: 0.7009381055831909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 26/86 [D loss: 0.6918922364711761, acc.: 52.69%] [G loss: 0.7014355063438416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 27/86 [D loss: 0.6906673014163971, acc.: 54.44%] [G loss: 0.700736403465271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 28/86 [D loss: 0.6919840574264526, acc.: 51.46%] [G loss: 0.7005850672721863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 29/86 [D loss: 0.6914555728435516, acc.: 52.93%] [G loss: 0.7009603381156921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 30/86 [D loss: 0.691301167011261, acc.: 54.00%] [G loss: 0.7000783681869507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 31/86 [D loss: 0.6919935941696167, acc.: 52.25%] [G loss: 0.7008228898048401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 32/86 [D loss: 0.6921206414699554, acc.: 51.46%] [G loss: 0.7005636096000671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 33/86 [D loss: 0.6914290487766266, acc.: 53.81%] [G loss: 0.6998932361602783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 34/86 [D loss: 0.6918632686138153, acc.: 51.66%] [G loss: 0.7010279297828674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 35/86 [D loss: 0.6910730004310608, acc.: 55.08%] [G loss: 0.7009401321411133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 36/86 [D loss: 0.692112922668457, acc.: 52.69%] [G loss: 0.7003883123397827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 37/86 [D loss: 0.6918549835681915, acc.: 51.12%] [G loss: 0.7003730535507202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 38/86 [D loss: 0.6922094523906708, acc.: 50.49%] [G loss: 0.7009773850440979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 39/86 [D loss: 0.6918116509914398, acc.: 52.93%] [G loss: 0.701041042804718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 40/86 [D loss: 0.6920410692691803, acc.: 51.71%] [G loss: 0.7009183168411255]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 41/86 [D loss: 0.6914712488651276, acc.: 53.22%] [G loss: 0.7018668055534363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 42/86 [D loss: 0.6908657848834991, acc.: 53.76%] [G loss: 0.7001137137413025]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 43/86 [D loss: 0.6925598084926605, acc.: 50.49%] [G loss: 0.7018597722053528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 44/86 [D loss: 0.691991925239563, acc.: 52.83%] [G loss: 0.7009274959564209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 45/86 [D loss: 0.6918081641197205, acc.: 53.37%] [G loss: 0.7000270485877991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 46/86 [D loss: 0.691055029630661, acc.: 53.08%] [G loss: 0.7020940184593201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 47/86 [D loss: 0.6924620270729065, acc.: 51.17%] [G loss: 0.7016569375991821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 48/86 [D loss: 0.6916387379169464, acc.: 52.83%] [G loss: 0.7002638578414917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 49/86 [D loss: 0.6927037835121155, acc.: 50.29%] [G loss: 0.7009290456771851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 50/86 [D loss: 0.6917631030082703, acc.: 51.61%] [G loss: 0.7004677653312683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 51/86 [D loss: 0.6918969750404358, acc.: 52.25%] [G loss: 0.7011194825172424]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 52/86 [D loss: 0.6919532120227814, acc.: 52.54%] [G loss: 0.700499415397644]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 53/86 [D loss: 0.6906558275222778, acc.: 53.81%] [G loss: 0.7019152641296387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 54/86 [D loss: 0.6922744512557983, acc.: 50.59%] [G loss: 0.7014061808586121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 55/86 [D loss: 0.6915164589881897, acc.: 53.12%] [G loss: 0.7012037634849548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 56/86 [D loss: 0.6925613284111023, acc.: 50.73%] [G loss: 0.7014890909194946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 57/86 [D loss: 0.6918662190437317, acc.: 52.78%] [G loss: 0.7005380988121033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 58/86 [D loss: 0.6924692094326019, acc.: 50.93%] [G loss: 0.7011150121688843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 59/86 [D loss: 0.6919685304164886, acc.: 52.05%] [G loss: 0.701175332069397]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 60/86 [D loss: 0.6920782923698425, acc.: 52.49%] [G loss: 0.7012339234352112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 61/86 [D loss: 0.6911061704158783, acc.: 54.25%] [G loss: 0.7003382444381714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 62/86 [D loss: 0.6909555196762085, acc.: 54.15%] [G loss: 0.7011773586273193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 63/86 [D loss: 0.6911978125572205, acc.: 53.86%] [G loss: 0.701457679271698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 64/86 [D loss: 0.691849410533905, acc.: 52.00%] [G loss: 0.7016058564186096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 65/86 [D loss: 0.6915201544761658, acc.: 53.37%] [G loss: 0.6999884843826294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 66/86 [D loss: 0.6911408603191376, acc.: 53.32%] [G loss: 0.7015504240989685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 67/86 [D loss: 0.6915484964847565, acc.: 53.03%] [G loss: 0.7026847004890442]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 68/86 [D loss: 0.6914675235748291, acc.: 53.42%] [G loss: 0.7013376951217651]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 69/86 [D loss: 0.6926413476467133, acc.: 50.78%] [G loss: 0.6999326944351196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 70/86 [D loss: 0.6927329897880554, acc.: 51.03%] [G loss: 0.7015717029571533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 71/86 [D loss: 0.6912297904491425, acc.: 52.29%] [G loss: 0.7004154920578003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 72/86 [D loss: 0.6918872892856598, acc.: 53.03%] [G loss: 0.7015116214752197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 73/86 [D loss: 0.6914284825325012, acc.: 52.93%] [G loss: 0.7004795670509338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 74/86 [D loss: 0.6927314698696136, acc.: 50.15%] [G loss: 0.7007414698600769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 75/86 [D loss: 0.6920954883098602, acc.: 52.49%] [G loss: 0.7013632655143738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 76/86 [D loss: 0.6918728351593018, acc.: 51.86%] [G loss: 0.7008020281791687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 77/86 [D loss: 0.6913387179374695, acc.: 54.44%] [G loss: 0.7009809613227844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 78/86 [D loss: 0.6929041147232056, acc.: 50.73%] [G loss: 0.7002599239349365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 79/86 [D loss: 0.691476434469223, acc.: 53.08%] [G loss: 0.7003786563873291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 80/86 [D loss: 0.6919814944267273, acc.: 51.12%] [G loss: 0.698300838470459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 81/86 [D loss: 0.6919345855712891, acc.: 52.69%] [G loss: 0.7003501057624817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 82/86 [D loss: 0.691277027130127, acc.: 54.10%] [G loss: 0.7005559802055359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 83/86 [D loss: 0.6939907371997833, acc.: 48.39%] [G loss: 0.698767364025116]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 84/86 [D loss: 0.6924081444740295, acc.: 51.27%] [G loss: 0.7016307711601257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 85/86 [D loss: 0.6922549605369568, acc.: 51.37%] [G loss: 0.6983723640441895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 86/86 [D loss: 0.6919631361961365, acc.: 52.05%] [G loss: 0.7017742991447449]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 1/86 [D loss: 0.6908872127532959, acc.: 54.44%] [G loss: 0.6965906620025635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 2/86 [D loss: 0.6965278685092926, acc.: 44.87%] [G loss: 0.7008976936340332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 3/86 [D loss: 0.6901655197143555, acc.: 56.10%] [G loss: 0.6985031366348267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 4/86 [D loss: 0.6944200098514557, acc.: 47.85%] [G loss: 0.6985769271850586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 5/86 [D loss: 0.6915479004383087, acc.: 53.37%] [G loss: 0.7010915875434875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 6/86 [D loss: 0.6921513080596924, acc.: 52.88%] [G loss: 0.6935303807258606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 7/86 [D loss: 0.6958163976669312, acc.: 45.70%] [G loss: 0.7021052837371826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 8/86 [D loss: 0.69106724858284, acc.: 54.10%] [G loss: 0.6984018683433533]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 9/86 [D loss: 0.6949985325336456, acc.: 47.22%] [G loss: 0.6978617310523987]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 60/200, Batch 10/86 [D loss: 0.6917197406291962, acc.: 52.29%] [G loss: 0.7013647556304932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 11/86 [D loss: 0.6929931342601776, acc.: 50.59%] [G loss: 0.6924596428871155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 12/86 [D loss: 0.6957148313522339, acc.: 46.44%] [G loss: 0.7021019458770752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 13/86 [D loss: 0.6911415457725525, acc.: 52.98%] [G loss: 0.6975278854370117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 14/86 [D loss: 0.6941227912902832, acc.: 47.36%] [G loss: 0.7005690932273865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 15/86 [D loss: 0.6921461522579193, acc.: 52.25%] [G loss: 0.701304018497467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 16/86 [D loss: 0.6923093497753143, acc.: 51.42%] [G loss: 0.6961224675178528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 17/86 [D loss: 0.6947159767150879, acc.: 48.05%] [G loss: 0.7013974189758301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 18/86 [D loss: 0.690687745809555, acc.: 54.74%] [G loss: 0.7006350755691528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 19/86 [D loss: 0.6923514008522034, acc.: 51.76%] [G loss: 0.6977506875991821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 20/86 [D loss: 0.6917163133621216, acc.: 52.83%] [G loss: 0.7001600861549377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 21/86 [D loss: 0.6929402947425842, acc.: 50.29%] [G loss: 0.6985691785812378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 22/86 [D loss: 0.6926374435424805, acc.: 51.66%] [G loss: 0.7019414901733398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 23/86 [D loss: 0.6914278864860535, acc.: 53.86%] [G loss: 0.6996265053749084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 24/86 [D loss: 0.6918002367019653, acc.: 52.88%] [G loss: 0.7004773020744324]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 25/86 [D loss: 0.6917303502559662, acc.: 52.69%] [G loss: 0.6999608278274536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 26/86 [D loss: 0.6917652189731598, acc.: 52.34%] [G loss: 0.6986702680587769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 27/86 [D loss: 0.6939230263233185, acc.: 49.85%] [G loss: 0.7010862827301025]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 28/86 [D loss: 0.6911624372005463, acc.: 52.83%] [G loss: 0.7005690336227417]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 29/86 [D loss: 0.6925466060638428, acc.: 51.81%] [G loss: 0.7013790011405945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 30/86 [D loss: 0.6930931210517883, acc.: 50.39%] [G loss: 0.7015190124511719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 31/86 [D loss: 0.6924332678318024, acc.: 51.12%] [G loss: 0.7000926733016968]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 32/86 [D loss: 0.6925186216831207, acc.: 52.10%] [G loss: 0.7022719979286194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 33/86 [D loss: 0.6921285688877106, acc.: 51.76%] [G loss: 0.7000330686569214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 34/86 [D loss: 0.6912592351436615, acc.: 53.22%] [G loss: 0.701410174369812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 35/86 [D loss: 0.6916332840919495, acc.: 52.39%] [G loss: 0.7004413604736328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 36/86 [D loss: 0.6923348903656006, acc.: 51.42%] [G loss: 0.7001423239707947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 37/86 [D loss: 0.6919450759887695, acc.: 52.69%] [G loss: 0.7000604271888733]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 38/86 [D loss: 0.6920391321182251, acc.: 52.44%] [G loss: 0.7003742456436157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 39/86 [D loss: 0.6911729574203491, acc.: 53.96%] [G loss: 0.7016236782073975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 40/86 [D loss: 0.6913900375366211, acc.: 54.39%] [G loss: 0.7008877396583557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 41/86 [D loss: 0.6918224096298218, acc.: 52.98%] [G loss: 0.7012472152709961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 42/86 [D loss: 0.6920575499534607, acc.: 51.61%] [G loss: 0.7011587619781494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 43/86 [D loss: 0.6907697916030884, acc.: 54.54%] [G loss: 0.7029268145561218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 44/86 [D loss: 0.690956175327301, acc.: 53.66%] [G loss: 0.7005255222320557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 45/86 [D loss: 0.6918438673019409, acc.: 53.27%] [G loss: 0.7013911604881287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 46/86 [D loss: 0.6919642388820648, acc.: 52.83%] [G loss: 0.699860692024231]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 47/86 [D loss: 0.6910735964775085, acc.: 54.15%] [G loss: 0.7008179426193237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 48/86 [D loss: 0.690700501203537, acc.: 53.81%] [G loss: 0.7002319693565369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 49/86 [D loss: 0.6919994652271271, acc.: 50.93%] [G loss: 0.6998738646507263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 50/86 [D loss: 0.691368818283081, acc.: 52.34%] [G loss: 0.7003337144851685]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 51/86 [D loss: 0.6908420026302338, acc.: 54.35%] [G loss: 0.7005726099014282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 52/86 [D loss: 0.6907392144203186, acc.: 53.56%] [G loss: 0.7016632556915283]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 53/86 [D loss: 0.6917992234230042, acc.: 52.49%] [G loss: 0.7010313272476196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 54/86 [D loss: 0.6912068128585815, acc.: 53.76%] [G loss: 0.7015477418899536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 55/86 [D loss: 0.6922417879104614, acc.: 52.05%] [G loss: 0.6994656920433044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 56/86 [D loss: 0.69124835729599, acc.: 52.54%] [G loss: 0.7012715935707092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 57/86 [D loss: 0.6917657852172852, acc.: 51.81%] [G loss: 0.7016102075576782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 58/86 [D loss: 0.692516565322876, acc.: 51.32%] [G loss: 0.7013168931007385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 59/86 [D loss: 0.6921040117740631, acc.: 52.15%] [G loss: 0.7021793127059937]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 60/86 [D loss: 0.691826343536377, acc.: 52.29%] [G loss: 0.7005001902580261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 61/86 [D loss: 0.6923411786556244, acc.: 52.54%] [G loss: 0.7004685997962952]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 62/86 [D loss: 0.69161057472229, acc.: 53.52%] [G loss: 0.7014455199241638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 63/86 [D loss: 0.6914813816547394, acc.: 54.05%] [G loss: 0.7016169428825378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 64/86 [D loss: 0.6926827132701874, acc.: 52.15%] [G loss: 0.7001290917396545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 65/86 [D loss: 0.6910706758499146, acc.: 54.10%] [G loss: 0.7003226280212402]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 66/86 [D loss: 0.6911025047302246, acc.: 53.12%] [G loss: 0.7017804384231567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 67/86 [D loss: 0.6907893717288971, acc.: 54.64%] [G loss: 0.700335681438446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 68/86 [D loss: 0.6912487745285034, acc.: 55.18%] [G loss: 0.7001991868019104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 69/86 [D loss: 0.690821498632431, acc.: 52.78%] [G loss: 0.7012172341346741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 70/86 [D loss: 0.6915940046310425, acc.: 53.42%] [G loss: 0.7012474536895752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 71/86 [D loss: 0.6917082667350769, acc.: 53.32%] [G loss: 0.7012911438941956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 72/86 [D loss: 0.6916100382804871, acc.: 53.52%] [G loss: 0.7013727426528931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 73/86 [D loss: 0.6911076307296753, acc.: 54.05%] [G loss: 0.7001323103904724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 74/86 [D loss: 0.6913966834545135, acc.: 53.22%] [G loss: 0.7006504535675049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 75/86 [D loss: 0.691443920135498, acc.: 52.88%] [G loss: 0.7005000114440918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 76/86 [D loss: 0.6903608739376068, acc.: 54.25%] [G loss: 0.7010184526443481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 77/86 [D loss: 0.6916394233703613, acc.: 52.69%] [G loss: 0.7025257349014282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 78/86 [D loss: 0.6907828152179718, acc.: 53.52%] [G loss: 0.7005884647369385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 79/86 [D loss: 0.6921049356460571, acc.: 52.49%] [G loss: 0.7004129886627197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 80/86 [D loss: 0.6906928420066833, acc.: 54.25%] [G loss: 0.7016739845275879]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 81/86 [D loss: 0.6917119920253754, acc.: 52.44%] [G loss: 0.7011966705322266]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 82/86 [D loss: 0.6912966966629028, acc.: 53.27%] [G loss: 0.7011101245880127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 83/86 [D loss: 0.6919227838516235, acc.: 52.34%] [G loss: 0.7002145051956177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 84/86 [D loss: 0.6921219825744629, acc.: 50.59%] [G loss: 0.7022825479507446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 85/86 [D loss: 0.6913251578807831, acc.: 53.12%] [G loss: 0.7011651992797852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 86/86 [D loss: 0.6916038691997528, acc.: 52.83%] [G loss: 0.7010033130645752]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 1/86 [D loss: 0.691035270690918, acc.: 53.27%] [G loss: 0.6997450590133667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 2/86 [D loss: 0.6924830377101898, acc.: 50.63%] [G loss: 0.7011709809303284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 3/86 [D loss: 0.6908096075057983, acc.: 54.05%] [G loss: 0.7019748091697693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 4/86 [D loss: 0.691591203212738, acc.: 52.25%] [G loss: 0.6993255615234375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 5/86 [D loss: 0.6914571821689606, acc.: 52.88%] [G loss: 0.699993371963501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 6/86 [D loss: 0.6922202110290527, acc.: 51.51%] [G loss: 0.6998446583747864]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 7/86 [D loss: 0.6913758218288422, acc.: 51.61%] [G loss: 0.7016832232475281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 8/86 [D loss: 0.6910342276096344, acc.: 53.42%] [G loss: 0.7004922032356262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 9/86 [D loss: 0.6919639706611633, acc.: 52.54%] [G loss: 0.7020274996757507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 10/86 [D loss: 0.6902344226837158, acc.: 54.93%] [G loss: 0.7001286745071411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 11/86 [D loss: 0.6920630931854248, acc.: 51.12%] [G loss: 0.7015630006790161]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 12/86 [D loss: 0.6909686028957367, acc.: 53.91%] [G loss: 0.700619101524353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 13/86 [D loss: 0.6925390958786011, acc.: 50.34%] [G loss: 0.6996179223060608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 14/86 [D loss: 0.6902226507663727, acc.: 54.49%] [G loss: 0.7019054889678955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 15/86 [D loss: 0.6916448473930359, acc.: 52.05%] [G loss: 0.6991879343986511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 16/86 [D loss: 0.6921921074390411, acc.: 50.78%] [G loss: 0.7012987732887268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 17/86 [D loss: 0.6903179883956909, acc.: 55.86%] [G loss: 0.7017276883125305]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 18/86 [D loss: 0.6914845705032349, acc.: 52.88%] [G loss: 0.7028000354766846]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 19/86 [D loss: 0.6909806430339813, acc.: 53.76%] [G loss: 0.7008020281791687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 20/86 [D loss: 0.6918061077594757, acc.: 52.73%] [G loss: 0.7014942169189453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 21/86 [D loss: 0.6918010711669922, acc.: 52.73%] [G loss: 0.7021567821502686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 22/86 [D loss: 0.6920308768749237, acc.: 52.78%] [G loss: 0.7019448280334473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 23/86 [D loss: 0.6910114586353302, acc.: 53.86%] [G loss: 0.7032215595245361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 24/86 [D loss: 0.6906012296676636, acc.: 54.30%] [G loss: 0.7016681432723999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 25/86 [D loss: 0.6915653049945831, acc.: 53.47%] [G loss: 0.7017489075660706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 26/86 [D loss: 0.6917248964309692, acc.: 52.15%] [G loss: 0.701469898223877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 27/86 [D loss: 0.6924427449703217, acc.: 51.17%] [G loss: 0.7005847096443176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 28/86 [D loss: 0.6906901299953461, acc.: 54.88%] [G loss: 0.7007768750190735]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 29/86 [D loss: 0.6915358304977417, acc.: 52.15%] [G loss: 0.7007769346237183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 30/86 [D loss: 0.6920756995677948, acc.: 51.61%] [G loss: 0.7014901638031006]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 31/86 [D loss: 0.6907521486282349, acc.: 54.35%] [G loss: 0.701320469379425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 32/86 [D loss: 0.6917257308959961, acc.: 54.39%] [G loss: 0.7023464441299438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 33/86 [D loss: 0.6910870969295502, acc.: 53.96%] [G loss: 0.701431393623352]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 34/86 [D loss: 0.6919010579586029, acc.: 53.86%] [G loss: 0.7014690041542053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 35/86 [D loss: 0.6906579434871674, acc.: 54.35%] [G loss: 0.7016512751579285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 36/86 [D loss: 0.6919057369232178, acc.: 52.29%] [G loss: 0.7007281184196472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 37/86 [D loss: 0.6915266215801239, acc.: 51.66%] [G loss: 0.7025482654571533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 38/86 [D loss: 0.6921176016330719, acc.: 52.10%] [G loss: 0.7016103267669678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 39/86 [D loss: 0.6925058960914612, acc.: 50.98%] [G loss: 0.703388512134552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 40/86 [D loss: 0.6913654208183289, acc.: 52.25%] [G loss: 0.7010074853897095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 41/86 [D loss: 0.6921220421791077, acc.: 52.64%] [G loss: 0.7027412056922913]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 42/86 [D loss: 0.6912926137447357, acc.: 54.10%] [G loss: 0.7030553221702576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 43/86 [D loss: 0.692749410867691, acc.: 51.12%] [G loss: 0.7016737461090088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 44/86 [D loss: 0.6908420324325562, acc.: 54.15%] [G loss: 0.7023705840110779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 45/86 [D loss: 0.6915276646614075, acc.: 53.12%] [G loss: 0.7034484148025513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 46/86 [D loss: 0.690872848033905, acc.: 53.52%] [G loss: 0.7015298008918762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 47/86 [D loss: 0.6916382014751434, acc.: 53.76%] [G loss: 0.701998233795166]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 61/200, Batch 48/86 [D loss: 0.6916838586330414, acc.: 52.64%] [G loss: 0.7026823163032532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 49/86 [D loss: 0.6913059651851654, acc.: 51.71%] [G loss: 0.7013958692550659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 50/86 [D loss: 0.6925221979618073, acc.: 52.15%] [G loss: 0.7025654315948486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 51/86 [D loss: 0.6910509467124939, acc.: 54.64%] [G loss: 0.6997848749160767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 52/86 [D loss: 0.6928637027740479, acc.: 50.10%] [G loss: 0.7022733092308044]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 53/86 [D loss: 0.6907812654972076, acc.: 54.54%] [G loss: 0.7025794386863708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 54/86 [D loss: 0.6929216086864471, acc.: 48.63%] [G loss: 0.701153576374054]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 55/86 [D loss: 0.6907452046871185, acc.: 53.91%] [G loss: 0.7031928896903992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 56/86 [D loss: 0.6916824281215668, acc.: 52.15%] [G loss: 0.699661374092102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 57/86 [D loss: 0.6925283968448639, acc.: 51.71%] [G loss: 0.7030267119407654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 58/86 [D loss: 0.6899509429931641, acc.: 54.98%] [G loss: 0.7024760842323303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 59/86 [D loss: 0.6914559602737427, acc.: 54.25%] [G loss: 0.702935516834259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 60/86 [D loss: 0.6919686496257782, acc.: 52.93%] [G loss: 0.7020419836044312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 61/86 [D loss: 0.6925686597824097, acc.: 51.95%] [G loss: 0.7001828551292419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 62/86 [D loss: 0.6929742097854614, acc.: 50.73%] [G loss: 0.702572226524353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 63/86 [D loss: 0.6911708116531372, acc.: 52.93%] [G loss: 0.7000213861465454]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 64/86 [D loss: 0.6907130777835846, acc.: 53.42%] [G loss: 0.7022466063499451]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 65/86 [D loss: 0.6911202371120453, acc.: 53.08%] [G loss: 0.7008187174797058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 66/86 [D loss: 0.6916249990463257, acc.: 52.83%] [G loss: 0.7031878232955933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 67/86 [D loss: 0.6908125281333923, acc.: 55.08%] [G loss: 0.7024039626121521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 68/86 [D loss: 0.6919987201690674, acc.: 52.34%] [G loss: 0.7018200159072876]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 69/86 [D loss: 0.6909960806369781, acc.: 53.91%] [G loss: 0.7020570039749146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 70/86 [D loss: 0.6902738809585571, acc.: 55.47%] [G loss: 0.6999176144599915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 71/86 [D loss: 0.6921623647212982, acc.: 52.49%] [G loss: 0.7015583515167236]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 72/86 [D loss: 0.6904972493648529, acc.: 53.03%] [G loss: 0.7032620310783386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 73/86 [D loss: 0.6901695132255554, acc.: 54.74%] [G loss: 0.7027515769004822]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 74/86 [D loss: 0.6906754374504089, acc.: 54.00%] [G loss: 0.7011932134628296]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 75/86 [D loss: 0.6915245056152344, acc.: 54.49%] [G loss: 0.7009560465812683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 76/86 [D loss: 0.6904561817646027, acc.: 54.49%] [G loss: 0.7028318047523499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 77/86 [D loss: 0.6902782618999481, acc.: 54.98%] [G loss: 0.7020000219345093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 78/86 [D loss: 0.6903685629367828, acc.: 54.59%] [G loss: 0.7017738819122314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 79/86 [D loss: 0.6931208372116089, acc.: 48.29%] [G loss: 0.7006776928901672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 80/86 [D loss: 0.6909549236297607, acc.: 55.08%] [G loss: 0.7033174633979797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 81/86 [D loss: 0.6906294822692871, acc.: 54.83%] [G loss: 0.702311635017395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 82/86 [D loss: 0.6916100084781647, acc.: 53.37%] [G loss: 0.7024216651916504]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 83/86 [D loss: 0.6902881264686584, acc.: 56.69%] [G loss: 0.6998945474624634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 84/86 [D loss: 0.6920528709888458, acc.: 51.61%] [G loss: 0.7016520500183105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 85/86 [D loss: 0.6897028982639313, acc.: 55.47%] [G loss: 0.7009363770484924]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 86/86 [D loss: 0.6928845047950745, acc.: 50.29%] [G loss: 0.7009934186935425]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 1/86 [D loss: 0.6912132501602173, acc.: 52.88%] [G loss: 0.7024239301681519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 2/86 [D loss: 0.6920547485351562, acc.: 53.17%] [G loss: 0.6987787485122681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 3/86 [D loss: 0.6913330256938934, acc.: 54.25%] [G loss: 0.7022199630737305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 4/86 [D loss: 0.691762238740921, acc.: 52.20%] [G loss: 0.7011079788208008]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 5/86 [D loss: 0.6916368007659912, acc.: 52.78%] [G loss: 0.7030500173568726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 6/86 [D loss: 0.6911097168922424, acc.: 53.71%] [G loss: 0.7001709938049316]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 7/86 [D loss: 0.6919925510883331, acc.: 52.00%] [G loss: 0.7014930844306946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 8/86 [D loss: 0.6900854110717773, acc.: 54.79%] [G loss: 0.7025642395019531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 9/86 [D loss: 0.6910629272460938, acc.: 53.37%] [G loss: 0.6999716758728027]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 10/86 [D loss: 0.6912862360477448, acc.: 53.71%] [G loss: 0.702669620513916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 11/86 [D loss: 0.6915354132652283, acc.: 53.32%] [G loss: 0.6979621052742004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 12/86 [D loss: 0.6939098536968231, acc.: 48.88%] [G loss: 0.7032787799835205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 13/86 [D loss: 0.6901432573795319, acc.: 54.30%] [G loss: 0.7005355954170227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 14/86 [D loss: 0.6920775175094604, acc.: 50.88%] [G loss: 0.7029844522476196]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 15/86 [D loss: 0.6903049945831299, acc.: 55.13%] [G loss: 0.7020950317382812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 16/86 [D loss: 0.6920397281646729, acc.: 53.17%] [G loss: 0.7007675170898438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 17/86 [D loss: 0.6923741698265076, acc.: 51.03%] [G loss: 0.7026662826538086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 18/86 [D loss: 0.6903786063194275, acc.: 54.44%] [G loss: 0.7007538676261902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 19/86 [D loss: 0.6925466656684875, acc.: 50.68%] [G loss: 0.7014754414558411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 20/86 [D loss: 0.6903774440288544, acc.: 54.54%] [G loss: 0.6998050212860107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 21/86 [D loss: 0.69302898645401, acc.: 50.20%] [G loss: 0.7010558843612671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 22/86 [D loss: 0.6910740435123444, acc.: 54.20%] [G loss: 0.7005370259284973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 23/86 [D loss: 0.6914868950843811, acc.: 53.27%] [G loss: 0.700589120388031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 24/86 [D loss: 0.6903611719608307, acc.: 54.20%] [G loss: 0.7017804384231567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 25/86 [D loss: 0.6918553113937378, acc.: 52.59%] [G loss: 0.6994630694389343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 26/86 [D loss: 0.6914520263671875, acc.: 53.08%] [G loss: 0.703079879283905]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 27/86 [D loss: 0.6908799111843109, acc.: 52.98%] [G loss: 0.7003146409988403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 28/86 [D loss: 0.6922559440135956, acc.: 52.54%] [G loss: 0.7017648220062256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 29/86 [D loss: 0.6900331377983093, acc.: 54.93%] [G loss: 0.7027369737625122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 30/86 [D loss: 0.6925102472305298, acc.: 51.17%] [G loss: 0.7005541324615479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 31/86 [D loss: 0.6910626590251923, acc.: 52.98%] [G loss: 0.7029427289962769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 32/86 [D loss: 0.6919242739677429, acc.: 50.98%] [G loss: 0.7001579999923706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 33/86 [D loss: 0.6909256875514984, acc.: 53.81%] [G loss: 0.7040024399757385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 34/86 [D loss: 0.6916559934616089, acc.: 53.37%] [G loss: 0.698562502861023]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 35/86 [D loss: 0.6930062174797058, acc.: 49.17%] [G loss: 0.7031416296958923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 36/86 [D loss: 0.6905115842819214, acc.: 54.30%] [G loss: 0.7023470997810364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 37/86 [D loss: 0.6921003758907318, acc.: 52.10%] [G loss: 0.7018171548843384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 38/86 [D loss: 0.6907556354999542, acc.: 53.86%] [G loss: 0.7020233273506165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 39/86 [D loss: 0.6922956109046936, acc.: 51.22%] [G loss: 0.6987136602401733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 40/86 [D loss: 0.6917255520820618, acc.: 52.29%] [G loss: 0.7029092907905579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 41/86 [D loss: 0.6919744312763214, acc.: 50.49%] [G loss: 0.7006993889808655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 42/86 [D loss: 0.6910735070705414, acc.: 52.54%] [G loss: 0.7028483152389526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 43/86 [D loss: 0.6896034777164459, acc.: 55.03%] [G loss: 0.7001233100891113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 44/86 [D loss: 0.6946160197257996, acc.: 48.00%] [G loss: 0.7009433507919312]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 45/86 [D loss: 0.6904411315917969, acc.: 54.00%] [G loss: 0.7027595043182373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 46/86 [D loss: 0.6906404793262482, acc.: 53.37%] [G loss: 0.7008522152900696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 47/86 [D loss: 0.6904803514480591, acc.: 54.69%] [G loss: 0.7044460773468018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 48/86 [D loss: 0.6907431483268738, acc.: 54.64%] [G loss: 0.6994760036468506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 49/86 [D loss: 0.6934385895729065, acc.: 49.71%] [G loss: 0.702321469783783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 50/86 [D loss: 0.6900371313095093, acc.: 55.52%] [G loss: 0.7015014290809631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 51/86 [D loss: 0.6912566125392914, acc.: 51.76%] [G loss: 0.7013673782348633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 52/86 [D loss: 0.6908504664897919, acc.: 53.76%] [G loss: 0.7020546793937683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 53/86 [D loss: 0.6910638213157654, acc.: 51.76%] [G loss: 0.7024651169776917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 54/86 [D loss: 0.6913895308971405, acc.: 52.69%] [G loss: 0.703093409538269]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 55/86 [D loss: 0.6897200644016266, acc.: 56.79%] [G loss: 0.702949583530426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 56/86 [D loss: 0.6905918419361115, acc.: 55.13%] [G loss: 0.7011644840240479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 57/86 [D loss: 0.6904284358024597, acc.: 54.10%] [G loss: 0.7007162570953369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 58/86 [D loss: 0.6913520693778992, acc.: 52.69%] [G loss: 0.7026866674423218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 59/86 [D loss: 0.6901472210884094, acc.: 55.62%] [G loss: 0.7027578353881836]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 60/86 [D loss: 0.6915391087532043, acc.: 52.29%] [G loss: 0.7031629681587219]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 61/86 [D loss: 0.6911546885967255, acc.: 52.73%] [G loss: 0.7033874988555908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 62/86 [D loss: 0.6903047263622284, acc.: 53.32%] [G loss: 0.7008350491523743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 63/86 [D loss: 0.6918901801109314, acc.: 51.86%] [G loss: 0.7040846347808838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 64/86 [D loss: 0.6906362473964691, acc.: 54.69%] [G loss: 0.7028328776359558]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 65/86 [D loss: 0.6904776990413666, acc.: 54.44%] [G loss: 0.7030820250511169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 66/86 [D loss: 0.6908056139945984, acc.: 53.56%] [G loss: 0.7023575901985168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 67/86 [D loss: 0.6911255121231079, acc.: 52.73%] [G loss: 0.702026903629303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 68/86 [D loss: 0.6914196908473969, acc.: 52.59%] [G loss: 0.7037636041641235]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 69/86 [D loss: 0.6904963850975037, acc.: 54.35%] [G loss: 0.7013487815856934]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 70/86 [D loss: 0.6912983655929565, acc.: 53.17%] [G loss: 0.702235221862793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 71/86 [D loss: 0.6903831958770752, acc.: 53.96%] [G loss: 0.7029768228530884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 72/86 [D loss: 0.6917727589607239, acc.: 52.39%] [G loss: 0.7029245495796204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 73/86 [D loss: 0.691547155380249, acc.: 52.73%] [G loss: 0.7010260224342346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 74/86 [D loss: 0.691586047410965, acc.: 52.64%] [G loss: 0.7014380693435669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 75/86 [D loss: 0.6907158493995667, acc.: 53.66%] [G loss: 0.702212393283844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 76/86 [D loss: 0.6912446916103363, acc.: 52.88%] [G loss: 0.7033846378326416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 77/86 [D loss: 0.6916723847389221, acc.: 53.47%] [G loss: 0.7047812342643738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 78/86 [D loss: 0.6911381185054779, acc.: 52.69%] [G loss: 0.7037780284881592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 79/86 [D loss: 0.6906281113624573, acc.: 55.08%] [G loss: 0.7038979530334473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 80/86 [D loss: 0.6908402442932129, acc.: 54.20%] [G loss: 0.701977550983429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 81/86 [D loss: 0.6906238496303558, acc.: 54.05%] [G loss: 0.7020829916000366]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 82/86 [D loss: 0.689816415309906, acc.: 54.30%] [G loss: 0.702987551689148]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 83/86 [D loss: 0.6908550262451172, acc.: 53.56%] [G loss: 0.7033171653747559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 84/86 [D loss: 0.6906421780586243, acc.: 53.81%] [G loss: 0.7037380337715149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 85/86 [D loss: 0.6901068985462189, acc.: 55.08%] [G loss: 0.7020900249481201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 86/86 [D loss: 0.6915982961654663, acc.: 52.54%] [G loss: 0.7018447518348694]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 1/86 [D loss: 0.6902818381786346, acc.: 56.30%] [G loss: 0.7023820877075195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 2/86 [D loss: 0.6914492547512054, acc.: 51.81%] [G loss: 0.702363133430481]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 3/86 [D loss: 0.6896733641624451, acc.: 55.71%] [G loss: 0.7010219097137451]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 4/86 [D loss: 0.6911121308803558, acc.: 55.13%] [G loss: 0.7011383175849915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 5/86 [D loss: 0.6899325251579285, acc.: 54.98%] [G loss: 0.7030094265937805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 6/86 [D loss: 0.6909945607185364, acc.: 54.44%] [G loss: 0.700377345085144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 7/86 [D loss: 0.6904652416706085, acc.: 53.91%] [G loss: 0.7019644975662231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 8/86 [D loss: 0.6907105445861816, acc.: 53.56%] [G loss: 0.7031722664833069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 9/86 [D loss: 0.6893320083618164, acc.: 56.40%] [G loss: 0.7017567157745361]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 10/86 [D loss: 0.6904000043869019, acc.: 54.25%] [G loss: 0.7031300067901611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 11/86 [D loss: 0.6910623610019684, acc.: 53.08%] [G loss: 0.7032173275947571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 12/86 [D loss: 0.6899591982364655, acc.: 54.83%] [G loss: 0.7028021216392517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 13/86 [D loss: 0.6901134848594666, acc.: 54.15%] [G loss: 0.7002272605895996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 14/86 [D loss: 0.6920533180236816, acc.: 51.03%] [G loss: 0.7036845088005066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 15/86 [D loss: 0.6905356347560883, acc.: 54.59%] [G loss: 0.7010929584503174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 16/86 [D loss: 0.6921526193618774, acc.: 51.12%] [G loss: 0.7020261287689209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 17/86 [D loss: 0.6909946203231812, acc.: 54.20%] [G loss: 0.6996886730194092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 18/86 [D loss: 0.6930881142616272, acc.: 50.05%] [G loss: 0.7020685076713562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 19/86 [D loss: 0.6912005841732025, acc.: 54.20%] [G loss: 0.701370358467102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 20/86 [D loss: 0.691706508398056, acc.: 54.25%] [G loss: 0.7010684013366699]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 21/86 [D loss: 0.6911763250827789, acc.: 52.49%] [G loss: 0.704415500164032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 22/86 [D loss: 0.690278947353363, acc.: 54.35%] [G loss: 0.6976653337478638]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 23/86 [D loss: 0.6938828825950623, acc.: 49.12%] [G loss: 0.7027973532676697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 24/86 [D loss: 0.6896713376045227, acc.: 54.44%] [G loss: 0.6993805170059204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 25/86 [D loss: 0.693914383649826, acc.: 48.73%] [G loss: 0.700394868850708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 26/86 [D loss: 0.6902368664741516, acc.: 53.91%] [G loss: 0.7030859589576721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 27/86 [D loss: 0.6925369799137115, acc.: 50.73%] [G loss: 0.6957552433013916]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 28/86 [D loss: 0.6935511827468872, acc.: 48.14%] [G loss: 0.7043879628181458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 29/86 [D loss: 0.691089391708374, acc.: 53.52%] [G loss: 0.7001644372940063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 30/86 [D loss: 0.6932976245880127, acc.: 48.88%] [G loss: 0.7011115550994873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 31/86 [D loss: 0.6900475323200226, acc.: 54.59%] [G loss: 0.7019956111907959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 32/86 [D loss: 0.6920261681079865, acc.: 52.29%] [G loss: 0.6977929472923279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 33/86 [D loss: 0.6923687756061554, acc.: 51.12%] [G loss: 0.7034906148910522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 34/86 [D loss: 0.68924680352211, acc.: 56.84%] [G loss: 0.7002571225166321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 35/86 [D loss: 0.6921006739139557, acc.: 52.34%] [G loss: 0.7019814848899841]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 36/86 [D loss: 0.6904098689556122, acc.: 54.98%] [G loss: 0.7021803855895996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 37/86 [D loss: 0.6908621490001678, acc.: 54.98%] [G loss: 0.7012089490890503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 38/86 [D loss: 0.6910282075405121, acc.: 53.08%] [G loss: 0.7030455470085144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 39/86 [D loss: 0.6910800337791443, acc.: 52.83%] [G loss: 0.7004615664482117]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 40/86 [D loss: 0.690824031829834, acc.: 53.17%] [G loss: 0.703322172164917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 41/86 [D loss: 0.6893658638000488, acc.: 56.05%] [G loss: 0.701524019241333]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 42/86 [D loss: 0.6905859708786011, acc.: 54.00%] [G loss: 0.701424241065979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 43/86 [D loss: 0.6890514492988586, acc.: 55.32%] [G loss: 0.7035208344459534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 44/86 [D loss: 0.6915569305419922, acc.: 53.03%] [G loss: 0.7005279064178467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 45/86 [D loss: 0.6915760338306427, acc.: 52.10%] [G loss: 0.7034210562705994]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 46/86 [D loss: 0.6900082230567932, acc.: 54.35%] [G loss: 0.7012673020362854]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 47/86 [D loss: 0.6930851638317108, acc.: 49.85%] [G loss: 0.7031610012054443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 48/86 [D loss: 0.6889406144618988, acc.: 56.79%] [G loss: 0.7025344371795654]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 49/86 [D loss: 0.6917368769645691, acc.: 52.78%] [G loss: 0.7020785212516785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 50/86 [D loss: 0.6904899179935455, acc.: 54.05%] [G loss: 0.70335853099823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 51/86 [D loss: 0.6913686990737915, acc.: 54.59%] [G loss: 0.7013905644416809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 52/86 [D loss: 0.6928216516971588, acc.: 51.56%] [G loss: 0.7030823230743408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 53/86 [D loss: 0.6897165477275848, acc.: 55.91%] [G loss: 0.7023012638092041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 54/86 [D loss: 0.690424919128418, acc.: 54.98%] [G loss: 0.7024358510971069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 55/86 [D loss: 0.6892827749252319, acc.: 54.79%] [G loss: 0.7032803297042847]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 56/86 [D loss: 0.6910524964332581, acc.: 54.00%] [G loss: 0.7019768357276917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 57/86 [D loss: 0.6914622783660889, acc.: 52.83%] [G loss: 0.7044522762298584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 58/86 [D loss: 0.6902240812778473, acc.: 54.44%] [G loss: 0.7029048204421997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 59/86 [D loss: 0.6907013058662415, acc.: 53.71%] [G loss: 0.7035309672355652]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 60/86 [D loss: 0.6892602145671844, acc.: 57.13%] [G loss: 0.7023058533668518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 61/86 [D loss: 0.6925375759601593, acc.: 51.90%] [G loss: 0.7027970552444458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 62/86 [D loss: 0.6909164190292358, acc.: 54.74%] [G loss: 0.7040098309516907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 63/86 [D loss: 0.6901001334190369, acc.: 54.54%] [G loss: 0.7024137377738953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 64/86 [D loss: 0.6912785172462463, acc.: 53.37%] [G loss: 0.7037616968154907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 65/86 [D loss: 0.6915613412857056, acc.: 53.86%] [G loss: 0.7007454037666321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 66/86 [D loss: 0.6908924281597137, acc.: 54.74%] [G loss: 0.7041083574295044]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 67/86 [D loss: 0.6908900737762451, acc.: 54.39%] [G loss: 0.7011457681655884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 68/86 [D loss: 0.6915289163589478, acc.: 53.17%] [G loss: 0.7027813196182251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 69/86 [D loss: 0.6904104351997375, acc.: 54.25%] [G loss: 0.7028253078460693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 70/86 [D loss: 0.6905816495418549, acc.: 53.76%] [G loss: 0.7032201290130615]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 71/86 [D loss: 0.6913994252681732, acc.: 52.20%] [G loss: 0.7045589685440063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 72/86 [D loss: 0.6904097497463226, acc.: 53.86%] [G loss: 0.7041620016098022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 73/86 [D loss: 0.690739631652832, acc.: 53.42%] [G loss: 0.7028122544288635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 74/86 [D loss: 0.6906458139419556, acc.: 54.05%] [G loss: 0.7019262313842773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 75/86 [D loss: 0.6908901631832123, acc.: 54.49%] [G loss: 0.7018164396286011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 76/86 [D loss: 0.6899729669094086, acc.: 54.59%] [G loss: 0.7019509673118591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 77/86 [D loss: 0.6915725469589233, acc.: 53.22%] [G loss: 0.7030978202819824]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 78/86 [D loss: 0.6906533241271973, acc.: 53.27%] [G loss: 0.7029986381530762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 79/86 [D loss: 0.6910312473773956, acc.: 52.69%] [G loss: 0.7014975547790527]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 80/86 [D loss: 0.6912619173526764, acc.: 53.22%] [G loss: 0.7034431099891663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 81/86 [D loss: 0.689927875995636, acc.: 54.00%] [G loss: 0.7022904753684998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 82/86 [D loss: 0.6931358873844147, acc.: 49.61%] [G loss: 0.7030596137046814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 83/86 [D loss: 0.68979412317276, acc.: 54.44%] [G loss: 0.7022265195846558]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 84/86 [D loss: 0.6920983195304871, acc.: 51.12%] [G loss: 0.7003492712974548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 85/86 [D loss: 0.6908731758594513, acc.: 54.00%] [G loss: 0.7024421691894531]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 86/86 [D loss: 0.6914777159690857, acc.: 53.03%] [G loss: 0.7007806897163391]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 1/86 [D loss: 0.6912548840045929, acc.: 53.32%] [G loss: 0.7031345963478088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 2/86 [D loss: 0.6916759312152863, acc.: 53.03%] [G loss: 0.7004832625389099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 3/86 [D loss: 0.6901910305023193, acc.: 54.69%] [G loss: 0.7016447186470032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 4/86 [D loss: 0.6910216212272644, acc.: 52.05%] [G loss: 0.7024515867233276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 5/86 [D loss: 0.6916002929210663, acc.: 52.00%] [G loss: 0.7020928263664246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 6/86 [D loss: 0.6898097395896912, acc.: 54.35%] [G loss: 0.7027893662452698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 7/86 [D loss: 0.6916109621524811, acc.: 52.69%] [G loss: 0.7025360465049744]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 8/86 [D loss: 0.6911728382110596, acc.: 52.49%] [G loss: 0.7047130465507507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 9/86 [D loss: 0.691369354724884, acc.: 52.73%] [G loss: 0.7026783227920532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 10/86 [D loss: 0.6910068392753601, acc.: 52.83%] [G loss: 0.703381359577179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 11/86 [D loss: 0.6895908713340759, acc.: 55.81%] [G loss: 0.7027324438095093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 12/86 [D loss: 0.6921135187149048, acc.: 52.29%] [G loss: 0.7034724354743958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 13/86 [D loss: 0.6905032098293304, acc.: 54.30%] [G loss: 0.701211154460907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 14/86 [D loss: 0.6913548111915588, acc.: 53.81%] [G loss: 0.7017639875411987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 15/86 [D loss: 0.6905357539653778, acc.: 52.88%] [G loss: 0.702697217464447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 16/86 [D loss: 0.6904623508453369, acc.: 54.00%] [G loss: 0.7001189589500427]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 17/86 [D loss: 0.691894918680191, acc.: 51.95%] [G loss: 0.7033376693725586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 18/86 [D loss: 0.6906826496124268, acc.: 53.42%] [G loss: 0.7041105031967163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 19/86 [D loss: 0.6926611363887787, acc.: 49.80%] [G loss: 0.7029435634613037]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 20/86 [D loss: 0.690488874912262, acc.: 54.98%] [G loss: 0.7032115459442139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 21/86 [D loss: 0.6900152564048767, acc.: 54.98%] [G loss: 0.7001050710678101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 22/86 [D loss: 0.6931900084018707, acc.: 50.34%] [G loss: 0.704504132270813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 23/86 [D loss: 0.6888924837112427, acc.: 56.88%] [G loss: 0.702296257019043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 24/86 [D loss: 0.6911949217319489, acc.: 50.78%] [G loss: 0.6998474597930908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 25/86 [D loss: 0.6892931163311005, acc.: 55.32%] [G loss: 0.701675534248352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 26/86 [D loss: 0.6916267275810242, acc.: 52.10%] [G loss: 0.7005856037139893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 27/86 [D loss: 0.6920267343521118, acc.: 52.34%] [G loss: 0.7025269269943237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 28/86 [D loss: 0.6921653151512146, acc.: 52.44%] [G loss: 0.6987542510032654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 29/86 [D loss: 0.6914490461349487, acc.: 53.22%] [G loss: 0.7036619782447815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 30/86 [D loss: 0.6904315948486328, acc.: 53.71%] [G loss: 0.6995428204536438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 31/86 [D loss: 0.6918433308601379, acc.: 51.51%] [G loss: 0.7003850340843201]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 32/86 [D loss: 0.6898564696311951, acc.: 54.64%] [G loss: 0.7018964290618896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 33/86 [D loss: 0.6909808218479156, acc.: 53.17%] [G loss: 0.6996797919273376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 34/86 [D loss: 0.6909147799015045, acc.: 52.15%] [G loss: 0.7022022008895874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 35/86 [D loss: 0.6907863914966583, acc.: 53.42%] [G loss: 0.7016098499298096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 36/86 [D loss: 0.6933749616146088, acc.: 49.17%] [G loss: 0.7015208005905151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 37/86 [D loss: 0.6902258098125458, acc.: 55.96%] [G loss: 0.7010095119476318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 38/86 [D loss: 0.6925833225250244, acc.: 50.44%] [G loss: 0.6989583969116211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 39/86 [D loss: 0.6915597021579742, acc.: 52.83%] [G loss: 0.70380699634552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 40/86 [D loss: 0.6899979710578918, acc.: 55.52%] [G loss: 0.6978957056999207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 41/86 [D loss: 0.6939413249492645, acc.: 49.07%] [G loss: 0.7037515640258789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 42/86 [D loss: 0.6899072527885437, acc.: 54.30%] [G loss: 0.701927900314331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 43/86 [D loss: 0.6927241384983063, acc.: 51.71%] [G loss: 0.7009909152984619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 44/86 [D loss: 0.6913301050662994, acc.: 51.12%] [G loss: 0.7027223706245422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 45/86 [D loss: 0.691235363483429, acc.: 54.10%] [G loss: 0.7004073262214661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 46/86 [D loss: 0.6929974853992462, acc.: 49.07%] [G loss: 0.7021584510803223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 47/86 [D loss: 0.6908012628555298, acc.: 54.88%] [G loss: 0.7031134963035583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 48/86 [D loss: 0.6908825635910034, acc.: 53.61%] [G loss: 0.6999477744102478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 49/86 [D loss: 0.6905501782894135, acc.: 54.83%] [G loss: 0.7030072212219238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 50/86 [D loss: 0.6903237402439117, acc.: 54.44%] [G loss: 0.6981713771820068]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 51/86 [D loss: 0.6930366158485413, acc.: 50.05%] [G loss: 0.7031074166297913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 52/86 [D loss: 0.6895497739315033, acc.: 56.05%] [G loss: 0.7013013362884521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 53/86 [D loss: 0.6914963126182556, acc.: 51.95%] [G loss: 0.7009369134902954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 54/86 [D loss: 0.6909558475017548, acc.: 54.35%] [G loss: 0.7042964100837708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 55/86 [D loss: 0.6906830370426178, acc.: 54.30%] [G loss: 0.6984268426895142]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 56/86 [D loss: 0.6934797167778015, acc.: 48.68%] [G loss: 0.7033518552780151]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 57/86 [D loss: 0.689555823802948, acc.: 55.57%] [G loss: 0.7021040320396423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 58/86 [D loss: 0.6915283501148224, acc.: 52.05%] [G loss: 0.7022047638893127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 59/86 [D loss: 0.6903248429298401, acc.: 54.00%] [G loss: 0.7035283446311951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 60/86 [D loss: 0.6905122101306915, acc.: 54.64%] [G loss: 0.7000104188919067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 61/86 [D loss: 0.6935847699642181, acc.: 50.20%] [G loss: 0.7030664086341858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 62/86 [D loss: 0.6901748180389404, acc.: 55.37%] [G loss: 0.7013643980026245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 63/86 [D loss: 0.6911986470222473, acc.: 51.76%] [G loss: 0.6994609236717224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 64/86 [D loss: 0.6916102170944214, acc.: 51.76%] [G loss: 0.7027891874313354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 65/86 [D loss: 0.690485954284668, acc.: 54.83%] [G loss: 0.7005111575126648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 66/86 [D loss: 0.6929861009120941, acc.: 50.68%] [G loss: 0.7024509906768799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 67/86 [D loss: 0.6900346577167511, acc.: 54.98%] [G loss: 0.703323245048523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 68/86 [D loss: 0.6926696002483368, acc.: 49.71%] [G loss: 0.7020322680473328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 69/86 [D loss: 0.6908693313598633, acc.: 53.76%] [G loss: 0.7041756510734558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 70/86 [D loss: 0.6906836032867432, acc.: 53.56%] [G loss: 0.7018190622329712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 71/86 [D loss: 0.6926918923854828, acc.: 50.59%] [G loss: 0.7024245262145996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 72/86 [D loss: 0.6892388463020325, acc.: 56.01%] [G loss: 0.702693521976471]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 73/86 [D loss: 0.6918165683746338, acc.: 52.73%] [G loss: 0.7018487453460693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 74/86 [D loss: 0.6904414594173431, acc.: 53.91%] [G loss: 0.7050811052322388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 75/86 [D loss: 0.6904888451099396, acc.: 55.03%] [G loss: 0.7005341053009033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 76/86 [D loss: 0.691432535648346, acc.: 53.61%] [G loss: 0.702243447303772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 77/86 [D loss: 0.6903993785381317, acc.: 55.62%] [G loss: 0.7021746635437012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 78/86 [D loss: 0.6904816925525665, acc.: 52.93%] [G loss: 0.7007825374603271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 79/86 [D loss: 0.6904085576534271, acc.: 54.44%] [G loss: 0.7021651864051819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 80/86 [D loss: 0.6905089318752289, acc.: 53.76%] [G loss: 0.7006865739822388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 81/86 [D loss: 0.6921390295028687, acc.: 52.59%] [G loss: 0.7029218673706055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 82/86 [D loss: 0.6893050074577332, acc.: 55.86%] [G loss: 0.7028465270996094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 83/86 [D loss: 0.6917000114917755, acc.: 52.69%] [G loss: 0.7023395299911499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 84/86 [D loss: 0.6908072233200073, acc.: 52.93%] [G loss: 0.7045203447341919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 85/86 [D loss: 0.6909907460212708, acc.: 53.17%] [G loss: 0.7009958624839783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 86/86 [D loss: 0.6914319396018982, acc.: 52.44%] [G loss: 0.7041462063789368]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 1/86 [D loss: 0.6899511516094208, acc.: 56.01%] [G loss: 0.703278124332428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 2/86 [D loss: 0.6911723613739014, acc.: 53.52%] [G loss: 0.7019320130348206]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 3/86 [D loss: 0.6910627484321594, acc.: 51.76%] [G loss: 0.7024059891700745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 4/86 [D loss: 0.6914842128753662, acc.: 53.22%] [G loss: 0.70171719789505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 5/86 [D loss: 0.6908561885356903, acc.: 53.96%] [G loss: 0.7027177810668945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 6/86 [D loss: 0.690405011177063, acc.: 54.49%] [G loss: 0.7034841775894165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 7/86 [D loss: 0.6905451118946075, acc.: 54.83%] [G loss: 0.7015324831008911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 8/86 [D loss: 0.6900893151760101, acc.: 54.10%] [G loss: 0.7003961205482483]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 9/86 [D loss: 0.6900409162044525, acc.: 55.76%] [G loss: 0.702630877494812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 10/86 [D loss: 0.6899188160896301, acc.: 54.49%] [G loss: 0.7021473050117493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 11/86 [D loss: 0.691199004650116, acc.: 53.17%] [G loss: 0.7018903493881226]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 12/86 [D loss: 0.6899917721748352, acc.: 55.66%] [G loss: 0.7043505311012268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 13/86 [D loss: 0.690966784954071, acc.: 53.66%] [G loss: 0.7013399004936218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 14/86 [D loss: 0.692070871591568, acc.: 51.86%] [G loss: 0.7043678760528564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 15/86 [D loss: 0.6899605989456177, acc.: 54.88%] [G loss: 0.7033089995384216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 16/86 [D loss: 0.6908164322376251, acc.: 52.69%] [G loss: 0.7043384313583374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 17/86 [D loss: 0.6892062723636627, acc.: 55.76%] [G loss: 0.7037654519081116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 18/86 [D loss: 0.6920190155506134, acc.: 52.83%] [G loss: 0.702499270439148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 19/86 [D loss: 0.6897779703140259, acc.: 56.01%] [G loss: 0.7039909362792969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 20/86 [D loss: 0.6908360123634338, acc.: 52.29%] [G loss: 0.702724277973175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 21/86 [D loss: 0.690291166305542, acc.: 54.10%] [G loss: 0.7051628828048706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 22/86 [D loss: 0.6908069252967834, acc.: 53.96%] [G loss: 0.7015467286109924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 23/86 [D loss: 0.692142128944397, acc.: 51.32%] [G loss: 0.7021851539611816]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 24/86 [D loss: 0.6900230646133423, acc.: 54.64%] [G loss: 0.7029451727867126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 25/86 [D loss: 0.6905223429203033, acc.: 53.81%] [G loss: 0.7032101154327393]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 26/86 [D loss: 0.6901713907718658, acc.: 54.64%] [G loss: 0.7031911015510559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 27/86 [D loss: 0.69015172123909, acc.: 54.20%] [G loss: 0.7018235325813293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 28/86 [D loss: 0.6907857656478882, acc.: 54.15%] [G loss: 0.7037129998207092]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 29/86 [D loss: 0.6897219717502594, acc.: 54.83%] [G loss: 0.7038959264755249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 30/86 [D loss: 0.6903464794158936, acc.: 54.39%] [G loss: 0.7041046619415283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 31/86 [D loss: 0.6914927959442139, acc.: 53.12%] [G loss: 0.7032490968704224]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 32/86 [D loss: 0.6922263205051422, acc.: 51.42%] [G loss: 0.7026950120925903]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 33/86 [D loss: 0.691564679145813, acc.: 53.32%] [G loss: 0.703533411026001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 34/86 [D loss: 0.6905433833599091, acc.: 54.35%] [G loss: 0.7011465430259705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 35/86 [D loss: 0.6919166743755341, acc.: 52.20%] [G loss: 0.7042589783668518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 36/86 [D loss: 0.689520537853241, acc.: 56.01%] [G loss: 0.7029026746749878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 37/86 [D loss: 0.6926555037498474, acc.: 50.05%] [G loss: 0.7008471488952637]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 38/86 [D loss: 0.6897926330566406, acc.: 55.22%] [G loss: 0.7036649584770203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 39/86 [D loss: 0.6919410526752472, acc.: 51.95%] [G loss: 0.7034939527511597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 40/86 [D loss: 0.6909255683422089, acc.: 52.25%] [G loss: 0.703407883644104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 41/86 [D loss: 0.6901271343231201, acc.: 54.79%] [G loss: 0.7006403207778931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 42/86 [D loss: 0.6921185255050659, acc.: 50.68%] [G loss: 0.7025504112243652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 43/86 [D loss: 0.6894406676292419, acc.: 55.47%] [G loss: 0.7037575244903564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 44/86 [D loss: 0.6913388669490814, acc.: 52.34%] [G loss: 0.7030526995658875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 45/86 [D loss: 0.6896997690200806, acc.: 54.69%] [G loss: 0.7038331627845764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 46/86 [D loss: 0.6908935308456421, acc.: 53.86%] [G loss: 0.7007908225059509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 47/86 [D loss: 0.6917787790298462, acc.: 50.93%] [G loss: 0.7044442296028137]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 48/86 [D loss: 0.6902231574058533, acc.: 53.91%] [G loss: 0.7013627290725708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 49/86 [D loss: 0.6912418603897095, acc.: 53.47%] [G loss: 0.7018651962280273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 50/86 [D loss: 0.6896335780620575, acc.: 55.71%] [G loss: 0.7031868696212769]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 51/86 [D loss: 0.6906041204929352, acc.: 53.91%] [G loss: 0.7031169533729553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 52/86 [D loss: 0.6915750205516815, acc.: 53.47%] [G loss: 0.7048431038856506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 53/86 [D loss: 0.6904065310955048, acc.: 54.39%] [G loss: 0.7031445503234863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 54/86 [D loss: 0.6912731230258942, acc.: 52.88%] [G loss: 0.70343017578125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 55/86 [D loss: 0.6903459429740906, acc.: 54.74%] [G loss: 0.7036486268043518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 56/86 [D loss: 0.6911118924617767, acc.: 53.76%] [G loss: 0.7029850482940674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 57/86 [D loss: 0.690384566783905, acc.: 54.15%] [G loss: 0.704744815826416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 58/86 [D loss: 0.6907682716846466, acc.: 53.52%] [G loss: 0.7021008729934692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 59/86 [D loss: 0.6903569102287292, acc.: 54.30%] [G loss: 0.70355224609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 60/86 [D loss: 0.6904434859752655, acc.: 54.10%] [G loss: 0.7026064991950989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 61/86 [D loss: 0.6929180026054382, acc.: 50.73%] [G loss: 0.7033404111862183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 62/86 [D loss: 0.6902791261672974, acc.: 54.15%] [G loss: 0.7047024965286255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 63/86 [D loss: 0.6914968192577362, acc.: 51.66%] [G loss: 0.7046886086463928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 64/86 [D loss: 0.690408855676651, acc.: 53.91%] [G loss: 0.7043333053588867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 65/86 [D loss: 0.6900770664215088, acc.: 54.39%] [G loss: 0.7041743397712708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 66/86 [D loss: 0.6909035444259644, acc.: 53.12%] [G loss: 0.7035143375396729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 67/86 [D loss: 0.6899059414863586, acc.: 54.98%] [G loss: 0.7044302225112915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 68/86 [D loss: 0.6898735165596008, acc.: 55.13%] [G loss: 0.7037917375564575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 69/86 [D loss: 0.6911023557186127, acc.: 53.47%] [G loss: 0.7058836221694946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 70/86 [D loss: 0.6898253560066223, acc.: 54.83%] [G loss: 0.702713131904602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 71/86 [D loss: 0.69071164727211, acc.: 54.00%] [G loss: 0.7030094861984253]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 72/86 [D loss: 0.6903394758701324, acc.: 54.59%] [G loss: 0.70182865858078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 73/86 [D loss: 0.6906783878803253, acc.: 54.10%] [G loss: 0.7038378715515137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 74/86 [D loss: 0.6901347637176514, acc.: 55.32%] [G loss: 0.7042253017425537]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 75/86 [D loss: 0.6914551556110382, acc.: 52.64%] [G loss: 0.7028855085372925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 76/86 [D loss: 0.6917724013328552, acc.: 52.34%] [G loss: 0.7051548957824707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 77/86 [D loss: 0.6903228163719177, acc.: 54.69%] [G loss: 0.7031416893005371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 78/86 [D loss: 0.6905034184455872, acc.: 53.08%] [G loss: 0.7042190432548523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 79/86 [D loss: 0.690113365650177, acc.: 55.57%] [G loss: 0.705338716506958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 80/86 [D loss: 0.6915305554866791, acc.: 52.64%] [G loss: 0.7026168704032898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 81/86 [D loss: 0.6912451088428497, acc.: 53.61%] [G loss: 0.7051528692245483]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 82/86 [D loss: 0.6904764473438263, acc.: 53.81%] [G loss: 0.7035059928894043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 83/86 [D loss: 0.6913867890834808, acc.: 52.20%] [G loss: 0.7036817073822021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 84/86 [D loss: 0.6907276213169098, acc.: 54.10%] [G loss: 0.7018134593963623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 85/86 [D loss: 0.6922682225704193, acc.: 51.22%] [G loss: 0.7023743987083435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 86/86 [D loss: 0.6906386613845825, acc.: 54.64%] [G loss: 0.7031584978103638]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 1/86 [D loss: 0.6915499567985535, acc.: 50.68%] [G loss: 0.7019748687744141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 2/86 [D loss: 0.6900267899036407, acc.: 54.10%] [G loss: 0.7047483325004578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 3/86 [D loss: 0.6908357739448547, acc.: 53.17%] [G loss: 0.7030429840087891]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 4/86 [D loss: 0.6902554631233215, acc.: 53.91%] [G loss: 0.7040165662765503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 5/86 [D loss: 0.6894226670265198, acc.: 56.01%] [G loss: 0.7045249342918396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 6/86 [D loss: 0.690619021654129, acc.: 54.15%] [G loss: 0.7031155228614807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 7/86 [D loss: 0.690497487783432, acc.: 54.15%] [G loss: 0.7040690779685974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 8/86 [D loss: 0.691042959690094, acc.: 53.12%] [G loss: 0.7040181159973145]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 9/86 [D loss: 0.6911376416683197, acc.: 54.05%] [G loss: 0.7044156789779663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 10/86 [D loss: 0.688899427652359, acc.: 56.84%] [G loss: 0.7039836049079895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 11/86 [D loss: 0.6915184557437897, acc.: 52.64%] [G loss: 0.7035853266716003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 12/86 [D loss: 0.6894556879997253, acc.: 55.08%] [G loss: 0.7028248906135559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 13/86 [D loss: 0.6903063952922821, acc.: 53.96%] [G loss: 0.7040160298347473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 14/86 [D loss: 0.6907985210418701, acc.: 53.17%] [G loss: 0.7054084539413452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 15/86 [D loss: 0.6904937624931335, acc.: 53.37%] [G loss: 0.7041022777557373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 16/86 [D loss: 0.6913636922836304, acc.: 53.22%] [G loss: 0.7045261859893799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 17/86 [D loss: 0.6891833543777466, acc.: 55.86%] [G loss: 0.7036356925964355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 18/86 [D loss: 0.6910299062728882, acc.: 53.27%] [G loss: 0.7052630186080933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 19/86 [D loss: 0.6901515126228333, acc.: 55.08%] [G loss: 0.7079365849494934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 20/86 [D loss: 0.6910930275917053, acc.: 54.00%] [G loss: 0.7029520273208618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 21/86 [D loss: 0.6905732452869415, acc.: 54.44%] [G loss: 0.7053618431091309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 22/86 [D loss: 0.6896772086620331, acc.: 55.96%] [G loss: 0.7032767534255981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 23/86 [D loss: 0.6905236542224884, acc.: 52.44%] [G loss: 0.7038028240203857]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 24/86 [D loss: 0.6898460984230042, acc.: 54.98%] [G loss: 0.7015705108642578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 25/86 [D loss: 0.6914263069629669, acc.: 53.47%] [G loss: 0.7041826844215393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 26/86 [D loss: 0.6905713379383087, acc.: 53.66%] [G loss: 0.7031285762786865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 27/86 [D loss: 0.6912097632884979, acc.: 52.10%] [G loss: 0.7018921375274658]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 28/86 [D loss: 0.6909239888191223, acc.: 53.47%] [G loss: 0.7058181166648865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 29/86 [D loss: 0.6906814873218536, acc.: 53.03%] [G loss: 0.7037690281867981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 30/86 [D loss: 0.6908845901489258, acc.: 53.52%] [G loss: 0.7038343548774719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 31/86 [D loss: 0.6899322271347046, acc.: 54.35%] [G loss: 0.7041179537773132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 32/86 [D loss: 0.6917442679405212, acc.: 52.25%] [G loss: 0.7020941972732544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 33/86 [D loss: 0.691563218832016, acc.: 52.39%] [G loss: 0.7049887776374817]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 34/86 [D loss: 0.6902790069580078, acc.: 53.22%] [G loss: 0.7030782699584961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 35/86 [D loss: 0.6907455325126648, acc.: 53.03%] [G loss: 0.7068686485290527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 36/86 [D loss: 0.6897779703140259, acc.: 55.42%] [G loss: 0.701286256313324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 37/86 [D loss: 0.6921334266662598, acc.: 51.51%] [G loss: 0.7063614726066589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 38/86 [D loss: 0.6883968710899353, acc.: 57.62%] [G loss: 0.7041191458702087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 39/86 [D loss: 0.6912677884101868, acc.: 53.37%] [G loss: 0.7017589807510376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 40/86 [D loss: 0.6899703741073608, acc.: 53.91%] [G loss: 0.7054829597473145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 41/86 [D loss: 0.6913384199142456, acc.: 52.39%] [G loss: 0.7000884413719177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 42/86 [D loss: 0.6923357248306274, acc.: 51.22%] [G loss: 0.7048484683036804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 43/86 [D loss: 0.6892423331737518, acc.: 56.05%] [G loss: 0.7026122808456421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 44/86 [D loss: 0.6917390823364258, acc.: 52.93%] [G loss: 0.7036230564117432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 45/86 [D loss: 0.6896395981311798, acc.: 56.05%] [G loss: 0.7032492160797119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 46/86 [D loss: 0.6916198134422302, acc.: 52.59%] [G loss: 0.7000953555107117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 47/86 [D loss: 0.6917981505393982, acc.: 51.95%] [G loss: 0.7059370875358582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 48/86 [D loss: 0.6901057660579681, acc.: 53.81%] [G loss: 0.7006101012229919]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 49/86 [D loss: 0.6910123527050018, acc.: 52.15%] [G loss: 0.703726053237915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 50/86 [D loss: 0.6893203556537628, acc.: 56.49%] [G loss: 0.7012645602226257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 51/86 [D loss: 0.6919190585613251, acc.: 52.59%] [G loss: 0.7041196823120117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 52/86 [D loss: 0.688299834728241, acc.: 56.88%] [G loss: 0.7054786086082458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 53/86 [D loss: 0.6926759481430054, acc.: 50.59%] [G loss: 0.7030923962593079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 54/86 [D loss: 0.6907080411911011, acc.: 53.86%] [G loss: 0.7048248648643494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 55/86 [D loss: 0.6912092566490173, acc.: 52.78%] [G loss: 0.7003443837165833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 56/86 [D loss: 0.6928767263889313, acc.: 50.68%] [G loss: 0.7072064876556396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 57/86 [D loss: 0.6870509386062622, acc.: 58.79%] [G loss: 0.7034487724304199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 58/86 [D loss: 0.6927106082439423, acc.: 51.17%] [G loss: 0.7031103372573853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 59/86 [D loss: 0.689146101474762, acc.: 56.59%] [G loss: 0.7069973349571228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 60/86 [D loss: 0.6906657516956329, acc.: 53.66%] [G loss: 0.6989831924438477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 61/86 [D loss: 0.6935482025146484, acc.: 49.90%] [G loss: 0.705253005027771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 62/86 [D loss: 0.6896718144416809, acc.: 54.54%] [G loss: 0.7024765610694885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 63/86 [D loss: 0.6926748156547546, acc.: 50.78%] [G loss: 0.7036836743354797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 64/86 [D loss: 0.6893075704574585, acc.: 56.35%] [G loss: 0.7051125764846802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 65/86 [D loss: 0.6913094520568848, acc.: 53.42%] [G loss: 0.7030353546142578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 66/86 [D loss: 0.6919977962970734, acc.: 52.93%] [G loss: 0.7049513459205627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 67/86 [D loss: 0.6904956698417664, acc.: 54.83%] [G loss: 0.701015830039978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 68/86 [D loss: 0.6913370192050934, acc.: 53.81%] [G loss: 0.7046824097633362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 69/86 [D loss: 0.6893876492977142, acc.: 56.15%] [G loss: 0.705442488193512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 70/86 [D loss: 0.6918065547943115, acc.: 51.27%] [G loss: 0.7034508585929871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 71/86 [D loss: 0.6897322833538055, acc.: 55.76%] [G loss: 0.7030550837516785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 72/86 [D loss: 0.6899770498275757, acc.: 54.74%] [G loss: 0.7039721012115479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 73/86 [D loss: 0.690289169549942, acc.: 54.30%] [G loss: 0.7058956027030945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 74/86 [D loss: 0.6901530623435974, acc.: 54.39%] [G loss: 0.7040855884552002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 75/86 [D loss: 0.6919536590576172, acc.: 51.56%] [G loss: 0.7041205167770386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 76/86 [D loss: 0.6897028088569641, acc.: 55.62%] [G loss: 0.7058252096176147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 77/86 [D loss: 0.6899338662624359, acc.: 54.88%] [G loss: 0.7045879364013672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 78/86 [D loss: 0.6904009282588959, acc.: 54.20%] [G loss: 0.7049196362495422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 79/86 [D loss: 0.6894142031669617, acc.: 55.47%] [G loss: 0.7047818899154663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 80/86 [D loss: 0.6895004212856293, acc.: 55.91%] [G loss: 0.704340398311615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 81/86 [D loss: 0.6901736557483673, acc.: 53.71%] [G loss: 0.7051186561584473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 82/86 [D loss: 0.6904464066028595, acc.: 52.54%] [G loss: 0.7039068341255188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 83/86 [D loss: 0.690718948841095, acc.: 53.52%] [G loss: 0.7057064175605774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 84/86 [D loss: 0.6906483769416809, acc.: 54.35%] [G loss: 0.7047041058540344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 85/86 [D loss: 0.6909842491149902, acc.: 52.59%] [G loss: 0.7046715021133423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 86/86 [D loss: 0.6898184418678284, acc.: 54.69%] [G loss: 0.7059906721115112]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 1/86 [D loss: 0.6899740397930145, acc.: 55.47%] [G loss: 0.7049874067306519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 2/86 [D loss: 0.6895580291748047, acc.: 55.37%] [G loss: 0.7061648368835449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 3/86 [D loss: 0.6908955574035645, acc.: 52.25%] [G loss: 0.7052434682846069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 4/86 [D loss: 0.6913406252861023, acc.: 51.51%] [G loss: 0.7074400782585144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 5/86 [D loss: 0.6895690560340881, acc.: 55.71%] [G loss: 0.7074815630912781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 6/86 [D loss: 0.6912314891815186, acc.: 53.22%] [G loss: 0.7059667706489563]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 7/86 [D loss: 0.6905946731567383, acc.: 53.42%] [G loss: 0.7076390981674194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 8/86 [D loss: 0.6911442875862122, acc.: 53.66%] [G loss: 0.7052960395812988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 9/86 [D loss: 0.6897540092468262, acc.: 55.32%] [G loss: 0.7068646550178528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 10/86 [D loss: 0.6913072168827057, acc.: 52.93%] [G loss: 0.7033233642578125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 11/86 [D loss: 0.6910403072834015, acc.: 53.66%] [G loss: 0.7063387036323547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 12/86 [D loss: 0.6894259452819824, acc.: 55.42%] [G loss: 0.7032487392425537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 13/86 [D loss: 0.6930621862411499, acc.: 50.68%] [G loss: 0.706132709980011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 14/86 [D loss: 0.6893641948699951, acc.: 56.01%] [G loss: 0.7036606073379517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 15/86 [D loss: 0.6920971870422363, acc.: 50.73%] [G loss: 0.7048478722572327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 16/86 [D loss: 0.6905505061149597, acc.: 54.93%] [G loss: 0.7069297432899475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 17/86 [D loss: 0.6895258724689484, acc.: 54.79%] [G loss: 0.7005230188369751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 18/86 [D loss: 0.6924218535423279, acc.: 52.73%] [G loss: 0.7081148028373718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 19/86 [D loss: 0.687847375869751, acc.: 56.98%] [G loss: 0.7043876051902771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 20/86 [D loss: 0.6922273933887482, acc.: 51.90%] [G loss: 0.7039767503738403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 21/86 [D loss: 0.6899914741516113, acc.: 54.64%] [G loss: 0.7072352170944214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 22/86 [D loss: 0.691405326128006, acc.: 52.54%] [G loss: 0.6991344690322876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 23/86 [D loss: 0.6934636235237122, acc.: 49.61%] [G loss: 0.7068427801132202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 24/86 [D loss: 0.6893787384033203, acc.: 55.18%] [G loss: 0.7019233107566833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 25/86 [D loss: 0.6926936209201813, acc.: 51.22%] [G loss: 0.7030899524688721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 26/86 [D loss: 0.6896893680095673, acc.: 55.08%] [G loss: 0.7061350345611572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 27/86 [D loss: 0.6912626624107361, acc.: 53.47%] [G loss: 0.701330840587616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 28/86 [D loss: 0.6915013194084167, acc.: 52.88%] [G loss: 0.7086915969848633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 29/86 [D loss: 0.6893511116504669, acc.: 55.71%] [G loss: 0.7022989988327026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 30/86 [D loss: 0.6921117007732391, acc.: 51.22%] [G loss: 0.7050554156303406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 31/86 [D loss: 0.6897304952144623, acc.: 55.76%] [G loss: 0.7058781385421753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 32/86 [D loss: 0.6915848851203918, acc.: 51.27%] [G loss: 0.700695812702179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 33/86 [D loss: 0.691725492477417, acc.: 52.15%] [G loss: 0.7088879942893982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 34/86 [D loss: 0.6897720098495483, acc.: 54.79%] [G loss: 0.702411413192749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 35/86 [D loss: 0.6922103762626648, acc.: 51.81%] [G loss: 0.7048917412757874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 36/86 [D loss: 0.6894486546516418, acc.: 55.27%] [G loss: 0.705864667892456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 37/86 [D loss: 0.6898543834686279, acc.: 55.27%] [G loss: 0.6987021565437317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 38/86 [D loss: 0.692586213350296, acc.: 50.59%] [G loss: 0.7069894671440125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 39/86 [D loss: 0.6896846890449524, acc.: 54.20%] [G loss: 0.7018212080001831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 40/86 [D loss: 0.6930056214332581, acc.: 49.85%] [G loss: 0.702489972114563]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 41/86 [D loss: 0.6899934113025665, acc.: 54.39%] [G loss: 0.7075033783912659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 42/86 [D loss: 0.6909059286117554, acc.: 52.20%] [G loss: 0.7016475200653076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 43/86 [D loss: 0.6930713653564453, acc.: 49.41%] [G loss: 0.7051148414611816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 44/86 [D loss: 0.6895061135292053, acc.: 56.35%] [G loss: 0.7044832706451416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 45/86 [D loss: 0.6921335458755493, acc.: 50.83%] [G loss: 0.7018339037895203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 46/86 [D loss: 0.6915966272354126, acc.: 52.73%] [G loss: 0.7071678638458252]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 47/86 [D loss: 0.6889852583408356, acc.: 56.25%] [G loss: 0.7009921669960022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 48/86 [D loss: 0.6930738091468811, acc.: 50.15%] [G loss: 0.7040877342224121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 49/86 [D loss: 0.6877227127552032, acc.: 57.91%] [G loss: 0.7040461897850037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 50/86 [D loss: 0.6931591033935547, acc.: 49.71%] [G loss: 0.7034333944320679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 51/86 [D loss: 0.6906141638755798, acc.: 54.20%] [G loss: 0.7063128352165222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 52/86 [D loss: 0.690354734659195, acc.: 53.22%] [G loss: 0.7010065913200378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 53/86 [D loss: 0.6923076510429382, acc.: 51.90%] [G loss: 0.7032294869422913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 54/86 [D loss: 0.6890699565410614, acc.: 57.47%] [G loss: 0.703474760055542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 55/86 [D loss: 0.6925062239170074, acc.: 51.27%] [G loss: 0.7012020945549011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 56/86 [D loss: 0.6910082399845123, acc.: 53.66%] [G loss: 0.7086089253425598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 57/86 [D loss: 0.6890755295753479, acc.: 56.64%] [G loss: 0.7021750211715698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 58/86 [D loss: 0.6913525760173798, acc.: 50.83%] [G loss: 0.7013669013977051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 59/86 [D loss: 0.689785361289978, acc.: 55.52%] [G loss: 0.7054494023323059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 60/86 [D loss: 0.6901532113552094, acc.: 53.08%] [G loss: 0.703730583190918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 61/86 [D loss: 0.69267737865448, acc.: 49.95%] [G loss: 0.7045757174491882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 62/86 [D loss: 0.687765896320343, acc.: 58.54%] [G loss: 0.7029351592063904]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 63/86 [D loss: 0.6926928758621216, acc.: 52.10%] [G loss: 0.7020973563194275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 64/86 [D loss: 0.6908852756023407, acc.: 53.08%] [G loss: 0.7065900564193726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 65/86 [D loss: 0.6906521618366241, acc.: 53.71%] [G loss: 0.7039270401000977]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 66/86 [D loss: 0.6907736957073212, acc.: 53.81%] [G loss: 0.7067146301269531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 67/86 [D loss: 0.6899297535419464, acc.: 54.54%] [G loss: 0.7042452096939087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 68/86 [D loss: 0.6913130283355713, acc.: 53.76%] [G loss: 0.7040009498596191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 69/86 [D loss: 0.69090735912323, acc.: 53.22%] [G loss: 0.7069219946861267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 70/86 [D loss: 0.6892851889133453, acc.: 54.20%] [G loss: 0.7038165330886841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 71/86 [D loss: 0.690136581659317, acc.: 53.56%] [G loss: 0.706093430519104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 72/86 [D loss: 0.6883170008659363, acc.: 57.28%] [G loss: 0.7039775252342224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 73/86 [D loss: 0.6908456981182098, acc.: 54.35%] [G loss: 0.705565869808197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 74/86 [D loss: 0.6888549625873566, acc.: 55.71%] [G loss: 0.7050976157188416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 75/86 [D loss: 0.6896764636039734, acc.: 55.32%] [G loss: 0.70595782995224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 76/86 [D loss: 0.69106325507164, acc.: 52.44%] [G loss: 0.7067431211471558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 77/86 [D loss: 0.6875747442245483, acc.: 57.96%] [G loss: 0.7063242197036743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 78/86 [D loss: 0.6902718245983124, acc.: 53.66%] [G loss: 0.7063686847686768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 79/86 [D loss: 0.6899231672286987, acc.: 55.03%] [G loss: 0.7073987722396851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 80/86 [D loss: 0.6892913579940796, acc.: 55.03%] [G loss: 0.7061420679092407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 81/86 [D loss: 0.6896555423736572, acc.: 54.49%] [G loss: 0.707969069480896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 82/86 [D loss: 0.6898222863674164, acc.: 54.69%] [G loss: 0.7057642936706543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 83/86 [D loss: 0.6917095184326172, acc.: 52.29%] [G loss: 0.7055650949478149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 84/86 [D loss: 0.6890433430671692, acc.: 56.20%] [G loss: 0.7041376829147339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 85/86 [D loss: 0.6893649697303772, acc.: 55.27%] [G loss: 0.7042420506477356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 86/86 [D loss: 0.6898186206817627, acc.: 54.54%] [G loss: 0.7069711685180664]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 68/200, Batch 1/86 [D loss: 0.6899361312389374, acc.: 54.83%] [G loss: 0.7054963707923889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 2/86 [D loss: 0.6902572512626648, acc.: 54.59%] [G loss: 0.7067368030548096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 3/86 [D loss: 0.6897633671760559, acc.: 53.86%] [G loss: 0.7063934803009033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 4/86 [D loss: 0.6896320581436157, acc.: 54.54%] [G loss: 0.7048106789588928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 5/86 [D loss: 0.689408004283905, acc.: 54.64%] [G loss: 0.7057121396064758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 6/86 [D loss: 0.6907365918159485, acc.: 54.20%] [G loss: 0.7050259113311768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 7/86 [D loss: 0.6903527081012726, acc.: 53.12%] [G loss: 0.705706000328064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 8/86 [D loss: 0.6883639693260193, acc.: 56.98%] [G loss: 0.7039275765419006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 9/86 [D loss: 0.6901189982891083, acc.: 54.54%] [G loss: 0.7047647833824158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 10/86 [D loss: 0.6884685754776001, acc.: 57.62%] [G loss: 0.7048012018203735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 11/86 [D loss: 0.6900779902935028, acc.: 54.15%] [G loss: 0.702176034450531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 12/86 [D loss: 0.6907809972763062, acc.: 52.93%] [G loss: 0.7040454149246216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 13/86 [D loss: 0.6893186271190643, acc.: 56.45%] [G loss: 0.705292284488678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 14/86 [D loss: 0.6903052031993866, acc.: 52.93%] [G loss: 0.7035675644874573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 15/86 [D loss: 0.6888227462768555, acc.: 55.22%] [G loss: 0.7060934901237488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 16/86 [D loss: 0.6908629834651947, acc.: 53.27%] [G loss: 0.7018517255783081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 17/86 [D loss: 0.6926723718643188, acc.: 51.12%] [G loss: 0.7077302932739258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 18/86 [D loss: 0.6883593201637268, acc.: 56.30%] [G loss: 0.7039109468460083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 19/86 [D loss: 0.6919865012168884, acc.: 50.68%] [G loss: 0.7037845253944397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 20/86 [D loss: 0.6894688606262207, acc.: 53.22%] [G loss: 0.7088351845741272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 21/86 [D loss: 0.6910218000411987, acc.: 53.03%] [G loss: 0.7012343406677246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 22/86 [D loss: 0.6938378810882568, acc.: 48.88%] [G loss: 0.7066648006439209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 23/86 [D loss: 0.6891184151172638, acc.: 56.20%] [G loss: 0.7010656595230103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 24/86 [D loss: 0.6924704909324646, acc.: 50.88%] [G loss: 0.7007652521133423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 25/86 [D loss: 0.6884861886501312, acc.: 55.66%] [G loss: 0.7066222429275513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 26/86 [D loss: 0.6901220381259918, acc.: 54.74%] [G loss: 0.6990134716033936]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 27/86 [D loss: 0.6944102942943573, acc.: 47.61%] [G loss: 0.7056008577346802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 28/86 [D loss: 0.6872814297676086, acc.: 58.50%] [G loss: 0.7019068002700806]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 29/86 [D loss: 0.6955127120018005, acc.: 47.31%] [G loss: 0.7004374265670776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 30/86 [D loss: 0.6903233826160431, acc.: 53.91%] [G loss: 0.7062719464302063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 31/86 [D loss: 0.6886177062988281, acc.: 56.98%] [G loss: 0.6974765062332153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 32/86 [D loss: 0.695040225982666, acc.: 47.80%] [G loss: 0.7011042833328247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 33/86 [D loss: 0.6879658401012421, acc.: 57.42%] [G loss: 0.7042359113693237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 34/86 [D loss: 0.693943589925766, acc.: 49.07%] [G loss: 0.6986026167869568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 35/86 [D loss: 0.6929285228252411, acc.: 49.51%] [G loss: 0.7064501643180847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 36/86 [D loss: 0.6879197061061859, acc.: 56.98%] [G loss: 0.697578489780426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 37/86 [D loss: 0.6945702135562897, acc.: 48.68%] [G loss: 0.6973239183425903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 38/86 [D loss: 0.6896927952766418, acc.: 56.49%] [G loss: 0.706257700920105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 39/86 [D loss: 0.6911174654960632, acc.: 52.83%] [G loss: 0.6986494660377502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 40/86 [D loss: 0.6931940913200378, acc.: 50.29%] [G loss: 0.7033277153968811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 41/86 [D loss: 0.6896270513534546, acc.: 53.71%] [G loss: 0.704149603843689]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 42/86 [D loss: 0.6921091079711914, acc.: 50.63%] [G loss: 0.6991013288497925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 43/86 [D loss: 0.692716658115387, acc.: 49.41%] [G loss: 0.706308126449585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 44/86 [D loss: 0.6894436478614807, acc.: 55.27%] [G loss: 0.7022953033447266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 45/86 [D loss: 0.6931656897068024, acc.: 49.12%] [G loss: 0.7020936608314514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 46/86 [D loss: 0.6900274753570557, acc.: 54.20%] [G loss: 0.706145703792572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 47/86 [D loss: 0.689515620470047, acc.: 55.71%] [G loss: 0.7006116509437561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 48/86 [D loss: 0.6923001110553741, acc.: 50.59%] [G loss: 0.703479528427124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 49/86 [D loss: 0.6875530183315277, acc.: 58.35%] [G loss: 0.7049344182014465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 50/86 [D loss: 0.690779834985733, acc.: 51.71%] [G loss: 0.7032355666160583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 51/86 [D loss: 0.6891287565231323, acc.: 56.10%] [G loss: 0.7061861753463745]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 52/86 [D loss: 0.6897887289524078, acc.: 55.08%] [G loss: 0.7034363150596619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 53/86 [D loss: 0.692494809627533, acc.: 51.76%] [G loss: 0.7050337791442871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 54/86 [D loss: 0.6900255084037781, acc.: 53.76%] [G loss: 0.7073694467544556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 55/86 [D loss: 0.6897300779819489, acc.: 54.74%] [G loss: 0.7044428586959839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 56/86 [D loss: 0.6910195350646973, acc.: 52.49%] [G loss: 0.7065726518630981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 57/86 [D loss: 0.6897144019603729, acc.: 55.18%] [G loss: 0.7053876519203186]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 58/86 [D loss: 0.6908345520496368, acc.: 52.73%] [G loss: 0.7026230692863464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 59/86 [D loss: 0.6910184919834137, acc.: 53.52%] [G loss: 0.7067009806632996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 60/86 [D loss: 0.6890393495559692, acc.: 56.15%] [G loss: 0.7043042778968811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 61/86 [D loss: 0.6911217570304871, acc.: 52.88%] [G loss: 0.7068370580673218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 62/86 [D loss: 0.6893103122711182, acc.: 54.83%] [G loss: 0.7061840891838074]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 63/86 [D loss: 0.6910140216350555, acc.: 54.44%] [G loss: 0.7031694650650024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 64/86 [D loss: 0.6907866299152374, acc.: 52.44%] [G loss: 0.7058416604995728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 65/86 [D loss: 0.6896464228630066, acc.: 55.47%] [G loss: 0.7055120468139648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 66/86 [D loss: 0.6906462013721466, acc.: 54.00%] [G loss: 0.7037709951400757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 67/86 [D loss: 0.6888343393802643, acc.: 56.20%] [G loss: 0.7073097229003906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 68/86 [D loss: 0.6881206929683685, acc.: 57.42%] [G loss: 0.7035791873931885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 69/86 [D loss: 0.6919335126876831, acc.: 51.81%] [G loss: 0.7054104804992676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 70/86 [D loss: 0.6882363855838776, acc.: 58.06%] [G loss: 0.7064505219459534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 71/86 [D loss: 0.6911564469337463, acc.: 53.27%] [G loss: 0.7047414183616638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 72/86 [D loss: 0.69102543592453, acc.: 53.22%] [G loss: 0.7069010734558105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 73/86 [D loss: 0.6890380084514618, acc.: 55.27%] [G loss: 0.7063528895378113]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 74/86 [D loss: 0.6919618546962738, acc.: 52.39%] [G loss: 0.7048869729042053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 75/86 [D loss: 0.6907867789268494, acc.: 54.05%] [G loss: 0.7083235383033752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 76/86 [D loss: 0.689991295337677, acc.: 54.15%] [G loss: 0.7029690146446228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 77/86 [D loss: 0.6916361153125763, acc.: 51.56%] [G loss: 0.7075963020324707]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 78/86 [D loss: 0.69004225730896, acc.: 54.35%] [G loss: 0.7060578465461731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 79/86 [D loss: 0.6916548609733582, acc.: 51.56%] [G loss: 0.7031888961791992]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 80/86 [D loss: 0.6911004185676575, acc.: 52.93%] [G loss: 0.7070584297180176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 81/86 [D loss: 0.6897643804550171, acc.: 56.45%] [G loss: 0.7039228677749634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 82/86 [D loss: 0.6907568275928497, acc.: 53.37%] [G loss: 0.7044140100479126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 83/86 [D loss: 0.6890963912010193, acc.: 54.64%] [G loss: 0.7079063057899475]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 84/86 [D loss: 0.690091997385025, acc.: 54.35%] [G loss: 0.7041649222373962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 85/86 [D loss: 0.6924248337745667, acc.: 51.66%] [G loss: 0.7069368362426758]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 86/86 [D loss: 0.6882338225841522, acc.: 56.01%] [G loss: 0.705320417881012]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 1/86 [D loss: 0.691701740026474, acc.: 51.27%] [G loss: 0.7045717835426331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 2/86 [D loss: 0.6903232336044312, acc.: 54.64%] [G loss: 0.7076427340507507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 3/86 [D loss: 0.6883968710899353, acc.: 56.49%] [G loss: 0.7013369202613831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 4/86 [D loss: 0.6936673820018768, acc.: 48.44%] [G loss: 0.7046573162078857]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 5/86 [D loss: 0.6888795793056488, acc.: 55.47%] [G loss: 0.7051899433135986]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 6/86 [D loss: 0.6917275190353394, acc.: 51.86%] [G loss: 0.6988327503204346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 7/86 [D loss: 0.6920883655548096, acc.: 52.25%] [G loss: 0.7056282162666321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 8/86 [D loss: 0.6869548261165619, acc.: 59.33%] [G loss: 0.7046685218811035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 9/86 [D loss: 0.6910412311553955, acc.: 53.27%] [G loss: 0.6977224349975586]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 10/86 [D loss: 0.6926168203353882, acc.: 50.63%] [G loss: 0.7087875604629517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 11/86 [D loss: 0.6886215209960938, acc.: 56.10%] [G loss: 0.7013048529624939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 12/86 [D loss: 0.6925836801528931, acc.: 50.49%] [G loss: 0.7031988501548767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 13/86 [D loss: 0.6898048222064972, acc.: 55.42%] [G loss: 0.705764651298523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 14/86 [D loss: 0.6907928586006165, acc.: 54.05%] [G loss: 0.6992948055267334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 15/86 [D loss: 0.6928667426109314, acc.: 50.73%] [G loss: 0.704332709312439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 16/86 [D loss: 0.6894335746765137, acc.: 55.37%] [G loss: 0.7036982178688049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 17/86 [D loss: 0.6928785145282745, acc.: 50.34%] [G loss: 0.7032549381256104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 18/86 [D loss: 0.6910466253757477, acc.: 53.08%] [G loss: 0.70511394739151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 19/86 [D loss: 0.6877543330192566, acc.: 56.25%] [G loss: 0.7021690011024475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 20/86 [D loss: 0.6928839385509491, acc.: 49.46%] [G loss: 0.7013301253318787]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 21/86 [D loss: 0.6912127435207367, acc.: 53.08%] [G loss: 0.7048863172531128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 22/86 [D loss: 0.6901607513427734, acc.: 54.15%] [G loss: 0.7030137181282043]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 23/86 [D loss: 0.6912219822406769, acc.: 52.73%] [G loss: 0.7058825492858887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 24/86 [D loss: 0.6880694627761841, acc.: 56.74%] [G loss: 0.7040746808052063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 25/86 [D loss: 0.6893656253814697, acc.: 53.76%] [G loss: 0.7009668946266174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 26/86 [D loss: 0.6924133002758026, acc.: 51.86%] [G loss: 0.7058449983596802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 27/86 [D loss: 0.6889565587043762, acc.: 55.71%] [G loss: 0.7036609649658203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 28/86 [D loss: 0.6911908388137817, acc.: 53.37%] [G loss: 0.7029681205749512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 29/86 [D loss: 0.6897215247154236, acc.: 55.18%] [G loss: 0.7056818604469299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 30/86 [D loss: 0.6886269450187683, acc.: 56.84%] [G loss: 0.7030609250068665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 31/86 [D loss: 0.6916927397251129, acc.: 52.64%] [G loss: 0.7051790952682495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 32/86 [D loss: 0.6906789243221283, acc.: 53.03%] [G loss: 0.7050162553787231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 33/86 [D loss: 0.6899335980415344, acc.: 54.00%] [G loss: 0.7048888802528381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 34/86 [D loss: 0.6892780661582947, acc.: 54.83%] [G loss: 0.7057335376739502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 35/86 [D loss: 0.6894743144512177, acc.: 54.39%] [G loss: 0.7045274972915649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 36/86 [D loss: 0.6895227432250977, acc.: 55.71%] [G loss: 0.7035402655601501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 37/86 [D loss: 0.6903634667396545, acc.: 54.64%] [G loss: 0.7080607414245605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 38/86 [D loss: 0.6896872818470001, acc.: 54.88%] [G loss: 0.7044947743415833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 39/86 [D loss: 0.6904228925704956, acc.: 53.47%] [G loss: 0.7046943306922913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 40/86 [D loss: 0.6896042227745056, acc.: 54.69%] [G loss: 0.7041851878166199]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 41/86 [D loss: 0.6901053190231323, acc.: 54.59%] [G loss: 0.7027699947357178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 42/86 [D loss: 0.6914204359054565, acc.: 52.64%] [G loss: 0.7051335573196411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 43/86 [D loss: 0.6888555288314819, acc.: 56.20%] [G loss: 0.7046359777450562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 44/86 [D loss: 0.690993458032608, acc.: 51.66%] [G loss: 0.7020452618598938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 45/86 [D loss: 0.6912477314472198, acc.: 52.78%] [G loss: 0.7065925002098083]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 46/86 [D loss: 0.6879934072494507, acc.: 58.06%] [G loss: 0.7034995555877686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 47/86 [D loss: 0.6919474303722382, acc.: 51.95%] [G loss: 0.7033696174621582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 48/86 [D loss: 0.6900073289871216, acc.: 54.00%] [G loss: 0.7072132229804993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 49/86 [D loss: 0.6913549900054932, acc.: 53.32%] [G loss: 0.7009208798408508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 50/86 [D loss: 0.690804660320282, acc.: 53.56%] [G loss: 0.7051450610160828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 51/86 [D loss: 0.6892494857311249, acc.: 54.59%] [G loss: 0.7044025659561157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 52/86 [D loss: 0.6903857886791229, acc.: 53.37%] [G loss: 0.7039082050323486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 53/86 [D loss: 0.6903969049453735, acc.: 54.30%] [G loss: 0.7063826322555542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 54/86 [D loss: 0.6894161999225616, acc.: 55.52%] [G loss: 0.7034095525741577]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 55/86 [D loss: 0.6909251809120178, acc.: 53.61%] [G loss: 0.7058128714561462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 56/86 [D loss: 0.689783900976181, acc.: 54.98%] [G loss: 0.7043891549110413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 57/86 [D loss: 0.6906554400920868, acc.: 53.47%] [G loss: 0.7030468583106995]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 58/86 [D loss: 0.6906742453575134, acc.: 51.61%] [G loss: 0.7054181098937988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 59/86 [D loss: 0.6880213022232056, acc.: 55.86%] [G loss: 0.7057617902755737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 60/86 [D loss: 0.6916772425174713, acc.: 52.25%] [G loss: 0.7059043049812317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 61/86 [D loss: 0.6891501545906067, acc.: 55.18%] [G loss: 0.704308271408081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 62/86 [D loss: 0.6884182989597321, acc.: 57.62%] [G loss: 0.7035015821456909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 63/86 [D loss: 0.6920659840106964, acc.: 50.34%] [G loss: 0.7054247856140137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 64/86 [D loss: 0.6877749562263489, acc.: 56.93%] [G loss: 0.7053651213645935]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 65/86 [D loss: 0.6911269128322601, acc.: 53.12%] [G loss: 0.7013792991638184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 66/86 [D loss: 0.6898277103900909, acc.: 54.10%] [G loss: 0.7071139812469482]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 67/86 [D loss: 0.6892836689949036, acc.: 56.35%] [G loss: 0.7025721073150635]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 68/86 [D loss: 0.6918283998966217, acc.: 51.56%] [G loss: 0.7032749652862549]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 69/86 [D loss: 0.6876759231090546, acc.: 56.74%] [G loss: 0.7076295614242554]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 70/86 [D loss: 0.6916971504688263, acc.: 51.81%] [G loss: 0.7015632390975952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 71/86 [D loss: 0.6913230121135712, acc.: 53.08%] [G loss: 0.7081273198127747]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 72/86 [D loss: 0.6873175501823425, acc.: 58.79%] [G loss: 0.7034045457839966]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 73/86 [D loss: 0.6921992301940918, acc.: 51.32%] [G loss: 0.6992107033729553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 74/86 [D loss: 0.6900534927845001, acc.: 54.39%] [G loss: 0.7088757157325745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 75/86 [D loss: 0.6887127459049225, acc.: 56.59%] [G loss: 0.7022886872291565]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 76/86 [D loss: 0.6919941306114197, acc.: 51.71%] [G loss: 0.7051033973693848]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 77/86 [D loss: 0.6890931725502014, acc.: 55.76%] [G loss: 0.7068158984184265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 78/86 [D loss: 0.6901620626449585, acc.: 54.39%] [G loss: 0.7009962201118469]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 79/86 [D loss: 0.6940473020076752, acc.: 49.56%] [G loss: 0.7051986455917358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 80/86 [D loss: 0.6877407729625702, acc.: 58.25%] [G loss: 0.7062526345252991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 81/86 [D loss: 0.6912198960781097, acc.: 52.93%] [G loss: 0.7005637884140015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 82/86 [D loss: 0.6902233958244324, acc.: 53.17%] [G loss: 0.706732451915741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 83/86 [D loss: 0.6874077916145325, acc.: 57.37%] [G loss: 0.7029846906661987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 84/86 [D loss: 0.6931088268756866, acc.: 51.17%] [G loss: 0.69889897108078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 85/86 [D loss: 0.6911121606826782, acc.: 52.88%] [G loss: 0.7075943350791931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 86/86 [D loss: 0.6888781189918518, acc.: 56.25%] [G loss: 0.702892005443573]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 1/86 [D loss: 0.6911526918411255, acc.: 53.66%] [G loss: 0.7042601704597473]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 2/86 [D loss: 0.6896206140518188, acc.: 54.30%] [G loss: 0.706691563129425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 3/86 [D loss: 0.6904071271419525, acc.: 53.32%] [G loss: 0.7006759643554688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 4/86 [D loss: 0.6916509568691254, acc.: 51.03%] [G loss: 0.7049048542976379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 5/86 [D loss: 0.6883919835090637, acc.: 55.52%] [G loss: 0.7055245637893677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 6/86 [D loss: 0.690517783164978, acc.: 52.98%] [G loss: 0.7040351629257202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 7/86 [D loss: 0.690446525812149, acc.: 53.03%] [G loss: 0.7077516913414001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 8/86 [D loss: 0.6887397468090057, acc.: 56.88%] [G loss: 0.7028467655181885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 9/86 [D loss: 0.6919713914394379, acc.: 51.42%] [G loss: 0.7027710676193237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 10/86 [D loss: 0.689339280128479, acc.: 56.01%] [G loss: 0.7071061134338379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 11/86 [D loss: 0.6892260313034058, acc.: 55.62%] [G loss: 0.7019889950752258]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 12/86 [D loss: 0.6931571364402771, acc.: 50.29%] [G loss: 0.7069332599639893]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 13/86 [D loss: 0.6896955966949463, acc.: 55.13%] [G loss: 0.7068030834197998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 14/86 [D loss: 0.690470427274704, acc.: 53.81%] [G loss: 0.6988738775253296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 15/86 [D loss: 0.6925291419029236, acc.: 50.83%] [G loss: 0.7064088582992554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 16/86 [D loss: 0.6885740160942078, acc.: 56.15%] [G loss: 0.7018674612045288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 17/86 [D loss: 0.6924866437911987, acc.: 50.34%] [G loss: 0.7027544975280762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 18/86 [D loss: 0.6901558637619019, acc.: 54.39%] [G loss: 0.707551121711731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 19/86 [D loss: 0.6887969076633453, acc.: 57.71%] [G loss: 0.7037441730499268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 20/86 [D loss: 0.6932261884212494, acc.: 50.93%] [G loss: 0.7022032141685486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 21/86 [D loss: 0.6909829676151276, acc.: 53.12%] [G loss: 0.7066878080368042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 22/86 [D loss: 0.6894649565219879, acc.: 55.13%] [G loss: 0.7011631727218628]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 23/86 [D loss: 0.6906185448169708, acc.: 53.71%] [G loss: 0.7023900747299194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 24/86 [D loss: 0.6892828345298767, acc.: 55.96%] [G loss: 0.7079300880432129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 25/86 [D loss: 0.6908632218837738, acc.: 53.76%] [G loss: 0.7001239061355591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 26/86 [D loss: 0.6946019530296326, acc.: 49.85%] [G loss: 0.7050836086273193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 27/86 [D loss: 0.6883769929409027, acc.: 57.81%] [G loss: 0.7052465677261353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 28/86 [D loss: 0.6908221244812012, acc.: 53.17%] [G loss: 0.700365424156189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 29/86 [D loss: 0.6901702284812927, acc.: 54.98%] [G loss: 0.7055116295814514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 30/86 [D loss: 0.6876200735569, acc.: 57.23%] [G loss: 0.7062362432479858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 31/86 [D loss: 0.6909770369529724, acc.: 53.42%] [G loss: 0.7011725902557373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 32/86 [D loss: 0.691865086555481, acc.: 52.20%] [G loss: 0.7074323892593384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 33/86 [D loss: 0.6878255605697632, acc.: 57.52%] [G loss: 0.7034955024719238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 34/86 [D loss: 0.6915188133716583, acc.: 50.98%] [G loss: 0.704075813293457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 35/86 [D loss: 0.6892559230327606, acc.: 55.81%] [G loss: 0.7068299651145935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 36/86 [D loss: 0.6890206038951874, acc.: 54.83%] [G loss: 0.7030797004699707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 37/86 [D loss: 0.691580206155777, acc.: 51.42%] [G loss: 0.7037979960441589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 38/86 [D loss: 0.6892350912094116, acc.: 55.47%] [G loss: 0.7050536870956421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 39/86 [D loss: 0.6885640919208527, acc.: 56.45%] [G loss: 0.7026186585426331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 40/86 [D loss: 0.6898695826530457, acc.: 54.69%] [G loss: 0.704925000667572]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 41/86 [D loss: 0.689525306224823, acc.: 55.52%] [G loss: 0.7054810523986816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 42/86 [D loss: 0.6905108690261841, acc.: 53.71%] [G loss: 0.704189121723175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 43/86 [D loss: 0.6894997954368591, acc.: 54.54%] [G loss: 0.7057210206985474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 44/86 [D loss: 0.6889530420303345, acc.: 55.57%] [G loss: 0.7062578201293945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 45/86 [D loss: 0.690604954957962, acc.: 53.27%] [G loss: 0.7034849524497986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 46/86 [D loss: 0.6907878816127777, acc.: 54.05%] [G loss: 0.7068305611610413]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 47/86 [D loss: 0.6898338794708252, acc.: 54.15%] [G loss: 0.7069380283355713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 48/86 [D loss: 0.6898525357246399, acc.: 54.59%] [G loss: 0.7041003108024597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 49/86 [D loss: 0.688909113407135, acc.: 54.05%] [G loss: 0.7061759233474731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 50/86 [D loss: 0.6894513070583344, acc.: 54.69%] [G loss: 0.7062639594078064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 51/86 [D loss: 0.688424289226532, acc.: 56.59%] [G loss: 0.7069545388221741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 52/86 [D loss: 0.6887882947921753, acc.: 56.30%] [G loss: 0.7087898850440979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 53/86 [D loss: 0.6894545257091522, acc.: 56.10%] [G loss: 0.7052680253982544]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 54/86 [D loss: 0.6892282962799072, acc.: 55.81%] [G loss: 0.7066998481750488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 55/86 [D loss: 0.6896756291389465, acc.: 55.13%] [G loss: 0.7056340575218201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 56/86 [D loss: 0.69073286652565, acc.: 53.56%] [G loss: 0.7063495516777039]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 57/86 [D loss: 0.6896456182003021, acc.: 54.59%] [G loss: 0.7073682546615601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 58/86 [D loss: 0.6897109150886536, acc.: 53.22%] [G loss: 0.7056383490562439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 59/86 [D loss: 0.6907310783863068, acc.: 53.08%] [G loss: 0.7072737812995911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 60/86 [D loss: 0.6891307830810547, acc.: 55.66%] [G loss: 0.7072432041168213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 61/86 [D loss: 0.6890181601047516, acc.: 55.22%] [G loss: 0.7058509588241577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 62/86 [D loss: 0.6873279213905334, acc.: 58.98%] [G loss: 0.7069756388664246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 63/86 [D loss: 0.6888217329978943, acc.: 55.91%] [G loss: 0.7079050540924072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 64/86 [D loss: 0.6897989511489868, acc.: 54.05%] [G loss: 0.7060440182685852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 65/86 [D loss: 0.6887049973011017, acc.: 55.81%] [G loss: 0.7070062756538391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 66/86 [D loss: 0.6886913180351257, acc.: 55.42%] [G loss: 0.7048511505126953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 67/86 [D loss: 0.6902804672718048, acc.: 53.12%] [G loss: 0.7051819562911987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 68/86 [D loss: 0.6891999542713165, acc.: 54.44%] [G loss: 0.705923318862915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 69/86 [D loss: 0.6894253492355347, acc.: 54.88%] [G loss: 0.7061397433280945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 70/86 [D loss: 0.6887407898902893, acc.: 55.96%] [G loss: 0.7059035897254944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 71/86 [D loss: 0.6886405050754547, acc.: 56.20%] [G loss: 0.7072864770889282]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 72/86 [D loss: 0.6893962919712067, acc.: 54.05%] [G loss: 0.7061042189598083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 73/86 [D loss: 0.6888780295848846, acc.: 56.64%] [G loss: 0.7086045742034912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 74/86 [D loss: 0.6888632476329803, acc.: 54.39%] [G loss: 0.7052142024040222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 75/86 [D loss: 0.6883096098899841, acc.: 55.66%] [G loss: 0.7054110169410706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 76/86 [D loss: 0.6886277198791504, acc.: 55.13%] [G loss: 0.7065441608428955]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 77/86 [D loss: 0.6896209716796875, acc.: 53.91%] [G loss: 0.7062250971794128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 78/86 [D loss: 0.6901200413703918, acc.: 54.15%] [G loss: 0.7074465751647949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 79/86 [D loss: 0.6897337138652802, acc.: 54.74%] [G loss: 0.7059424519538879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 80/86 [D loss: 0.6896994709968567, acc.: 54.54%] [G loss: 0.7071367502212524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 81/86 [D loss: 0.6901214420795441, acc.: 53.61%] [G loss: 0.706621527671814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 82/86 [D loss: 0.688913106918335, acc.: 53.81%] [G loss: 0.7051932215690613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 83/86 [D loss: 0.6900905668735504, acc.: 53.71%] [G loss: 0.708450436592102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 84/86 [D loss: 0.6893320083618164, acc.: 55.03%] [G loss: 0.7057818174362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 85/86 [D loss: 0.690421849489212, acc.: 55.66%] [G loss: 0.7052197456359863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 86/86 [D loss: 0.6888919770717621, acc.: 56.35%] [G loss: 0.705340564250946]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 1/86 [D loss: 0.6905208826065063, acc.: 53.76%] [G loss: 0.7047368884086609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 2/86 [D loss: 0.6904962360858917, acc.: 53.08%] [G loss: 0.7055274248123169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 3/86 [D loss: 0.6891126930713654, acc.: 55.86%] [G loss: 0.7054343819618225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 4/86 [D loss: 0.6904863119125366, acc.: 54.88%] [G loss: 0.7061563730239868]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 5/86 [D loss: 0.6887214183807373, acc.: 56.10%] [G loss: 0.7066168189048767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 6/86 [D loss: 0.6878657937049866, acc.: 57.67%] [G loss: 0.7035024166107178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 7/86 [D loss: 0.6895262002944946, acc.: 54.49%] [G loss: 0.7062892317771912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 8/86 [D loss: 0.6883189082145691, acc.: 56.05%] [G loss: 0.7046874165534973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 9/86 [D loss: 0.6907615661621094, acc.: 52.73%] [G loss: 0.7045837640762329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 10/86 [D loss: 0.6884885430335999, acc.: 55.76%] [G loss: 0.7066109776496887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 11/86 [D loss: 0.6905996203422546, acc.: 53.08%] [G loss: 0.7055383324623108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 12/86 [D loss: 0.6890272498130798, acc.: 53.71%] [G loss: 0.7061693072319031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 13/86 [D loss: 0.6883822083473206, acc.: 56.45%] [G loss: 0.7039166688919067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 14/86 [D loss: 0.6914009749889374, acc.: 53.17%] [G loss: 0.705233097076416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 15/86 [D loss: 0.6889001429080963, acc.: 55.66%] [G loss: 0.7060602903366089]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 16/86 [D loss: 0.6899612247943878, acc.: 52.78%] [G loss: 0.7008457779884338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 17/86 [D loss: 0.6907986104488373, acc.: 53.12%] [G loss: 0.7093726396560669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 18/86 [D loss: 0.6877705454826355, acc.: 56.98%] [G loss: 0.7019224166870117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 19/86 [D loss: 0.6939573287963867, acc.: 49.56%] [G loss: 0.7026112079620361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 20/86 [D loss: 0.68769970536232, acc.: 56.93%] [G loss: 0.7069069147109985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 21/86 [D loss: 0.6928799152374268, acc.: 49.07%] [G loss: 0.6989012956619263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 22/86 [D loss: 0.6901949346065521, acc.: 52.69%] [G loss: 0.7075282335281372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 23/86 [D loss: 0.6872866451740265, acc.: 57.23%] [G loss: 0.7022064924240112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 24/86 [D loss: 0.6928886771202087, acc.: 51.32%] [G loss: 0.6991498470306396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 25/86 [D loss: 0.6905567348003387, acc.: 52.69%] [G loss: 0.708163321018219]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 26/86 [D loss: 0.688959002494812, acc.: 54.83%] [G loss: 0.7012195587158203]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 27/86 [D loss: 0.6911932826042175, acc.: 50.05%] [G loss: 0.7042120099067688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 28/86 [D loss: 0.6879888474941254, acc.: 56.15%] [G loss: 0.7079343795776367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 29/86 [D loss: 0.6893516778945923, acc.: 54.79%] [G loss: 0.7002662420272827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 30/86 [D loss: 0.6913464367389679, acc.: 52.83%] [G loss: 0.7069747447967529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 31/86 [D loss: 0.688263326883316, acc.: 57.47%] [G loss: 0.7015337944030762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 32/86 [D loss: 0.6909327805042267, acc.: 53.37%] [G loss: 0.7032063007354736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 33/86 [D loss: 0.6912056803703308, acc.: 52.98%] [G loss: 0.7070577144622803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 34/86 [D loss: 0.6885337829589844, acc.: 56.69%] [G loss: 0.7021187543869019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 35/86 [D loss: 0.692091703414917, acc.: 52.10%] [G loss: 0.7026006579399109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 36/86 [D loss: 0.6872694492340088, acc.: 58.35%] [G loss: 0.7068716287612915]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 37/86 [D loss: 0.690680056810379, acc.: 52.83%] [G loss: 0.7025043368339539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 38/86 [D loss: 0.6904726922512054, acc.: 52.78%] [G loss: 0.7060185670852661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 39/86 [D loss: 0.6885285973548889, acc.: 54.44%] [G loss: 0.7062731981277466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 40/86 [D loss: 0.6915552914142609, acc.: 52.34%] [G loss: 0.7028562426567078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 41/86 [D loss: 0.6900037527084351, acc.: 54.88%] [G loss: 0.7085222601890564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 42/86 [D loss: 0.6888969838619232, acc.: 54.93%] [G loss: 0.7045798897743225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 43/86 [D loss: 0.6901495158672333, acc.: 54.64%] [G loss: 0.7062167525291443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 44/86 [D loss: 0.6877197325229645, acc.: 58.06%] [G loss: 0.7075084447860718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 45/86 [D loss: 0.6896656155586243, acc.: 55.32%] [G loss: 0.7042772769927979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 46/86 [D loss: 0.6923747360706329, acc.: 51.27%] [G loss: 0.7064117193222046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 47/86 [D loss: 0.6876984238624573, acc.: 57.47%] [G loss: 0.7063231468200684]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 48/86 [D loss: 0.6907365620136261, acc.: 53.32%] [G loss: 0.7061097621917725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 49/86 [D loss: 0.6896907389163971, acc.: 54.59%] [G loss: 0.7079795598983765]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 50/86 [D loss: 0.6890051960945129, acc.: 54.83%] [G loss: 0.7045484781265259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 51/86 [D loss: 0.6905378699302673, acc.: 53.47%] [G loss: 0.7059330940246582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 52/86 [D loss: 0.6891433894634247, acc.: 54.79%] [G loss: 0.70637047290802]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 53/86 [D loss: 0.6891328692436218, acc.: 55.42%] [G loss: 0.705922544002533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 54/86 [D loss: 0.6895385980606079, acc.: 55.37%] [G loss: 0.7078476548194885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 55/86 [D loss: 0.6874657869338989, acc.: 57.03%] [G loss: 0.7057231068611145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 56/86 [D loss: 0.6915540993213654, acc.: 52.44%] [G loss: 0.704743504524231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 57/86 [D loss: 0.6874026656150818, acc.: 56.10%] [G loss: 0.7073590755462646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 58/86 [D loss: 0.6880160868167877, acc.: 56.15%] [G loss: 0.7054910063743591]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 59/86 [D loss: 0.6899329125881195, acc.: 53.37%] [G loss: 0.7078012228012085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 60/86 [D loss: 0.6894207894802094, acc.: 53.91%] [G loss: 0.7040742635726929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 61/86 [D loss: 0.6899600028991699, acc.: 53.47%] [G loss: 0.7049856781959534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 62/86 [D loss: 0.689767450094223, acc.: 55.13%] [G loss: 0.7078286409378052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 63/86 [D loss: 0.689111202955246, acc.: 55.47%] [G loss: 0.706771731376648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 64/86 [D loss: 0.689661830663681, acc.: 54.30%] [G loss: 0.7076035737991333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 65/86 [D loss: 0.6888567209243774, acc.: 55.57%] [G loss: 0.7064697742462158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 66/86 [D loss: 0.6889293491840363, acc.: 55.81%] [G loss: 0.7053797841072083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 67/86 [D loss: 0.6905112862586975, acc.: 53.08%] [G loss: 0.7041804194450378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 68/86 [D loss: 0.6888096630573273, acc.: 54.20%] [G loss: 0.70584636926651]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 69/86 [D loss: 0.6902749836444855, acc.: 53.71%] [G loss: 0.7045989632606506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 70/86 [D loss: 0.6902722418308258, acc.: 54.20%] [G loss: 0.7093424797058105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 71/86 [D loss: 0.6896184384822845, acc.: 54.10%] [G loss: 0.7063764333724976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 72/86 [D loss: 0.6882665455341339, acc.: 56.05%] [G loss: 0.7073399424552917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 73/86 [D loss: 0.6886672079563141, acc.: 54.30%] [G loss: 0.7069377899169922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 74/86 [D loss: 0.6901313662528992, acc.: 54.54%] [G loss: 0.7087286710739136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 75/86 [D loss: 0.6890529096126556, acc.: 54.10%] [G loss: 0.7071834206581116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 76/86 [D loss: 0.6892503499984741, acc.: 54.79%] [G loss: 0.7062952518463135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 77/86 [D loss: 0.6891913414001465, acc.: 54.54%] [G loss: 0.7073366641998291]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 78/86 [D loss: 0.6877550780773163, acc.: 56.59%] [G loss: 0.7082772850990295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 79/86 [D loss: 0.6884020864963531, acc.: 56.01%] [G loss: 0.7088627219200134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 80/86 [D loss: 0.6890124380588531, acc.: 54.93%] [G loss: 0.7083810567855835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 81/86 [D loss: 0.6885080337524414, acc.: 55.22%] [G loss: 0.7086865901947021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 82/86 [D loss: 0.6884655058383942, acc.: 55.37%] [G loss: 0.7070286273956299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 83/86 [D loss: 0.6883071660995483, acc.: 56.45%] [G loss: 0.7072668671607971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 84/86 [D loss: 0.6885707974433899, acc.: 55.37%] [G loss: 0.707353413105011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 85/86 [D loss: 0.6888010799884796, acc.: 55.81%] [G loss: 0.7061475515365601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 86/86 [D loss: 0.6887712776660919, acc.: 54.44%] [G loss: 0.7083436250686646]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 1/86 [D loss: 0.688631534576416, acc.: 55.18%] [G loss: 0.7055874466896057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 2/86 [D loss: 0.6902214288711548, acc.: 53.27%] [G loss: 0.7051981091499329]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 3/86 [D loss: 0.6891845464706421, acc.: 56.10%] [G loss: 0.7058378458023071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 4/86 [D loss: 0.6880480349063873, acc.: 56.20%] [G loss: 0.7074195146560669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 5/86 [D loss: 0.6900455057621002, acc.: 53.03%] [G loss: 0.7082310318946838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 6/86 [D loss: 0.6895864307880402, acc.: 54.35%] [G loss: 0.7077158689498901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 7/86 [D loss: 0.6884406208992004, acc.: 55.57%] [G loss: 0.7072131037712097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 8/86 [D loss: 0.6888065934181213, acc.: 56.49%] [G loss: 0.7061939835548401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 9/86 [D loss: 0.6889995038509369, acc.: 54.59%] [G loss: 0.7073752284049988]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 10/86 [D loss: 0.6888827979564667, acc.: 55.91%] [G loss: 0.7072899341583252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 11/86 [D loss: 0.6889385282993317, acc.: 55.18%] [G loss: 0.7079846858978271]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 12/86 [D loss: 0.6891022622585297, acc.: 55.91%] [G loss: 0.7066911458969116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 13/86 [D loss: 0.6891957521438599, acc.: 54.98%] [G loss: 0.706964373588562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 14/86 [D loss: 0.6893454790115356, acc.: 55.37%] [G loss: 0.7081683278083801]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 15/86 [D loss: 0.6878759860992432, acc.: 57.13%] [G loss: 0.7080602645874023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 16/86 [D loss: 0.6893333196640015, acc.: 54.39%] [G loss: 0.7063193917274475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 17/86 [D loss: 0.6890665292739868, acc.: 53.76%] [G loss: 0.7074064612388611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 18/86 [D loss: 0.6875192821025848, acc.: 56.93%] [G loss: 0.7069123983383179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 19/86 [D loss: 0.6891576051712036, acc.: 55.13%] [G loss: 0.7085074186325073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 20/86 [D loss: 0.6879690885543823, acc.: 56.10%] [G loss: 0.7081005573272705]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 21/86 [D loss: 0.6888027489185333, acc.: 54.69%] [G loss: 0.7067863941192627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 22/86 [D loss: 0.689497709274292, acc.: 54.35%] [G loss: 0.7053141593933105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 23/86 [D loss: 0.689416378736496, acc.: 54.69%] [G loss: 0.7063116431236267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 24/86 [D loss: 0.6901419162750244, acc.: 54.39%] [G loss: 0.7060778141021729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 25/86 [D loss: 0.6878646910190582, acc.: 56.05%] [G loss: 0.7089025378227234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 26/86 [D loss: 0.6898245811462402, acc.: 52.69%] [G loss: 0.705191969871521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 27/86 [D loss: 0.6889165639877319, acc.: 55.71%] [G loss: 0.7064580917358398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 28/86 [D loss: 0.6884649991989136, acc.: 55.62%] [G loss: 0.7073414325714111]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 29/86 [D loss: 0.6889228522777557, acc.: 54.10%] [G loss: 0.7059723138809204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 30/86 [D loss: 0.6877961456775665, acc.: 55.91%] [G loss: 0.7074723243713379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 31/86 [D loss: 0.6896201372146606, acc.: 53.71%] [G loss: 0.7043159604072571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 32/86 [D loss: 0.690038800239563, acc.: 53.96%] [G loss: 0.7078278064727783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 33/86 [D loss: 0.6884098052978516, acc.: 55.32%] [G loss: 0.7036567330360413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 34/86 [D loss: 0.6898244619369507, acc.: 53.27%] [G loss: 0.7060881853103638]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 35/86 [D loss: 0.687256783246994, acc.: 56.01%] [G loss: 0.7075100541114807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 36/86 [D loss: 0.6903657019138336, acc.: 54.00%] [G loss: 0.7042427659034729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 37/86 [D loss: 0.6888217329978943, acc.: 55.22%] [G loss: 0.7081894278526306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 38/86 [D loss: 0.6885789632797241, acc.: 54.35%] [G loss: 0.7030838131904602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 39/86 [D loss: 0.6916393339633942, acc.: 50.05%] [G loss: 0.7075954675674438]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 40/86 [D loss: 0.6871404945850372, acc.: 57.52%] [G loss: 0.7093884944915771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 41/86 [D loss: 0.6914150714874268, acc.: 51.90%] [G loss: 0.7028715014457703]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 42/86 [D loss: 0.6905599534511566, acc.: 52.39%] [G loss: 0.7081998586654663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 43/86 [D loss: 0.6900043487548828, acc.: 53.56%] [G loss: 0.7018265128135681]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 44/86 [D loss: 0.6913602948188782, acc.: 51.95%] [G loss: 0.7054990530014038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 45/86 [D loss: 0.686034083366394, acc.: 58.06%] [G loss: 0.7027338743209839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 46/86 [D loss: 0.6945070326328278, acc.: 49.46%] [G loss: 0.7022466063499451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 47/86 [D loss: 0.6855629980564117, acc.: 60.25%] [G loss: 0.7053343057632446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 48/86 [D loss: 0.6911178529262543, acc.: 52.64%] [G loss: 0.7016902565956116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 49/86 [D loss: 0.690286785364151, acc.: 52.88%] [G loss: 0.7065845131874084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 50/86 [D loss: 0.6862732768058777, acc.: 58.15%] [G loss: 0.7018188238143921]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 51/86 [D loss: 0.6945381760597229, acc.: 48.29%] [G loss: 0.7032508850097656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 52/86 [D loss: 0.6867800354957581, acc.: 58.15%] [G loss: 0.7077955603599548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 53/86 [D loss: 0.6916510164737701, acc.: 50.63%] [G loss: 0.6994773149490356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 54/86 [D loss: 0.6907338798046112, acc.: 53.17%] [G loss: 0.7067387700080872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 55/86 [D loss: 0.687755286693573, acc.: 54.74%] [G loss: 0.7024601697921753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 56/86 [D loss: 0.6922364234924316, acc.: 51.95%] [G loss: 0.7011910080909729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 57/86 [D loss: 0.69062939286232, acc.: 52.83%] [G loss: 0.709572970867157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 58/86 [D loss: 0.688044011592865, acc.: 55.96%] [G loss: 0.7018739581108093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 59/86 [D loss: 0.691333144903183, acc.: 51.95%] [G loss: 0.7042794227600098]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 60/86 [D loss: 0.6883790791034698, acc.: 55.27%] [G loss: 0.7068708539009094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 61/86 [D loss: 0.6888786852359772, acc.: 54.30%] [G loss: 0.7026705145835876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 62/86 [D loss: 0.6931868195533752, acc.: 49.12%] [G loss: 0.7070038318634033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 63/86 [D loss: 0.6875267326831818, acc.: 56.79%] [G loss: 0.7067850232124329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 64/86 [D loss: 0.6898652911186218, acc.: 53.66%] [G loss: 0.7061065435409546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 65/86 [D loss: 0.689421683549881, acc.: 54.44%] [G loss: 0.7083369493484497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 66/86 [D loss: 0.689315676689148, acc.: 54.88%] [G loss: 0.7042514085769653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 67/86 [D loss: 0.6916947364807129, acc.: 53.71%] [G loss: 0.7033867239952087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 68/86 [D loss: 0.6887731552124023, acc.: 56.25%] [G loss: 0.7070159912109375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 69/86 [D loss: 0.6878972947597504, acc.: 56.93%] [G loss: 0.705075204372406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 70/86 [D loss: 0.6904513835906982, acc.: 53.76%] [G loss: 0.707658052444458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 71/86 [D loss: 0.6891435086727142, acc.: 55.91%] [G loss: 0.708583652973175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 72/86 [D loss: 0.6904778778553009, acc.: 52.59%] [G loss: 0.704247772693634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 73/86 [D loss: 0.6905405819416046, acc.: 53.76%] [G loss: 0.7092620730400085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 74/86 [D loss: 0.6869164109230042, acc.: 57.81%] [G loss: 0.7077268958091736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 75/86 [D loss: 0.6905708909034729, acc.: 52.64%] [G loss: 0.7071018218994141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 76/86 [D loss: 0.6883029043674469, acc.: 55.66%] [G loss: 0.708231508731842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 77/86 [D loss: 0.6884517073631287, acc.: 56.88%] [G loss: 0.7061033248901367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 78/86 [D loss: 0.6919365227222443, acc.: 52.98%] [G loss: 0.7068282961845398]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 79/86 [D loss: 0.6877229809761047, acc.: 56.74%] [G loss: 0.7084711194038391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 80/86 [D loss: 0.6886135637760162, acc.: 55.52%] [G loss: 0.7062993049621582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 81/86 [D loss: 0.6888169348239899, acc.: 55.22%] [G loss: 0.708326518535614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 82/86 [D loss: 0.6881431937217712, acc.: 55.91%] [G loss: 0.7052986025810242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 83/86 [D loss: 0.6904306411743164, acc.: 53.22%] [G loss: 0.7074474096298218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 84/86 [D loss: 0.6874878406524658, acc.: 56.64%] [G loss: 0.7104706764221191]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 85/86 [D loss: 0.6886038482189178, acc.: 55.96%] [G loss: 0.7076960802078247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 86/86 [D loss: 0.6893297135829926, acc.: 54.59%] [G loss: 0.7066398859024048]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 1/86 [D loss: 0.6881765723228455, acc.: 55.27%] [G loss: 0.7072103023529053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 2/86 [D loss: 0.6895541250705719, acc.: 54.93%] [G loss: 0.7050325274467468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 3/86 [D loss: 0.6891533732414246, acc.: 54.20%] [G loss: 0.7075484991073608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 4/86 [D loss: 0.6894452571868896, acc.: 54.79%] [G loss: 0.7058795690536499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 5/86 [D loss: 0.6896512806415558, acc.: 53.47%] [G loss: 0.7093443870544434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 6/86 [D loss: 0.6886666417121887, acc.: 55.76%] [G loss: 0.7064090371131897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 7/86 [D loss: 0.6895515024662018, acc.: 54.49%] [G loss: 0.7070459127426147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 8/86 [D loss: 0.6879639327526093, acc.: 56.64%] [G loss: 0.7063548564910889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 9/86 [D loss: 0.688880980014801, acc.: 54.93%] [G loss: 0.7069951295852661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 10/86 [D loss: 0.68802809715271, acc.: 56.01%] [G loss: 0.7055992484092712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 11/86 [D loss: 0.6904285848140717, acc.: 52.10%] [G loss: 0.7079577445983887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 12/86 [D loss: 0.6885586678981781, acc.: 56.30%] [G loss: 0.7074035406112671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 13/86 [D loss: 0.6912292540073395, acc.: 52.15%] [G loss: 0.7054771184921265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 14/86 [D loss: 0.6876660883426666, acc.: 57.32%] [G loss: 0.7089186310768127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 15/86 [D loss: 0.690263032913208, acc.: 53.32%] [G loss: 0.7073709964752197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 16/86 [D loss: 0.6891810595989227, acc.: 54.74%] [G loss: 0.7104219198226929]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 17/86 [D loss: 0.6891417503356934, acc.: 54.15%] [G loss: 0.7083224058151245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 18/86 [D loss: 0.6911837458610535, acc.: 52.05%] [G loss: 0.7082209587097168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 19/86 [D loss: 0.6892495453357697, acc.: 54.49%] [G loss: 0.7073315382003784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 20/86 [D loss: 0.6884272992610931, acc.: 56.10%] [G loss: 0.7072944641113281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 21/86 [D loss: 0.6879048645496368, acc.: 56.54%] [G loss: 0.7087686061859131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 22/86 [D loss: 0.6891556680202484, acc.: 54.25%] [G loss: 0.7046123147010803]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 23/86 [D loss: 0.6883982419967651, acc.: 56.25%] [G loss: 0.7071137428283691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 24/86 [D loss: 0.6883262395858765, acc.: 57.13%] [G loss: 0.7049465775489807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 25/86 [D loss: 0.6899019777774811, acc.: 53.08%] [G loss: 0.705409586429596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 26/86 [D loss: 0.6882733404636383, acc.: 55.52%] [G loss: 0.7083451747894287]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 27/86 [D loss: 0.6889021992683411, acc.: 55.37%] [G loss: 0.7075328826904297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 28/86 [D loss: 0.6903706192970276, acc.: 54.44%] [G loss: 0.7089749574661255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 29/86 [D loss: 0.6895341277122498, acc.: 54.79%] [G loss: 0.7066134214401245]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 30/86 [D loss: 0.6886423826217651, acc.: 54.74%] [G loss: 0.707507848739624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 31/86 [D loss: 0.6888976097106934, acc.: 54.15%] [G loss: 0.7090436816215515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 32/86 [D loss: 0.6902705729007721, acc.: 52.69%] [G loss: 0.7072944045066833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 33/86 [D loss: 0.6898705065250397, acc.: 53.71%] [G loss: 0.7078808546066284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 34/86 [D loss: 0.688702791929245, acc.: 54.74%] [G loss: 0.7079139351844788]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 35/86 [D loss: 0.6882030069828033, acc.: 54.69%] [G loss: 0.7081544399261475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 36/86 [D loss: 0.6881823837757111, acc.: 56.49%] [G loss: 0.7104611396789551]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 37/86 [D loss: 0.6879000961780548, acc.: 56.25%] [G loss: 0.7088003158569336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 38/86 [D loss: 0.6891461312770844, acc.: 53.32%] [G loss: 0.7070564031600952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 39/86 [D loss: 0.6884652972221375, acc.: 55.71%] [G loss: 0.7081700563430786]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 40/86 [D loss: 0.688752144575119, acc.: 55.57%] [G loss: 0.7068624496459961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 41/86 [D loss: 0.6889947354793549, acc.: 55.52%] [G loss: 0.7078241109848022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 42/86 [D loss: 0.687904953956604, acc.: 56.25%] [G loss: 0.7064465284347534]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 43/86 [D loss: 0.6901189982891083, acc.: 53.96%] [G loss: 0.7053696513175964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 44/86 [D loss: 0.6897223591804504, acc.: 53.71%] [G loss: 0.7077465057373047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 45/86 [D loss: 0.689661979675293, acc.: 54.25%] [G loss: 0.7069369554519653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 46/86 [D loss: 0.6896403431892395, acc.: 54.69%] [G loss: 0.708694577217102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 47/86 [D loss: 0.6901257932186127, acc.: 55.08%] [G loss: 0.7054226994514465]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 48/86 [D loss: 0.6903500258922577, acc.: 53.91%] [G loss: 0.7074673771858215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 49/86 [D loss: 0.6884713470935822, acc.: 55.13%] [G loss: 0.7080132365226746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 50/86 [D loss: 0.6900292634963989, acc.: 52.88%] [G loss: 0.707197368144989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 51/86 [D loss: 0.6884033977985382, acc.: 56.15%] [G loss: 0.7085357308387756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 52/86 [D loss: 0.6885711848735809, acc.: 55.57%] [G loss: 0.7048050165176392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 53/86 [D loss: 0.6904712617397308, acc.: 52.69%] [G loss: 0.7084336280822754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 54/86 [D loss: 0.6873882114887238, acc.: 56.54%] [G loss: 0.7040168046951294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 55/86 [D loss: 0.6903701424598694, acc.: 52.83%] [G loss: 0.7056007981300354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 56/86 [D loss: 0.6894274353981018, acc.: 54.74%] [G loss: 0.7093422412872314]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 57/86 [D loss: 0.6882180869579315, acc.: 56.20%] [G loss: 0.7048510909080505]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 58/86 [D loss: 0.691039651632309, acc.: 53.32%] [G loss: 0.7074128985404968]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 59/86 [D loss: 0.686163455247879, acc.: 58.59%] [G loss: 0.7086300849914551]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 60/86 [D loss: 0.6909627616405487, acc.: 52.54%] [G loss: 0.7024779319763184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 61/86 [D loss: 0.6908024251461029, acc.: 52.29%] [G loss: 0.708561897277832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 62/86 [D loss: 0.6882551312446594, acc.: 55.52%] [G loss: 0.7041361331939697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 63/86 [D loss: 0.6928858458995819, acc.: 50.98%] [G loss: 0.706349790096283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 64/86 [D loss: 0.6867481470108032, acc.: 58.40%] [G loss: 0.7077045440673828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 65/86 [D loss: 0.6915861964225769, acc.: 52.59%] [G loss: 0.7013962268829346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 66/86 [D loss: 0.6906361877918243, acc.: 53.03%] [G loss: 0.7072449922561646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 67/86 [D loss: 0.6874295175075531, acc.: 57.52%] [G loss: 0.7026159763336182]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 68/86 [D loss: 0.6957090198993683, acc.: 47.36%] [G loss: 0.7038059830665588]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 69/86 [D loss: 0.6874427795410156, acc.: 57.13%] [G loss: 0.707262396812439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 70/86 [D loss: 0.6933844685554504, acc.: 48.49%] [G loss: 0.7006883025169373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 71/86 [D loss: 0.6916859149932861, acc.: 50.88%] [G loss: 0.7084617614746094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 72/86 [D loss: 0.6858585476875305, acc.: 58.74%] [G loss: 0.7033478021621704]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 73/86 [D loss: 0.6942190825939178, acc.: 48.68%] [G loss: 0.7004990577697754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 74/86 [D loss: 0.6880475282669067, acc.: 56.05%] [G loss: 0.7091138362884521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 75/86 [D loss: 0.6904073655605316, acc.: 52.73%] [G loss: 0.701390266418457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 76/86 [D loss: 0.6915240585803986, acc.: 52.29%] [G loss: 0.7061401605606079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 77/86 [D loss: 0.6878696084022522, acc.: 55.57%] [G loss: 0.7058861255645752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 78/86 [D loss: 0.6901299357414246, acc.: 53.96%] [G loss: 0.6997029781341553]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 79/86 [D loss: 0.6914149522781372, acc.: 51.95%] [G loss: 0.7090511918067932]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 80/86 [D loss: 0.6883427500724792, acc.: 56.05%] [G loss: 0.7057108879089355]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 81/86 [D loss: 0.6898375153541565, acc.: 53.91%] [G loss: 0.7053956985473633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 82/86 [D loss: 0.6875425279140472, acc.: 57.03%] [G loss: 0.7090001106262207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 83/86 [D loss: 0.6893294155597687, acc.: 56.20%] [G loss: 0.7052637338638306]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 84/86 [D loss: 0.6921137571334839, acc.: 50.39%] [G loss: 0.7062664031982422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 85/86 [D loss: 0.6873091757297516, acc.: 57.42%] [G loss: 0.7078669667243958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 86/86 [D loss: 0.6891849637031555, acc.: 55.18%] [G loss: 0.7069684863090515]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 1/86 [D loss: 0.688117504119873, acc.: 56.35%] [G loss: 0.7099000215530396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 2/86 [D loss: 0.6887061297893524, acc.: 55.91%] [G loss: 0.7084313631057739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 3/86 [D loss: 0.6885727643966675, acc.: 54.00%] [G loss: 0.7070228457450867]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 4/86 [D loss: 0.6893820464611053, acc.: 54.59%] [G loss: 0.7098588943481445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 5/86 [D loss: 0.6871819496154785, acc.: 56.74%] [G loss: 0.7084303498268127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 6/86 [D loss: 0.6887699067592621, acc.: 55.52%] [G loss: 0.7059913277626038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 7/86 [D loss: 0.6874728202819824, acc.: 56.20%] [G loss: 0.707697331905365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 8/86 [D loss: 0.6891968250274658, acc.: 54.88%] [G loss: 0.7060719132423401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 9/86 [D loss: 0.6882903277873993, acc.: 55.96%] [G loss: 0.7079458832740784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 10/86 [D loss: 0.6887246072292328, acc.: 53.71%] [G loss: 0.7077741622924805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 11/86 [D loss: 0.6882103085517883, acc.: 54.74%] [G loss: 0.7087465524673462]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 12/86 [D loss: 0.6883275508880615, acc.: 56.88%] [G loss: 0.7106501460075378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 13/86 [D loss: 0.687995046377182, acc.: 55.08%] [G loss: 0.7080302834510803]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 14/86 [D loss: 0.6900363266468048, acc.: 54.88%] [G loss: 0.7092010378837585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 15/86 [D loss: 0.6876622438430786, acc.: 55.18%] [G loss: 0.7079833149909973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 16/86 [D loss: 0.6881843209266663, acc.: 54.49%] [G loss: 0.7070250511169434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 17/86 [D loss: 0.6885931193828583, acc.: 54.79%] [G loss: 0.7101314663887024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 18/86 [D loss: 0.6884666979312897, acc.: 55.66%] [G loss: 0.7074031233787537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 19/86 [D loss: 0.6902223229408264, acc.: 53.22%] [G loss: 0.7074989080429077]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 20/86 [D loss: 0.6868425607681274, acc.: 57.76%] [G loss: 0.7082928419113159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 21/86 [D loss: 0.6888367235660553, acc.: 54.20%] [G loss: 0.706291139125824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 22/86 [D loss: 0.6880696415901184, acc.: 55.47%] [G loss: 0.7079684734344482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 23/86 [D loss: 0.6892502009868622, acc.: 54.83%] [G loss: 0.7077409625053406]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 24/86 [D loss: 0.6898023784160614, acc.: 53.61%] [G loss: 0.7071256041526794]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 25/86 [D loss: 0.6890427768230438, acc.: 54.35%] [G loss: 0.7087101936340332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 26/86 [D loss: 0.6877085864543915, acc.: 56.93%] [G loss: 0.7068963050842285]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 27/86 [D loss: 0.6876466572284698, acc.: 57.13%] [G loss: 0.7091214060783386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 28/86 [D loss: 0.6899965703487396, acc.: 54.10%] [G loss: 0.706018328666687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 29/86 [D loss: 0.6884596049785614, acc.: 56.05%] [G loss: 0.7080190181732178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 30/86 [D loss: 0.6881102323532104, acc.: 54.93%] [G loss: 0.7057713866233826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 31/86 [D loss: 0.6878517270088196, acc.: 57.13%] [G loss: 0.7040219306945801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 32/86 [D loss: 0.6890737116336823, acc.: 54.54%] [G loss: 0.7071576714515686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 33/86 [D loss: 0.6879494786262512, acc.: 55.66%] [G loss: 0.7076235413551331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 34/86 [D loss: 0.6901364028453827, acc.: 53.91%] [G loss: 0.7081433534622192]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 35/86 [D loss: 0.6898268163204193, acc.: 53.32%] [G loss: 0.7089714407920837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 36/86 [D loss: 0.6890614926815033, acc.: 54.64%] [G loss: 0.7087714672088623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 37/86 [D loss: 0.6878065168857574, acc.: 55.18%] [G loss: 0.7091413736343384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 38/86 [D loss: 0.6893333494663239, acc.: 55.08%] [G loss: 0.7076630592346191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 39/86 [D loss: 0.6876765489578247, acc.: 56.98%] [G loss: 0.7071928977966309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 40/86 [D loss: 0.688275009393692, acc.: 56.59%] [G loss: 0.7061942219734192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 41/86 [D loss: 0.6887962818145752, acc.: 55.81%] [G loss: 0.7051138877868652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 42/86 [D loss: 0.6878011226654053, acc.: 58.06%] [G loss: 0.7107561230659485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 43/86 [D loss: 0.6889304518699646, acc.: 54.64%] [G loss: 0.7077040672302246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 44/86 [D loss: 0.6879639029502869, acc.: 55.76%] [G loss: 0.7066452503204346]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 74/200, Batch 45/86 [D loss: 0.6879845261573792, acc.: 54.98%] [G loss: 0.7100809216499329]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 46/86 [D loss: 0.6891330480575562, acc.: 54.69%] [G loss: 0.708310604095459]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 47/86 [D loss: 0.6889722347259521, acc.: 53.61%] [G loss: 0.7092272639274597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 48/86 [D loss: 0.6886433959007263, acc.: 55.81%] [G loss: 0.7088418006896973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 49/86 [D loss: 0.687345027923584, acc.: 56.64%] [G loss: 0.7062361836433411]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 50/86 [D loss: 0.6895468533039093, acc.: 54.69%] [G loss: 0.7076407670974731]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 51/86 [D loss: 0.6870838701725006, acc.: 56.54%] [G loss: 0.7086478471755981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 52/86 [D loss: 0.688850611448288, acc.: 54.35%] [G loss: 0.7078185677528381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 53/86 [D loss: 0.6882829070091248, acc.: 55.32%] [G loss: 0.7087861895561218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 54/86 [D loss: 0.688440352678299, acc.: 56.64%] [G loss: 0.7063525319099426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 55/86 [D loss: 0.6891891360282898, acc.: 54.79%] [G loss: 0.707713782787323]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 56/86 [D loss: 0.6882778406143188, acc.: 55.03%] [G loss: 0.7075750827789307]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 57/86 [D loss: 0.689661979675293, acc.: 53.47%] [G loss: 0.7066051363945007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 58/86 [D loss: 0.688495934009552, acc.: 53.66%] [G loss: 0.7074978351593018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 59/86 [D loss: 0.6896148025989532, acc.: 54.54%] [G loss: 0.7073401212692261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 60/86 [D loss: 0.6890789866447449, acc.: 55.22%] [G loss: 0.7102653980255127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 61/86 [D loss: 0.6883673071861267, acc.: 55.66%] [G loss: 0.7074403762817383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 62/86 [D loss: 0.6888293623924255, acc.: 54.93%] [G loss: 0.7087170481681824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 63/86 [D loss: 0.6891184747219086, acc.: 55.32%] [G loss: 0.7115378379821777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 64/86 [D loss: 0.6868812143802643, acc.: 56.93%] [G loss: 0.7089167237281799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 65/86 [D loss: 0.6887615323066711, acc.: 55.13%] [G loss: 0.7089816331863403]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 66/86 [D loss: 0.6883241236209869, acc.: 56.35%] [G loss: 0.7092838883399963]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 74/200, Batch 67/86 [D loss: 0.6887768507003784, acc.: 54.74%] [G loss: 0.7106091380119324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 68/86 [D loss: 0.6890657842159271, acc.: 54.44%] [G loss: 0.7101014852523804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 69/86 [D loss: 0.689957469701767, acc.: 54.10%] [G loss: 0.7080265283584595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 70/86 [D loss: 0.6888833940029144, acc.: 54.88%] [G loss: 0.7101873159408569]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 71/86 [D loss: 0.688294380903244, acc.: 55.62%] [G loss: 0.7086765170097351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 72/86 [D loss: 0.6888633668422699, acc.: 54.44%] [G loss: 0.7097005248069763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 73/86 [D loss: 0.687872439622879, acc.: 56.69%] [G loss: 0.7090634107589722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 74/86 [D loss: 0.6900525689125061, acc.: 53.37%] [G loss: 0.7087768912315369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 75/86 [D loss: 0.6886406540870667, acc.: 55.08%] [G loss: 0.710407018661499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 76/86 [D loss: 0.6881548464298248, acc.: 54.88%] [G loss: 0.7094488143920898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 77/86 [D loss: 0.6885398626327515, acc.: 55.57%] [G loss: 0.7102838754653931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 78/86 [D loss: 0.6882280111312866, acc.: 55.18%] [G loss: 0.7077394723892212]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 79/86 [D loss: 0.6898544132709503, acc.: 53.47%] [G loss: 0.7077808380126953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 80/86 [D loss: 0.6882712244987488, acc.: 54.44%] [G loss: 0.7100630402565002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 81/86 [D loss: 0.6895955502986908, acc.: 54.20%] [G loss: 0.7083340883255005]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 82/86 [D loss: 0.6890766620635986, acc.: 53.96%] [G loss: 0.7102770805358887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 83/86 [D loss: 0.6883343458175659, acc.: 55.13%] [G loss: 0.7074100375175476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 84/86 [D loss: 0.6908193826675415, acc.: 52.34%] [G loss: 0.7098100185394287]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 85/86 [D loss: 0.6875565052032471, acc.: 55.47%] [G loss: 0.7100471258163452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 86/86 [D loss: 0.6886309683322906, acc.: 54.74%] [G loss: 0.709721565246582]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 1/86 [D loss: 0.6895627379417419, acc.: 53.08%] [G loss: 0.7102336883544922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 2/86 [D loss: 0.686085432767868, acc.: 57.52%] [G loss: 0.7055968642234802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 3/86 [D loss: 0.6912720501422882, acc.: 52.49%] [G loss: 0.7129906415939331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 4/86 [D loss: 0.6875119209289551, acc.: 55.62%] [G loss: 0.7046506404876709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 5/86 [D loss: 0.6924876868724823, acc.: 50.93%] [G loss: 0.7092396020889282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 6/86 [D loss: 0.6868375837802887, acc.: 57.08%] [G loss: 0.7104471921920776]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 7/86 [D loss: 0.6900350153446198, acc.: 51.95%] [G loss: 0.7061630487442017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 8/86 [D loss: 0.6900532841682434, acc.: 54.88%] [G loss: 0.7099337577819824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 9/86 [D loss: 0.687075287103653, acc.: 56.15%] [G loss: 0.7048409581184387]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 10/86 [D loss: 0.6903089582920074, acc.: 53.61%] [G loss: 0.7066715955734253]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 11/86 [D loss: 0.6865440905094147, acc.: 58.20%] [G loss: 0.7086841464042664]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 12/86 [D loss: 0.691038966178894, acc.: 52.98%] [G loss: 0.7045920491218567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 13/86 [D loss: 0.6901877820491791, acc.: 53.42%] [G loss: 0.7093176245689392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 14/86 [D loss: 0.6880666315555573, acc.: 54.98%] [G loss: 0.7046543955802917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 15/86 [D loss: 0.6887378692626953, acc.: 54.25%] [G loss: 0.7085440158843994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 16/86 [D loss: 0.6871083974838257, acc.: 57.03%] [G loss: 0.7062329053878784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 17/86 [D loss: 0.6913052499294281, acc.: 52.25%] [G loss: 0.7049932479858398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 18/86 [D loss: 0.6894335150718689, acc.: 54.15%] [G loss: 0.7100024223327637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 19/86 [D loss: 0.688698947429657, acc.: 53.42%] [G loss: 0.7055632472038269]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 20/86 [D loss: 0.69106724858284, acc.: 53.17%] [G loss: 0.7110699415206909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 21/86 [D loss: 0.6882590353488922, acc.: 55.96%] [G loss: 0.7042960524559021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 22/86 [D loss: 0.6906432807445526, acc.: 52.49%] [G loss: 0.7077934145927429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 23/86 [D loss: 0.6879908442497253, acc.: 55.32%] [G loss: 0.7089105844497681]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 24/86 [D loss: 0.6909653842449188, acc.: 52.25%] [G loss: 0.7064430117607117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 25/86 [D loss: 0.6891436278820038, acc.: 54.44%] [G loss: 0.7080346941947937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 26/86 [D loss: 0.6885693073272705, acc.: 55.37%] [G loss: 0.7078567147254944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 27/86 [D loss: 0.6906851530075073, acc.: 53.66%] [G loss: 0.7088613510131836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 28/86 [D loss: 0.6868962347507477, acc.: 56.69%] [G loss: 0.7092556357383728]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 29/86 [D loss: 0.6898437142372131, acc.: 53.66%] [G loss: 0.7076854109764099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 30/86 [D loss: 0.6887880861759186, acc.: 54.69%] [G loss: 0.7083351612091064]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 31/86 [D loss: 0.6873776614665985, acc.: 56.35%] [G loss: 0.705924391746521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 32/86 [D loss: 0.6905324459075928, acc.: 52.15%] [G loss: 0.7052454352378845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 33/86 [D loss: 0.6882618367671967, acc.: 56.69%] [G loss: 0.709010660648346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 34/86 [D loss: 0.6897806525230408, acc.: 53.96%] [G loss: 0.706653892993927]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 35/86 [D loss: 0.6906918287277222, acc.: 51.27%] [G loss: 0.709356963634491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 36/86 [D loss: 0.6903021037578583, acc.: 53.81%] [G loss: 0.7057960629463196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 37/86 [D loss: 0.6903862953186035, acc.: 53.08%] [G loss: 0.7102478742599487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 38/86 [D loss: 0.6877715587615967, acc.: 55.32%] [G loss: 0.7092124223709106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 39/86 [D loss: 0.6900951564311981, acc.: 53.12%] [G loss: 0.7068572640419006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 40/86 [D loss: 0.6877943873405457, acc.: 55.18%] [G loss: 0.7097906470298767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 41/86 [D loss: 0.6882884800434113, acc.: 55.03%] [G loss: 0.7070815563201904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 42/86 [D loss: 0.6898031532764435, acc.: 54.49%] [G loss: 0.7081456184387207]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 43/86 [D loss: 0.6877122521400452, acc.: 54.88%] [G loss: 0.7070035934448242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 44/86 [D loss: 0.6904327571392059, acc.: 52.49%] [G loss: 0.7083317637443542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 45/86 [D loss: 0.687452107667923, acc.: 56.30%] [G loss: 0.7098048329353333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 46/86 [D loss: 0.6883505582809448, acc.: 54.83%] [G loss: 0.7083108425140381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 47/86 [D loss: 0.6894696056842804, acc.: 55.03%] [G loss: 0.7084953784942627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 48/86 [D loss: 0.687416672706604, acc.: 55.22%] [G loss: 0.7078613042831421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 49/86 [D loss: 0.6887734234333038, acc.: 55.32%] [G loss: 0.7085452079772949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 50/86 [D loss: 0.686605840921402, acc.: 57.32%] [G loss: 0.7091560363769531]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 51/86 [D loss: 0.6888224184513092, acc.: 54.69%] [G loss: 0.707309365272522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 52/86 [D loss: 0.6890392899513245, acc.: 54.83%] [G loss: 0.7090717554092407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 53/86 [D loss: 0.6892822682857513, acc.: 54.59%] [G loss: 0.7112208604812622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 54/86 [D loss: 0.6872524321079254, acc.: 56.84%] [G loss: 0.7094753384590149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 55/86 [D loss: 0.6881862580776215, acc.: 54.64%] [G loss: 0.712834358215332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 56/86 [D loss: 0.6891508400440216, acc.: 54.64%] [G loss: 0.7090324759483337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 57/86 [D loss: 0.6889864802360535, acc.: 54.88%] [G loss: 0.7080914974212646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 58/86 [D loss: 0.6880660653114319, acc.: 56.64%] [G loss: 0.7101333141326904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 59/86 [D loss: 0.6877010762691498, acc.: 54.98%] [G loss: 0.7094352841377258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 60/86 [D loss: 0.6877985596656799, acc.: 56.84%] [G loss: 0.708486795425415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 61/86 [D loss: 0.6883978545665741, acc.: 55.13%] [G loss: 0.7121373414993286]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 62/86 [D loss: 0.6886506080627441, acc.: 54.20%] [G loss: 0.7089459896087646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 63/86 [D loss: 0.6890740990638733, acc.: 54.79%] [G loss: 0.7112873792648315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 64/86 [D loss: 0.6870926320552826, acc.: 57.62%] [G loss: 0.7112404108047485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 65/86 [D loss: 0.6898813247680664, acc.: 54.00%] [G loss: 0.7097988724708557]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 66/86 [D loss: 0.688079297542572, acc.: 55.13%] [G loss: 0.711122989654541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 67/86 [D loss: 0.6895710825920105, acc.: 53.42%] [G loss: 0.7080710530281067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 68/86 [D loss: 0.6890074908733368, acc.: 54.49%] [G loss: 0.7108414173126221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 69/86 [D loss: 0.6895267963409424, acc.: 54.83%] [G loss: 0.7085446715354919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 70/86 [D loss: 0.690776139497757, acc.: 51.86%] [G loss: 0.7105022668838501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 71/86 [D loss: 0.6888238191604614, acc.: 56.20%] [G loss: 0.7103351354598999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 72/86 [D loss: 0.689246416091919, acc.: 52.83%] [G loss: 0.7087777853012085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 73/86 [D loss: 0.6871142983436584, acc.: 56.35%] [G loss: 0.7105345129966736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 74/86 [D loss: 0.6875724792480469, acc.: 56.79%] [G loss: 0.708172082901001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 75/86 [D loss: 0.6887940466403961, acc.: 54.35%] [G loss: 0.7084892392158508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 76/86 [D loss: 0.6891151070594788, acc.: 55.52%] [G loss: 0.7099134922027588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 77/86 [D loss: 0.6882829070091248, acc.: 55.47%] [G loss: 0.707665205001831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 78/86 [D loss: 0.6878091394901276, acc.: 56.05%] [G loss: 0.7103275656700134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 79/86 [D loss: 0.6871024370193481, acc.: 56.69%] [G loss: 0.7076542377471924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 80/86 [D loss: 0.6889320909976959, acc.: 55.03%] [G loss: 0.710117757320404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 81/86 [D loss: 0.6885877549648285, acc.: 55.13%] [G loss: 0.7089177966117859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 82/86 [D loss: 0.6881609857082367, acc.: 55.42%] [G loss: 0.7099148631095886]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 83/86 [D loss: 0.6875665485858917, acc.: 57.32%] [G loss: 0.7112001180648804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 84/86 [D loss: 0.6879013478755951, acc.: 56.93%] [G loss: 0.711167573928833]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 85/86 [D loss: 0.6877301037311554, acc.: 55.08%] [G loss: 0.7104755640029907]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 86/86 [D loss: 0.6872332692146301, acc.: 56.74%] [G loss: 0.708631157875061]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 1/86 [D loss: 0.6901231110095978, acc.: 53.03%] [G loss: 0.711036205291748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 2/86 [D loss: 0.6878871321678162, acc.: 56.30%] [G loss: 0.7078379988670349]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 3/86 [D loss: 0.6899703145027161, acc.: 52.59%] [G loss: 0.70875483751297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 4/86 [D loss: 0.6895765364170074, acc.: 54.15%] [G loss: 0.71100914478302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 5/86 [D loss: 0.6897674202919006, acc.: 53.42%] [G loss: 0.7071720957756042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 6/86 [D loss: 0.6884936392307281, acc.: 55.62%] [G loss: 0.7084356546401978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 7/86 [D loss: 0.6875404715538025, acc.: 56.49%] [G loss: 0.7064802646636963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 8/86 [D loss: 0.6896737217903137, acc.: 54.79%] [G loss: 0.7101724147796631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 9/86 [D loss: 0.6891264915466309, acc.: 53.56%] [G loss: 0.7114137411117554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 10/86 [D loss: 0.6878067553043365, acc.: 56.25%] [G loss: 0.7047826051712036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 11/86 [D loss: 0.6887666881084442, acc.: 53.81%] [G loss: 0.7093656063079834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 12/86 [D loss: 0.6895142793655396, acc.: 54.20%] [G loss: 0.7091135382652283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 13/86 [D loss: 0.688557505607605, acc.: 53.12%] [G loss: 0.7100861072540283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 14/86 [D loss: 0.6881172955036163, acc.: 56.88%] [G loss: 0.7103332281112671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 15/86 [D loss: 0.6892658174037933, acc.: 53.66%] [G loss: 0.7088736295700073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 16/86 [D loss: 0.6895458698272705, acc.: 54.49%] [G loss: 0.7089332938194275]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 17/86 [D loss: 0.6895298063755035, acc.: 53.86%] [G loss: 0.7083175778388977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 18/86 [D loss: 0.6866799592971802, acc.: 57.62%] [G loss: 0.7124712467193604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 19/86 [D loss: 0.6885111629962921, acc.: 54.79%] [G loss: 0.7094036340713501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 20/86 [D loss: 0.6889116168022156, acc.: 54.93%] [G loss: 0.7102260589599609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 21/86 [D loss: 0.6887369751930237, acc.: 54.93%] [G loss: 0.7114876508712769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 22/86 [D loss: 0.6884939670562744, acc.: 54.98%] [G loss: 0.7081355452537537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 23/86 [D loss: 0.6889053285121918, acc.: 53.27%] [G loss: 0.7105705738067627]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 24/86 [D loss: 0.6862273514270782, acc.: 57.47%] [G loss: 0.7073363661766052]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 25/86 [D loss: 0.6903351843357086, acc.: 53.76%] [G loss: 0.7121917605400085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 26/86 [D loss: 0.6868135333061218, acc.: 56.74%] [G loss: 0.7112042307853699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 27/86 [D loss: 0.69035804271698, acc.: 52.39%] [G loss: 0.7103134393692017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 28/86 [D loss: 0.6889671087265015, acc.: 53.81%] [G loss: 0.7118062973022461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 29/86 [D loss: 0.6882317066192627, acc.: 56.10%] [G loss: 0.7057965397834778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 30/86 [D loss: 0.6902510523796082, acc.: 52.64%] [G loss: 0.713373601436615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 31/86 [D loss: 0.6877838671207428, acc.: 56.01%] [G loss: 0.7072004675865173]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 32/86 [D loss: 0.6917508244514465, acc.: 52.10%] [G loss: 0.7099438905715942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 33/86 [D loss: 0.68638476729393, acc.: 56.98%] [G loss: 0.7085632085800171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 34/86 [D loss: 0.6918396055698395, acc.: 50.88%] [G loss: 0.7073978781700134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 35/86 [D loss: 0.6861425936222076, acc.: 58.06%] [G loss: 0.7109557390213013]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 36/86 [D loss: 0.6904957294464111, acc.: 53.32%] [G loss: 0.6999320387840271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 37/86 [D loss: 0.6912810802459717, acc.: 51.66%] [G loss: 0.7138356566429138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 38/86 [D loss: 0.6864855587482452, acc.: 57.28%] [G loss: 0.6992597579956055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 39/86 [D loss: 0.695543497800827, acc.: 47.66%] [G loss: 0.7109470367431641]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 40/86 [D loss: 0.6845757067203522, acc.: 59.52%] [G loss: 0.7032313942909241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 41/86 [D loss: 0.6947649419307709, acc.: 47.66%] [G loss: 0.6984899640083313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 42/86 [D loss: 0.690296083688736, acc.: 52.25%] [G loss: 0.7150522470474243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 43/86 [D loss: 0.6847434043884277, acc.: 60.01%] [G loss: 0.7016595602035522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 44/86 [D loss: 0.6978275179862976, acc.: 46.14%] [G loss: 0.7027243375778198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 45/86 [D loss: 0.6861567795276642, acc.: 58.84%] [G loss: 0.7079020738601685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 46/86 [D loss: 0.6947261393070221, acc.: 47.41%] [G loss: 0.6977102756500244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 47/86 [D loss: 0.6908195316791534, acc.: 52.44%] [G loss: 0.7118769884109497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 48/86 [D loss: 0.6860902309417725, acc.: 58.64%] [G loss: 0.7035773992538452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 49/86 [D loss: 0.6951601803302765, acc.: 47.85%] [G loss: 0.6993755102157593]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 50/86 [D loss: 0.6885997653007507, acc.: 55.08%] [G loss: 0.7083398699760437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 51/86 [D loss: 0.6906442642211914, acc.: 52.34%] [G loss: 0.7015128135681152]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 52/86 [D loss: 0.6900413632392883, acc.: 52.34%] [G loss: 0.7083179354667664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 53/86 [D loss: 0.6861717104911804, acc.: 57.57%] [G loss: 0.7046681046485901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 54/86 [D loss: 0.6924295723438263, acc.: 50.44%] [G loss: 0.7017917037010193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 55/86 [D loss: 0.6888058185577393, acc.: 55.22%] [G loss: 0.7101540565490723]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 56/86 [D loss: 0.6883645057678223, acc.: 55.22%] [G loss: 0.704223096370697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 57/86 [D loss: 0.6905593276023865, acc.: 51.56%] [G loss: 0.7083917856216431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 58/86 [D loss: 0.6887770891189575, acc.: 54.20%] [G loss: 0.7114064693450928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 59/86 [D loss: 0.6876530647277832, acc.: 56.15%] [G loss: 0.7041978240013123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 60/86 [D loss: 0.6900848150253296, acc.: 52.88%] [G loss: 0.7070882320404053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 61/86 [D loss: 0.686305046081543, acc.: 58.06%] [G loss: 0.7133660912513733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 62/86 [D loss: 0.6878856420516968, acc.: 56.30%] [G loss: 0.7067672610282898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 63/86 [D loss: 0.6895549595355988, acc.: 54.10%] [G loss: 0.7120073437690735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 64/86 [D loss: 0.6868712604045868, acc.: 55.57%] [G loss: 0.7083515524864197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 65/86 [D loss: 0.6904078722000122, acc.: 53.76%] [G loss: 0.7073979377746582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 66/86 [D loss: 0.6882636547088623, acc.: 55.42%] [G loss: 0.7095888257026672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 67/86 [D loss: 0.6883702278137207, acc.: 54.44%] [G loss: 0.7101547718048096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 68/86 [D loss: 0.6887670457363129, acc.: 54.98%] [G loss: 0.7114548087120056]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 69/86 [D loss: 0.6874972283840179, acc.: 57.28%] [G loss: 0.7083754539489746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 70/86 [D loss: 0.6901195049285889, acc.: 52.98%] [G loss: 0.7085941433906555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 71/86 [D loss: 0.6876834630966187, acc.: 55.42%] [G loss: 0.7117565274238586]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 72/86 [D loss: 0.6888828277587891, acc.: 54.39%] [G loss: 0.7108761072158813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 73/86 [D loss: 0.6883475482463837, acc.: 55.32%] [G loss: 0.7111895680427551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 74/86 [D loss: 0.6885986030101776, acc.: 55.52%] [G loss: 0.7116631269454956]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 75/86 [D loss: 0.6893641352653503, acc.: 54.35%] [G loss: 0.7116802930831909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 76/86 [D loss: 0.6886318624019623, acc.: 55.03%] [G loss: 0.7125160694122314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 77/86 [D loss: 0.6874514818191528, acc.: 56.05%] [G loss: 0.7117898464202881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 78/86 [D loss: 0.6891142129898071, acc.: 53.61%] [G loss: 0.7098559737205505]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 79/86 [D loss: 0.6884709894657135, acc.: 55.71%] [G loss: 0.7113534212112427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 80/86 [D loss: 0.6872022449970245, acc.: 56.64%] [G loss: 0.7110574245452881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 81/86 [D loss: 0.6893119513988495, acc.: 53.91%] [G loss: 0.7126431465148926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 82/86 [D loss: 0.6879554092884064, acc.: 55.52%] [G loss: 0.7105550169944763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 83/86 [D loss: 0.6882611811161041, acc.: 55.22%] [G loss: 0.7123317122459412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 84/86 [D loss: 0.6879632472991943, acc.: 55.52%] [G loss: 0.710590124130249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 85/86 [D loss: 0.6870330274105072, acc.: 56.30%] [G loss: 0.7109596729278564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 86/86 [D loss: 0.6892163455486298, acc.: 54.79%] [G loss: 0.7103126049041748]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 1/86 [D loss: 0.6882431507110596, acc.: 55.96%] [G loss: 0.7123062610626221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 2/86 [D loss: 0.6888167858123779, acc.: 54.05%] [G loss: 0.7097659111022949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 3/86 [D loss: 0.6886900067329407, acc.: 55.37%] [G loss: 0.7111684679985046]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 4/86 [D loss: 0.685969740152359, acc.: 58.20%] [G loss: 0.7125306129455566]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 5/86 [D loss: 0.69019615650177, acc.: 53.17%] [G loss: 0.7075070142745972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 6/86 [D loss: 0.688675045967102, acc.: 55.13%] [G loss: 0.7100714445114136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 7/86 [D loss: 0.689206600189209, acc.: 54.10%] [G loss: 0.7097102403640747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 8/86 [D loss: 0.687974601984024, acc.: 55.18%] [G loss: 0.7095832824707031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 9/86 [D loss: 0.6876022815704346, acc.: 55.42%] [G loss: 0.7109779715538025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 10/86 [D loss: 0.6880261301994324, acc.: 54.49%] [G loss: 0.7113732695579529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 11/86 [D loss: 0.6889540255069733, acc.: 54.69%] [G loss: 0.7104740738868713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 12/86 [D loss: 0.6864768862724304, acc.: 56.49%] [G loss: 0.7094482779502869]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 13/86 [D loss: 0.6886942386627197, acc.: 53.91%] [G loss: 0.7099118232727051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 14/86 [D loss: 0.6877449750900269, acc.: 56.74%] [G loss: 0.7115975618362427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 15/86 [D loss: 0.6885956525802612, acc.: 54.39%] [G loss: 0.7096582651138306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 16/86 [D loss: 0.6897412836551666, acc.: 54.30%] [G loss: 0.7096351981163025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 17/86 [D loss: 0.6875412166118622, acc.: 56.15%] [G loss: 0.7096641659736633]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 18/86 [D loss: 0.6884627044200897, acc.: 54.35%] [G loss: 0.710793673992157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 19/86 [D loss: 0.6856225430965424, acc.: 58.84%] [G loss: 0.7105191349983215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 20/86 [D loss: 0.6874944269657135, acc.: 57.37%] [G loss: 0.7093185186386108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 21/86 [D loss: 0.6880304217338562, acc.: 54.69%] [G loss: 0.7121036648750305]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 22/86 [D loss: 0.6866359412670135, acc.: 57.57%] [G loss: 0.7072715759277344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 23/86 [D loss: 0.6895894110202789, acc.: 54.74%] [G loss: 0.708689272403717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 24/86 [D loss: 0.6863961815834045, acc.: 56.74%] [G loss: 0.7111817002296448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 25/86 [D loss: 0.688274472951889, acc.: 54.83%] [G loss: 0.7081845998764038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 26/86 [D loss: 0.6904947757720947, acc.: 52.15%] [G loss: 0.7128987312316895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 27/86 [D loss: 0.6873635053634644, acc.: 54.83%] [G loss: 0.7068105340003967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 28/86 [D loss: 0.6904698014259338, acc.: 53.22%] [G loss: 0.7066250443458557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 29/86 [D loss: 0.6886266469955444, acc.: 54.83%] [G loss: 0.7101887464523315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 30/86 [D loss: 0.6853297054767609, acc.: 59.28%] [G loss: 0.705085277557373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 31/86 [D loss: 0.6927627921104431, acc.: 50.93%] [G loss: 0.7126675248146057]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 32/86 [D loss: 0.6839515268802643, acc.: 59.38%] [G loss: 0.7084239721298218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 33/86 [D loss: 0.6944931447505951, acc.: 46.88%] [G loss: 0.7033601999282837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 34/86 [D loss: 0.690873384475708, acc.: 51.51%] [G loss: 0.7171456813812256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 35/86 [D loss: 0.6880335509777069, acc.: 55.18%] [G loss: 0.7001845240592957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 36/86 [D loss: 0.6973294317722321, acc.: 45.61%] [G loss: 0.7116114497184753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 37/86 [D loss: 0.6824891567230225, acc.: 61.72%] [G loss: 0.7067924737930298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 38/86 [D loss: 0.6956462562084198, acc.: 46.39%] [G loss: 0.6968450546264648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 39/86 [D loss: 0.6902203857898712, acc.: 52.49%] [G loss: 0.7148970365524292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 40/86 [D loss: 0.6845827996730804, acc.: 59.96%] [G loss: 0.7000439167022705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 41/86 [D loss: 0.6980355381965637, acc.: 45.61%] [G loss: 0.6959050893783569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 42/86 [D loss: 0.6881793141365051, acc.: 54.93%] [G loss: 0.711802065372467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 43/86 [D loss: 0.691007673740387, acc.: 52.00%] [G loss: 0.6953873634338379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 44/86 [D loss: 0.6926375925540924, acc.: 50.73%] [G loss: 0.706322431564331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 45/86 [D loss: 0.6835622489452362, acc.: 59.96%] [G loss: 0.7099071741104126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 46/86 [D loss: 0.6917385756969452, acc.: 51.86%] [G loss: 0.6992212533950806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 47/86 [D loss: 0.6910481750965118, acc.: 52.44%] [G loss: 0.7074950933456421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 48/86 [D loss: 0.6847318410873413, acc.: 59.67%] [G loss: 0.7065265774726868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 49/86 [D loss: 0.6895360052585602, acc.: 53.76%] [G loss: 0.7036845088005066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 50/86 [D loss: 0.6904237568378448, acc.: 53.17%] [G loss: 0.7089164853096008]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 51/86 [D loss: 0.6870200037956238, acc.: 55.37%] [G loss: 0.7086880207061768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 52/86 [D loss: 0.6910040378570557, acc.: 52.73%] [G loss: 0.7079191207885742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 53/86 [D loss: 0.6878025829792023, acc.: 55.27%] [G loss: 0.7100338339805603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 54/86 [D loss: 0.687643438577652, acc.: 55.52%] [G loss: 0.7057159543037415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 55/86 [D loss: 0.6890232861042023, acc.: 55.08%] [G loss: 0.70741868019104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 56/86 [D loss: 0.6875122785568237, acc.: 57.42%] [G loss: 0.7089133262634277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 57/86 [D loss: 0.6885198056697845, acc.: 56.01%] [G loss: 0.7075575590133667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 58/86 [D loss: 0.6886394917964935, acc.: 54.00%] [G loss: 0.7101403474807739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 59/86 [D loss: 0.687980443239212, acc.: 55.57%] [G loss: 0.7114620804786682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 60/86 [D loss: 0.6872907876968384, acc.: 56.54%] [G loss: 0.7094111442565918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 61/86 [D loss: 0.6873148381710052, acc.: 57.18%] [G loss: 0.7105894684791565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 62/86 [D loss: 0.6869796812534332, acc.: 56.35%] [G loss: 0.7117881774902344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 63/86 [D loss: 0.688738614320755, acc.: 53.81%] [G loss: 0.7105826139450073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 64/86 [D loss: 0.6883939802646637, acc.: 55.47%] [G loss: 0.710877001285553]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 65/86 [D loss: 0.6880049109458923, acc.: 54.69%] [G loss: 0.7098692059516907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 66/86 [D loss: 0.6873906552791595, acc.: 55.81%] [G loss: 0.710315465927124]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 67/86 [D loss: 0.687830924987793, acc.: 54.00%] [G loss: 0.7121739983558655]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 68/86 [D loss: 0.6872957944869995, acc.: 55.76%] [G loss: 0.7096848487854004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 69/86 [D loss: 0.6880969703197479, acc.: 55.08%] [G loss: 0.7102715969085693]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 70/86 [D loss: 0.6871775984764099, acc.: 55.91%] [G loss: 0.7110356688499451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 71/86 [D loss: 0.6882602274417877, acc.: 55.66%] [G loss: 0.7097406387329102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 72/86 [D loss: 0.6879412531852722, acc.: 57.03%] [G loss: 0.7104684710502625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 73/86 [D loss: 0.686088889837265, acc.: 57.42%] [G loss: 0.7128785252571106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 74/86 [D loss: 0.6869757771492004, acc.: 55.52%] [G loss: 0.7090516090393066]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 75/86 [D loss: 0.6877594888210297, acc.: 54.39%] [G loss: 0.7125723361968994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 76/86 [D loss: 0.6888707280158997, acc.: 55.27%] [G loss: 0.7108035683631897]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 77/86 [D loss: 0.6893860697746277, acc.: 53.71%] [G loss: 0.7124217748641968]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 78/86 [D loss: 0.687693178653717, acc.: 55.62%] [G loss: 0.7109361886978149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 79/86 [D loss: 0.6872300207614899, acc.: 55.91%] [G loss: 0.7102311849594116]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 80/86 [D loss: 0.6865743696689606, acc.: 56.69%] [G loss: 0.7116715312004089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 81/86 [D loss: 0.6875331997871399, acc.: 56.74%] [G loss: 0.7109992504119873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 82/86 [D loss: 0.6871972978115082, acc.: 56.84%] [G loss: 0.7123737931251526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 83/86 [D loss: 0.6893601715564728, acc.: 53.52%] [G loss: 0.7109848260879517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 84/86 [D loss: 0.6873510181903839, acc.: 56.20%] [G loss: 0.7119792103767395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 85/86 [D loss: 0.6863978803157806, acc.: 56.93%] [G loss: 0.7131572365760803]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 86/86 [D loss: 0.6876244843006134, acc.: 56.59%] [G loss: 0.7097416520118713]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 1/86 [D loss: 0.6876793503761292, acc.: 56.15%] [G loss: 0.7137591242790222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 2/86 [D loss: 0.6872916221618652, acc.: 55.76%] [G loss: 0.7119126319885254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 3/86 [D loss: 0.6867750287055969, acc.: 55.91%] [G loss: 0.7131170034408569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 4/86 [D loss: 0.6876892149448395, acc.: 55.57%] [G loss: 0.714125394821167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 5/86 [D loss: 0.6865290999412537, acc.: 56.59%] [G loss: 0.710817277431488]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 6/86 [D loss: 0.6889973878860474, acc.: 54.20%] [G loss: 0.7108327746391296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 7/86 [D loss: 0.686707615852356, acc.: 56.84%] [G loss: 0.7126303911209106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 8/86 [D loss: 0.688075065612793, acc.: 56.54%] [G loss: 0.711243748664856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 9/86 [D loss: 0.6884920299053192, acc.: 55.32%] [G loss: 0.7110630869865417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 10/86 [D loss: 0.6870939135551453, acc.: 55.42%] [G loss: 0.712026059627533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 11/86 [D loss: 0.6873742938041687, acc.: 55.86%] [G loss: 0.7117030620574951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 12/86 [D loss: 0.6867983937263489, acc.: 55.81%] [G loss: 0.7099367380142212]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 13/86 [D loss: 0.6897984445095062, acc.: 52.29%] [G loss: 0.7081369161605835]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 14/86 [D loss: 0.6878848075866699, acc.: 56.20%] [G loss: 0.7114806771278381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 15/86 [D loss: 0.6855846345424652, acc.: 58.20%] [G loss: 0.709865152835846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 16/86 [D loss: 0.6890011131763458, acc.: 54.74%] [G loss: 0.7091776132583618]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 17/86 [D loss: 0.688724547624588, acc.: 55.76%] [G loss: 0.711005687713623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 18/86 [D loss: 0.6880460381507874, acc.: 56.69%] [G loss: 0.7076587080955505]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 19/86 [D loss: 0.6894842684268951, acc.: 52.98%] [G loss: 0.7122513651847839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 20/86 [D loss: 0.6877621710300446, acc.: 55.37%] [G loss: 0.7115578651428223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 21/86 [D loss: 0.6906335949897766, acc.: 53.56%] [G loss: 0.7085034251213074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 22/86 [D loss: 0.6871337890625, acc.: 56.88%] [G loss: 0.7128309011459351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 23/86 [D loss: 0.6871789395809174, acc.: 55.62%] [G loss: 0.7059271335601807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 24/86 [D loss: 0.69168820977211, acc.: 52.78%] [G loss: 0.7103285789489746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 25/86 [D loss: 0.6861158311367035, acc.: 56.54%] [G loss: 0.7107121348381042]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 26/86 [D loss: 0.6890519857406616, acc.: 54.20%] [G loss: 0.7026150226593018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 27/86 [D loss: 0.6924430727958679, acc.: 50.44%] [G loss: 0.7166565656661987]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 28/86 [D loss: 0.684892863035202, acc.: 58.98%] [G loss: 0.7038789391517639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 29/86 [D loss: 0.6905207633972168, acc.: 50.88%] [G loss: 0.7030288577079773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 30/86 [D loss: 0.6872667968273163, acc.: 56.15%] [G loss: 0.7117561101913452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 31/86 [D loss: 0.6871250867843628, acc.: 56.05%] [G loss: 0.7046180963516235]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 32/86 [D loss: 0.6928264200687408, acc.: 50.68%] [G loss: 0.7105357646942139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 33/86 [D loss: 0.685570478439331, acc.: 59.62%] [G loss: 0.7093408703804016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 34/86 [D loss: 0.691940575838089, acc.: 50.88%] [G loss: 0.7057750225067139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 35/86 [D loss: 0.6878497004508972, acc.: 55.47%] [G loss: 0.7129068970680237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 36/86 [D loss: 0.6858706772327423, acc.: 59.03%] [G loss: 0.7075380086898804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 37/86 [D loss: 0.6924068033695221, acc.: 50.05%] [G loss: 0.7020201683044434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 38/86 [D loss: 0.6878571510314941, acc.: 55.52%] [G loss: 0.7130473852157593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 39/86 [D loss: 0.6879319250583649, acc.: 55.42%] [G loss: 0.7056738138198853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 40/86 [D loss: 0.6905372738838196, acc.: 52.73%] [G loss: 0.7094770073890686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 41/86 [D loss: 0.6857272684574127, acc.: 56.93%] [G loss: 0.7087825536727905]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 42/86 [D loss: 0.6883662641048431, acc.: 55.66%] [G loss: 0.7039797306060791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 43/86 [D loss: 0.6891892552375793, acc.: 52.64%] [G loss: 0.7089595794677734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 44/86 [D loss: 0.6873508393764496, acc.: 55.37%] [G loss: 0.7091896533966064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 45/86 [D loss: 0.6885705292224884, acc.: 54.25%] [G loss: 0.7055224776268005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 46/86 [D loss: 0.6883499920368195, acc.: 54.74%] [G loss: 0.7142122387886047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 47/86 [D loss: 0.6868440806865692, acc.: 56.30%] [G loss: 0.7095990180969238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 48/86 [D loss: 0.6906571388244629, acc.: 51.76%] [G loss: 0.7090866565704346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 49/86 [D loss: 0.6874881982803345, acc.: 56.93%] [G loss: 0.7129996418952942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 50/86 [D loss: 0.6898689866065979, acc.: 52.49%] [G loss: 0.7094020843505859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 51/86 [D loss: 0.6889284253120422, acc.: 53.66%] [G loss: 0.7126432061195374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 52/86 [D loss: 0.6873962879180908, acc.: 56.49%] [G loss: 0.7097476720809937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 53/86 [D loss: 0.689816951751709, acc.: 53.32%] [G loss: 0.7099490761756897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 54/86 [D loss: 0.6883897185325623, acc.: 54.88%] [G loss: 0.7138243317604065]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 55/86 [D loss: 0.6866253018379211, acc.: 56.10%] [G loss: 0.7076468467712402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 56/86 [D loss: 0.6883015632629395, acc.: 54.83%] [G loss: 0.7098936438560486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 57/86 [D loss: 0.6872294247150421, acc.: 56.59%] [G loss: 0.711559534072876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 58/86 [D loss: 0.6892243325710297, acc.: 54.30%] [G loss: 0.7086161971092224]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 59/86 [D loss: 0.6877367496490479, acc.: 56.01%] [G loss: 0.7109045386314392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 60/86 [D loss: 0.6871442794799805, acc.: 55.81%] [G loss: 0.7087587118148804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 61/86 [D loss: 0.688561737537384, acc.: 55.03%] [G loss: 0.710397481918335]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 62/86 [D loss: 0.6875382959842682, acc.: 56.64%] [G loss: 0.7114744186401367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 63/86 [D loss: 0.68797767162323, acc.: 55.22%] [G loss: 0.7113882303237915]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 64/86 [D loss: 0.6887238621711731, acc.: 54.88%] [G loss: 0.7104362845420837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 65/86 [D loss: 0.6871355473995209, acc.: 56.69%] [G loss: 0.7115368247032166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 66/86 [D loss: 0.6868308782577515, acc.: 56.25%] [G loss: 0.71156245470047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 67/86 [D loss: 0.6878926753997803, acc.: 55.42%] [G loss: 0.7111414670944214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 68/86 [D loss: 0.6881733238697052, acc.: 56.35%] [G loss: 0.7111108303070068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 69/86 [D loss: 0.688802570104599, acc.: 54.15%] [G loss: 0.7135592103004456]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 70/86 [D loss: 0.6862338483333588, acc.: 57.37%] [G loss: 0.7106751203536987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 71/86 [D loss: 0.6882340312004089, acc.: 55.96%] [G loss: 0.7101726531982422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 72/86 [D loss: 0.6864712238311768, acc.: 56.64%] [G loss: 0.7122408747673035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 73/86 [D loss: 0.6871528029441833, acc.: 56.40%] [G loss: 0.7115824818611145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 74/86 [D loss: 0.6866115927696228, acc.: 56.69%] [G loss: 0.7126898765563965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 75/86 [D loss: 0.6876927614212036, acc.: 55.91%] [G loss: 0.7114278674125671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 76/86 [D loss: 0.6869506239891052, acc.: 57.52%] [G loss: 0.7126315236091614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 77/86 [D loss: 0.6860105097293854, acc.: 58.69%] [G loss: 0.7115143537521362]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 78/86 [D loss: 0.6886178553104401, acc.: 55.22%] [G loss: 0.713005542755127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 79/86 [D loss: 0.6882336139678955, acc.: 55.08%] [G loss: 0.7131711840629578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 80/86 [D loss: 0.6876140236854553, acc.: 55.76%] [G loss: 0.7131249904632568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 81/86 [D loss: 0.6870504021644592, acc.: 56.74%] [G loss: 0.7102869153022766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 82/86 [D loss: 0.6876219511032104, acc.: 56.35%] [G loss: 0.7116739749908447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 83/86 [D loss: 0.686211347579956, acc.: 58.74%] [G loss: 0.7128049731254578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 84/86 [D loss: 0.6871711313724518, acc.: 54.20%] [G loss: 0.709314227104187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 85/86 [D loss: 0.6883975565433502, acc.: 53.91%] [G loss: 0.7119173407554626]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 86/86 [D loss: 0.6856254637241364, acc.: 57.32%] [G loss: 0.7117033004760742]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 1/86 [D loss: 0.6865487396717072, acc.: 57.57%] [G loss: 0.7099822759628296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 2/86 [D loss: 0.6883569359779358, acc.: 53.91%] [G loss: 0.7095996737480164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 3/86 [D loss: 0.6860935688018799, acc.: 57.37%] [G loss: 0.7100698947906494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 4/86 [D loss: 0.687335342168808, acc.: 56.64%] [G loss: 0.7129164934158325]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 5/86 [D loss: 0.6868496537208557, acc.: 58.25%] [G loss: 0.7128506898880005]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 6/86 [D loss: 0.687044620513916, acc.: 57.08%] [G loss: 0.7101447582244873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 7/86 [D loss: 0.6881852447986603, acc.: 56.54%] [G loss: 0.7110052704811096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 8/86 [D loss: 0.6851690411567688, acc.: 59.03%] [G loss: 0.7115843296051025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 9/86 [D loss: 0.687142014503479, acc.: 55.71%] [G loss: 0.7114886045455933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 10/86 [D loss: 0.6879228055477142, acc.: 56.20%] [G loss: 0.7113502621650696]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 11/86 [D loss: 0.6890636086463928, acc.: 54.54%] [G loss: 0.7101284265518188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 12/86 [D loss: 0.6879863739013672, acc.: 55.62%] [G loss: 0.7134256362915039]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 13/86 [D loss: 0.6887127161026001, acc.: 53.66%] [G loss: 0.7113168239593506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 14/86 [D loss: 0.6891507208347321, acc.: 53.91%] [G loss: 0.7103532552719116]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 15/86 [D loss: 0.6847108602523804, acc.: 59.72%] [G loss: 0.712311863899231]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 16/86 [D loss: 0.6877497136592865, acc.: 55.37%] [G loss: 0.7084552645683289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 17/86 [D loss: 0.6879067718982697, acc.: 54.88%] [G loss: 0.7130908966064453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 18/86 [D loss: 0.6871691942214966, acc.: 56.49%] [G loss: 0.7071895003318787]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 19/86 [D loss: 0.6888183355331421, acc.: 55.52%] [G loss: 0.7106854915618896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 20/86 [D loss: 0.6867154538631439, acc.: 56.93%] [G loss: 0.7069423198699951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 21/86 [D loss: 0.6881348788738251, acc.: 54.05%] [G loss: 0.7084313631057739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 22/86 [D loss: 0.6859090924263, acc.: 57.47%] [G loss: 0.7155841588973999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 23/86 [D loss: 0.6882122457027435, acc.: 53.61%] [G loss: 0.7069538831710815]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 24/86 [D loss: 0.6893835067749023, acc.: 53.22%] [G loss: 0.7151746153831482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 25/86 [D loss: 0.6864016652107239, acc.: 57.23%] [G loss: 0.7077370882034302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 26/86 [D loss: 0.6899047195911407, acc.: 52.69%] [G loss: 0.7100720405578613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 27/86 [D loss: 0.6873228549957275, acc.: 56.10%] [G loss: 0.7110385298728943]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 28/86 [D loss: 0.6872686445713043, acc.: 56.15%] [G loss: 0.7050451636314392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 29/86 [D loss: 0.6895718276500702, acc.: 53.22%] [G loss: 0.7146328091621399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 30/86 [D loss: 0.6854892075061798, acc.: 57.23%] [G loss: 0.7074876427650452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 31/86 [D loss: 0.6900681555271149, acc.: 52.93%] [G loss: 0.7085807919502258]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 32/86 [D loss: 0.6857891380786896, acc.: 56.59%] [G loss: 0.712860107421875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 33/86 [D loss: 0.6884816884994507, acc.: 54.20%] [G loss: 0.7048326730728149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 34/86 [D loss: 0.6903927028179169, acc.: 52.83%] [G loss: 0.7128547430038452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 35/86 [D loss: 0.6856765449047089, acc.: 57.86%] [G loss: 0.7047848105430603]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 36/86 [D loss: 0.6905494332313538, acc.: 52.64%] [G loss: 0.7061049938201904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 37/86 [D loss: 0.6877414584159851, acc.: 56.05%] [G loss: 0.7118590474128723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 38/86 [D loss: 0.6870453357696533, acc.: 56.98%] [G loss: 0.7093342542648315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 39/86 [D loss: 0.6916448473930359, acc.: 53.61%] [G loss: 0.7096428275108337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 40/86 [D loss: 0.6834433674812317, acc.: 59.86%] [G loss: 0.7137949466705322]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 41/86 [D loss: 0.6915623545646667, acc.: 51.66%] [G loss: 0.7054721117019653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 42/86 [D loss: 0.6890912055969238, acc.: 53.66%] [G loss: 0.7117328643798828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 43/86 [D loss: 0.6853793263435364, acc.: 58.15%] [G loss: 0.707237720489502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 44/86 [D loss: 0.6929357051849365, acc.: 51.12%] [G loss: 0.7099495530128479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 45/86 [D loss: 0.6853868961334229, acc.: 58.79%] [G loss: 0.7125605940818787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 46/86 [D loss: 0.692541092634201, acc.: 50.10%] [G loss: 0.7042825222015381]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 47/86 [D loss: 0.6872516572475433, acc.: 55.03%] [G loss: 0.7122453451156616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 48/86 [D loss: 0.6858758628368378, acc.: 56.40%] [G loss: 0.7074701189994812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 49/86 [D loss: 0.6896151602268219, acc.: 53.71%] [G loss: 0.7056583166122437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 50/86 [D loss: 0.6884035766124725, acc.: 54.59%] [G loss: 0.7110572457313538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 51/86 [D loss: 0.6876193583011627, acc.: 55.37%] [G loss: 0.7052428126335144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 52/86 [D loss: 0.6928209960460663, acc.: 50.63%] [G loss: 0.7097593545913696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 53/86 [D loss: 0.6872397661209106, acc.: 55.42%] [G loss: 0.7126319408416748]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 54/86 [D loss: 0.6890781223773956, acc.: 54.25%] [G loss: 0.7052049040794373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 55/86 [D loss: 0.6884205937385559, acc.: 55.81%] [G loss: 0.7108830809593201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 56/86 [D loss: 0.6856210231781006, acc.: 57.86%] [G loss: 0.7091274261474609]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 57/86 [D loss: 0.6878767311573029, acc.: 54.88%] [G loss: 0.7105429172515869]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 58/86 [D loss: 0.6877759993076324, acc.: 55.18%] [G loss: 0.7118853330612183]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 59/86 [D loss: 0.6868118345737457, acc.: 56.49%] [G loss: 0.7104097008705139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 60/86 [D loss: 0.6891835331916809, acc.: 54.39%] [G loss: 0.7114318013191223]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 61/86 [D loss: 0.6865591406822205, acc.: 57.13%] [G loss: 0.7114804983139038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 62/86 [D loss: 0.6876056492328644, acc.: 55.47%] [G loss: 0.7138153314590454]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 63/86 [D loss: 0.6874062716960907, acc.: 56.15%] [G loss: 0.7116937041282654]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 64/86 [D loss: 0.6869407892227173, acc.: 56.40%] [G loss: 0.7108417749404907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 65/86 [D loss: 0.6874609887599945, acc.: 55.47%] [G loss: 0.7132931351661682]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 66/86 [D loss: 0.6876612603664398, acc.: 55.42%] [G loss: 0.7120538949966431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 67/86 [D loss: 0.6855838000774384, acc.: 57.76%] [G loss: 0.7126954197883606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 68/86 [D loss: 0.6871458888053894, acc.: 55.81%] [G loss: 0.713309109210968]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 69/86 [D loss: 0.6865592002868652, acc.: 56.15%] [G loss: 0.7123313546180725]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 70/86 [D loss: 0.6858095228672028, acc.: 58.30%] [G loss: 0.7139098644256592]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 71/86 [D loss: 0.6876441240310669, acc.: 57.23%] [G loss: 0.7114079594612122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 72/86 [D loss: 0.687804251909256, acc.: 55.03%] [G loss: 0.7123773694038391]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 73/86 [D loss: 0.6873605847358704, acc.: 55.37%] [G loss: 0.7117623090744019]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 74/86 [D loss: 0.6853589117527008, acc.: 57.76%] [G loss: 0.713098406791687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 75/86 [D loss: 0.6871778964996338, acc.: 54.98%] [G loss: 0.7129367589950562]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 79/200, Batch 76/86 [D loss: 0.6873321235179901, acc.: 56.01%] [G loss: 0.7131338715553284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 77/86 [D loss: 0.6890024840831757, acc.: 55.57%] [G loss: 0.7117474675178528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 78/86 [D loss: 0.6855268776416779, acc.: 59.42%] [G loss: 0.7123128771781921]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 79/86 [D loss: 0.6870878040790558, acc.: 56.05%] [G loss: 0.7137385606765747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 80/86 [D loss: 0.6862922310829163, acc.: 56.93%] [G loss: 0.7145733833312988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 81/86 [D loss: 0.6861790120601654, acc.: 57.57%] [G loss: 0.7112146019935608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 82/86 [D loss: 0.6878000795841217, acc.: 56.40%] [G loss: 0.7144281268119812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 83/86 [D loss: 0.68654465675354, acc.: 57.76%] [G loss: 0.7123532295227051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 84/86 [D loss: 0.686549037694931, acc.: 55.08%] [G loss: 0.7143075466156006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 85/86 [D loss: 0.6855623126029968, acc.: 58.15%] [G loss: 0.7161511182785034]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 86/86 [D loss: 0.6856635510921478, acc.: 58.50%] [G loss: 0.7096260786056519]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 1/86 [D loss: 0.6884592771530151, acc.: 54.98%] [G loss: 0.7140517234802246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 2/86 [D loss: 0.6860664784908295, acc.: 58.06%] [G loss: 0.7107293605804443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 3/86 [D loss: 0.6870610117912292, acc.: 55.52%] [G loss: 0.7091183662414551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 4/86 [D loss: 0.6872154176235199, acc.: 54.39%] [G loss: 0.7139188051223755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 5/86 [D loss: 0.6883411109447479, acc.: 54.35%] [G loss: 0.7127892971038818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 6/86 [D loss: 0.6884300112724304, acc.: 53.86%] [G loss: 0.7149343490600586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 7/86 [D loss: 0.6858045160770416, acc.: 58.01%] [G loss: 0.7133002877235413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 8/86 [D loss: 0.6879251301288605, acc.: 56.25%] [G loss: 0.7138470411300659]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 9/86 [D loss: 0.686605304479599, acc.: 56.40%] [G loss: 0.711114227771759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 10/86 [D loss: 0.6896259188652039, acc.: 54.00%] [G loss: 0.7172927856445312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 11/86 [D loss: 0.6853547394275665, acc.: 58.20%] [G loss: 0.7138450741767883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 12/86 [D loss: 0.6864465475082397, acc.: 56.88%] [G loss: 0.7117530107498169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 13/86 [D loss: 0.6888046562671661, acc.: 54.69%] [G loss: 0.7118048071861267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 14/86 [D loss: 0.6861766874790192, acc.: 56.45%] [G loss: 0.7128798365592957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 15/86 [D loss: 0.6875505745410919, acc.: 55.13%] [G loss: 0.7154380083084106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 16/86 [D loss: 0.6866995990276337, acc.: 57.18%] [G loss: 0.7137514352798462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 17/86 [D loss: 0.6863301396369934, acc.: 56.74%] [G loss: 0.7137224078178406]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 18/86 [D loss: 0.6882483065128326, acc.: 55.42%] [G loss: 0.7177459001541138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 19/86 [D loss: 0.6870449483394623, acc.: 55.71%] [G loss: 0.7168689370155334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 20/86 [D loss: 0.6867289841175079, acc.: 55.96%] [G loss: 0.712607741355896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 21/86 [D loss: 0.6872895658016205, acc.: 56.15%] [G loss: 0.7126882672309875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 22/86 [D loss: 0.685107558965683, acc.: 59.18%] [G loss: 0.7104904055595398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 23/86 [D loss: 0.686901867389679, acc.: 55.66%] [G loss: 0.7126005291938782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 24/86 [D loss: 0.6876066327095032, acc.: 55.47%] [G loss: 0.7124295830726624]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 25/86 [D loss: 0.6870397627353668, acc.: 55.81%] [G loss: 0.7126943469047546]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 26/86 [D loss: 0.6859310567378998, acc.: 56.64%] [G loss: 0.7086951732635498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 27/86 [D loss: 0.6891032755374908, acc.: 53.66%] [G loss: 0.7125648260116577]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 28/86 [D loss: 0.6847268640995026, acc.: 58.35%] [G loss: 0.7132823467254639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 29/86 [D loss: 0.6866122484207153, acc.: 57.18%] [G loss: 0.7155334949493408]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 30/86 [D loss: 0.6887312531471252, acc.: 54.05%] [G loss: 0.7134820818901062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 31/86 [D loss: 0.6881157457828522, acc.: 55.71%] [G loss: 0.7106902003288269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 32/86 [D loss: 0.6872730553150177, acc.: 56.79%] [G loss: 0.7109704613685608]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 33/86 [D loss: 0.6856509447097778, acc.: 57.03%] [G loss: 0.7130380272865295]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 34/86 [D loss: 0.6875750124454498, acc.: 55.91%] [G loss: 0.7130815982818604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 35/86 [D loss: 0.6882551610469818, acc.: 54.44%] [G loss: 0.7113742828369141]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 36/86 [D loss: 0.6888696551322937, acc.: 55.71%] [G loss: 0.7154068350791931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 37/86 [D loss: 0.6871339380741119, acc.: 56.10%] [G loss: 0.7150288820266724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 38/86 [D loss: 0.6871476769447327, acc.: 56.88%] [G loss: 0.7140727043151855]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 39/86 [D loss: 0.688307136297226, acc.: 55.32%] [G loss: 0.7138851881027222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 40/86 [D loss: 0.6864296495914459, acc.: 57.71%] [G loss: 0.7133370637893677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 41/86 [D loss: 0.6845220923423767, acc.: 58.20%] [G loss: 0.7124823927879333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 42/86 [D loss: 0.6862854063510895, acc.: 55.62%] [G loss: 0.7119218111038208]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 43/86 [D loss: 0.6860058307647705, acc.: 57.52%] [G loss: 0.7138947248458862]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 44/86 [D loss: 0.687211662530899, acc.: 55.86%] [G loss: 0.7088474631309509]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 45/86 [D loss: 0.6862423121929169, acc.: 55.96%] [G loss: 0.7122586369514465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 46/86 [D loss: 0.6872385442256927, acc.: 55.03%] [G loss: 0.7111137509346008]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 47/86 [D loss: 0.6872411966323853, acc.: 55.81%] [G loss: 0.710956871509552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 48/86 [D loss: 0.6861921548843384, acc.: 57.32%] [G loss: 0.7127981185913086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 49/86 [D loss: 0.6875952780246735, acc.: 55.57%] [G loss: 0.7118964195251465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 50/86 [D loss: 0.6872229874134064, acc.: 56.64%] [G loss: 0.7108860611915588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 51/86 [D loss: 0.6872327327728271, acc.: 56.25%] [G loss: 0.7121665477752686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 52/86 [D loss: 0.688120424747467, acc.: 55.42%] [G loss: 0.7120404243469238]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 53/86 [D loss: 0.6894917786121368, acc.: 53.96%] [G loss: 0.712120532989502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 54/86 [D loss: 0.6848479807376862, acc.: 58.30%] [G loss: 0.7114081978797913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 55/86 [D loss: 0.6877390146255493, acc.: 55.71%] [G loss: 0.7149366736412048]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 56/86 [D loss: 0.6876736283302307, acc.: 53.56%] [G loss: 0.7113661766052246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 57/86 [D loss: 0.686838686466217, acc.: 56.88%] [G loss: 0.7135656476020813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 58/86 [D loss: 0.6853593289852142, acc.: 57.03%] [G loss: 0.7135675549507141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 59/86 [D loss: 0.6873190402984619, acc.: 55.76%] [G loss: 0.7103294134140015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 60/86 [D loss: 0.6885837316513062, acc.: 54.39%] [G loss: 0.7110514044761658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 61/86 [D loss: 0.685981273651123, acc.: 56.69%] [G loss: 0.7129421234130859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 62/86 [D loss: 0.6863380074501038, acc.: 57.23%] [G loss: 0.7128219604492188]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 63/86 [D loss: 0.6864053010940552, acc.: 56.64%] [G loss: 0.7112939357757568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 64/86 [D loss: 0.6873715221881866, acc.: 54.83%] [G loss: 0.7140909433364868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 65/86 [D loss: 0.6873224377632141, acc.: 54.69%] [G loss: 0.7123519778251648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 66/86 [D loss: 0.6861439645290375, acc.: 57.57%] [G loss: 0.7113375067710876]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 67/86 [D loss: 0.6871049404144287, acc.: 55.22%] [G loss: 0.7093149423599243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 68/86 [D loss: 0.6872130930423737, acc.: 55.81%] [G loss: 0.7147417068481445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 69/86 [D loss: 0.6876495778560638, acc.: 54.54%] [G loss: 0.7127128839492798]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 70/86 [D loss: 0.6856592893600464, acc.: 57.42%] [G loss: 0.7116618752479553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 71/86 [D loss: 0.687730073928833, acc.: 54.49%] [G loss: 0.7152722477912903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 72/86 [D loss: 0.6862660050392151, acc.: 57.52%] [G loss: 0.7133083343505859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 73/86 [D loss: 0.6868084371089935, acc.: 56.84%] [G loss: 0.7147836685180664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 74/86 [D loss: 0.6869539618492126, acc.: 56.98%] [G loss: 0.7146860361099243]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 75/86 [D loss: 0.6873309314250946, acc.: 57.18%] [G loss: 0.7135288119316101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 76/86 [D loss: 0.6873433887958527, acc.: 56.79%] [G loss: 0.7144401669502258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 77/86 [D loss: 0.6857589781284332, acc.: 57.08%] [G loss: 0.7127649188041687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 78/86 [D loss: 0.687247097492218, acc.: 55.52%] [G loss: 0.7125365734100342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 79/86 [D loss: 0.6871088445186615, acc.: 55.03%] [G loss: 0.7136138677597046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 80/86 [D loss: 0.6877578794956207, acc.: 55.27%] [G loss: 0.7134116888046265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 81/86 [D loss: 0.6865483522415161, acc.: 56.69%] [G loss: 0.7089129090309143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 82/86 [D loss: 0.6876421570777893, acc.: 55.37%] [G loss: 0.7128216028213501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 83/86 [D loss: 0.6869932413101196, acc.: 56.30%] [G loss: 0.7129168510437012]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 84/86 [D loss: 0.6867652833461761, acc.: 56.01%] [G loss: 0.7127418518066406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 85/86 [D loss: 0.6876171827316284, acc.: 55.81%] [G loss: 0.7153561115264893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 86/86 [D loss: 0.6881038546562195, acc.: 55.62%] [G loss: 0.715401828289032]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 1/86 [D loss: 0.6880413889884949, acc.: 53.47%] [G loss: 0.7129942178726196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 2/86 [D loss: 0.68686643242836, acc.: 54.69%] [G loss: 0.7122020721435547]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 3/86 [D loss: 0.685726672410965, acc.: 57.47%] [G loss: 0.713002622127533]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 4/86 [D loss: 0.6898655593395233, acc.: 53.22%] [G loss: 0.7128506898880005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 5/86 [D loss: 0.685835063457489, acc.: 56.79%] [G loss: 0.7107704281806946]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 6/86 [D loss: 0.688505083322525, acc.: 53.96%] [G loss: 0.7104994654655457]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 7/86 [D loss: 0.6874152719974518, acc.: 56.25%] [G loss: 0.7125192284584045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 8/86 [D loss: 0.6862254738807678, acc.: 55.91%] [G loss: 0.7089401483535767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 9/86 [D loss: 0.688659280538559, acc.: 54.00%] [G loss: 0.7140384316444397]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 10/86 [D loss: 0.686530351638794, acc.: 56.59%] [G loss: 0.7107970714569092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 11/86 [D loss: 0.689224362373352, acc.: 53.42%] [G loss: 0.714584469795227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 12/86 [D loss: 0.6876429319381714, acc.: 54.35%] [G loss: 0.7161243557929993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 13/86 [D loss: 0.6876862943172455, acc.: 55.37%] [G loss: 0.7103527188301086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 14/86 [D loss: 0.6871597468852997, acc.: 55.86%] [G loss: 0.7130982875823975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 15/86 [D loss: 0.6867086291313171, acc.: 57.32%] [G loss: 0.7092183232307434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 16/86 [D loss: 0.6903219819068909, acc.: 53.66%] [G loss: 0.711713969707489]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 17/86 [D loss: 0.6852039694786072, acc.: 58.64%] [G loss: 0.7137437462806702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 18/86 [D loss: 0.6895573139190674, acc.: 52.73%] [G loss: 0.7115963101387024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 19/86 [D loss: 0.6899581849575043, acc.: 53.22%] [G loss: 0.7154773473739624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 20/86 [D loss: 0.6874108016490936, acc.: 56.15%] [G loss: 0.707952082157135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 21/86 [D loss: 0.6908833384513855, acc.: 53.08%] [G loss: 0.7152883410453796]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 22/86 [D loss: 0.6850688457489014, acc.: 56.88%] [G loss: 0.7096734642982483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 23/86 [D loss: 0.6906899213790894, acc.: 51.76%] [G loss: 0.7096999287605286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 24/86 [D loss: 0.6885898411273956, acc.: 55.08%] [G loss: 0.7159909605979919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 25/86 [D loss: 0.6880854666233063, acc.: 54.59%] [G loss: 0.7042471170425415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 26/86 [D loss: 0.6904168426990509, acc.: 52.05%] [G loss: 0.7188913822174072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 27/86 [D loss: 0.6865702569484711, acc.: 56.79%] [G loss: 0.706136167049408]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 28/86 [D loss: 0.6913018524646759, acc.: 53.22%] [G loss: 0.7129857540130615]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 29/86 [D loss: 0.6843124330043793, acc.: 57.86%] [G loss: 0.7137322425842285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 30/86 [D loss: 0.6932848691940308, acc.: 50.44%] [G loss: 0.7079316973686218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 31/86 [D loss: 0.6843617558479309, acc.: 57.76%] [G loss: 0.7081348896026611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 32/86 [D loss: 0.6905598342418671, acc.: 52.00%] [G loss: 0.705436646938324]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 33/86 [D loss: 0.6872007846832275, acc.: 55.08%] [G loss: 0.7181157469749451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 34/86 [D loss: 0.6856814026832581, acc.: 56.93%] [G loss: 0.7014033794403076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 35/86 [D loss: 0.6938705742359161, acc.: 51.42%] [G loss: 0.7152044773101807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 36/86 [D loss: 0.6849772930145264, acc.: 57.67%] [G loss: 0.7047879695892334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 37/86 [D loss: 0.6917842626571655, acc.: 52.59%] [G loss: 0.7062126994132996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 38/86 [D loss: 0.6886653900146484, acc.: 54.79%] [G loss: 0.7124979496002197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 39/86 [D loss: 0.6865969896316528, acc.: 55.42%] [G loss: 0.7039417028427124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 40/86 [D loss: 0.6947818994522095, acc.: 48.83%] [G loss: 0.7145336866378784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 41/86 [D loss: 0.6857446432113647, acc.: 57.28%] [G loss: 0.7070801854133606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 42/86 [D loss: 0.6955444514751434, acc.: 47.12%] [G loss: 0.7037703990936279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 43/86 [D loss: 0.6864733099937439, acc.: 56.30%] [G loss: 0.719764769077301]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 44/86 [D loss: 0.6872281134128571, acc.: 55.62%] [G loss: 0.7046394348144531]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 45/86 [D loss: 0.6923825740814209, acc.: 50.00%] [G loss: 0.7119916677474976]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 46/86 [D loss: 0.683989405632019, acc.: 58.89%] [G loss: 0.709395170211792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 47/86 [D loss: 0.6929121613502502, acc.: 50.29%] [G loss: 0.7082517147064209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 48/86 [D loss: 0.6892462968826294, acc.: 54.15%] [G loss: 0.7159862518310547]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 49/86 [D loss: 0.6857922375202179, acc.: 56.40%] [G loss: 0.7074022889137268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 50/86 [D loss: 0.6931764781475067, acc.: 49.66%] [G loss: 0.7113023400306702]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 51/86 [D loss: 0.6854643523693085, acc.: 58.15%] [G loss: 0.7124646902084351]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 52/86 [D loss: 0.6903519928455353, acc.: 53.08%] [G loss: 0.7069377899169922]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 53/86 [D loss: 0.6887896955013275, acc.: 54.25%] [G loss: 0.7108623385429382]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 54/86 [D loss: 0.684314101934433, acc.: 58.69%] [G loss: 0.7086775898933411]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 55/86 [D loss: 0.6911242604255676, acc.: 52.34%] [G loss: 0.7102813124656677]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 56/86 [D loss: 0.685163825750351, acc.: 58.98%] [G loss: 0.712535560131073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 57/86 [D loss: 0.6862350404262543, acc.: 56.01%] [G loss: 0.7098050117492676]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 58/86 [D loss: 0.6883241534233093, acc.: 53.42%] [G loss: 0.7139098644256592]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 59/86 [D loss: 0.6882359087467194, acc.: 54.59%] [G loss: 0.7088476419448853]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 60/86 [D loss: 0.6911942362785339, acc.: 51.86%] [G loss: 0.7140474319458008]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 61/86 [D loss: 0.6861824989318848, acc.: 56.88%] [G loss: 0.7139397859573364]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 62/86 [D loss: 0.6869669258594513, acc.: 56.05%] [G loss: 0.7109142541885376]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 63/86 [D loss: 0.6883503794670105, acc.: 54.59%] [G loss: 0.7131376266479492]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 64/86 [D loss: 0.6867315769195557, acc.: 56.49%] [G loss: 0.7131620645523071]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 65/86 [D loss: 0.6869112253189087, acc.: 54.74%] [G loss: 0.7118159532546997]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 66/86 [D loss: 0.6881098747253418, acc.: 56.20%] [G loss: 0.7124886512756348]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 67/86 [D loss: 0.6865866780281067, acc.: 55.18%] [G loss: 0.7118536233901978]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 68/86 [D loss: 0.6895557940006256, acc.: 53.66%] [G loss: 0.71168053150177]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 69/86 [D loss: 0.6849998533725739, acc.: 56.84%] [G loss: 0.7131187319755554]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 70/86 [D loss: 0.6865613758563995, acc.: 55.47%] [G loss: 0.7141448259353638]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 71/86 [D loss: 0.6875718235969543, acc.: 55.62%] [G loss: 0.7128596901893616]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 72/86 [D loss: 0.6873732209205627, acc.: 56.20%] [G loss: 0.7143983840942383]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 73/86 [D loss: 0.6876209080219269, acc.: 54.44%] [G loss: 0.7125272750854492]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 74/86 [D loss: 0.6866238415241241, acc.: 56.69%] [G loss: 0.713796079158783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 75/86 [D loss: 0.6855194270610809, acc.: 58.01%] [G loss: 0.7106003165245056]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 76/86 [D loss: 0.687819093465805, acc.: 55.27%] [G loss: 0.7150292992591858]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 77/86 [D loss: 0.6873815059661865, acc.: 55.27%] [G loss: 0.7131446599960327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 78/86 [D loss: 0.6878428161144257, acc.: 54.49%] [G loss: 0.7158704400062561]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 79/86 [D loss: 0.6851241290569305, acc.: 58.64%] [G loss: 0.7154755592346191]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 80/86 [D loss: 0.6896860897541046, acc.: 53.47%] [G loss: 0.7141628265380859]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 81/86 [D loss: 0.6881256997585297, acc.: 56.10%] [G loss: 0.7118780016899109]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 82/86 [D loss: 0.6852361559867859, acc.: 58.11%] [G loss: 0.7137399911880493]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 83/86 [D loss: 0.6860233247280121, acc.: 55.76%] [G loss: 0.7145220637321472]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 84/86 [D loss: 0.6860894560813904, acc.: 57.32%] [G loss: 0.7164009213447571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 85/86 [D loss: 0.686407059431076, acc.: 55.32%] [G loss: 0.7150482535362244]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 86/86 [D loss: 0.6858297884464264, acc.: 57.96%] [G loss: 0.7164218425750732]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 1/86 [D loss: 0.6891167163848877, acc.: 53.96%] [G loss: 0.7130414843559265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 2/86 [D loss: 0.6845993399620056, acc.: 57.91%] [G loss: 0.7160972356796265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 3/86 [D loss: 0.6863035261631012, acc.: 56.84%] [G loss: 0.7153651714324951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 4/86 [D loss: 0.6875308752059937, acc.: 54.25%] [G loss: 0.7154648303985596]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 5/86 [D loss: 0.68579962849617, acc.: 56.98%] [G loss: 0.7140102386474609]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 6/86 [D loss: 0.6859429478645325, acc.: 57.57%] [G loss: 0.7131091952323914]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 82/200, Batch 7/86 [D loss: 0.6879340410232544, acc.: 54.25%] [G loss: 0.714864194393158]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 8/86 [D loss: 0.6857459247112274, acc.: 57.08%] [G loss: 0.7130196690559387]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 9/86 [D loss: 0.6875872015953064, acc.: 55.76%] [G loss: 0.7159032225608826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 10/86 [D loss: 0.6852860748767853, acc.: 57.47%] [G loss: 0.7132103443145752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 11/86 [D loss: 0.6867347359657288, acc.: 56.10%] [G loss: 0.7113255262374878]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 12/86 [D loss: 0.6865340769290924, acc.: 55.62%] [G loss: 0.7133197784423828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 13/86 [D loss: 0.6853547096252441, acc.: 57.28%] [G loss: 0.7132046818733215]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 14/86 [D loss: 0.6886670589447021, acc.: 54.74%] [G loss: 0.7119594216346741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 15/86 [D loss: 0.6860552728176117, acc.: 57.03%] [G loss: 0.7127079367637634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 16/86 [D loss: 0.6888974606990814, acc.: 53.61%] [G loss: 0.7111223936080933]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 82/200, Batch 17/86 [D loss: 0.6869342029094696, acc.: 56.45%] [G loss: 0.7158195972442627]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 18/86 [D loss: 0.6846287250518799, acc.: 58.35%] [G loss: 0.7128329873085022]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 19/86 [D loss: 0.6885805130004883, acc.: 54.59%] [G loss: 0.7143977284431458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 20/86 [D loss: 0.6852867901325226, acc.: 57.67%] [G loss: 0.7114921808242798]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 82/200, Batch 21/86 [D loss: 0.6874232292175293, acc.: 55.08%] [G loss: 0.7145569920539856]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 22/86 [D loss: 0.6870382726192474, acc.: 55.42%] [G loss: 0.714470624923706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 23/86 [D loss: 0.6872988641262054, acc.: 54.74%] [G loss: 0.7100679874420166]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 24/86 [D loss: 0.6907475292682648, acc.: 51.86%] [G loss: 0.7168816924095154]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 25/86 [D loss: 0.6850084662437439, acc.: 57.32%] [G loss: 0.7143651247024536]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 26/86 [D loss: 0.6886750757694244, acc.: 54.59%] [G loss: 0.7122958898544312]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 27/86 [D loss: 0.6872642934322357, acc.: 56.30%] [G loss: 0.7181524634361267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 28/86 [D loss: 0.6854229271411896, acc.: 57.91%] [G loss: 0.7100908756256104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 29/86 [D loss: 0.6888907253742218, acc.: 53.81%] [G loss: 0.7166473865509033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 30/86 [D loss: 0.6847887933254242, acc.: 58.69%] [G loss: 0.7115950584411621]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 31/86 [D loss: 0.6886457204818726, acc.: 52.98%] [G loss: 0.713249683380127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 32/86 [D loss: 0.6860719323158264, acc.: 56.49%] [G loss: 0.715526282787323]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 33/86 [D loss: 0.687152236700058, acc.: 55.71%] [G loss: 0.7090703248977661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 34/86 [D loss: 0.6909417510032654, acc.: 52.83%] [G loss: 0.7187831997871399]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 35/86 [D loss: 0.6850467324256897, acc.: 58.25%] [G loss: 0.7086345553398132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 36/86 [D loss: 0.6924022138118744, acc.: 50.68%] [G loss: 0.7106778621673584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 37/86 [D loss: 0.6863681972026825, acc.: 56.74%] [G loss: 0.7198960781097412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 38/86 [D loss: 0.6860890984535217, acc.: 57.52%] [G loss: 0.707882821559906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 39/86 [D loss: 0.693615734577179, acc.: 50.29%] [G loss: 0.7181111574172974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 40/86 [D loss: 0.6828296184539795, acc.: 60.30%] [G loss: 0.7089638113975525]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 41/86 [D loss: 0.6933338642120361, acc.: 48.88%] [G loss: 0.7114038467407227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 42/86 [D loss: 0.688165158033371, acc.: 54.88%] [G loss: 0.7210310697555542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 43/86 [D loss: 0.6859898269176483, acc.: 57.91%] [G loss: 0.7070444226264954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 44/86 [D loss: 0.6932138800621033, acc.: 49.85%] [G loss: 0.7177491784095764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 45/86 [D loss: 0.6830308437347412, acc.: 60.16%] [G loss: 0.7156368494033813]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 46/86 [D loss: 0.6926685273647308, acc.: 50.24%] [G loss: 0.7067770957946777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 47/86 [D loss: 0.6874198317527771, acc.: 54.35%] [G loss: 0.7154759764671326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 48/86 [D loss: 0.6856408715248108, acc.: 57.08%] [G loss: 0.7064810991287231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 49/86 [D loss: 0.693314403295517, acc.: 48.68%] [G loss: 0.7198742628097534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 50/86 [D loss: 0.6839037239551544, acc.: 57.91%] [G loss: 0.7162885069847107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 51/86 [D loss: 0.6929207146167755, acc.: 50.49%] [G loss: 0.705744743347168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 52/86 [D loss: 0.6881736814975739, acc.: 54.30%] [G loss: 0.7162685990333557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 53/86 [D loss: 0.6846485435962677, acc.: 56.54%] [G loss: 0.7081298232078552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 54/86 [D loss: 0.6924043893814087, acc.: 49.95%] [G loss: 0.7085451483726501]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 55/86 [D loss: 0.6851849257946014, acc.: 58.94%] [G loss: 0.7191662192344666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 56/86 [D loss: 0.688644528388977, acc.: 53.56%] [G loss: 0.7056551575660706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 57/86 [D loss: 0.6886903941631317, acc.: 54.74%] [G loss: 0.7147394418716431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 58/86 [D loss: 0.6859422326087952, acc.: 57.57%] [G loss: 0.7153726816177368]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 59/86 [D loss: 0.6898753345012665, acc.: 53.17%] [G loss: 0.708832859992981]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 60/86 [D loss: 0.6886802613735199, acc.: 53.52%] [G loss: 0.7158811092376709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 61/86 [D loss: 0.6861551702022552, acc.: 55.91%] [G loss: 0.7085784077644348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 62/86 [D loss: 0.6897233724594116, acc.: 52.98%] [G loss: 0.71437668800354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 63/86 [D loss: 0.6854150295257568, acc.: 57.42%] [G loss: 0.7134085893630981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 64/86 [D loss: 0.6869125366210938, acc.: 56.69%] [G loss: 0.7096091508865356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 65/86 [D loss: 0.6875541508197784, acc.: 55.96%] [G loss: 0.714689314365387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 66/86 [D loss: 0.686487466096878, acc.: 55.71%] [G loss: 0.7098223567008972]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 67/86 [D loss: 0.6892378628253937, acc.: 53.08%] [G loss: 0.7104689478874207]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 68/86 [D loss: 0.6859485507011414, acc.: 57.96%] [G loss: 0.715918242931366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 69/86 [D loss: 0.6876175701618195, acc.: 53.66%] [G loss: 0.7121039032936096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 70/86 [D loss: 0.6881193220615387, acc.: 54.54%] [G loss: 0.7139706611633301]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 71/86 [D loss: 0.68597811460495, acc.: 55.66%] [G loss: 0.712580144405365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 72/86 [D loss: 0.6870594024658203, acc.: 55.86%] [G loss: 0.7135802507400513]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 73/86 [D loss: 0.6879527866840363, acc.: 54.93%] [G loss: 0.7160465717315674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 74/86 [D loss: 0.6859679222106934, acc.: 56.64%] [G loss: 0.7142491936683655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 75/86 [D loss: 0.6862478256225586, acc.: 56.59%] [G loss: 0.7141133546829224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 76/86 [D loss: 0.6853972375392914, acc.: 57.62%] [G loss: 0.7147544026374817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 77/86 [D loss: 0.6875912249088287, acc.: 55.76%] [G loss: 0.7142837643623352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 78/86 [D loss: 0.6880078911781311, acc.: 54.20%] [G loss: 0.7135100960731506]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 79/86 [D loss: 0.6864160895347595, acc.: 57.18%] [G loss: 0.713405191898346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 80/86 [D loss: 0.6880750358104706, acc.: 54.15%] [G loss: 0.7164783477783203]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 81/86 [D loss: 0.6864281892776489, acc.: 56.25%] [G loss: 0.7157142162322998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 82/86 [D loss: 0.6849384009838104, acc.: 57.13%] [G loss: 0.7127280235290527]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 83/86 [D loss: 0.6885280609130859, acc.: 53.91%] [G loss: 0.7172921299934387]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 84/86 [D loss: 0.6845168173313141, acc.: 57.62%] [G loss: 0.7152551412582397]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 85/86 [D loss: 0.6849897503852844, acc.: 56.20%] [G loss: 0.7150848507881165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 86/86 [D loss: 0.6848697662353516, acc.: 57.71%] [G loss: 0.7173972725868225]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 1/86 [D loss: 0.6876808404922485, acc.: 55.37%] [G loss: 0.7154160141944885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 2/86 [D loss: 0.6849800944328308, acc.: 55.76%] [G loss: 0.7184178233146667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 3/86 [D loss: 0.6862479746341705, acc.: 55.47%] [G loss: 0.7167571783065796]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 4/86 [D loss: 0.6861959099769592, acc.: 55.76%] [G loss: 0.7182791233062744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 5/86 [D loss: 0.6845836937427521, acc.: 55.71%] [G loss: 0.7167145013809204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 6/86 [D loss: 0.6845089793205261, acc.: 57.23%] [G loss: 0.7159228324890137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 7/86 [D loss: 0.6869245767593384, acc.: 56.84%] [G loss: 0.7166696190834045]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 8/86 [D loss: 0.6866792142391205, acc.: 56.79%] [G loss: 0.71639084815979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 9/86 [D loss: 0.6867194473743439, acc.: 57.37%] [G loss: 0.7152528762817383]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 10/86 [D loss: 0.6881012916564941, acc.: 55.62%] [G loss: 0.7169880867004395]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 11/86 [D loss: 0.6860836744308472, acc.: 56.20%] [G loss: 0.716783344745636]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 12/86 [D loss: 0.6877315044403076, acc.: 54.69%] [G loss: 0.7162160277366638]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 13/86 [D loss: 0.686426192522049, acc.: 55.62%] [G loss: 0.7172815799713135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 14/86 [D loss: 0.6873228549957275, acc.: 55.22%] [G loss: 0.7154253721237183]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 15/86 [D loss: 0.6854041218757629, acc.: 56.20%] [G loss: 0.7165017127990723]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 16/86 [D loss: 0.6876140534877777, acc.: 54.35%] [G loss: 0.7137641906738281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 17/86 [D loss: 0.6862210035324097, acc.: 56.15%] [G loss: 0.7159680128097534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 18/86 [D loss: 0.6879567503929138, acc.: 54.64%] [G loss: 0.714935839176178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 19/86 [D loss: 0.6854201257228851, acc.: 57.32%] [G loss: 0.7167747616767883]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 20/86 [D loss: 0.6843868494033813, acc.: 58.11%] [G loss: 0.7107360363006592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 21/86 [D loss: 0.6860689520835876, acc.: 56.40%] [G loss: 0.7156043648719788]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 22/86 [D loss: 0.6868618130683899, acc.: 55.42%] [G loss: 0.7137663960456848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 23/86 [D loss: 0.6864585280418396, acc.: 55.76%] [G loss: 0.7141550183296204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 24/86 [D loss: 0.6868587136268616, acc.: 55.76%] [G loss: 0.7147819995880127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 25/86 [D loss: 0.6878506541252136, acc.: 55.08%] [G loss: 0.7131011486053467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 26/86 [D loss: 0.6870084702968597, acc.: 55.03%] [G loss: 0.7153329849243164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 27/86 [D loss: 0.6855928003787994, acc.: 57.32%] [G loss: 0.7135949730873108]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 28/86 [D loss: 0.6885923147201538, acc.: 53.42%] [G loss: 0.7120850086212158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 29/86 [D loss: 0.6836078763008118, acc.: 58.20%] [G loss: 0.7155718803405762]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 30/86 [D loss: 0.6862461268901825, acc.: 56.54%] [G loss: 0.7099955081939697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 31/86 [D loss: 0.6888658702373505, acc.: 52.25%] [G loss: 0.7191805243492126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 32/86 [D loss: 0.6845914423465729, acc.: 58.64%] [G loss: 0.7135313749313354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 33/86 [D loss: 0.6892200410366058, acc.: 52.73%] [G loss: 0.7148617506027222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 34/86 [D loss: 0.6854038834571838, acc.: 57.08%] [G loss: 0.7164335250854492]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 35/86 [D loss: 0.688130110502243, acc.: 55.37%] [G loss: 0.7111557126045227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 36/86 [D loss: 0.6892518401145935, acc.: 54.00%] [G loss: 0.7147418856620789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 37/86 [D loss: 0.6853419542312622, acc.: 57.47%] [G loss: 0.7089330554008484]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 38/86 [D loss: 0.6870988607406616, acc.: 56.10%] [G loss: 0.7104565501213074]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 39/86 [D loss: 0.6843494176864624, acc.: 57.23%] [G loss: 0.7172253727912903]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 40/86 [D loss: 0.68805992603302, acc.: 54.79%] [G loss: 0.7040649652481079]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 41/86 [D loss: 0.6924943923950195, acc.: 51.86%] [G loss: 0.7174824476242065]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 42/86 [D loss: 0.6828395128250122, acc.: 58.59%] [G loss: 0.7050111293792725]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 43/86 [D loss: 0.6924655437469482, acc.: 51.32%] [G loss: 0.7057124376296997]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 44/86 [D loss: 0.6872448921203613, acc.: 54.93%] [G loss: 0.7223140597343445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 45/86 [D loss: 0.6833504438400269, acc.: 59.38%] [G loss: 0.7066077589988708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 46/86 [D loss: 0.6959660649299622, acc.: 48.83%] [G loss: 0.7155085802078247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 47/86 [D loss: 0.6824997961521149, acc.: 59.08%] [G loss: 0.7124977707862854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 48/86 [D loss: 0.6942681074142456, acc.: 48.97%] [G loss: 0.6997263431549072]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 49/86 [D loss: 0.6874417066574097, acc.: 55.03%] [G loss: 0.7191185355186462]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 50/86 [D loss: 0.6827386319637299, acc.: 58.98%] [G loss: 0.7060046792030334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 51/86 [D loss: 0.6955821514129639, acc.: 47.41%] [G loss: 0.710043728351593]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 52/86 [D loss: 0.682040274143219, acc.: 60.30%] [G loss: 0.712978720664978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 53/86 [D loss: 0.6895280182361603, acc.: 52.88%] [G loss: 0.7009880542755127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 54/86 [D loss: 0.6909343004226685, acc.: 52.44%] [G loss: 0.7139081954956055]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 55/86 [D loss: 0.6832760572433472, acc.: 59.77%] [G loss: 0.7106671333312988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 56/86 [D loss: 0.693042665719986, acc.: 50.15%] [G loss: 0.7048254609107971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 57/86 [D loss: 0.6874023675918579, acc.: 55.37%] [G loss: 0.715745747089386]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 58/86 [D loss: 0.6867246329784393, acc.: 55.86%] [G loss: 0.7096855044364929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 59/86 [D loss: 0.6886771619319916, acc.: 52.78%] [G loss: 0.7114179730415344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 60/86 [D loss: 0.6865453124046326, acc.: 56.45%] [G loss: 0.7182224988937378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 61/86 [D loss: 0.6854741871356964, acc.: 56.69%] [G loss: 0.7129247188568115]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 83/200, Batch 62/86 [D loss: 0.6906879842281342, acc.: 53.22%] [G loss: 0.7149489521980286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 63/86 [D loss: 0.6842890083789825, acc.: 57.52%] [G loss: 0.7142277359962463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 64/86 [D loss: 0.6881972849369049, acc.: 54.49%] [G loss: 0.7128962874412537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 65/86 [D loss: 0.6852068603038788, acc.: 56.64%] [G loss: 0.7156700491905212]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 66/86 [D loss: 0.6869524717330933, acc.: 56.30%] [G loss: 0.7161240577697754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 67/86 [D loss: 0.6874277591705322, acc.: 55.76%] [G loss: 0.7149065136909485]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 68/86 [D loss: 0.685383677482605, acc.: 56.20%] [G loss: 0.7153482437133789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 69/86 [D loss: 0.6876234710216522, acc.: 55.08%] [G loss: 0.7148212790489197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 70/86 [D loss: 0.6865230798721313, acc.: 56.10%] [G loss: 0.7162400484085083]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 71/86 [D loss: 0.6844423711299896, acc.: 57.62%] [G loss: 0.7146790027618408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 72/86 [D loss: 0.6867098808288574, acc.: 55.27%] [G loss: 0.7152138948440552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 73/86 [D loss: 0.6824494898319244, acc.: 59.91%] [G loss: 0.7160496711730957]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 74/86 [D loss: 0.6863143146038055, acc.: 56.40%] [G loss: 0.7135916948318481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 75/86 [D loss: 0.6872650384902954, acc.: 54.64%] [G loss: 0.7132374048233032]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 76/86 [D loss: 0.6855775117874146, acc.: 56.74%] [G loss: 0.7167046070098877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 77/86 [D loss: 0.6855596303939819, acc.: 56.79%] [G loss: 0.7135658860206604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 78/86 [D loss: 0.686755359172821, acc.: 55.52%] [G loss: 0.715496301651001]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 79/86 [D loss: 0.683957427740097, acc.: 58.84%] [G loss: 0.7152441740036011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 80/86 [D loss: 0.6861733794212341, acc.: 56.10%] [G loss: 0.714853823184967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 81/86 [D loss: 0.6858089864253998, acc.: 55.42%] [G loss: 0.7148700952529907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 82/86 [D loss: 0.6854017078876495, acc.: 57.91%] [G loss: 0.716218888759613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 83/86 [D loss: 0.6858098804950714, acc.: 58.15%] [G loss: 0.7135952115058899]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 84/86 [D loss: 0.6874276697635651, acc.: 55.13%] [G loss: 0.7144090533256531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 85/86 [D loss: 0.6882768273353577, acc.: 54.88%] [G loss: 0.7157326936721802]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 86/86 [D loss: 0.6858241558074951, acc.: 57.76%] [G loss: 0.7159072756767273]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 1/86 [D loss: 0.686655730009079, acc.: 56.15%] [G loss: 0.7167306542396545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 2/86 [D loss: 0.686516523361206, acc.: 55.18%] [G loss: 0.7134120464324951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 3/86 [D loss: 0.6854044497013092, acc.: 55.76%] [G loss: 0.71330726146698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 4/86 [D loss: 0.6858615279197693, acc.: 56.30%] [G loss: 0.7183530926704407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 5/86 [D loss: 0.6871412992477417, acc.: 54.79%] [G loss: 0.7177003026008606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 6/86 [D loss: 0.6851134300231934, acc.: 56.69%] [G loss: 0.716524600982666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 7/86 [D loss: 0.6857604682445526, acc.: 56.59%] [G loss: 0.7173017859458923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 8/86 [D loss: 0.6848595142364502, acc.: 55.71%] [G loss: 0.7177242040634155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 9/86 [D loss: 0.686822921037674, acc.: 54.88%] [G loss: 0.7143946886062622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 10/86 [D loss: 0.6867504417896271, acc.: 54.88%] [G loss: 0.7188376188278198]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 11/86 [D loss: 0.6874719560146332, acc.: 55.08%] [G loss: 0.711902379989624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 12/86 [D loss: 0.6903598308563232, acc.: 52.83%] [G loss: 0.7116492390632629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 13/86 [D loss: 0.6868003904819489, acc.: 55.18%] [G loss: 0.716249942779541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 14/86 [D loss: 0.6864262819290161, acc.: 55.42%] [G loss: 0.7121104001998901]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 15/86 [D loss: 0.6874562501907349, acc.: 55.08%] [G loss: 0.7168313264846802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 16/86 [D loss: 0.6839702129364014, acc.: 57.91%] [G loss: 0.7171117067337036]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 17/86 [D loss: 0.6878341138362885, acc.: 54.98%] [G loss: 0.7113112211227417]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 18/86 [D loss: 0.6862552762031555, acc.: 55.71%] [G loss: 0.7157618999481201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 19/86 [D loss: 0.6849301755428314, acc.: 57.08%] [G loss: 0.7114406824111938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 20/86 [D loss: 0.6875911951065063, acc.: 54.79%] [G loss: 0.7143797874450684]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 21/86 [D loss: 0.6853339672088623, acc.: 57.42%] [G loss: 0.7157070636749268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 22/86 [D loss: 0.6857026219367981, acc.: 55.76%] [G loss: 0.7126246094703674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 23/86 [D loss: 0.6887007057666779, acc.: 53.47%] [G loss: 0.7147770524024963]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 24/86 [D loss: 0.6856462955474854, acc.: 56.35%] [G loss: 0.7125709056854248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 25/86 [D loss: 0.6882160604000092, acc.: 53.52%] [G loss: 0.7135224342346191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 26/86 [D loss: 0.6869333982467651, acc.: 55.18%] [G loss: 0.7126443982124329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 27/86 [D loss: 0.6867372393608093, acc.: 56.01%] [G loss: 0.7115192413330078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 28/86 [D loss: 0.6869121193885803, acc.: 54.54%] [G loss: 0.7154378294944763]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 29/86 [D loss: 0.6831964254379272, acc.: 59.67%] [G loss: 0.714522123336792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 30/86 [D loss: 0.6883533298969269, acc.: 55.13%] [G loss: 0.7147246599197388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 31/86 [D loss: 0.6852682828903198, acc.: 56.59%] [G loss: 0.7139850854873657]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 32/86 [D loss: 0.6882157325744629, acc.: 54.10%] [G loss: 0.7100716233253479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 33/86 [D loss: 0.6887992024421692, acc.: 52.98%] [G loss: 0.7153458595275879]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 34/86 [D loss: 0.6857289671897888, acc.: 55.71%] [G loss: 0.7131855487823486]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 35/86 [D loss: 0.6876725852489471, acc.: 53.86%] [G loss: 0.7172586917877197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 36/86 [D loss: 0.6866225600242615, acc.: 55.22%] [G loss: 0.7159899473190308]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 37/86 [D loss: 0.683692455291748, acc.: 58.84%] [G loss: 0.7133205533027649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 38/86 [D loss: 0.6868920922279358, acc.: 55.37%] [G loss: 0.7173097133636475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 39/86 [D loss: 0.6847117841243744, acc.: 57.91%] [G loss: 0.7144300937652588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 40/86 [D loss: 0.6898779273033142, acc.: 52.34%] [G loss: 0.7105389833450317]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 41/86 [D loss: 0.6852834224700928, acc.: 56.64%] [G loss: 0.7174007296562195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 42/86 [D loss: 0.6842561364173889, acc.: 58.59%] [G loss: 0.7123057842254639]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 43/86 [D loss: 0.6890809834003448, acc.: 53.86%] [G loss: 0.7175290584564209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 44/86 [D loss: 0.6849521398544312, acc.: 58.01%] [G loss: 0.7147610783576965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 45/86 [D loss: 0.6882548928260803, acc.: 53.66%] [G loss: 0.710912823677063]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 46/86 [D loss: 0.6881279647350311, acc.: 55.32%] [G loss: 0.7200110554695129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 47/86 [D loss: 0.6861476302146912, acc.: 56.05%] [G loss: 0.7071495056152344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 48/86 [D loss: 0.691536009311676, acc.: 50.63%] [G loss: 0.7143857479095459]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 49/86 [D loss: 0.6828515529632568, acc.: 59.96%] [G loss: 0.7130815982818604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 50/86 [D loss: 0.689170777797699, acc.: 51.95%] [G loss: 0.7063374519348145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 51/86 [D loss: 0.6889825463294983, acc.: 53.12%] [G loss: 0.7192167043685913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 52/86 [D loss: 0.6833404898643494, acc.: 59.81%] [G loss: 0.7079014778137207]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 53/86 [D loss: 0.6935146152973175, acc.: 50.15%] [G loss: 0.7103685140609741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 54/86 [D loss: 0.6830517947673798, acc.: 58.98%] [G loss: 0.7109704613685608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 55/86 [D loss: 0.6907064020633698, acc.: 51.61%] [G loss: 0.7040573358535767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 56/86 [D loss: 0.6875607073307037, acc.: 54.44%] [G loss: 0.717322826385498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 57/86 [D loss: 0.6823486983776093, acc.: 60.11%] [G loss: 0.7088810205459595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 58/86 [D loss: 0.6956148147583008, acc.: 47.90%] [G loss: 0.7050685882568359]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 59/86 [D loss: 0.6852535605430603, acc.: 58.64%] [G loss: 0.7163522243499756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 60/86 [D loss: 0.6906609535217285, acc.: 52.83%] [G loss: 0.7047751545906067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 61/86 [D loss: 0.6913098096847534, acc.: 50.68%] [G loss: 0.7155608534812927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 62/86 [D loss: 0.6833762526512146, acc.: 58.30%] [G loss: 0.7117957472801208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 63/86 [D loss: 0.6883941888809204, acc.: 55.13%] [G loss: 0.7065110206604004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 64/86 [D loss: 0.6871612370014191, acc.: 56.88%] [G loss: 0.7167550921440125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 65/86 [D loss: 0.6858816146850586, acc.: 54.74%] [G loss: 0.7110031247138977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 66/86 [D loss: 0.6881076395511627, acc.: 54.35%] [G loss: 0.709734320640564]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 67/86 [D loss: 0.6854115724563599, acc.: 55.71%] [G loss: 0.7171564698219299]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 68/86 [D loss: 0.6845891177654266, acc.: 56.98%] [G loss: 0.7133108377456665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 69/86 [D loss: 0.6878156661987305, acc.: 54.93%] [G loss: 0.71204674243927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 70/86 [D loss: 0.6856007874011993, acc.: 56.15%] [G loss: 0.7136895060539246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 71/86 [D loss: 0.6886619925498962, acc.: 54.49%] [G loss: 0.712936520576477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 72/86 [D loss: 0.6863387823104858, acc.: 54.79%] [G loss: 0.715811014175415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 73/86 [D loss: 0.6844130456447601, acc.: 57.91%] [G loss: 0.7160824537277222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 74/86 [D loss: 0.6871861815452576, acc.: 54.39%] [G loss: 0.7117941379547119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 75/86 [D loss: 0.6871209740638733, acc.: 56.20%] [G loss: 0.7162265181541443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 76/86 [D loss: 0.6844321489334106, acc.: 57.08%] [G loss: 0.7161738276481628]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 77/86 [D loss: 0.6860984265804291, acc.: 55.81%] [G loss: 0.7151502966880798]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 78/86 [D loss: 0.6861365437507629, acc.: 55.42%] [G loss: 0.716742992401123]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 79/86 [D loss: 0.6866399347782135, acc.: 55.32%] [G loss: 0.7147564888000488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 80/86 [D loss: 0.6881999373435974, acc.: 55.96%] [G loss: 0.7172374725341797]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 81/86 [D loss: 0.6868548393249512, acc.: 55.42%] [G loss: 0.7167882919311523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 82/86 [D loss: 0.6861115097999573, acc.: 55.52%] [G loss: 0.7147141695022583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 83/86 [D loss: 0.6840951442718506, acc.: 56.40%] [G loss: 0.7183330655097961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 84/86 [D loss: 0.6867389380931854, acc.: 54.49%] [G loss: 0.7181810736656189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 85/86 [D loss: 0.6849859058856964, acc.: 57.47%] [G loss: 0.7161906957626343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 86/86 [D loss: 0.6845769882202148, acc.: 57.37%] [G loss: 0.7146121859550476]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 85/200, Batch 1/86 [D loss: 0.6867741942405701, acc.: 55.57%] [G loss: 0.7130861282348633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 2/86 [D loss: 0.6847335696220398, acc.: 57.71%] [G loss: 0.7193396687507629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 3/86 [D loss: 0.6851774156093597, acc.: 57.52%] [G loss: 0.7181620001792908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 4/86 [D loss: 0.6863672137260437, acc.: 55.86%] [G loss: 0.7162117958068848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 5/86 [D loss: 0.6840973794460297, acc.: 58.11%] [G loss: 0.7181395888328552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 6/86 [D loss: 0.687528133392334, acc.: 55.81%] [G loss: 0.7168749570846558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 7/86 [D loss: 0.6856704950332642, acc.: 56.25%] [G loss: 0.7184386253356934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 8/86 [D loss: 0.6856750249862671, acc.: 55.37%] [G loss: 0.7168606519699097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 9/86 [D loss: 0.6881327629089355, acc.: 55.57%] [G loss: 0.7161169052124023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 10/86 [D loss: 0.6852333247661591, acc.: 57.18%] [G loss: 0.7203205823898315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 11/86 [D loss: 0.6835365891456604, acc.: 56.69%] [G loss: 0.713573157787323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 12/86 [D loss: 0.6864645183086395, acc.: 55.66%] [G loss: 0.717745304107666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 13/86 [D loss: 0.6872182488441467, acc.: 55.08%] [G loss: 0.7163336873054504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 14/86 [D loss: 0.6861916482448578, acc.: 56.98%] [G loss: 0.7117958068847656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 15/86 [D loss: 0.6856832504272461, acc.: 56.69%] [G loss: 0.7177847027778625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 16/86 [D loss: 0.6871229410171509, acc.: 55.37%] [G loss: 0.7109347581863403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 17/86 [D loss: 0.6850932538509369, acc.: 56.88%] [G loss: 0.7141990661621094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 18/86 [D loss: 0.6849243342876434, acc.: 56.45%] [G loss: 0.7143164277076721]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 19/86 [D loss: 0.685979425907135, acc.: 57.18%] [G loss: 0.7150142788887024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 20/86 [D loss: 0.6853660047054291, acc.: 56.10%] [G loss: 0.7174426317214966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 21/86 [D loss: 0.6862191557884216, acc.: 55.91%] [G loss: 0.7145850658416748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 22/86 [D loss: 0.6856277883052826, acc.: 56.20%] [G loss: 0.7171677350997925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 23/86 [D loss: 0.687177300453186, acc.: 54.93%] [G loss: 0.719093382358551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 24/86 [D loss: 0.6864160895347595, acc.: 56.15%] [G loss: 0.7146627306938171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 25/86 [D loss: 0.6877836585044861, acc.: 55.37%] [G loss: 0.7152607440948486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 26/86 [D loss: 0.6852587759494781, acc.: 56.54%] [G loss: 0.7199046611785889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 27/86 [D loss: 0.6851515173912048, acc.: 55.81%] [G loss: 0.7138751149177551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 28/86 [D loss: 0.6866302490234375, acc.: 54.74%] [G loss: 0.7205615639686584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 29/86 [D loss: 0.6827034652233124, acc.: 58.45%] [G loss: 0.715400755405426]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 30/86 [D loss: 0.6889082789421082, acc.: 52.73%] [G loss: 0.7121590375900269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 31/86 [D loss: 0.686652421951294, acc.: 55.27%] [G loss: 0.7190661430358887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 32/86 [D loss: 0.6843084096908569, acc.: 57.57%] [G loss: 0.7145714163780212]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 33/86 [D loss: 0.6889054179191589, acc.: 53.86%] [G loss: 0.7175453901290894]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 34/86 [D loss: 0.6849294006824493, acc.: 55.91%] [G loss: 0.7213548421859741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 35/86 [D loss: 0.6860489249229431, acc.: 57.42%] [G loss: 0.7079033255577087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 36/86 [D loss: 0.6890195906162262, acc.: 52.98%] [G loss: 0.7189761996269226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 37/86 [D loss: 0.6845221817493439, acc.: 58.98%] [G loss: 0.713681697845459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 38/86 [D loss: 0.6873835325241089, acc.: 54.15%] [G loss: 0.7132235765457153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 39/86 [D loss: 0.6867042779922485, acc.: 54.64%] [G loss: 0.7200667858123779]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 40/86 [D loss: 0.6864258348941803, acc.: 55.91%] [G loss: 0.714870274066925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 41/86 [D loss: 0.6873873174190521, acc.: 54.35%] [G loss: 0.7150360345840454]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 42/86 [D loss: 0.6836016774177551, acc.: 58.15%] [G loss: 0.7150143384933472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 43/86 [D loss: 0.6853917837142944, acc.: 56.64%] [G loss: 0.7123883962631226]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 44/86 [D loss: 0.6863053739070892, acc.: 55.37%] [G loss: 0.7157537937164307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 45/86 [D loss: 0.6830780208110809, acc.: 58.11%] [G loss: 0.714553952217102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 46/86 [D loss: 0.6877691447734833, acc.: 54.30%] [G loss: 0.7144413590431213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 47/86 [D loss: 0.684366762638092, acc.: 58.11%] [G loss: 0.7196479439735413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 48/86 [D loss: 0.6868893206119537, acc.: 55.18%] [G loss: 0.7143166661262512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 49/86 [D loss: 0.6872462630271912, acc.: 52.64%] [G loss: 0.7175658345222473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 50/86 [D loss: 0.6870865523815155, acc.: 54.88%] [G loss: 0.7173977494239807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 51/86 [D loss: 0.6868804097175598, acc.: 56.54%] [G loss: 0.713613748550415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 52/86 [D loss: 0.6856574714183807, acc.: 57.86%] [G loss: 0.7171432971954346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 53/86 [D loss: 0.6824804544448853, acc.: 59.08%] [G loss: 0.714438796043396]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 54/86 [D loss: 0.6878846287727356, acc.: 55.52%] [G loss: 0.7161755561828613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 55/86 [D loss: 0.6852610409259796, acc.: 55.71%] [G loss: 0.7176456451416016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 56/86 [D loss: 0.6852234303951263, acc.: 57.76%] [G loss: 0.7177684307098389]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 57/86 [D loss: 0.6863702237606049, acc.: 56.01%] [G loss: 0.7167244553565979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 58/86 [D loss: 0.6840045154094696, acc.: 56.98%] [G loss: 0.7193642258644104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 59/86 [D loss: 0.6866686046123505, acc.: 54.79%] [G loss: 0.7175745964050293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 60/86 [D loss: 0.6880204975605011, acc.: 54.88%] [G loss: 0.7195277214050293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 61/86 [D loss: 0.6833643317222595, acc.: 58.84%] [G loss: 0.7160300016403198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 62/86 [D loss: 0.6865674555301666, acc.: 55.66%] [G loss: 0.7164192199707031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 63/86 [D loss: 0.6832478046417236, acc.: 58.11%] [G loss: 0.7148406505584717]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 64/86 [D loss: 0.6862834095954895, acc.: 55.96%] [G loss: 0.7156563401222229]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 65/86 [D loss: 0.6850390136241913, acc.: 57.71%] [G loss: 0.7146239876747131]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 66/86 [D loss: 0.6832901835441589, acc.: 59.42%] [G loss: 0.7175165414810181]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 67/86 [D loss: 0.689132571220398, acc.: 53.22%] [G loss: 0.7168708443641663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 68/86 [D loss: 0.6855791211128235, acc.: 57.32%] [G loss: 0.7172033190727234]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 69/86 [D loss: 0.686585396528244, acc.: 55.71%] [G loss: 0.7179660201072693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 70/86 [D loss: 0.684538334608078, acc.: 58.01%] [G loss: 0.7179570198059082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 71/86 [D loss: 0.685511440038681, acc.: 56.79%] [G loss: 0.7180076241493225]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 72/86 [D loss: 0.6841323375701904, acc.: 56.64%] [G loss: 0.7148961424827576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 73/86 [D loss: 0.6859703361988068, acc.: 55.13%] [G loss: 0.7170189619064331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 74/86 [D loss: 0.6850647032260895, acc.: 57.08%] [G loss: 0.7166559100151062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 75/86 [D loss: 0.6848946213722229, acc.: 56.79%] [G loss: 0.7193853855133057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 76/86 [D loss: 0.6864914894104004, acc.: 56.74%] [G loss: 0.7159057855606079]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 77/86 [D loss: 0.6842314004898071, acc.: 57.52%] [G loss: 0.7155617475509644]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 78/86 [D loss: 0.684852659702301, acc.: 57.47%] [G loss: 0.7176012992858887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 79/86 [D loss: 0.6864365637302399, acc.: 56.15%] [G loss: 0.7142254710197449]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 80/86 [D loss: 0.686939001083374, acc.: 55.57%] [G loss: 0.7169474959373474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 81/86 [D loss: 0.6857554316520691, acc.: 56.84%] [G loss: 0.7150419354438782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 82/86 [D loss: 0.6864993870258331, acc.: 56.74%] [G loss: 0.7129398584365845]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 83/86 [D loss: 0.6850564479827881, acc.: 57.42%] [G loss: 0.7143855094909668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 84/86 [D loss: 0.6860983967781067, acc.: 56.15%] [G loss: 0.7125625610351562]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 85/86 [D loss: 0.6860122680664062, acc.: 55.32%] [G loss: 0.7142230272293091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 86/86 [D loss: 0.6844459772109985, acc.: 57.57%] [G loss: 0.7148311138153076]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 1/86 [D loss: 0.6846950650215149, acc.: 58.01%] [G loss: 0.7160757780075073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 2/86 [D loss: 0.6840454638004303, acc.: 58.01%] [G loss: 0.7178863286972046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 3/86 [D loss: 0.6863722503185272, acc.: 56.49%] [G loss: 0.717700183391571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 4/86 [D loss: 0.6850304305553436, acc.: 56.88%] [G loss: 0.716865062713623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 5/86 [D loss: 0.6845871210098267, acc.: 57.47%] [G loss: 0.7162411212921143]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 6/86 [D loss: 0.6868718862533569, acc.: 55.91%] [G loss: 0.7154655456542969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 7/86 [D loss: 0.6841014921665192, acc.: 58.20%] [G loss: 0.7162759304046631]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 8/86 [D loss: 0.6871350109577179, acc.: 56.05%] [G loss: 0.7183853387832642]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 9/86 [D loss: 0.6842645108699799, acc.: 56.84%] [G loss: 0.7175120711326599]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 10/86 [D loss: 0.6859555542469025, acc.: 55.42%] [G loss: 0.7188869714736938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 11/86 [D loss: 0.6858221292495728, acc.: 56.30%] [G loss: 0.7208606004714966]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 12/86 [D loss: 0.6840287148952484, acc.: 58.54%] [G loss: 0.71547931432724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 13/86 [D loss: 0.6851614415645599, acc.: 56.35%] [G loss: 0.7192309498786926]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 14/86 [D loss: 0.6848693788051605, acc.: 56.79%] [G loss: 0.7190978527069092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 15/86 [D loss: 0.6866583824157715, acc.: 56.69%] [G loss: 0.7119113206863403]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 16/86 [D loss: 0.6868268251419067, acc.: 54.79%] [G loss: 0.7185261249542236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 17/86 [D loss: 0.6848689615726471, acc.: 55.57%] [G loss: 0.7147820591926575]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 18/86 [D loss: 0.6837361752986908, acc.: 59.42%] [G loss: 0.7161015868186951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 19/86 [D loss: 0.6839596629142761, acc.: 57.47%] [G loss: 0.7194479703903198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 20/86 [D loss: 0.6859223544597626, acc.: 55.81%] [G loss: 0.7161255478858948]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 21/86 [D loss: 0.6837240755558014, acc.: 57.91%] [G loss: 0.7204894423484802]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 22/86 [D loss: 0.6840359270572662, acc.: 56.69%] [G loss: 0.7138625383377075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 23/86 [D loss: 0.6867924630641937, acc.: 55.62%] [G loss: 0.7146710157394409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 24/86 [D loss: 0.6838840246200562, acc.: 59.42%] [G loss: 0.7151675224304199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 25/86 [D loss: 0.6861613392829895, acc.: 57.28%] [G loss: 0.7138912081718445]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 26/86 [D loss: 0.6840849816799164, acc.: 58.50%] [G loss: 0.7190954685211182]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 27/86 [D loss: 0.684111624956131, acc.: 56.74%] [G loss: 0.7164293527603149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 28/86 [D loss: 0.6857037544250488, acc.: 54.59%] [G loss: 0.716949999332428]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 29/86 [D loss: 0.6848488748073578, acc.: 57.96%] [G loss: 0.7185372114181519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 30/86 [D loss: 0.6864169538021088, acc.: 55.71%] [G loss: 0.7183132767677307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 31/86 [D loss: 0.6855766177177429, acc.: 56.54%] [G loss: 0.7161983251571655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 32/86 [D loss: 0.6860804259777069, acc.: 55.86%] [G loss: 0.7162752747535706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 33/86 [D loss: 0.686030387878418, acc.: 55.52%] [G loss: 0.7179343700408936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 34/86 [D loss: 0.6850376725196838, acc.: 56.49%] [G loss: 0.714935302734375]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 35/86 [D loss: 0.6863049566745758, acc.: 56.20%] [G loss: 0.7180379629135132]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 36/86 [D loss: 0.6862956583499908, acc.: 54.74%] [G loss: 0.716934084892273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 37/86 [D loss: 0.6859269738197327, acc.: 54.15%] [G loss: 0.7169325351715088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 38/86 [D loss: 0.6873327195644379, acc.: 54.15%] [G loss: 0.7182119488716125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 39/86 [D loss: 0.6849872171878815, acc.: 56.88%] [G loss: 0.7193806767463684]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 40/86 [D loss: 0.6877652406692505, acc.: 54.25%] [G loss: 0.7188819646835327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 41/86 [D loss: 0.6860391497612, acc.: 55.86%] [G loss: 0.7193105220794678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 42/86 [D loss: 0.6853958964347839, acc.: 56.74%] [G loss: 0.7131524085998535]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 43/86 [D loss: 0.6878803372383118, acc.: 54.64%] [G loss: 0.7162764668464661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 44/86 [D loss: 0.6831121146678925, acc.: 58.79%] [G loss: 0.715990424156189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 45/86 [D loss: 0.6877760887145996, acc.: 54.83%] [G loss: 0.7151705026626587]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 46/86 [D loss: 0.6847251951694489, acc.: 57.62%] [G loss: 0.7201361656188965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 47/86 [D loss: 0.6858413815498352, acc.: 55.32%] [G loss: 0.7150733470916748]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 48/86 [D loss: 0.6881073713302612, acc.: 54.44%] [G loss: 0.7196391224861145]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 49/86 [D loss: 0.6839210987091064, acc.: 57.86%] [G loss: 0.713864803314209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 50/86 [D loss: 0.6908913552761078, acc.: 52.44%] [G loss: 0.7146207690238953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 51/86 [D loss: 0.6854562163352966, acc.: 55.91%] [G loss: 0.7159255146980286]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 52/86 [D loss: 0.6867709457874298, acc.: 54.64%] [G loss: 0.7113610506057739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 53/86 [D loss: 0.6860645115375519, acc.: 55.03%] [G loss: 0.7191116809844971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 54/86 [D loss: 0.6853287816047668, acc.: 57.18%] [G loss: 0.7096836566925049]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 55/86 [D loss: 0.6924137473106384, acc.: 51.12%] [G loss: 0.720136821269989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 56/86 [D loss: 0.6832139194011688, acc.: 57.32%] [G loss: 0.714867115020752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 57/86 [D loss: 0.6906079053878784, acc.: 53.27%] [G loss: 0.7146685123443604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 58/86 [D loss: 0.6837462782859802, acc.: 57.86%] [G loss: 0.7211194634437561]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 59/86 [D loss: 0.6861042082309723, acc.: 55.27%] [G loss: 0.7093096971511841]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 60/86 [D loss: 0.6870661973953247, acc.: 55.52%] [G loss: 0.7230274677276611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 61/86 [D loss: 0.6843582391738892, acc.: 56.74%] [G loss: 0.716144323348999]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 62/86 [D loss: 0.687035322189331, acc.: 54.64%] [G loss: 0.7162041664123535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 63/86 [D loss: 0.6858243644237518, acc.: 56.64%] [G loss: 0.7210818529129028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 64/86 [D loss: 0.6868795454502106, acc.: 54.88%] [G loss: 0.7109524011611938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 65/86 [D loss: 0.6892121434211731, acc.: 53.86%] [G loss: 0.7189782857894897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 66/86 [D loss: 0.6862948536872864, acc.: 55.66%] [G loss: 0.7156319618225098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 67/86 [D loss: 0.6888925135135651, acc.: 53.22%] [G loss: 0.7163906097412109]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 68/86 [D loss: 0.684058427810669, acc.: 57.81%] [G loss: 0.7193233370780945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 69/86 [D loss: 0.6842606365680695, acc.: 57.03%] [G loss: 0.7085505127906799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 70/86 [D loss: 0.6897089779376984, acc.: 52.93%] [G loss: 0.7147530913352966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 71/86 [D loss: 0.6840955913066864, acc.: 57.37%] [G loss: 0.7164185047149658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 72/86 [D loss: 0.6886399984359741, acc.: 51.95%] [G loss: 0.7119852304458618]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 73/86 [D loss: 0.6857850849628448, acc.: 55.76%] [G loss: 0.7185518145561218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 74/86 [D loss: 0.6864335238933563, acc.: 55.42%] [G loss: 0.7164715528488159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 75/86 [D loss: 0.6870397627353668, acc.: 54.44%] [G loss: 0.715703010559082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 76/86 [D loss: 0.6830921471118927, acc.: 58.45%] [G loss: 0.7156618237495422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 77/86 [D loss: 0.6867515444755554, acc.: 55.76%] [G loss: 0.7142860889434814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 78/86 [D loss: 0.6876170933246613, acc.: 55.08%] [G loss: 0.7177198529243469]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 79/86 [D loss: 0.6852146983146667, acc.: 56.15%] [G loss: 0.716303288936615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 80/86 [D loss: 0.6858685314655304, acc.: 55.91%] [G loss: 0.7163230180740356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 81/86 [D loss: 0.685033917427063, acc.: 56.59%] [G loss: 0.71844482421875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 82/86 [D loss: 0.6861083209514618, acc.: 55.62%] [G loss: 0.7199761867523193]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 86/200, Batch 83/86 [D loss: 0.6858484745025635, acc.: 55.66%] [G loss: 0.7191062569618225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 84/86 [D loss: 0.6850304305553436, acc.: 56.30%] [G loss: 0.7178007960319519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 85/86 [D loss: 0.6871644854545593, acc.: 55.52%] [G loss: 0.7168199419975281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 86/86 [D loss: 0.6863234043121338, acc.: 54.64%] [G loss: 0.717189610004425]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 1/86 [D loss: 0.6844545900821686, acc.: 57.08%] [G loss: 0.715004563331604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 2/86 [D loss: 0.6876198649406433, acc.: 55.37%] [G loss: 0.7180453538894653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 3/86 [D loss: 0.6834228336811066, acc.: 58.59%] [G loss: 0.7184994220733643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 4/86 [D loss: 0.6862095296382904, acc.: 55.37%] [G loss: 0.7115786075592041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 5/86 [D loss: 0.6878859996795654, acc.: 54.49%] [G loss: 0.717585563659668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 6/86 [D loss: 0.6869222521781921, acc.: 53.96%] [G loss: 0.7113512754440308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 7/86 [D loss: 0.6884883344173431, acc.: 52.59%] [G loss: 0.7180010080337524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 8/86 [D loss: 0.6815389692783356, acc.: 59.52%] [G loss: 0.7173483967781067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 9/86 [D loss: 0.6888138651847839, acc.: 53.22%] [G loss: 0.7167777419090271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 10/86 [D loss: 0.6848474144935608, acc.: 57.47%] [G loss: 0.7170896530151367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 11/86 [D loss: 0.6853908896446228, acc.: 57.18%] [G loss: 0.7104299068450928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 12/86 [D loss: 0.6864842176437378, acc.: 55.03%] [G loss: 0.7201524972915649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 13/86 [D loss: 0.6831952035427094, acc.: 58.30%] [G loss: 0.7181339859962463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 14/86 [D loss: 0.6899040639400482, acc.: 52.05%] [G loss: 0.7142125964164734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 15/86 [D loss: 0.6854020059108734, acc.: 56.88%] [G loss: 0.7210122346878052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 16/86 [D loss: 0.6846812665462494, acc.: 55.96%] [G loss: 0.7121003270149231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 17/86 [D loss: 0.6856606900691986, acc.: 55.42%] [G loss: 0.7163570523262024]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 87/200, Batch 18/86 [D loss: 0.6859934329986572, acc.: 57.03%] [G loss: 0.7216506600379944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 19/86 [D loss: 0.6869502663612366, acc.: 56.59%] [G loss: 0.7115033268928528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 20/86 [D loss: 0.6875066161155701, acc.: 54.00%] [G loss: 0.7197644710540771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 21/86 [D loss: 0.6848822832107544, acc.: 57.37%] [G loss: 0.7153857946395874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 22/86 [D loss: 0.6894907355308533, acc.: 52.73%] [G loss: 0.7174773216247559]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 23/86 [D loss: 0.6827133297920227, acc.: 58.11%] [G loss: 0.7173318862915039]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 24/86 [D loss: 0.6886963546276093, acc.: 53.86%] [G loss: 0.7120445966720581]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 25/86 [D loss: 0.6853013038635254, acc.: 56.64%] [G loss: 0.7206560969352722]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 26/86 [D loss: 0.6856576800346375, acc.: 56.74%] [G loss: 0.7124330401420593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 27/86 [D loss: 0.6894164383411407, acc.: 53.56%] [G loss: 0.718536376953125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 28/86 [D loss: 0.6847372353076935, acc.: 56.01%] [G loss: 0.715939998626709]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 29/86 [D loss: 0.6864528656005859, acc.: 55.37%] [G loss: 0.7085197567939758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 30/86 [D loss: 0.6879889070987701, acc.: 53.66%] [G loss: 0.7169463038444519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 31/86 [D loss: 0.6841107308864594, acc.: 55.86%] [G loss: 0.7134382128715515]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 32/86 [D loss: 0.6855828166007996, acc.: 56.49%] [G loss: 0.715043306350708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 33/86 [D loss: 0.6823940873146057, acc.: 58.59%] [G loss: 0.7251324653625488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 34/86 [D loss: 0.6864569783210754, acc.: 55.62%] [G loss: 0.7111542224884033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 35/86 [D loss: 0.685017466545105, acc.: 56.69%] [G loss: 0.7176600098609924]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 36/86 [D loss: 0.6851730346679688, acc.: 57.08%] [G loss: 0.7112870216369629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 37/86 [D loss: 0.6879493892192841, acc.: 53.42%] [G loss: 0.7162349224090576]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 38/86 [D loss: 0.6826009750366211, acc.: 57.57%] [G loss: 0.7164839506149292]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 39/86 [D loss: 0.6861855387687683, acc.: 55.76%] [G loss: 0.7169399261474609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 40/86 [D loss: 0.6857995092868805, acc.: 55.27%] [G loss: 0.718462347984314]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 41/86 [D loss: 0.688050776720047, acc.: 54.88%] [G loss: 0.7158102989196777]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 87/200, Batch 42/86 [D loss: 0.6855809390544891, acc.: 56.01%] [G loss: 0.7184394598007202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 43/86 [D loss: 0.6849627494812012, acc.: 55.96%] [G loss: 0.7161574363708496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 44/86 [D loss: 0.6854838728904724, acc.: 56.15%] [G loss: 0.7164722681045532]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 45/86 [D loss: 0.6836914718151093, acc.: 58.84%] [G loss: 0.7163230180740356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 46/86 [D loss: 0.6852754056453705, acc.: 55.47%] [G loss: 0.7152301073074341]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 47/86 [D loss: 0.6862006187438965, acc.: 56.30%] [G loss: 0.7187176942825317]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 48/86 [D loss: 0.6867935657501221, acc.: 54.05%] [G loss: 0.7189700603485107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 49/86 [D loss: 0.6863220036029816, acc.: 55.52%] [G loss: 0.7191256284713745]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 50/86 [D loss: 0.6857911050319672, acc.: 55.37%] [G loss: 0.7185708284378052]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 51/86 [D loss: 0.6833082437515259, acc.: 57.42%] [G loss: 0.7171906232833862]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 52/86 [D loss: 0.6851989328861237, acc.: 56.05%] [G loss: 0.72073894739151]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 53/86 [D loss: 0.6845535933971405, acc.: 56.49%] [G loss: 0.7228814363479614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 54/86 [D loss: 0.6852503716945648, acc.: 56.10%] [G loss: 0.7156264781951904]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 55/86 [D loss: 0.6856119334697723, acc.: 54.93%] [G loss: 0.7213827967643738]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 56/86 [D loss: 0.6840523481369019, acc.: 56.79%] [G loss: 0.7189255356788635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 57/86 [D loss: 0.6846292614936829, acc.: 56.79%] [G loss: 0.718329131603241]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 58/86 [D loss: 0.6846134066581726, acc.: 57.62%] [G loss: 0.7184486389160156]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 59/86 [D loss: 0.6856945753097534, acc.: 56.54%] [G loss: 0.7187448143959045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 60/86 [D loss: 0.6866117119789124, acc.: 55.91%] [G loss: 0.7200719118118286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 61/86 [D loss: 0.6839390993118286, acc.: 58.98%] [G loss: 0.7182239294052124]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 62/86 [D loss: 0.6872222721576691, acc.: 55.81%] [G loss: 0.7177027463912964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 63/86 [D loss: 0.6830584704875946, acc.: 57.67%] [G loss: 0.718876838684082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 64/86 [D loss: 0.6864128410816193, acc.: 55.96%] [G loss: 0.7179132699966431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 65/86 [D loss: 0.6860160827636719, acc.: 55.91%] [G loss: 0.7226535081863403]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 66/86 [D loss: 0.683901309967041, acc.: 57.03%] [G loss: 0.7203314304351807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 67/86 [D loss: 0.6871998012065887, acc.: 55.52%] [G loss: 0.7215986847877502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 68/86 [D loss: 0.6839536428451538, acc.: 57.13%] [G loss: 0.7225689888000488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 69/86 [D loss: 0.6847539246082306, acc.: 56.01%] [G loss: 0.7172262668609619]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 70/86 [D loss: 0.6853882372379303, acc.: 55.37%] [G loss: 0.7218354940414429]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 71/86 [D loss: 0.6839044392108917, acc.: 57.67%] [G loss: 0.7223689556121826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 72/86 [D loss: 0.6880218684673309, acc.: 54.15%] [G loss: 0.720133900642395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 73/86 [D loss: 0.6840240061283112, acc.: 55.71%] [G loss: 0.7196193337440491]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 74/86 [D loss: 0.6851058304309845, acc.: 56.79%] [G loss: 0.7200811505317688]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 75/86 [D loss: 0.6853899359703064, acc.: 56.15%] [G loss: 0.7201645970344543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 76/86 [D loss: 0.6831543147563934, acc.: 58.45%] [G loss: 0.7187123894691467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 77/86 [D loss: 0.6875252723693848, acc.: 54.69%] [G loss: 0.7178054451942444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 78/86 [D loss: 0.6875237822532654, acc.: 54.39%] [G loss: 0.7182630300521851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 79/86 [D loss: 0.6835757791996002, acc.: 58.69%] [G loss: 0.7167099118232727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 80/86 [D loss: 0.6883218586444855, acc.: 55.27%] [G loss: 0.7180228233337402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 81/86 [D loss: 0.6835090816020966, acc.: 58.45%] [G loss: 0.7209026217460632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 82/86 [D loss: 0.6843727231025696, acc.: 55.96%] [G loss: 0.7164627313613892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 83/86 [D loss: 0.6869178712368011, acc.: 56.15%] [G loss: 0.7187293171882629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 84/86 [D loss: 0.6838355958461761, acc.: 56.69%] [G loss: 0.7217625975608826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 85/86 [D loss: 0.6861979365348816, acc.: 55.42%] [G loss: 0.7240122556686401]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 86/86 [D loss: 0.6812281012535095, acc.: 60.40%] [G loss: 0.7177951335906982]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 1/86 [D loss: 0.6864217519760132, acc.: 56.59%] [G loss: 0.7145318984985352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 2/86 [D loss: 0.6882894933223724, acc.: 53.22%] [G loss: 0.7181369662284851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 3/86 [D loss: 0.6849648654460907, acc.: 56.25%] [G loss: 0.718864917755127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 4/86 [D loss: 0.6854727864265442, acc.: 55.91%] [G loss: 0.7201409935951233]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 5/86 [D loss: 0.6828613579273224, acc.: 59.13%] [G loss: 0.7152162194252014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 6/86 [D loss: 0.6854186952114105, acc.: 55.71%] [G loss: 0.7174769639968872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 7/86 [D loss: 0.6842735111713409, acc.: 57.62%] [G loss: 0.7213713526725769]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 8/86 [D loss: 0.6852604150772095, acc.: 55.76%] [G loss: 0.7145334482192993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 9/86 [D loss: 0.6867544949054718, acc.: 54.54%] [G loss: 0.7172771692276001]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 10/86 [D loss: 0.6828681826591492, acc.: 57.52%] [G loss: 0.716763973236084]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 11/86 [D loss: 0.6871138215065002, acc.: 53.52%] [G loss: 0.7175919413566589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 12/86 [D loss: 0.6860453188419342, acc.: 55.03%] [G loss: 0.7188940048217773]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 13/86 [D loss: 0.6840801239013672, acc.: 57.23%] [G loss: 0.7140030860900879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 14/86 [D loss: 0.6889780461788177, acc.: 52.10%] [G loss: 0.7199395298957825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 15/86 [D loss: 0.6841181218624115, acc.: 56.93%] [G loss: 0.7177914381027222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 16/86 [D loss: 0.687746673822403, acc.: 53.32%] [G loss: 0.7121545672416687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 17/86 [D loss: 0.6873776018619537, acc.: 54.20%] [G loss: 0.7214431166648865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 18/86 [D loss: 0.6821740865707397, acc.: 59.08%] [G loss: 0.7185249924659729]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 19/86 [D loss: 0.6917176842689514, acc.: 52.59%] [G loss: 0.714323103427887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 20/86 [D loss: 0.6839422285556793, acc.: 58.35%] [G loss: 0.7162907123565674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 21/86 [D loss: 0.6866203248500824, acc.: 56.35%] [G loss: 0.7089296579360962]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 22/86 [D loss: 0.688233882188797, acc.: 52.69%] [G loss: 0.7195074558258057]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 23/86 [D loss: 0.6801989376544952, acc.: 60.74%] [G loss: 0.715966522693634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 24/86 [D loss: 0.6938373744487762, acc.: 50.20%] [G loss: 0.715437650680542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 25/86 [D loss: 0.6828183531761169, acc.: 58.15%] [G loss: 0.7162891030311584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 26/86 [D loss: 0.6899854242801666, acc.: 52.93%] [G loss: 0.7068890333175659]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 27/86 [D loss: 0.6856601536273956, acc.: 55.71%] [G loss: 0.7198150753974915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 28/86 [D loss: 0.6823669672012329, acc.: 57.57%] [G loss: 0.7129165530204773]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 29/86 [D loss: 0.6910536885261536, acc.: 52.69%] [G loss: 0.7137289643287659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 30/86 [D loss: 0.682931661605835, acc.: 58.94%] [G loss: 0.7145946621894836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 31/86 [D loss: 0.6887584328651428, acc.: 53.37%] [G loss: 0.7092846632003784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 32/86 [D loss: 0.6865279972553253, acc.: 55.96%] [G loss: 0.7211259007453918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 33/86 [D loss: 0.6819860339164734, acc.: 59.47%] [G loss: 0.7131381034851074]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 34/86 [D loss: 0.6898757219314575, acc.: 52.78%] [G loss: 0.7172330021858215]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 35/86 [D loss: 0.68222975730896, acc.: 59.28%] [G loss: 0.7177940607070923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 36/86 [D loss: 0.6903314590454102, acc.: 52.34%] [G loss: 0.7110801935195923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 37/86 [D loss: 0.6867158114910126, acc.: 54.74%] [G loss: 0.7221326231956482]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 38/86 [D loss: 0.6809731721878052, acc.: 59.86%] [G loss: 0.7178866267204285]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 39/86 [D loss: 0.6915484368801117, acc.: 52.20%] [G loss: 0.7186144590377808]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 40/86 [D loss: 0.6823227107524872, acc.: 58.35%] [G loss: 0.7182245850563049]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 41/86 [D loss: 0.687581866979599, acc.: 54.69%] [G loss: 0.7128152847290039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 42/86 [D loss: 0.6869018375873566, acc.: 53.86%] [G loss: 0.7222934365272522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 43/86 [D loss: 0.6819820702075958, acc.: 58.74%] [G loss: 0.7179480195045471]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 44/86 [D loss: 0.6923333704471588, acc.: 50.59%] [G loss: 0.714569091796875]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 45/86 [D loss: 0.6850452423095703, acc.: 56.59%] [G loss: 0.7228033542633057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 46/86 [D loss: 0.6854203343391418, acc.: 55.66%] [G loss: 0.7148247957229614]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 47/86 [D loss: 0.6865949928760529, acc.: 54.64%] [G loss: 0.7194532155990601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 48/86 [D loss: 0.6841179728507996, acc.: 57.67%] [G loss: 0.7218298316001892]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 49/86 [D loss: 0.6858842968940735, acc.: 55.86%] [G loss: 0.7166591882705688]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 88/200, Batch 50/86 [D loss: 0.6860845386981964, acc.: 56.49%] [G loss: 0.7228533029556274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 51/86 [D loss: 0.6845088303089142, acc.: 58.30%] [G loss: 0.7186136245727539]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 52/86 [D loss: 0.6846368908882141, acc.: 56.98%] [G loss: 0.7166754007339478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 53/86 [D loss: 0.685449630022049, acc.: 56.69%] [G loss: 0.7185847759246826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 54/86 [D loss: 0.6844545602798462, acc.: 57.28%] [G loss: 0.7186828851699829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 55/86 [D loss: 0.6867415010929108, acc.: 55.66%] [G loss: 0.7217384576797485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 56/86 [D loss: 0.6837973296642303, acc.: 58.06%] [G loss: 0.717760443687439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 57/86 [D loss: 0.6859913766384125, acc.: 56.20%] [G loss: 0.7169037461280823]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 58/86 [D loss: 0.6837002635002136, acc.: 57.47%] [G loss: 0.720188319683075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 59/86 [D loss: 0.6855108141899109, acc.: 56.20%] [G loss: 0.7198147773742676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 60/86 [D loss: 0.6840856075286865, acc.: 56.40%] [G loss: 0.7189697623252869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 61/86 [D loss: 0.68442502617836, acc.: 57.52%] [G loss: 0.7224366664886475]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 62/86 [D loss: 0.684523731470108, acc.: 56.35%] [G loss: 0.7201876044273376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 63/86 [D loss: 0.6846712529659271, acc.: 56.93%] [G loss: 0.7214438915252686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 64/86 [D loss: 0.6832473874092102, acc.: 57.32%] [G loss: 0.7207494974136353]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 65/86 [D loss: 0.6835205256938934, acc.: 56.79%] [G loss: 0.7202900052070618]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 66/86 [D loss: 0.6842888593673706, acc.: 57.28%] [G loss: 0.7186117172241211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 67/86 [D loss: 0.6848902404308319, acc.: 56.15%] [G loss: 0.7202624082565308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 68/86 [D loss: 0.6838090121746063, acc.: 56.74%] [G loss: 0.7201031446456909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 69/86 [D loss: 0.6846746206283569, acc.: 57.42%] [G loss: 0.7221274971961975]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 70/86 [D loss: 0.6861909627914429, acc.: 53.71%] [G loss: 0.7196141481399536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 71/86 [D loss: 0.6843985319137573, acc.: 57.57%] [G loss: 0.7189767360687256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 72/86 [D loss: 0.6830872595310211, acc.: 56.54%] [G loss: 0.7188677191734314]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 73/86 [D loss: 0.6842891275882721, acc.: 58.30%] [G loss: 0.7192913889884949]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 74/86 [D loss: 0.6851284801959991, acc.: 56.20%] [G loss: 0.7215197682380676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 75/86 [D loss: 0.683973491191864, acc.: 57.57%] [G loss: 0.7167922854423523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 76/86 [D loss: 0.6862260103225708, acc.: 56.05%] [G loss: 0.7184616923332214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 77/86 [D loss: 0.6848291456699371, acc.: 56.69%] [G loss: 0.7199575304985046]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 78/86 [D loss: 0.6869140565395355, acc.: 54.59%] [G loss: 0.7225842475891113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 79/86 [D loss: 0.6828209161758423, acc.: 58.25%] [G loss: 0.7211304306983948]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 80/86 [D loss: 0.6832473874092102, acc.: 57.57%] [G loss: 0.7187244892120361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 81/86 [D loss: 0.68720343708992, acc.: 54.64%] [G loss: 0.7235040664672852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 82/86 [D loss: 0.6829308569431305, acc.: 59.08%] [G loss: 0.7182343006134033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 83/86 [D loss: 0.6859691441059113, acc.: 55.32%] [G loss: 0.7199503183364868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 84/86 [D loss: 0.6853788793087006, acc.: 57.03%] [G loss: 0.7197859287261963]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 85/86 [D loss: 0.6842837631702423, acc.: 56.64%] [G loss: 0.7189728617668152]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 86/86 [D loss: 0.6848161518573761, acc.: 56.69%] [G loss: 0.7226249575614929]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 1/86 [D loss: 0.6844435036182404, acc.: 56.59%] [G loss: 0.7204137444496155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 2/86 [D loss: 0.6857180297374725, acc.: 56.64%] [G loss: 0.7177582383155823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 3/86 [D loss: 0.6844711303710938, acc.: 55.96%] [G loss: 0.7199978828430176]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 4/86 [D loss: 0.683598130941391, acc.: 58.64%] [G loss: 0.7187104225158691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 5/86 [D loss: 0.6865480542182922, acc.: 54.30%] [G loss: 0.7207337617874146]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 6/86 [D loss: 0.6855882704257965, acc.: 56.69%] [G loss: 0.7221558094024658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 7/86 [D loss: 0.6846722364425659, acc.: 56.30%] [G loss: 0.7193915843963623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 8/86 [D loss: 0.6869587302207947, acc.: 54.93%] [G loss: 0.7219420671463013]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 9/86 [D loss: 0.6863496601581573, acc.: 55.27%] [G loss: 0.7208028435707092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 10/86 [D loss: 0.6843277812004089, acc.: 57.37%] [G loss: 0.7200679779052734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 11/86 [D loss: 0.6844609677791595, acc.: 56.88%] [G loss: 0.7196166515350342]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 12/86 [D loss: 0.6826078593730927, acc.: 59.47%] [G loss: 0.7202479243278503]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 13/86 [D loss: 0.6856322586536407, acc.: 55.47%] [G loss: 0.7208447456359863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 14/86 [D loss: 0.6827256381511688, acc.: 58.79%] [G loss: 0.719055712223053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 15/86 [D loss: 0.6861270070075989, acc.: 56.40%] [G loss: 0.7175325751304626]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 16/86 [D loss: 0.6842374205589294, acc.: 58.15%] [G loss: 0.7204791307449341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 17/86 [D loss: 0.6862505674362183, acc.: 54.74%] [G loss: 0.720766544342041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 18/86 [D loss: 0.685492217540741, acc.: 56.98%] [G loss: 0.7207158803939819]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 19/86 [D loss: 0.6836084425449371, acc.: 56.64%] [G loss: 0.7220063805580139]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 20/86 [D loss: 0.6852296590805054, acc.: 56.98%] [G loss: 0.7199375033378601]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 21/86 [D loss: 0.6848503649234772, acc.: 56.45%] [G loss: 0.7199500799179077]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 22/86 [D loss: 0.6843521296977997, acc.: 56.88%] [G loss: 0.7233215570449829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 23/86 [D loss: 0.6851462423801422, acc.: 55.81%] [G loss: 0.7195929288864136]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 24/86 [D loss: 0.6845390200614929, acc.: 56.40%] [G loss: 0.7199172377586365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 25/86 [D loss: 0.6854847073554993, acc.: 56.69%] [G loss: 0.7217206358909607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 26/86 [D loss: 0.6840302348136902, acc.: 57.18%] [G loss: 0.719106912612915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 27/86 [D loss: 0.685803234577179, acc.: 56.59%] [G loss: 0.7196860313415527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 28/86 [D loss: 0.685743898153305, acc.: 55.71%] [G loss: 0.7198611497879028]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 29/86 [D loss: 0.6836846470832825, acc.: 57.18%] [G loss: 0.7212649583816528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 30/86 [D loss: 0.6847482621669769, acc.: 55.57%] [G loss: 0.7224164605140686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 31/86 [D loss: 0.6848920583724976, acc.: 56.05%] [G loss: 0.7163928747177124]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 32/86 [D loss: 0.6839024722576141, acc.: 57.28%] [G loss: 0.7191205620765686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 33/86 [D loss: 0.6847053468227386, acc.: 56.54%] [G loss: 0.7201540470123291]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 34/86 [D loss: 0.683368593454361, acc.: 57.13%] [G loss: 0.7222914099693298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 35/86 [D loss: 0.6850629448890686, acc.: 56.35%] [G loss: 0.7187367081642151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 36/86 [D loss: 0.6840912997722626, acc.: 57.13%] [G loss: 0.7214377522468567]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 37/86 [D loss: 0.6829854846000671, acc.: 57.32%] [G loss: 0.7230944037437439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 38/86 [D loss: 0.6835969984531403, acc.: 56.45%] [G loss: 0.7205327153205872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 39/86 [D loss: 0.6861176788806915, acc.: 55.13%] [G loss: 0.7209272384643555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 40/86 [D loss: 0.6840126514434814, acc.: 56.59%] [G loss: 0.7217373847961426]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 41/86 [D loss: 0.685202032327652, acc.: 56.25%] [G loss: 0.7206992506980896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 42/86 [D loss: 0.684166669845581, acc.: 57.52%] [G loss: 0.7197352647781372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 43/86 [D loss: 0.6861574053764343, acc.: 55.57%] [G loss: 0.7216562032699585]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 44/86 [D loss: 0.6857176423072815, acc.: 56.59%] [G loss: 0.7234503030776978]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 45/86 [D loss: 0.6836108267307281, acc.: 57.23%] [G loss: 0.7200074195861816]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 46/86 [D loss: 0.6842705607414246, acc.: 55.47%] [G loss: 0.7191265821456909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 47/86 [D loss: 0.6852038204669952, acc.: 56.54%] [G loss: 0.7181144952774048]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 48/86 [D loss: 0.68450927734375, acc.: 56.40%] [G loss: 0.7147393226623535]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 49/86 [D loss: 0.6860097050666809, acc.: 56.74%] [G loss: 0.7191454768180847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 50/86 [D loss: 0.6831881701946259, acc.: 58.45%] [G loss: 0.7186875343322754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 51/86 [D loss: 0.6872170865535736, acc.: 54.69%] [G loss: 0.7164811491966248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 52/86 [D loss: 0.685410350561142, acc.: 57.42%] [G loss: 0.7169952392578125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 53/86 [D loss: 0.6864527463912964, acc.: 55.37%] [G loss: 0.7169085741043091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 54/86 [D loss: 0.6891171932220459, acc.: 52.20%] [G loss: 0.7225087285041809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 55/86 [D loss: 0.6851552426815033, acc.: 56.64%] [G loss: 0.7179688215255737]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 89/200, Batch 56/86 [D loss: 0.6875743269920349, acc.: 53.27%] [G loss: 0.7192363142967224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 57/86 [D loss: 0.6871316134929657, acc.: 53.32%] [G loss: 0.7210423350334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 58/86 [D loss: 0.6849312484264374, acc.: 56.64%] [G loss: 0.7131515145301819]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 59/86 [D loss: 0.6867561042308807, acc.: 55.03%] [G loss: 0.729775071144104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 60/86 [D loss: 0.6831028461456299, acc.: 58.15%] [G loss: 0.7143965363502502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 61/86 [D loss: 0.6908035278320312, acc.: 51.12%] [G loss: 0.7178734540939331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 62/86 [D loss: 0.6830517053604126, acc.: 57.32%] [G loss: 0.7235561013221741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 63/86 [D loss: 0.6879257261753082, acc.: 54.00%] [G loss: 0.7156990766525269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 64/86 [D loss: 0.6868237555027008, acc.: 56.49%] [G loss: 0.7192403674125671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 65/86 [D loss: 0.6864686906337738, acc.: 54.88%] [G loss: 0.7104660272598267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 66/86 [D loss: 0.687054306268692, acc.: 53.32%] [G loss: 0.7176233530044556]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 67/86 [D loss: 0.6833194494247437, acc.: 55.66%] [G loss: 0.7181943655014038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 68/86 [D loss: 0.6920419335365295, acc.: 50.49%] [G loss: 0.7134345173835754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 69/86 [D loss: 0.6829044818878174, acc.: 57.42%] [G loss: 0.7212063670158386]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 70/86 [D loss: 0.6908370852470398, acc.: 52.49%] [G loss: 0.7082288861274719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 71/86 [D loss: 0.6877486109733582, acc.: 53.96%] [G loss: 0.7210730314254761]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 72/86 [D loss: 0.6827508211135864, acc.: 58.20%] [G loss: 0.7069578170776367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 73/86 [D loss: 0.6974168419837952, acc.: 46.88%] [G loss: 0.7230836749076843]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 74/86 [D loss: 0.6776339411735535, acc.: 61.67%] [G loss: 0.7089417576789856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 75/86 [D loss: 0.6933011710643768, acc.: 50.29%] [G loss: 0.7072524428367615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 76/86 [D loss: 0.684913694858551, acc.: 56.30%] [G loss: 0.7226187586784363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 77/86 [D loss: 0.6806725561618805, acc.: 59.13%] [G loss: 0.7128127813339233]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 78/86 [D loss: 0.6925521194934845, acc.: 51.03%] [G loss: 0.7179960608482361]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 79/86 [D loss: 0.6815516650676727, acc.: 59.52%] [G loss: 0.7154273986816406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 80/86 [D loss: 0.6935765445232391, acc.: 49.61%] [G loss: 0.7057084441184998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 81/86 [D loss: 0.6872190535068512, acc.: 54.20%] [G loss: 0.7233432531356812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 82/86 [D loss: 0.6818543374538422, acc.: 58.94%] [G loss: 0.7164496779441833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 83/86 [D loss: 0.6945282518863678, acc.: 50.05%] [G loss: 0.7183557748794556]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 84/86 [D loss: 0.6833871603012085, acc.: 57.62%] [G loss: 0.7176403999328613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 85/86 [D loss: 0.6876545548439026, acc.: 54.49%] [G loss: 0.7075817584991455]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 86/86 [D loss: 0.6859908103942871, acc.: 55.13%] [G loss: 0.7222389578819275]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 1/86 [D loss: 0.6804295480251312, acc.: 60.06%] [G loss: 0.7164947390556335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 2/86 [D loss: 0.6937871873378754, acc.: 50.29%] [G loss: 0.7113621234893799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 3/86 [D loss: 0.6845851838588715, acc.: 56.64%] [G loss: 0.7239494323730469]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 4/86 [D loss: 0.6865725517272949, acc.: 54.54%] [G loss: 0.712431788444519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 5/86 [D loss: 0.6870361268520355, acc.: 54.98%] [G loss: 0.7236875295639038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 6/86 [D loss: 0.6825718581676483, acc.: 57.18%] [G loss: 0.7208111882209778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 7/86 [D loss: 0.6891893744468689, acc.: 53.91%] [G loss: 0.7155154347419739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 8/86 [D loss: 0.6851939260959625, acc.: 57.47%] [G loss: 0.7223485708236694]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 9/86 [D loss: 0.6870477795600891, acc.: 55.37%] [G loss: 0.7178294658660889]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 10/86 [D loss: 0.6865282654762268, acc.: 55.86%] [G loss: 0.7200380563735962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 11/86 [D loss: 0.6828044652938843, acc.: 58.20%] [G loss: 0.7182617783546448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 12/86 [D loss: 0.6852232813835144, acc.: 56.64%] [G loss: 0.7176653742790222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 13/86 [D loss: 0.683679610490799, acc.: 57.13%] [G loss: 0.7230827212333679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 14/86 [D loss: 0.6831922233104706, acc.: 57.76%] [G loss: 0.7195096015930176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 15/86 [D loss: 0.6850430965423584, acc.: 57.23%] [G loss: 0.7193264961242676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 16/86 [D loss: 0.6857340633869171, acc.: 55.08%] [G loss: 0.7211817502975464]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 17/86 [D loss: 0.6835631728172302, acc.: 58.11%] [G loss: 0.719435453414917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 18/86 [D loss: 0.6839604675769806, acc.: 57.32%] [G loss: 0.719244122505188]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 19/86 [D loss: 0.683464765548706, acc.: 56.20%] [G loss: 0.7198307514190674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 20/86 [D loss: 0.6848711967468262, acc.: 56.84%] [G loss: 0.716484546661377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 21/86 [D loss: 0.6848291754722595, acc.: 56.79%] [G loss: 0.7182137966156006]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 22/86 [D loss: 0.6829643845558167, acc.: 57.47%] [G loss: 0.718959629535675]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 23/86 [D loss: 0.6862240135669708, acc.: 54.74%] [G loss: 0.717285692691803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 24/86 [D loss: 0.683736652135849, acc.: 57.62%] [G loss: 0.7185449600219727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 25/86 [D loss: 0.6842943131923676, acc.: 56.74%] [G loss: 0.7179017066955566]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 26/86 [D loss: 0.6848699450492859, acc.: 55.91%] [G loss: 0.7168382406234741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 27/86 [D loss: 0.6831566691398621, acc.: 57.08%] [G loss: 0.7216616272926331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 28/86 [D loss: 0.68595951795578, acc.: 55.81%] [G loss: 0.7185649275779724]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 29/86 [D loss: 0.6858092546463013, acc.: 55.91%] [G loss: 0.7198606729507446]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 30/86 [D loss: 0.684680849313736, acc.: 56.05%] [G loss: 0.721893310546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 31/86 [D loss: 0.6839242577552795, acc.: 55.52%] [G loss: 0.7194260954856873]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 32/86 [D loss: 0.683927059173584, acc.: 56.49%] [G loss: 0.7228960990905762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 33/86 [D loss: 0.6845141649246216, acc.: 55.81%] [G loss: 0.7196855545043945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 34/86 [D loss: 0.6861604154109955, acc.: 55.22%] [G loss: 0.7227113842964172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 35/86 [D loss: 0.6805345416069031, acc.: 59.47%] [G loss: 0.7219910025596619]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 36/86 [D loss: 0.6849022805690765, acc.: 55.57%] [G loss: 0.7208254933357239]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 37/86 [D loss: 0.682155042886734, acc.: 56.35%] [G loss: 0.7225752472877502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 38/86 [D loss: 0.6837754845619202, acc.: 56.40%] [G loss: 0.7198749780654907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 39/86 [D loss: 0.68514284491539, acc.: 55.71%] [G loss: 0.7233454585075378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 40/86 [D loss: 0.6823360323905945, acc.: 58.25%] [G loss: 0.7218483686447144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 41/86 [D loss: 0.6848657429218292, acc.: 55.91%] [G loss: 0.7216596603393555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 42/86 [D loss: 0.6852456629276276, acc.: 56.54%] [G loss: 0.7259819507598877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 43/86 [D loss: 0.6833820641040802, acc.: 58.25%] [G loss: 0.7207356095314026]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 44/86 [D loss: 0.6845496594905853, acc.: 56.69%] [G loss: 0.7244364023208618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 45/86 [D loss: 0.682762861251831, acc.: 57.57%] [G loss: 0.7224700450897217]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 46/86 [D loss: 0.6853445172309875, acc.: 56.20%] [G loss: 0.7183368802070618]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 47/86 [D loss: 0.6842368841171265, acc.: 56.59%] [G loss: 0.7195793390274048]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 48/86 [D loss: 0.6835885047912598, acc.: 57.32%] [G loss: 0.719774603843689]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 49/86 [D loss: 0.6865753531455994, acc.: 55.32%] [G loss: 0.7230436205863953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 50/86 [D loss: 0.6808931231498718, acc.: 59.77%] [G loss: 0.7194386720657349]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 51/86 [D loss: 0.6869516670703888, acc.: 55.81%] [G loss: 0.7190134525299072]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 52/86 [D loss: 0.6842465996742249, acc.: 56.98%] [G loss: 0.721879780292511]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 53/86 [D loss: 0.6835476756095886, acc.: 56.93%] [G loss: 0.7151408195495605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 54/86 [D loss: 0.6882710456848145, acc.: 54.05%] [G loss: 0.7224420309066772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 55/86 [D loss: 0.681349366903305, acc.: 59.33%] [G loss: 0.7165042757987976]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 56/86 [D loss: 0.688001960515976, acc.: 54.30%] [G loss: 0.7153845429420471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 57/86 [D loss: 0.685234397649765, acc.: 56.45%] [G loss: 0.7232438325881958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 58/86 [D loss: 0.6828030347824097, acc.: 57.91%] [G loss: 0.7126122117042542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 59/86 [D loss: 0.6888402700424194, acc.: 53.32%] [G loss: 0.7234945297241211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 60/86 [D loss: 0.6813571155071259, acc.: 59.18%] [G loss: 0.7146802544593811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 61/86 [D loss: 0.6909540295600891, acc.: 52.10%] [G loss: 0.7118039131164551]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 62/86 [D loss: 0.6853339970111847, acc.: 54.25%] [G loss: 0.7261251211166382]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 63/86 [D loss: 0.6817239224910736, acc.: 58.40%] [G loss: 0.7072084546089172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 64/86 [D loss: 0.6944268643856049, acc.: 49.32%] [G loss: 0.7209486961364746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 65/86 [D loss: 0.6780765056610107, acc.: 60.21%] [G loss: 0.7175098061561584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 66/86 [D loss: 0.6918087601661682, acc.: 50.73%] [G loss: 0.7107307314872742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 67/86 [D loss: 0.6874698400497437, acc.: 53.03%] [G loss: 0.7248586416244507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 68/86 [D loss: 0.6803810298442841, acc.: 60.25%] [G loss: 0.711902916431427]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 69/86 [D loss: 0.6955114603042603, acc.: 48.58%] [G loss: 0.7191014289855957]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 70/86 [D loss: 0.6785595417022705, acc.: 60.45%] [G loss: 0.7192803621292114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 71/86 [D loss: 0.6923581659793854, acc.: 51.51%] [G loss: 0.7062789797782898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 72/86 [D loss: 0.6865251958370209, acc.: 54.98%] [G loss: 0.7214820384979248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 73/86 [D loss: 0.6798099875450134, acc.: 59.86%] [G loss: 0.711250364780426]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 74/86 [D loss: 0.6926800906658173, acc.: 50.39%] [G loss: 0.7103548049926758]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 75/86 [D loss: 0.6845867931842804, acc.: 55.57%] [G loss: 0.7224367260932922]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 76/86 [D loss: 0.686305969953537, acc.: 54.35%] [G loss: 0.7063672542572021]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 77/86 [D loss: 0.6867939829826355, acc.: 53.56%] [G loss: 0.719103991985321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 78/86 [D loss: 0.6802632212638855, acc.: 59.08%] [G loss: 0.7162012457847595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 79/86 [D loss: 0.6908330917358398, acc.: 51.61%] [G loss: 0.7116536498069763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 80/86 [D loss: 0.6852012872695923, acc.: 56.01%] [G loss: 0.7219460010528564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 81/86 [D loss: 0.6861835718154907, acc.: 54.93%] [G loss: 0.7130048871040344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 82/86 [D loss: 0.6846969127655029, acc.: 55.37%] [G loss: 0.7193500399589539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 83/86 [D loss: 0.6847727596759796, acc.: 57.03%] [G loss: 0.7198814153671265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 84/86 [D loss: 0.683834046125412, acc.: 58.30%] [G loss: 0.7155779004096985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 85/86 [D loss: 0.6845309436321259, acc.: 56.93%] [G loss: 0.72044438123703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 86/86 [D loss: 0.6855500638484955, acc.: 56.25%] [G loss: 0.7199567556381226]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 1/86 [D loss: 0.6858889162540436, acc.: 53.66%] [G loss: 0.7193899154663086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 2/86 [D loss: 0.6832405924797058, acc.: 58.74%] [G loss: 0.721463680267334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 3/86 [D loss: 0.6830960512161255, acc.: 57.28%] [G loss: 0.720268964767456]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 4/86 [D loss: 0.6853133141994476, acc.: 56.54%] [G loss: 0.7222661972045898]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 5/86 [D loss: 0.6836768686771393, acc.: 56.93%] [G loss: 0.722073495388031]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 6/86 [D loss: 0.6837745904922485, acc.: 57.13%] [G loss: 0.7209079265594482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 7/86 [D loss: 0.6845177710056305, acc.: 56.74%] [G loss: 0.7224432229995728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 8/86 [D loss: 0.6859158575534821, acc.: 55.13%] [G loss: 0.7233593463897705]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 9/86 [D loss: 0.6835964918136597, acc.: 57.67%] [G loss: 0.7196148633956909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 10/86 [D loss: 0.6863492131233215, acc.: 54.64%] [G loss: 0.7236067652702332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 11/86 [D loss: 0.6836029291152954, acc.: 58.06%] [G loss: 0.718981146812439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 12/86 [D loss: 0.683456301689148, acc.: 57.76%] [G loss: 0.7180370092391968]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 13/86 [D loss: 0.6842707693576813, acc.: 56.05%] [G loss: 0.7218316197395325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 14/86 [D loss: 0.684583306312561, acc.: 57.08%] [G loss: 0.7219036817550659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 15/86 [D loss: 0.6834483742713928, acc.: 57.37%] [G loss: 0.7199723720550537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 16/86 [D loss: 0.683939516544342, acc.: 57.13%] [G loss: 0.7209844589233398]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 17/86 [D loss: 0.6841844320297241, acc.: 56.10%] [G loss: 0.7199172377586365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 18/86 [D loss: 0.6860533952713013, acc.: 54.98%] [G loss: 0.7218335866928101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 19/86 [D loss: 0.6854982376098633, acc.: 55.71%] [G loss: 0.7224606871604919]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 20/86 [D loss: 0.6823138296604156, acc.: 57.96%] [G loss: 0.7219873070716858]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 21/86 [D loss: 0.68678417801857, acc.: 55.27%] [G loss: 0.7219403386116028]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 22/86 [D loss: 0.683307558298111, acc.: 55.42%] [G loss: 0.7209228277206421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 23/86 [D loss: 0.6851764023303986, acc.: 55.47%] [G loss: 0.7219065427780151]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 24/86 [D loss: 0.686182290315628, acc.: 55.13%] [G loss: 0.7214829921722412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 25/86 [D loss: 0.6821798384189606, acc.: 58.01%] [G loss: 0.7215338945388794]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 26/86 [D loss: 0.6834012866020203, acc.: 55.96%] [G loss: 0.7202354669570923]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 27/86 [D loss: 0.6843386292457581, acc.: 57.47%] [G loss: 0.721872091293335]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 28/86 [D loss: 0.6847794950008392, acc.: 56.69%] [G loss: 0.7224853038787842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 29/86 [D loss: 0.6840466856956482, acc.: 56.45%] [G loss: 0.7187056541442871]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 30/86 [D loss: 0.6839073598384857, acc.: 57.76%] [G loss: 0.7253092527389526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 31/86 [D loss: 0.6833207607269287, acc.: 57.32%] [G loss: 0.7218008041381836]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 32/86 [D loss: 0.6845232248306274, acc.: 56.05%] [G loss: 0.7238866090774536]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 33/86 [D loss: 0.6842953562736511, acc.: 54.79%] [G loss: 0.7232812643051147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 34/86 [D loss: 0.6855016052722931, acc.: 55.66%] [G loss: 0.7224678993225098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 35/86 [D loss: 0.6858003437519073, acc.: 56.84%] [G loss: 0.7215385437011719]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 36/86 [D loss: 0.6822578608989716, acc.: 57.67%] [G loss: 0.7238099575042725]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 37/86 [D loss: 0.6846623718738556, acc.: 56.59%] [G loss: 0.7212758660316467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 38/86 [D loss: 0.6833313703536987, acc.: 56.84%] [G loss: 0.7197780609130859]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 39/86 [D loss: 0.6829963624477386, acc.: 58.84%] [G loss: 0.720572292804718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 40/86 [D loss: 0.6859821975231171, acc.: 54.88%] [G loss: 0.7230013012886047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 41/86 [D loss: 0.683655709028244, acc.: 56.74%] [G loss: 0.7236653566360474]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 42/86 [D loss: 0.685517430305481, acc.: 57.08%] [G loss: 0.7237966060638428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 43/86 [D loss: 0.6838991045951843, acc.: 56.79%] [G loss: 0.721406102180481]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 44/86 [D loss: 0.6826435923576355, acc.: 57.28%] [G loss: 0.7203165888786316]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 45/86 [D loss: 0.6860446035861969, acc.: 54.98%] [G loss: 0.7192214131355286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 46/86 [D loss: 0.6843335032463074, acc.: 56.10%] [G loss: 0.7220455408096313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 47/86 [D loss: 0.6848027408123016, acc.: 56.20%] [G loss: 0.7172865867614746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 48/86 [D loss: 0.6863469481468201, acc.: 55.47%] [G loss: 0.7191059589385986]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 49/86 [D loss: 0.6823140680789948, acc.: 57.08%] [G loss: 0.7172254323959351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 50/86 [D loss: 0.6828398704528809, acc.: 57.52%] [G loss: 0.7216112017631531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 51/86 [D loss: 0.6827050745487213, acc.: 58.59%] [G loss: 0.7232564687728882]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 52/86 [D loss: 0.6836057007312775, acc.: 56.64%] [G loss: 0.7175896763801575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 53/86 [D loss: 0.6855462193489075, acc.: 55.81%] [G loss: 0.7206698060035706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 54/86 [D loss: 0.6823307871818542, acc.: 58.20%] [G loss: 0.7214418053627014]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 55/86 [D loss: 0.685762345790863, acc.: 54.30%] [G loss: 0.7199174165725708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 56/86 [D loss: 0.6867771148681641, acc.: 55.81%] [G loss: 0.7248350381851196]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 57/86 [D loss: 0.6823663115501404, acc.: 57.23%] [G loss: 0.7232213020324707]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 58/86 [D loss: 0.6865279376506805, acc.: 53.81%] [G loss: 0.7162492871284485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 59/86 [D loss: 0.6851297914981842, acc.: 57.03%] [G loss: 0.7217575907707214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 60/86 [D loss: 0.6866052746772766, acc.: 54.49%] [G loss: 0.7166597247123718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 61/86 [D loss: 0.686646580696106, acc.: 54.25%] [G loss: 0.7196893692016602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 62/86 [D loss: 0.6829011738300323, acc.: 57.03%] [G loss: 0.7240700721740723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 63/86 [D loss: 0.6835847496986389, acc.: 56.79%] [G loss: 0.7176862359046936]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 64/86 [D loss: 0.6827681958675385, acc.: 57.18%] [G loss: 0.7222407460212708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 65/86 [D loss: 0.6833427846431732, acc.: 57.03%] [G loss: 0.7172849178314209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 66/86 [D loss: 0.6870748698711395, acc.: 53.61%] [G loss: 0.7180742621421814]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 67/86 [D loss: 0.6860367953777313, acc.: 54.69%] [G loss: 0.7246674299240112]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 68/86 [D loss: 0.6849671006202698, acc.: 55.42%] [G loss: 0.7179158926010132]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 69/86 [D loss: 0.6867249608039856, acc.: 54.69%] [G loss: 0.7248840928077698]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 70/86 [D loss: 0.6830137073993683, acc.: 57.23%] [G loss: 0.7202613949775696]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 71/86 [D loss: 0.6887357234954834, acc.: 53.12%] [G loss: 0.7193028926849365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 72/86 [D loss: 0.6832334101200104, acc.: 57.81%] [G loss: 0.7266827821731567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 73/86 [D loss: 0.6846257448196411, acc.: 55.66%] [G loss: 0.7154276371002197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 74/86 [D loss: 0.6870520412921906, acc.: 54.20%] [G loss: 0.722220778465271]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 75/86 [D loss: 0.6806888580322266, acc.: 58.30%] [G loss: 0.7152841687202454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 76/86 [D loss: 0.6867332756519318, acc.: 54.79%] [G loss: 0.7195463180541992]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 77/86 [D loss: 0.6871514320373535, acc.: 53.66%] [G loss: 0.726146936416626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 78/86 [D loss: 0.6829160749912262, acc.: 57.91%] [G loss: 0.7177156209945679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 79/86 [D loss: 0.6876972615718842, acc.: 53.96%] [G loss: 0.7270355820655823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 80/86 [D loss: 0.6802845299243927, acc.: 58.98%] [G loss: 0.7204099893569946]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 81/86 [D loss: 0.690123587846756, acc.: 52.54%] [G loss: 0.7153226137161255]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 82/86 [D loss: 0.6842232942581177, acc.: 55.96%] [G loss: 0.7285006046295166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 83/86 [D loss: 0.6836227178573608, acc.: 56.88%] [G loss: 0.7152283191680908]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 84/86 [D loss: 0.6914601624011993, acc.: 52.05%] [G loss: 0.7214502692222595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 85/86 [D loss: 0.682541161775589, acc.: 57.23%] [G loss: 0.7211184501647949]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 86/86 [D loss: 0.6880137622356415, acc.: 53.71%] [G loss: 0.717096745967865]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 1/86 [D loss: 0.6826935112476349, acc.: 56.84%] [G loss: 0.7241911292076111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 2/86 [D loss: 0.6819841861724854, acc.: 58.50%] [G loss: 0.7185742855072021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 3/86 [D loss: 0.6908809244632721, acc.: 52.83%] [G loss: 0.7198052406311035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 4/86 [D loss: 0.6825348734855652, acc.: 56.25%] [G loss: 0.7201687097549438]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 5/86 [D loss: 0.6850080788135529, acc.: 54.54%] [G loss: 0.7139267921447754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 6/86 [D loss: 0.6868722438812256, acc.: 54.93%] [G loss: 0.7230145335197449]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 7/86 [D loss: 0.6800668835639954, acc.: 58.25%] [G loss: 0.719459593296051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 8/86 [D loss: 0.687819093465805, acc.: 53.12%] [G loss: 0.7149602770805359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 9/86 [D loss: 0.6840987503528595, acc.: 57.08%] [G loss: 0.7232935428619385]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 10/86 [D loss: 0.6835914850234985, acc.: 56.93%] [G loss: 0.7142548561096191]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 11/86 [D loss: 0.6862199902534485, acc.: 55.37%] [G loss: 0.7215908765792847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 12/86 [D loss: 0.6819643378257751, acc.: 58.15%] [G loss: 0.7226715683937073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 13/86 [D loss: 0.6878780424594879, acc.: 54.49%] [G loss: 0.7142794728279114]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 14/86 [D loss: 0.6886590719223022, acc.: 53.22%] [G loss: 0.7273436784744263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 15/86 [D loss: 0.6816459596157074, acc.: 59.08%] [G loss: 0.7204233407974243]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 16/86 [D loss: 0.6857273280620575, acc.: 55.66%] [G loss: 0.7201573848724365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 17/86 [D loss: 0.6836503446102142, acc.: 57.32%] [G loss: 0.7253549098968506]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 18/86 [D loss: 0.6819893717765808, acc.: 58.89%] [G loss: 0.7205225825309753]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 19/86 [D loss: 0.6892763674259186, acc.: 53.08%] [G loss: 0.722335934638977]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 20/86 [D loss: 0.6828226447105408, acc.: 57.86%] [G loss: 0.7236777544021606]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 21/86 [D loss: 0.6853106021881104, acc.: 55.57%] [G loss: 0.7195004820823669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 22/86 [D loss: 0.6850422024726868, acc.: 55.18%] [G loss: 0.7230419516563416]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 23/86 [D loss: 0.6802711188793182, acc.: 58.79%] [G loss: 0.7240858674049377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 24/86 [D loss: 0.6896642446517944, acc.: 52.83%] [G loss: 0.7194882035255432]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 25/86 [D loss: 0.6818522214889526, acc.: 58.50%] [G loss: 0.7262383103370667]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 26/86 [D loss: 0.6845089197158813, acc.: 56.10%] [G loss: 0.7180730104446411]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 27/86 [D loss: 0.6848811507225037, acc.: 56.15%] [G loss: 0.7235768437385559]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 28/86 [D loss: 0.6828444302082062, acc.: 58.94%] [G loss: 0.7250527739524841]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 29/86 [D loss: 0.6863011419773102, acc.: 54.69%] [G loss: 0.7226450443267822]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 30/86 [D loss: 0.68406081199646, acc.: 56.35%] [G loss: 0.7265073657035828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 31/86 [D loss: 0.6849468052387238, acc.: 55.66%] [G loss: 0.7189421653747559]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 32/86 [D loss: 0.6856925189495087, acc.: 55.42%] [G loss: 0.7252465486526489]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 33/86 [D loss: 0.682910829782486, acc.: 57.13%] [G loss: 0.7216789722442627]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 34/86 [D loss: 0.6848742365837097, acc.: 55.86%] [G loss: 0.7218923568725586]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 35/86 [D loss: 0.6860924661159515, acc.: 55.91%] [G loss: 0.722703754901886]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 36/86 [D loss: 0.6824641227722168, acc.: 56.59%] [G loss: 0.7256443500518799]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 37/86 [D loss: 0.6840980052947998, acc.: 55.52%] [G loss: 0.726454496383667]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 38/86 [D loss: 0.6821835339069366, acc.: 57.96%] [G loss: 0.7244560718536377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 39/86 [D loss: 0.6829414963722229, acc.: 57.57%] [G loss: 0.7200192213058472]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 40/86 [D loss: 0.685307115316391, acc.: 56.59%] [G loss: 0.7233942747116089]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 41/86 [D loss: 0.6839154958724976, acc.: 57.03%] [G loss: 0.7225983142852783]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 42/86 [D loss: 0.6854429841041565, acc.: 54.79%] [G loss: 0.7224697470664978]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 43/86 [D loss: 0.6836880743503571, acc.: 56.05%] [G loss: 0.7196024060249329]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 44/86 [D loss: 0.6810154020786285, acc.: 57.91%] [G loss: 0.72209233045578]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 45/86 [D loss: 0.6861453056335449, acc.: 54.64%] [G loss: 0.7203740477561951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 46/86 [D loss: 0.6832273006439209, acc.: 57.23%] [G loss: 0.7231022715568542]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 47/86 [D loss: 0.683748722076416, acc.: 55.81%] [G loss: 0.7199996709823608]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 48/86 [D loss: 0.684422492980957, acc.: 56.10%] [G loss: 0.7207088470458984]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 49/86 [D loss: 0.681763082742691, acc.: 58.98%] [G loss: 0.7219852805137634]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 50/86 [D loss: 0.6846479177474976, acc.: 56.10%] [G loss: 0.7222400903701782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 51/86 [D loss: 0.6841985881328583, acc.: 56.20%] [G loss: 0.7214437127113342]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 52/86 [D loss: 0.682377964258194, acc.: 59.33%] [G loss: 0.7210205793380737]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 53/86 [D loss: 0.6837868690490723, acc.: 56.25%] [G loss: 0.7221947312355042]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 54/86 [D loss: 0.6821650266647339, acc.: 56.30%] [G loss: 0.7199394702911377]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 55/86 [D loss: 0.6844869256019592, acc.: 56.30%] [G loss: 0.720285177230835]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 56/86 [D loss: 0.6842409372329712, acc.: 55.96%] [G loss: 0.7203571200370789]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 57/86 [D loss: 0.6823097467422485, acc.: 58.20%] [G loss: 0.7210385203361511]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 58/86 [D loss: 0.6841664910316467, acc.: 56.64%] [G loss: 0.7242385149002075]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 59/86 [D loss: 0.6825290024280548, acc.: 57.71%] [G loss: 0.725834846496582]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 60/86 [D loss: 0.6826176643371582, acc.: 57.28%] [G loss: 0.720806360244751]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 61/86 [D loss: 0.6859007775783539, acc.: 55.52%] [G loss: 0.7232505083084106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 62/86 [D loss: 0.6834122538566589, acc.: 56.45%] [G loss: 0.7192243933677673]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 63/86 [D loss: 0.6852043569087982, acc.: 55.18%] [G loss: 0.7222613096237183]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 64/86 [D loss: 0.6822943687438965, acc.: 58.15%] [G loss: 0.7210077047348022]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 65/86 [D loss: 0.6836352944374084, acc.: 57.18%] [G loss: 0.7181656360626221]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 66/86 [D loss: 0.6857621967792511, acc.: 54.54%] [G loss: 0.7209955453872681]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 67/86 [D loss: 0.6838731169700623, acc.: 56.35%] [G loss: 0.7241929769515991]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 92/200, Batch 68/86 [D loss: 0.684017539024353, acc.: 56.74%] [G loss: 0.720148503780365]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 69/86 [D loss: 0.6856274902820587, acc.: 54.59%] [G loss: 0.7230730652809143]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 92/200, Batch 70/86 [D loss: 0.6837368905544281, acc.: 56.93%] [G loss: 0.7250946164131165]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 71/86 [D loss: 0.6848764717578888, acc.: 55.42%] [G loss: 0.723613977432251]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 72/86 [D loss: 0.6837178468704224, acc.: 57.23%] [G loss: 0.7225162386894226]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 73/86 [D loss: 0.6842981278896332, acc.: 57.57%] [G loss: 0.7192355990409851]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 74/86 [D loss: 0.6854965686798096, acc.: 53.81%] [G loss: 0.723610520362854]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 75/86 [D loss: 0.6828937828540802, acc.: 57.37%] [G loss: 0.723456084728241]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 76/86 [D loss: 0.685806542634964, acc.: 55.66%] [G loss: 0.7175219058990479]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 77/86 [D loss: 0.6861549913883209, acc.: 56.40%] [G loss: 0.7256072163581848]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 78/86 [D loss: 0.6823281943798065, acc.: 57.76%] [G loss: 0.7202218770980835]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 79/86 [D loss: 0.6865347921848297, acc.: 53.61%] [G loss: 0.7218965291976929]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 80/86 [D loss: 0.6835323572158813, acc.: 57.13%] [G loss: 0.7236652374267578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 81/86 [D loss: 0.6848816275596619, acc.: 55.22%] [G loss: 0.7183371186256409]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 82/86 [D loss: 0.6858214139938354, acc.: 56.59%] [G loss: 0.7249992489814758]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 83/86 [D loss: 0.6807776987552643, acc.: 58.59%] [G loss: 0.7234275341033936]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 84/86 [D loss: 0.6856541037559509, acc.: 54.93%] [G loss: 0.7201686501502991]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 85/86 [D loss: 0.6838648617267609, acc.: 56.45%] [G loss: 0.726333498954773]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 86/86 [D loss: 0.6846025586128235, acc.: 55.91%] [G loss: 0.7187730669975281]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 1/86 [D loss: 0.68614262342453, acc.: 55.37%] [G loss: 0.7274912595748901]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 93/200, Batch 2/86 [D loss: 0.6821997165679932, acc.: 58.69%] [G loss: 0.7193819284439087]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 3/86 [D loss: 0.6885183155536652, acc.: 53.71%] [G loss: 0.7184445261955261]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 93/200, Batch 4/86 [D loss: 0.6852069199085236, acc.: 55.47%] [G loss: 0.7243431806564331]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 93/200, Batch 5/86 [D loss: 0.6826633214950562, acc.: 57.67%] [G loss: 0.7189701795578003]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 93/200, Batch 6/86 [D loss: 0.6877978146076202, acc.: 55.66%] [G loss: 0.7231931686401367]\n",
      "32/32 [==============================] - 2s 62ms/step\n",
      "Epoch 93/200, Batch 7/86 [D loss: 0.6828177571296692, acc.: 56.98%] [G loss: 0.7208967208862305]\n",
      "32/32 [==============================] - 2s 63ms/step\n",
      "Epoch 93/200, Batch 8/86 [D loss: 0.685038149356842, acc.: 55.37%] [G loss: 0.7200210690498352]\n",
      "32/32 [==============================] - 2s 60ms/step\n",
      "Epoch 93/200, Batch 9/86 [D loss: 0.6836909353733063, acc.: 56.88%] [G loss: 0.7253385782241821]\n",
      "32/32 [==============================] - 2s 61ms/step\n",
      "Epoch 93/200, Batch 10/86 [D loss: 0.6840038001537323, acc.: 56.84%] [G loss: 0.7219340801239014]\n",
      "32/32 [==============================] - 2s 58ms/step\n",
      "Epoch 93/200, Batch 11/86 [D loss: 0.6834175288677216, acc.: 57.62%] [G loss: 0.7236998081207275]\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "Epoch 93/200, Batch 12/86 [D loss: 0.6827953457832336, acc.: 58.15%] [G loss: 0.7203148603439331]\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "Epoch 93/200, Batch 13/86 [D loss: 0.684017688035965, acc.: 55.81%] [G loss: 0.7224608063697815]\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "Epoch 93/200, Batch 14/86 [D loss: 0.6845234334468842, acc.: 55.03%] [G loss: 0.7253027558326721]\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "Epoch 93/200, Batch 15/86 [D loss: 0.683801531791687, acc.: 56.25%] [G loss: 0.7211416959762573]\n",
      "32/32 [==============================] - 2s 59ms/step\n",
      "Epoch 93/200, Batch 16/86 [D loss: 0.6857593655586243, acc.: 54.98%] [G loss: 0.7234910726547241]\n",
      "32/32 [==============================] - 2s 60ms/step\n",
      "Epoch 93/200, Batch 17/86 [D loss: 0.6843360662460327, acc.: 56.74%] [G loss: 0.7249844074249268]\n",
      "32/32 [==============================] - 1s 39ms/step\n",
      "Epoch 93/200, Batch 18/86 [D loss: 0.6823195815086365, acc.: 57.32%] [G loss: 0.7192556858062744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 19/86 [D loss: 0.6830719709396362, acc.: 56.35%] [G loss: 0.7306292653083801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 20/86 [D loss: 0.6817585229873657, acc.: 58.50%] [G loss: 0.7217010855674744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 21/86 [D loss: 0.6844930946826935, acc.: 55.86%] [G loss: 0.7207055687904358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 22/86 [D loss: 0.6841281354427338, acc.: 57.08%] [G loss: 0.7233366370201111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 23/86 [D loss: 0.6860218644142151, acc.: 56.05%] [G loss: 0.7188225388526917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 24/86 [D loss: 0.6827009916305542, acc.: 56.69%] [G loss: 0.7226558923721313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 25/86 [D loss: 0.6833316087722778, acc.: 56.69%] [G loss: 0.7201458811759949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 26/86 [D loss: 0.6861290633678436, acc.: 54.74%] [G loss: 0.722712516784668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 27/86 [D loss: 0.6843459904193878, acc.: 56.20%] [G loss: 0.7230088114738464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 28/86 [D loss: 0.6843936443328857, acc.: 55.62%] [G loss: 0.7243177890777588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 29/86 [D loss: 0.6844875812530518, acc.: 57.03%] [G loss: 0.725267231464386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 30/86 [D loss: 0.6822828650474548, acc.: 57.47%] [G loss: 0.7200903296470642]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 31/86 [D loss: 0.6879052817821503, acc.: 54.49%] [G loss: 0.724116861820221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 32/86 [D loss: 0.6785706877708435, acc.: 61.18%] [G loss: 0.7263922691345215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 33/86 [D loss: 0.682883620262146, acc.: 56.98%] [G loss: 0.724325954914093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 34/86 [D loss: 0.6821842789649963, acc.: 57.03%] [G loss: 0.7238707542419434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 35/86 [D loss: 0.684127539396286, acc.: 54.83%] [G loss: 0.7220122814178467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 36/86 [D loss: 0.6859745979309082, acc.: 55.13%] [G loss: 0.7199181318283081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 37/86 [D loss: 0.6850375235080719, acc.: 55.96%] [G loss: 0.7242777347564697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 38/86 [D loss: 0.6825471222400665, acc.: 57.47%] [G loss: 0.7226338386535645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 39/86 [D loss: 0.6867747604846954, acc.: 54.10%] [G loss: 0.7204364538192749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 40/86 [D loss: 0.681501179933548, acc.: 58.40%] [G loss: 0.7217327952384949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 41/86 [D loss: 0.6851606070995331, acc.: 55.62%] [G loss: 0.721064567565918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 42/86 [D loss: 0.6863420903682709, acc.: 55.71%] [G loss: 0.7224165201187134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 43/86 [D loss: 0.6819380223751068, acc.: 58.79%] [G loss: 0.7217644453048706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 44/86 [D loss: 0.6822939515113831, acc.: 56.01%] [G loss: 0.7207199335098267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 45/86 [D loss: 0.6821643114089966, acc.: 56.49%] [G loss: 0.7233231067657471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 46/86 [D loss: 0.6832722723484039, acc.: 58.69%] [G loss: 0.7185801267623901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 47/86 [D loss: 0.6844858229160309, acc.: 57.28%] [G loss: 0.7234888076782227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 48/86 [D loss: 0.6848579943180084, acc.: 55.57%] [G loss: 0.718101441860199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 49/86 [D loss: 0.6847066581249237, acc.: 56.69%] [G loss: 0.7136390209197998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 50/86 [D loss: 0.6871383488178253, acc.: 54.39%] [G loss: 0.7230916023254395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 51/86 [D loss: 0.6825562417507172, acc.: 57.28%] [G loss: 0.7241373062133789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 52/86 [D loss: 0.6865554749965668, acc.: 54.83%] [G loss: 0.7260022163391113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 53/86 [D loss: 0.68348827958107, acc.: 56.64%] [G loss: 0.7206697463989258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 54/86 [D loss: 0.6839343309402466, acc.: 56.35%] [G loss: 0.7158458232879639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 55/86 [D loss: 0.6853719353675842, acc.: 54.98%] [G loss: 0.7223351001739502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 56/86 [D loss: 0.6811100840568542, acc.: 57.81%] [G loss: 0.7257654070854187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 57/86 [D loss: 0.6870488822460175, acc.: 55.37%] [G loss: 0.7227601408958435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 58/86 [D loss: 0.6831622421741486, acc.: 57.42%] [G loss: 0.725026547908783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 59/86 [D loss: 0.6836507022380829, acc.: 56.35%] [G loss: 0.7174943089485168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 60/86 [D loss: 0.6855569183826447, acc.: 53.96%] [G loss: 0.7232404351234436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 61/86 [D loss: 0.6813880801200867, acc.: 58.45%] [G loss: 0.7262135148048401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 62/86 [D loss: 0.6865909993648529, acc.: 54.83%] [G loss: 0.7197140455245972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 63/86 [D loss: 0.6822886765003204, acc.: 58.50%] [G loss: 0.7252694368362427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 64/86 [D loss: 0.6856400668621063, acc.: 54.93%] [G loss: 0.718239426612854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 65/86 [D loss: 0.6851652562618256, acc.: 55.62%] [G loss: 0.7217204570770264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 66/86 [D loss: 0.6808257699012756, acc.: 59.42%] [G loss: 0.7186100482940674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 67/86 [D loss: 0.6844058632850647, acc.: 56.01%] [G loss: 0.7202132940292358]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 68/86 [D loss: 0.6850773394107819, acc.: 56.20%] [G loss: 0.727066159248352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 69/86 [D loss: 0.6830383241176605, acc.: 56.69%] [G loss: 0.7200721502304077]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 70/86 [D loss: 0.6888546049594879, acc.: 51.56%] [G loss: 0.7172998189926147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 71/86 [D loss: 0.6832482218742371, acc.: 56.59%] [G loss: 0.7260804772377014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 72/86 [D loss: 0.6826860904693604, acc.: 56.49%] [G loss: 0.7203254699707031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 73/86 [D loss: 0.6882929801940918, acc.: 53.12%] [G loss: 0.7249891757965088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 74/86 [D loss: 0.6777132451534271, acc.: 60.21%] [G loss: 0.7241880297660828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 75/86 [D loss: 0.6870253086090088, acc.: 52.73%] [G loss: 0.7193297147750854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 76/86 [D loss: 0.6839579045772552, acc.: 55.18%] [G loss: 0.7252795100212097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 77/86 [D loss: 0.6814820170402527, acc.: 58.50%] [G loss: 0.7177973389625549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 78/86 [D loss: 0.6902617812156677, acc.: 51.90%] [G loss: 0.7251976728439331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 79/86 [D loss: 0.6802045404911041, acc.: 58.54%] [G loss: 0.7223001718521118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 80/86 [D loss: 0.6872681379318237, acc.: 54.35%] [G loss: 0.7138325572013855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 81/86 [D loss: 0.6844021379947662, acc.: 55.81%] [G loss: 0.7291162014007568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 82/86 [D loss: 0.6814611554145813, acc.: 58.40%] [G loss: 0.7165400981903076]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 83/86 [D loss: 0.6949810087680817, acc.: 48.00%] [G loss: 0.7228411436080933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 84/86 [D loss: 0.6795130372047424, acc.: 58.79%] [G loss: 0.7220541834831238]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 85/86 [D loss: 0.6877816021442413, acc.: 53.27%] [G loss: 0.7091879844665527]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 86/86 [D loss: 0.6878989040851593, acc.: 54.69%] [G loss: 0.7276378870010376]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 1/86 [D loss: 0.6819020807743073, acc.: 57.76%] [G loss: 0.7154442071914673]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 2/86 [D loss: 0.6921359300613403, acc.: 51.17%] [G loss: 0.7239929437637329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 3/86 [D loss: 0.6801349818706512, acc.: 60.30%] [G loss: 0.725696861743927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 4/86 [D loss: 0.6886608898639679, acc.: 52.15%] [G loss: 0.7151963114738464]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 5/86 [D loss: 0.6891306042671204, acc.: 52.20%] [G loss: 0.7279556393623352]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 6/86 [D loss: 0.680714875459671, acc.: 59.23%] [G loss: 0.7177890539169312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 7/86 [D loss: 0.6940386593341827, acc.: 50.34%] [G loss: 0.7205345034599304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 8/86 [D loss: 0.6825481057167053, acc.: 59.08%] [G loss: 0.7222771644592285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 9/86 [D loss: 0.6858621835708618, acc.: 54.59%] [G loss: 0.7141711711883545]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 10/86 [D loss: 0.6883866190910339, acc.: 52.49%] [G loss: 0.7230747938156128]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 11/86 [D loss: 0.6807681024074554, acc.: 57.52%] [G loss: 0.723664402961731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 12/86 [D loss: 0.6897696852684021, acc.: 52.44%] [G loss: 0.7178795337677002]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 13/86 [D loss: 0.6791108548641205, acc.: 61.67%] [G loss: 0.7245375514030457]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 14/86 [D loss: 0.6852513253688812, acc.: 55.86%] [G loss: 0.7175947427749634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 15/86 [D loss: 0.6868653893470764, acc.: 54.79%] [G loss: 0.7232105731964111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 16/86 [D loss: 0.6824718117713928, acc.: 57.86%] [G loss: 0.7252511978149414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 17/86 [D loss: 0.685665100812912, acc.: 54.15%] [G loss: 0.7218613624572754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 18/86 [D loss: 0.6856523156166077, acc.: 55.18%] [G loss: 0.7260929346084595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 19/86 [D loss: 0.6848944425582886, acc.: 55.62%] [G loss: 0.7257512807846069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 20/86 [D loss: 0.6837256550788879, acc.: 57.08%] [G loss: 0.7209263443946838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 21/86 [D loss: 0.6836825609207153, acc.: 56.45%] [G loss: 0.7277774810791016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 22/86 [D loss: 0.6822865307331085, acc.: 57.47%] [G loss: 0.7231066823005676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 23/86 [D loss: 0.6837813258171082, acc.: 56.35%] [G loss: 0.7251187562942505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 24/86 [D loss: 0.6849497854709625, acc.: 56.54%] [G loss: 0.723508358001709]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 25/86 [D loss: 0.6851860582828522, acc.: 55.22%] [G loss: 0.7193923592567444]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 26/86 [D loss: 0.684000551700592, acc.: 56.30%] [G loss: 0.724149227142334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 27/86 [D loss: 0.6826402544975281, acc.: 57.13%] [G loss: 0.7256710529327393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 28/86 [D loss: 0.6845093369483948, acc.: 56.79%] [G loss: 0.7231826782226562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 29/86 [D loss: 0.6863636672496796, acc.: 54.93%] [G loss: 0.7246091961860657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 30/86 [D loss: 0.6821807324886322, acc.: 57.03%] [G loss: 0.7221086025238037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 31/86 [D loss: 0.6843192279338837, acc.: 56.20%] [G loss: 0.7240961790084839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 32/86 [D loss: 0.6845213770866394, acc.: 55.66%] [G loss: 0.7215635180473328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 33/86 [D loss: 0.6827188730239868, acc.: 56.40%] [G loss: 0.7204464077949524]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 34/86 [D loss: 0.6881760954856873, acc.: 54.35%] [G loss: 0.7243707776069641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 35/86 [D loss: 0.6811497509479523, acc.: 59.33%] [G loss: 0.7220449447631836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 36/86 [D loss: 0.6863188743591309, acc.: 53.71%] [G loss: 0.716052234172821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 37/86 [D loss: 0.6832603812217712, acc.: 56.98%] [G loss: 0.7248119115829468]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 38/86 [D loss: 0.6818267107009888, acc.: 58.06%] [G loss: 0.7179914712905884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 39/86 [D loss: 0.6850903928279877, acc.: 55.71%] [G loss: 0.7251348495483398]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 40/86 [D loss: 0.6815308630466461, acc.: 58.54%] [G loss: 0.7234205603599548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 41/86 [D loss: 0.6829434931278229, acc.: 56.45%] [G loss: 0.7166252732276917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 42/86 [D loss: 0.68281489610672, acc.: 58.06%] [G loss: 0.7274999618530273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 43/86 [D loss: 0.6824823021888733, acc.: 56.64%] [G loss: 0.7236322164535522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 44/86 [D loss: 0.6851126253604889, acc.: 56.74%] [G loss: 0.720501720905304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 45/86 [D loss: 0.6848495006561279, acc.: 56.45%] [G loss: 0.722518801689148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 46/86 [D loss: 0.6829336881637573, acc.: 56.20%] [G loss: 0.7203884720802307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 47/86 [D loss: 0.6840679347515106, acc.: 56.10%] [G loss: 0.7210533618927002]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 48/86 [D loss: 0.6800751090049744, acc.: 59.13%] [G loss: 0.7278730273246765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 49/86 [D loss: 0.6831785142421722, acc.: 56.88%] [G loss: 0.7220348119735718]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 50/86 [D loss: 0.6836245059967041, acc.: 56.69%] [G loss: 0.7264883518218994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 51/86 [D loss: 0.6834897696971893, acc.: 57.03%] [G loss: 0.7284088730812073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 52/86 [D loss: 0.6866341531276703, acc.: 54.20%] [G loss: 0.7272771596908569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 53/86 [D loss: 0.6831801533699036, acc.: 57.96%] [G loss: 0.7255788445472717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 54/86 [D loss: 0.6806310415267944, acc.: 58.40%] [G loss: 0.7264953255653381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 55/86 [D loss: 0.6819361448287964, acc.: 58.06%] [G loss: 0.7224408388137817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 56/86 [D loss: 0.6798637211322784, acc.: 59.72%] [G loss: 0.7221429347991943]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 57/86 [D loss: 0.6833410263061523, acc.: 56.49%] [G loss: 0.7235438227653503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 58/86 [D loss: 0.683518260717392, acc.: 54.88%] [G loss: 0.7217375040054321]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 59/86 [D loss: 0.6836492419242859, acc.: 57.52%] [G loss: 0.7250405550003052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 60/86 [D loss: 0.6845446527004242, acc.: 57.37%] [G loss: 0.7241867780685425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 61/86 [D loss: 0.68455970287323, acc.: 55.81%] [G loss: 0.7184661626815796]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 62/86 [D loss: 0.6842155158519745, acc.: 56.59%] [G loss: 0.7241318225860596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 63/86 [D loss: 0.6810871064662933, acc.: 59.18%] [G loss: 0.7225023508071899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 64/86 [D loss: 0.6807198822498322, acc.: 58.74%] [G loss: 0.7215327024459839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 65/86 [D loss: 0.6824818849563599, acc.: 57.08%] [G loss: 0.7221748232841492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 66/86 [D loss: 0.6813660264015198, acc.: 58.06%] [G loss: 0.7225462198257446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 67/86 [D loss: 0.6841396689414978, acc.: 56.45%] [G loss: 0.7232335209846497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 68/86 [D loss: 0.6821398735046387, acc.: 56.59%] [G loss: 0.7225263714790344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 69/86 [D loss: 0.6817385256290436, acc.: 57.62%] [G loss: 0.7238596677780151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 70/86 [D loss: 0.6831031143665314, acc.: 56.84%] [G loss: 0.7255467772483826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 71/86 [D loss: 0.6840561032295227, acc.: 55.96%] [G loss: 0.724068284034729]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 72/86 [D loss: 0.6831927299499512, acc.: 57.37%] [G loss: 0.7264316082000732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 73/86 [D loss: 0.6835307478904724, acc.: 56.35%] [G loss: 0.7259719371795654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 74/86 [D loss: 0.6825538873672485, acc.: 56.30%] [G loss: 0.7259878516197205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 75/86 [D loss: 0.6826855838298798, acc.: 57.28%] [G loss: 0.7219545841217041]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 76/86 [D loss: 0.6812243163585663, acc.: 56.64%] [G loss: 0.7273159027099609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 77/86 [D loss: 0.6809602677822113, acc.: 58.30%] [G loss: 0.7233256697654724]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 78/86 [D loss: 0.6830866634845734, acc.: 56.64%] [G loss: 0.728162944316864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 79/86 [D loss: 0.6820281744003296, acc.: 57.32%] [G loss: 0.7239996194839478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 80/86 [D loss: 0.6827712059020996, acc.: 58.45%] [G loss: 0.7230813503265381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 81/86 [D loss: 0.6836831569671631, acc.: 55.32%] [G loss: 0.7236513495445251]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 94/200, Batch 82/86 [D loss: 0.6848797798156738, acc.: 55.22%] [G loss: 0.7225119471549988]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 83/86 [D loss: 0.6829969584941864, acc.: 56.45%] [G loss: 0.7256125807762146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 94/200, Batch 84/86 [D loss: 0.6826756596565247, acc.: 56.98%] [G loss: 0.7234386801719666]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 85/86 [D loss: 0.6838575303554535, acc.: 55.62%] [G loss: 0.7244438529014587]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 94/200, Batch 86/86 [D loss: 0.6846441626548767, acc.: 55.32%] [G loss: 0.7269097566604614]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 1/86 [D loss: 0.6829961538314819, acc.: 58.35%] [G loss: 0.7244953513145447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 2/86 [D loss: 0.6828840970993042, acc.: 56.25%] [G loss: 0.7233717441558838]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 3/86 [D loss: 0.6840271949768066, acc.: 55.81%] [G loss: 0.7235580682754517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 4/86 [D loss: 0.6868089735507965, acc.: 54.15%] [G loss: 0.7181912660598755]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 5/86 [D loss: 0.6861769556999207, acc.: 55.52%] [G loss: 0.7243990898132324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 6/86 [D loss: 0.6803367733955383, acc.: 58.35%] [G loss: 0.720205545425415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 7/86 [D loss: 0.6864924728870392, acc.: 54.88%] [G loss: 0.7246571779251099]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 8/86 [D loss: 0.6811308562755585, acc.: 57.67%] [G loss: 0.7283152341842651]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 9/86 [D loss: 0.685967355966568, acc.: 54.93%] [G loss: 0.7182110548019409]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 10/86 [D loss: 0.6839385628700256, acc.: 56.59%] [G loss: 0.7254196405410767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 11/86 [D loss: 0.679276704788208, acc.: 59.08%] [G loss: 0.7185060381889343]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 12/86 [D loss: 0.6866358816623688, acc.: 54.20%] [G loss: 0.7212954759597778]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 13/86 [D loss: 0.6829151213169098, acc.: 57.96%] [G loss: 0.7228015661239624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 14/86 [D loss: 0.6845602691173553, acc.: 55.71%] [G loss: 0.7207275032997131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 15/86 [D loss: 0.6835819780826569, acc.: 56.30%] [G loss: 0.7236679196357727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 16/86 [D loss: 0.681241363286972, acc.: 57.23%] [G loss: 0.7208356857299805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 17/86 [D loss: 0.6855132579803467, acc.: 55.03%] [G loss: 0.72502601146698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 18/86 [D loss: 0.6813996732234955, acc.: 58.15%] [G loss: 0.7256858348846436]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 19/86 [D loss: 0.683638870716095, acc.: 57.42%] [G loss: 0.7238237261772156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 20/86 [D loss: 0.6879263520240784, acc.: 54.35%] [G loss: 0.7298640608787537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 21/86 [D loss: 0.6793772578239441, acc.: 60.79%] [G loss: 0.7181578874588013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 22/86 [D loss: 0.686234325170517, acc.: 54.93%] [G loss: 0.7237762212753296]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 23/86 [D loss: 0.6832891404628754, acc.: 56.64%] [G loss: 0.7257890105247498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 24/86 [D loss: 0.6834222376346588, acc.: 55.91%] [G loss: 0.7216238975524902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 25/86 [D loss: 0.6851488351821899, acc.: 54.69%] [G loss: 0.7264754176139832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 26/86 [D loss: 0.6805406510829926, acc.: 58.45%] [G loss: 0.7201109528541565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 27/86 [D loss: 0.6853662431240082, acc.: 53.61%] [G loss: 0.7202286720275879]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 28/86 [D loss: 0.6817250847816467, acc.: 58.45%] [G loss: 0.7247685790061951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 29/86 [D loss: 0.6847286224365234, acc.: 56.40%] [G loss: 0.7194281220436096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 30/86 [D loss: 0.6872513294219971, acc.: 54.44%] [G loss: 0.7283287048339844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 31/86 [D loss: 0.68145352602005, acc.: 57.71%] [G loss: 0.7195779085159302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 32/86 [D loss: 0.6866335272789001, acc.: 54.35%] [G loss: 0.7240328788757324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 33/86 [D loss: 0.6827229261398315, acc.: 57.42%] [G loss: 0.7302064895629883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 34/86 [D loss: 0.6825514435768127, acc.: 57.57%] [G loss: 0.7201651930809021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 35/86 [D loss: 0.6835303008556366, acc.: 56.74%] [G loss: 0.7251031398773193]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 36/86 [D loss: 0.6814031898975372, acc.: 58.84%] [G loss: 0.719631016254425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 37/86 [D loss: 0.6831379532814026, acc.: 56.10%] [G loss: 0.7220472097396851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 38/86 [D loss: 0.6815700829029083, acc.: 57.86%] [G loss: 0.7223660945892334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 39/86 [D loss: 0.6836735308170319, acc.: 55.18%] [G loss: 0.720203161239624]\n",
      "32/32 [==============================] - 2s 51ms/step\n",
      "Epoch 95/200, Batch 40/86 [D loss: 0.6861864924430847, acc.: 55.42%] [G loss: 0.7248928546905518]\n",
      "32/32 [==============================] - 2s 51ms/step\n",
      "Epoch 95/200, Batch 41/86 [D loss: 0.6806278824806213, acc.: 58.50%] [G loss: 0.7264880537986755]\n",
      "32/32 [==============================] - 2s 55ms/step\n",
      "Epoch 95/200, Batch 42/86 [D loss: 0.6862242519855499, acc.: 53.17%] [G loss: 0.7184766530990601]\n",
      "32/32 [==============================] - 2s 51ms/step\n",
      "Epoch 95/200, Batch 43/86 [D loss: 0.684730738401413, acc.: 55.22%] [G loss: 0.7282754182815552]\n",
      "32/32 [==============================] - 2s 51ms/step\n",
      "Epoch 95/200, Batch 44/86 [D loss: 0.6801945567131042, acc.: 58.64%] [G loss: 0.7186641097068787]\n",
      "32/32 [==============================] - 2s 55ms/step\n",
      "Epoch 95/200, Batch 45/86 [D loss: 0.6882326304912567, acc.: 54.20%] [G loss: 0.7259251475334167]\n",
      "32/32 [==============================] - 2s 51ms/step\n",
      "Epoch 95/200, Batch 46/86 [D loss: 0.6778435707092285, acc.: 60.74%] [G loss: 0.7253407835960388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 47/86 [D loss: 0.6845647394657135, acc.: 54.10%] [G loss: 0.7193860411643982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 48/86 [D loss: 0.6835941672325134, acc.: 57.03%] [G loss: 0.7221416234970093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 49/86 [D loss: 0.6814877688884735, acc.: 59.23%] [G loss: 0.7218267321586609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 50/86 [D loss: 0.687010258436203, acc.: 53.56%] [G loss: 0.7224154472351074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 51/86 [D loss: 0.6807569563388824, acc.: 58.15%] [G loss: 0.7254384160041809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 52/86 [D loss: 0.6838668584823608, acc.: 55.71%] [G loss: 0.7193158268928528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 53/86 [D loss: 0.6871452629566193, acc.: 53.12%] [G loss: 0.7267330884933472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 54/86 [D loss: 0.6796527206897736, acc.: 58.74%] [G loss: 0.71871417760849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 55/86 [D loss: 0.689879983663559, acc.: 53.37%] [G loss: 0.7214733362197876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 56/86 [D loss: 0.6809499561786652, acc.: 59.03%] [G loss: 0.7255906462669373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 57/86 [D loss: 0.6832208633422852, acc.: 57.13%] [G loss: 0.7239944338798523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 58/86 [D loss: 0.6831329464912415, acc.: 55.71%] [G loss: 0.7286052107810974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 59/86 [D loss: 0.6811918020248413, acc.: 59.13%] [G loss: 0.7242023944854736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 60/86 [D loss: 0.6871878206729889, acc.: 53.76%] [G loss: 0.7246946692466736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 61/86 [D loss: 0.6789193451404572, acc.: 60.40%] [G loss: 0.7261455059051514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 62/86 [D loss: 0.6829430460929871, acc.: 56.10%] [G loss: 0.7249736785888672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 63/86 [D loss: 0.6847255527973175, acc.: 55.32%] [G loss: 0.7247331738471985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 64/86 [D loss: 0.6803698837757111, acc.: 58.50%] [G loss: 0.724866509437561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 65/86 [D loss: 0.6856987476348877, acc.: 55.57%] [G loss: 0.7261224985122681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 66/86 [D loss: 0.682506650686264, acc.: 57.57%] [G loss: 0.7281484007835388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 67/86 [D loss: 0.6832029521465302, acc.: 57.86%] [G loss: 0.7190113663673401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 68/86 [D loss: 0.6831133961677551, acc.: 56.88%] [G loss: 0.7234285473823547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 69/86 [D loss: 0.6819671392440796, acc.: 56.64%] [G loss: 0.7270939946174622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 70/86 [D loss: 0.6828739643096924, acc.: 56.74%] [G loss: 0.7233507633209229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 71/86 [D loss: 0.6842347979545593, acc.: 55.47%] [G loss: 0.7232577800750732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 72/86 [D loss: 0.685124397277832, acc.: 55.47%] [G loss: 0.7216455340385437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 73/86 [D loss: 0.6849528551101685, acc.: 55.27%] [G loss: 0.7200850248336792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 74/86 [D loss: 0.6826743185520172, acc.: 57.47%] [G loss: 0.7248808145523071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 75/86 [D loss: 0.6807736456394196, acc.: 59.18%] [G loss: 0.7228684425354004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 76/86 [D loss: 0.6808971464633942, acc.: 57.96%] [G loss: 0.7241389155387878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 77/86 [D loss: 0.6810657382011414, acc.: 58.84%] [G loss: 0.7259517908096313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 78/86 [D loss: 0.6831345558166504, acc.: 55.42%] [G loss: 0.726704478263855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 79/86 [D loss: 0.6830524504184723, acc.: 56.69%] [G loss: 0.7266714572906494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 80/86 [D loss: 0.6833377778530121, acc.: 56.93%] [G loss: 0.7259699106216431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 81/86 [D loss: 0.6843405663967133, acc.: 56.35%] [G loss: 0.7245246171951294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 82/86 [D loss: 0.6799309253692627, acc.: 57.91%] [G loss: 0.7249112129211426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 83/86 [D loss: 0.6847111880779266, acc.: 55.37%] [G loss: 0.7241195440292358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 84/86 [D loss: 0.6824564337730408, acc.: 57.18%] [G loss: 0.7251899242401123]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 95/200, Batch 85/86 [D loss: 0.6826278567314148, acc.: 56.64%] [G loss: 0.7286978363990784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 95/200, Batch 86/86 [D loss: 0.6844543218612671, acc.: 56.45%] [G loss: 0.7231656312942505]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 1/86 [D loss: 0.6829149425029755, acc.: 57.28%] [G loss: 0.7303107976913452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 2/86 [D loss: 0.682111382484436, acc.: 56.49%] [G loss: 0.7261764407157898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 3/86 [D loss: 0.6842665076255798, acc.: 56.35%] [G loss: 0.7284073829650879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 4/86 [D loss: 0.6823444962501526, acc.: 57.47%] [G loss: 0.7269158363342285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 5/86 [D loss: 0.6839287877082825, acc.: 56.64%] [G loss: 0.7201859951019287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 6/86 [D loss: 0.6824707686901093, acc.: 57.37%] [G loss: 0.7286920547485352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 7/86 [D loss: 0.6848454177379608, acc.: 55.57%] [G loss: 0.7249960899353027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 8/86 [D loss: 0.6828971803188324, acc.: 54.35%] [G loss: 0.7269802689552307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 9/86 [D loss: 0.6805896461009979, acc.: 57.71%] [G loss: 0.727260172367096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 10/86 [D loss: 0.6846692860126495, acc.: 55.96%] [G loss: 0.7248039841651917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 11/86 [D loss: 0.6811087727546692, acc.: 58.30%] [G loss: 0.7290699481964111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 12/86 [D loss: 0.6835637390613556, acc.: 56.64%] [G loss: 0.720421314239502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 13/86 [D loss: 0.6852410435676575, acc.: 54.79%] [G loss: 0.7258958220481873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 14/86 [D loss: 0.6818761229515076, acc.: 57.42%] [G loss: 0.725611686706543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 15/86 [D loss: 0.6837115585803986, acc.: 56.25%] [G loss: 0.7227915525436401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 16/86 [D loss: 0.6857881844043732, acc.: 56.40%] [G loss: 0.7326092720031738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 17/86 [D loss: 0.6826084554195404, acc.: 57.86%] [G loss: 0.7235260009765625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 18/86 [D loss: 0.6853730380535126, acc.: 53.37%] [G loss: 0.7247195243835449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 19/86 [D loss: 0.6802756488323212, acc.: 59.13%] [G loss: 0.7258362770080566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 20/86 [D loss: 0.6832980513572693, acc.: 56.05%] [G loss: 0.721464216709137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 21/86 [D loss: 0.6828532516956329, acc.: 55.81%] [G loss: 0.7283380031585693]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 22/86 [D loss: 0.6819290816783905, acc.: 57.32%] [G loss: 0.7223901152610779]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 23/86 [D loss: 0.686376690864563, acc.: 55.52%] [G loss: 0.7249434590339661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 24/86 [D loss: 0.6784578263759613, acc.: 60.06%] [G loss: 0.7222808599472046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 25/86 [D loss: 0.6872216165065765, acc.: 53.66%] [G loss: 0.7198958396911621]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 26/86 [D loss: 0.6836602091789246, acc.: 56.93%] [G loss: 0.7276585102081299]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 27/86 [D loss: 0.6810388565063477, acc.: 58.01%] [G loss: 0.7232042551040649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 28/86 [D loss: 0.6837197840213776, acc.: 56.84%] [G loss: 0.7186975479125977]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 29/86 [D loss: 0.6839053332805634, acc.: 55.42%] [G loss: 0.7240185737609863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 30/86 [D loss: 0.6806458830833435, acc.: 58.50%] [G loss: 0.7188314199447632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 31/86 [D loss: 0.6842072308063507, acc.: 56.69%] [G loss: 0.7254082560539246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 32/86 [D loss: 0.6793298423290253, acc.: 59.42%] [G loss: 0.7197853922843933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 33/86 [D loss: 0.6839891076087952, acc.: 56.40%] [G loss: 0.7228106260299683]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 34/86 [D loss: 0.6838623285293579, acc.: 55.76%] [G loss: 0.7270767092704773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 35/86 [D loss: 0.6837960779666901, acc.: 56.88%] [G loss: 0.724709153175354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 36/86 [D loss: 0.6848614513874054, acc.: 56.54%] [G loss: 0.7241340279579163]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 37/86 [D loss: 0.6808882057666779, acc.: 57.67%] [G loss: 0.7205655574798584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 38/86 [D loss: 0.6850263476371765, acc.: 53.32%] [G loss: 0.7195374965667725]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 39/86 [D loss: 0.6823116838932037, acc.: 57.08%] [G loss: 0.7270324230194092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 40/86 [D loss: 0.6843135356903076, acc.: 55.71%] [G loss: 0.7207881212234497]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 41/86 [D loss: 0.6853649020195007, acc.: 55.71%] [G loss: 0.7236189842224121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 42/86 [D loss: 0.6846638023853302, acc.: 55.03%] [G loss: 0.7257553935050964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 43/86 [D loss: 0.6822696030139923, acc.: 57.71%] [G loss: 0.7220427989959717]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 44/86 [D loss: 0.6834673583507538, acc.: 55.57%] [G loss: 0.7236722707748413]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 45/86 [D loss: 0.6825523674488068, acc.: 56.35%] [G loss: 0.7238044738769531]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 46/86 [D loss: 0.6802658140659332, acc.: 58.54%] [G loss: 0.7263926863670349]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 47/86 [D loss: 0.6829841136932373, acc.: 56.49%] [G loss: 0.7264292240142822]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 48/86 [D loss: 0.6820344626903534, acc.: 58.40%] [G loss: 0.7251339554786682]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 49/86 [D loss: 0.6822511553764343, acc.: 56.30%] [G loss: 0.7305325269699097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 50/86 [D loss: 0.6836628019809723, acc.: 55.27%] [G loss: 0.7270970344543457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 51/86 [D loss: 0.6815517246723175, acc.: 57.37%] [G loss: 0.7288368940353394]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 52/86 [D loss: 0.6811591386795044, acc.: 56.59%] [G loss: 0.7219666838645935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 53/86 [D loss: 0.6824460625648499, acc.: 57.03%] [G loss: 0.7258419990539551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 54/86 [D loss: 0.6816238760948181, acc.: 56.15%] [G loss: 0.7250030040740967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 55/86 [D loss: 0.6820070743560791, acc.: 57.32%] [G loss: 0.7281216382980347]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 56/86 [D loss: 0.6820843517780304, acc.: 57.62%] [G loss: 0.7229057550430298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 57/86 [D loss: 0.6816172003746033, acc.: 57.28%] [G loss: 0.7273289561271667]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 58/86 [D loss: 0.6837481260299683, acc.: 57.13%] [G loss: 0.7253333330154419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 59/86 [D loss: 0.6837538778781891, acc.: 55.62%] [G loss: 0.730828046798706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 60/86 [D loss: 0.6818155646324158, acc.: 56.30%] [G loss: 0.7212560772895813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 61/86 [D loss: 0.6810877919197083, acc.: 57.71%] [G loss: 0.7249965071678162]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 62/86 [D loss: 0.6818397641181946, acc.: 56.98%] [G loss: 0.7233428359031677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 63/86 [D loss: 0.6853829324245453, acc.: 54.79%] [G loss: 0.724631667137146]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 64/86 [D loss: 0.6832651793956757, acc.: 56.74%] [G loss: 0.7241160869598389]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 65/86 [D loss: 0.6800124645233154, acc.: 57.08%] [G loss: 0.7295347452163696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 66/86 [D loss: 0.6816136837005615, acc.: 56.79%] [G loss: 0.7311056852340698]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 67/86 [D loss: 0.6791152060031891, acc.: 58.94%] [G loss: 0.7316320538520813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 68/86 [D loss: 0.684638500213623, acc.: 55.91%] [G loss: 0.7228527069091797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 69/86 [D loss: 0.680553525686264, acc.: 57.47%] [G loss: 0.728118896484375]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 70/86 [D loss: 0.6806068122386932, acc.: 58.89%] [G loss: 0.7245296835899353]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 71/86 [D loss: 0.6853562891483307, acc.: 55.66%] [G loss: 0.725811779499054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 72/86 [D loss: 0.6799004971981049, acc.: 58.79%] [G loss: 0.7274054884910583]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 73/86 [D loss: 0.685922771692276, acc.: 54.88%] [G loss: 0.7243550419807434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 74/86 [D loss: 0.6831676363945007, acc.: 56.69%] [G loss: 0.7285890579223633]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 75/86 [D loss: 0.6816578507423401, acc.: 58.20%] [G loss: 0.723575234413147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 76/86 [D loss: 0.6839780211448669, acc.: 56.40%] [G loss: 0.72530198097229]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 96/200, Batch 77/86 [D loss: 0.6809422671794891, acc.: 58.74%] [G loss: 0.7250782251358032]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 78/86 [D loss: 0.682515949010849, acc.: 56.30%] [G loss: 0.724618136882782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 79/86 [D loss: 0.6832742393016815, acc.: 56.84%] [G loss: 0.7290695905685425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 80/86 [D loss: 0.6809165179729462, acc.: 56.74%] [G loss: 0.7271729111671448]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 96/200, Batch 81/86 [D loss: 0.68584805727005, acc.: 53.47%] [G loss: 0.7298743724822998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 96/200, Batch 82/86 [D loss: 0.6818084120750427, acc.: 57.47%] [G loss: 0.7230371236801147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 83/86 [D loss: 0.6861475110054016, acc.: 55.03%] [G loss: 0.7220488786697388]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 84/86 [D loss: 0.6847386062145233, acc.: 55.32%] [G loss: 0.7249504327774048]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 85/86 [D loss: 0.6823246479034424, acc.: 58.20%] [G loss: 0.7211208343505859]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 96/200, Batch 86/86 [D loss: 0.6852176785469055, acc.: 54.83%] [G loss: 0.7276656627655029]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 1/86 [D loss: 0.6821701526641846, acc.: 57.96%] [G loss: 0.7241329550743103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 2/86 [D loss: 0.6840257346630096, acc.: 55.08%] [G loss: 0.7170053124427795]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 3/86 [D loss: 0.6857793927192688, acc.: 54.59%] [G loss: 0.727847158908844]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 4/86 [D loss: 0.6811123490333557, acc.: 57.76%] [G loss: 0.7177719473838806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 5/86 [D loss: 0.6879667639732361, acc.: 54.15%] [G loss: 0.7252376675605774]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 6/86 [D loss: 0.6796202957630157, acc.: 60.01%] [G loss: 0.7254700660705566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 7/86 [D loss: 0.6851418316364288, acc.: 55.57%] [G loss: 0.7227293848991394]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 8/86 [D loss: 0.683804839849472, acc.: 55.03%] [G loss: 0.7332775592803955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 9/86 [D loss: 0.6811821460723877, acc.: 57.67%] [G loss: 0.7200168967247009]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 10/86 [D loss: 0.6842343807220459, acc.: 56.54%] [G loss: 0.7243530750274658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 11/86 [D loss: 0.6796371936798096, acc.: 58.94%] [G loss: 0.7230782508850098]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 12/86 [D loss: 0.6890988051891327, acc.: 53.27%] [G loss: 0.7188248634338379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 13/86 [D loss: 0.6823911368846893, acc.: 56.49%] [G loss: 0.7291102409362793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 14/86 [D loss: 0.6822969913482666, acc.: 56.64%] [G loss: 0.7185292840003967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 15/86 [D loss: 0.6864069700241089, acc.: 56.93%] [G loss: 0.7255257964134216]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 16/86 [D loss: 0.6814217865467072, acc.: 58.74%] [G loss: 0.7235143780708313]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 17/86 [D loss: 0.6855550408363342, acc.: 54.93%] [G loss: 0.7228522300720215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 18/86 [D loss: 0.6819830536842346, acc.: 56.59%] [G loss: 0.7320414781570435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 19/86 [D loss: 0.6838752925395966, acc.: 56.98%] [G loss: 0.7174974083900452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 20/86 [D loss: 0.6846390962600708, acc.: 56.25%] [G loss: 0.7287217974662781]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 97/200, Batch 21/86 [D loss: 0.6777759194374084, acc.: 60.69%] [G loss: 0.7244027853012085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 22/86 [D loss: 0.6849953532218933, acc.: 55.03%] [G loss: 0.719597578048706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 23/86 [D loss: 0.6818332374095917, acc.: 56.54%] [G loss: 0.7282068133354187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 24/86 [D loss: 0.6797870099544525, acc.: 57.62%] [G loss: 0.7174685597419739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 25/86 [D loss: 0.6886107325553894, acc.: 53.03%] [G loss: 0.7328816652297974]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 26/86 [D loss: 0.6803669929504395, acc.: 57.91%] [G loss: 0.7247776389122009]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 27/86 [D loss: 0.6841838657855988, acc.: 54.39%] [G loss: 0.7216188907623291]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 28/86 [D loss: 0.6827885806560516, acc.: 55.86%] [G loss: 0.7271472811698914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 29/86 [D loss: 0.6793374121189117, acc.: 59.03%] [G loss: 0.7205890417098999]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 30/86 [D loss: 0.6889321208000183, acc.: 54.00%] [G loss: 0.7291024923324585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 31/86 [D loss: 0.6775209009647369, acc.: 60.84%] [G loss: 0.7265046834945679]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 32/86 [D loss: 0.6865069568157196, acc.: 53.71%] [G loss: 0.719744861125946]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 33/86 [D loss: 0.6845117807388306, acc.: 54.64%] [G loss: 0.7290705442428589]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 34/86 [D loss: 0.6792478263378143, acc.: 59.47%] [G loss: 0.7244176864624023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 35/86 [D loss: 0.6866494715213776, acc.: 55.13%] [G loss: 0.7314671874046326]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 36/86 [D loss: 0.6805792450904846, acc.: 58.94%] [G loss: 0.7227635383605957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 37/86 [D loss: 0.6848333775997162, acc.: 54.35%] [G loss: 0.7193218469619751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 38/86 [D loss: 0.6838609576225281, acc.: 56.88%] [G loss: 0.732393205165863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 39/86 [D loss: 0.6788067817687988, acc.: 58.45%] [G loss: 0.725988507270813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 40/86 [D loss: 0.684678465127945, acc.: 55.96%] [G loss: 0.7231204509735107]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 41/86 [D loss: 0.6837728917598724, acc.: 56.15%] [G loss: 0.7267570495605469]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 42/86 [D loss: 0.6799314320087433, acc.: 58.79%] [G loss: 0.7218937873840332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 43/86 [D loss: 0.6853774785995483, acc.: 54.74%] [G loss: 0.7246405482292175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 44/86 [D loss: 0.6819230616092682, acc.: 56.74%] [G loss: 0.7259441018104553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 45/86 [D loss: 0.6813471019268036, acc.: 57.81%] [G loss: 0.7244362831115723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 46/86 [D loss: 0.6829684674739838, acc.: 57.28%] [G loss: 0.7272791862487793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 47/86 [D loss: 0.6811862289905548, acc.: 58.25%] [G loss: 0.7234326601028442]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 48/86 [D loss: 0.6841567158699036, acc.: 55.52%] [G loss: 0.725512683391571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 49/86 [D loss: 0.6835562884807587, acc.: 56.20%] [G loss: 0.7309537529945374]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 50/86 [D loss: 0.6832043528556824, acc.: 57.28%] [G loss: 0.724422812461853]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 51/86 [D loss: 0.6829675137996674, acc.: 55.76%] [G loss: 0.7291355729103088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 52/86 [D loss: 0.6814535558223724, acc.: 56.84%] [G loss: 0.7237437963485718]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 53/86 [D loss: 0.6845135688781738, acc.: 55.76%] [G loss: 0.7214420437812805]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 54/86 [D loss: 0.6842150092124939, acc.: 56.64%] [G loss: 0.7310588955879211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 55/86 [D loss: 0.6832372546195984, acc.: 56.49%] [G loss: 0.7224479913711548]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 56/86 [D loss: 0.6831952631473541, acc.: 56.98%] [G loss: 0.7294003963470459]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 57/86 [D loss: 0.680268257856369, acc.: 58.59%] [G loss: 0.7232152223587036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 58/86 [D loss: 0.6832088530063629, acc.: 55.71%] [G loss: 0.7239864468574524]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 59/86 [D loss: 0.6821920573711395, acc.: 56.01%] [G loss: 0.7257757186889648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 60/86 [D loss: 0.6805728673934937, acc.: 59.47%] [G loss: 0.7210854291915894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 61/86 [D loss: 0.6864691972732544, acc.: 54.74%] [G loss: 0.7268434762954712]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 62/86 [D loss: 0.6785070896148682, acc.: 59.28%] [G loss: 0.7252256870269775]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 63/86 [D loss: 0.6857738196849823, acc.: 56.10%] [G loss: 0.7228642702102661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 64/86 [D loss: 0.6834834218025208, acc.: 56.10%] [G loss: 0.727622926235199]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 65/86 [D loss: 0.6802731454372406, acc.: 57.81%] [G loss: 0.7237191796302795]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 66/86 [D loss: 0.6875685453414917, acc.: 53.71%] [G loss: 0.7282840013504028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 67/86 [D loss: 0.682362824678421, acc.: 57.52%] [G loss: 0.7285402417182922]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 68/86 [D loss: 0.6847398579120636, acc.: 54.88%] [G loss: 0.719394326210022]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 69/86 [D loss: 0.6839328110218048, acc.: 56.45%] [G loss: 0.7297368049621582]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 70/86 [D loss: 0.6797835826873779, acc.: 58.20%] [G loss: 0.7238215208053589]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 71/86 [D loss: 0.6859246492385864, acc.: 55.57%] [G loss: 0.7234712839126587]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 72/86 [D loss: 0.6814625561237335, acc.: 57.18%] [G loss: 0.7278199195861816]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 73/86 [D loss: 0.6840389966964722, acc.: 57.42%] [G loss: 0.7196703553199768]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 74/86 [D loss: 0.6832512319087982, acc.: 55.08%] [G loss: 0.7308328747749329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 75/86 [D loss: 0.6785634458065033, acc.: 59.33%] [G loss: 0.7213306427001953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 76/86 [D loss: 0.6893777847290039, acc.: 51.66%] [G loss: 0.7240518927574158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 77/86 [D loss: 0.6804122626781464, acc.: 60.01%] [G loss: 0.7222335934638977]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 78/86 [D loss: 0.6856875121593475, acc.: 54.98%] [G loss: 0.7209010720252991]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 79/86 [D loss: 0.6830829977989197, acc.: 56.93%] [G loss: 0.7273296117782593]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 80/86 [D loss: 0.6811688542366028, acc.: 57.13%] [G loss: 0.7259337902069092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 81/86 [D loss: 0.6860915720462799, acc.: 54.25%] [G loss: 0.7326142191886902]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 82/86 [D loss: 0.6808618307113647, acc.: 57.03%] [G loss: 0.7313359379768372]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 83/86 [D loss: 0.6844178438186646, acc.: 56.49%] [G loss: 0.7246575355529785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 97/200, Batch 84/86 [D loss: 0.6827437579631805, acc.: 56.49%] [G loss: 0.7266885638237]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 85/86 [D loss: 0.6823163032531738, acc.: 56.64%] [G loss: 0.7282000780105591]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 97/200, Batch 86/86 [D loss: 0.682562530040741, acc.: 57.03%] [G loss: 0.727684497833252]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 1/86 [D loss: 0.6813461780548096, acc.: 57.91%] [G loss: 0.7234822511672974]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 2/86 [D loss: 0.6837634742259979, acc.: 56.40%] [G loss: 0.7270745038986206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 3/86 [D loss: 0.6833145618438721, acc.: 55.08%] [G loss: 0.7272794246673584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 4/86 [D loss: 0.6816586852073669, acc.: 57.62%] [G loss: 0.7240555286407471]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 5/86 [D loss: 0.6844436228275299, acc.: 56.10%] [G loss: 0.7275450825691223]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 6/86 [D loss: 0.6812968254089355, acc.: 57.96%] [G loss: 0.7271384596824646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 7/86 [D loss: 0.6844576001167297, acc.: 53.32%] [G loss: 0.7259073257446289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 8/86 [D loss: 0.6811026036739349, acc.: 57.37%] [G loss: 0.7267560958862305]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 9/86 [D loss: 0.6778808832168579, acc.: 58.89%] [G loss: 0.7265058755874634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 10/86 [D loss: 0.6882214546203613, acc.: 53.22%] [G loss: 0.7266511917114258]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 11/86 [D loss: 0.6832852065563202, acc.: 55.62%] [G loss: 0.7271382808685303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 12/86 [D loss: 0.6820085048675537, acc.: 57.13%] [G loss: 0.7282911539077759]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 13/86 [D loss: 0.6829421818256378, acc.: 57.18%] [G loss: 0.7252459526062012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 14/86 [D loss: 0.6833876073360443, acc.: 56.54%] [G loss: 0.7264517545700073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 15/86 [D loss: 0.6839411854743958, acc.: 56.98%] [G loss: 0.724321186542511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 16/86 [D loss: 0.6818941533565521, acc.: 58.54%] [G loss: 0.7275015711784363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 17/86 [D loss: 0.683619350194931, acc.: 55.76%] [G loss: 0.724881112575531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 18/86 [D loss: 0.6811601221561432, acc.: 57.08%] [G loss: 0.7279468178749084]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 19/86 [D loss: 0.6805725991725922, acc.: 57.96%] [G loss: 0.7254905104637146]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 20/86 [D loss: 0.6826057434082031, acc.: 57.76%] [G loss: 0.7301188707351685]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 21/86 [D loss: 0.6806140542030334, acc.: 58.40%] [G loss: 0.7292187809944153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 22/86 [D loss: 0.6828342080116272, acc.: 54.93%] [G loss: 0.7245165109634399]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 23/86 [D loss: 0.6839178204536438, acc.: 55.91%] [G loss: 0.7281907796859741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 24/86 [D loss: 0.6810786426067352, acc.: 56.64%] [G loss: 0.7254549860954285]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 25/86 [D loss: 0.686633825302124, acc.: 55.18%] [G loss: 0.7334930300712585]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 26/86 [D loss: 0.6823770403862, acc.: 58.01%] [G loss: 0.7250323295593262]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 27/86 [D loss: 0.6819077730178833, acc.: 56.35%] [G loss: 0.7239347696304321]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 28/86 [D loss: 0.6826381981372833, acc.: 55.71%] [G loss: 0.7283139228820801]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 29/86 [D loss: 0.6812919676303864, acc.: 56.88%] [G loss: 0.7247880101203918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 30/86 [D loss: 0.6853056848049164, acc.: 55.22%] [G loss: 0.7273574471473694]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 31/86 [D loss: 0.6788836419582367, acc.: 58.84%] [G loss: 0.7311416268348694]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 32/86 [D loss: 0.6802604794502258, acc.: 57.96%] [G loss: 0.7217501401901245]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 33/86 [D loss: 0.6793886125087738, acc.: 58.64%] [G loss: 0.7281072735786438]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 34/86 [D loss: 0.6803861260414124, acc.: 58.20%] [G loss: 0.7230944633483887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 35/86 [D loss: 0.68446284532547, acc.: 55.86%] [G loss: 0.7259162068367004]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 36/86 [D loss: 0.6807226538658142, acc.: 58.25%] [G loss: 0.7298181653022766]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 37/86 [D loss: 0.6819343268871307, acc.: 57.81%] [G loss: 0.7284051775932312]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 98/200, Batch 38/86 [D loss: 0.683875173330307, acc.: 56.40%] [G loss: 0.7247841954231262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 39/86 [D loss: 0.6811339557170868, acc.: 58.01%] [G loss: 0.7276016473770142]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 40/86 [D loss: 0.6834243535995483, acc.: 55.76%] [G loss: 0.7273519039154053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 41/86 [D loss: 0.6823860704898834, acc.: 58.11%] [G loss: 0.7271828651428223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 42/86 [D loss: 0.6826603710651398, acc.: 57.81%] [G loss: 0.7229366302490234]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 43/86 [D loss: 0.6815531551837921, acc.: 56.93%] [G loss: 0.725961446762085]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 44/86 [D loss: 0.6833421885967255, acc.: 57.52%] [G loss: 0.7275917530059814]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 45/86 [D loss: 0.6824910342693329, acc.: 56.49%] [G loss: 0.7306052446365356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 46/86 [D loss: 0.6799223721027374, acc.: 58.98%] [G loss: 0.7291523218154907]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 47/86 [D loss: 0.685832679271698, acc.: 55.18%] [G loss: 0.7255017161369324]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 48/86 [D loss: 0.6839188933372498, acc.: 55.81%] [G loss: 0.7280734181404114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 49/86 [D loss: 0.6834310293197632, acc.: 57.52%] [G loss: 0.725534200668335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 50/86 [D loss: 0.6818317770957947, acc.: 57.57%] [G loss: 0.7255895733833313]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 51/86 [D loss: 0.6802033483982086, acc.: 57.57%] [G loss: 0.7312932014465332]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 52/86 [D loss: 0.682534247636795, acc.: 56.79%] [G loss: 0.7229939103126526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 53/86 [D loss: 0.6841646730899811, acc.: 54.79%] [G loss: 0.7267464399337769]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 54/86 [D loss: 0.678041011095047, acc.: 58.59%] [G loss: 0.7271220684051514]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 55/86 [D loss: 0.6846260726451874, acc.: 55.71%] [G loss: 0.7291563153266907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 56/86 [D loss: 0.6770563125610352, acc.: 61.04%] [G loss: 0.728531539440155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 57/86 [D loss: 0.6866734623908997, acc.: 53.61%] [G loss: 0.7228708863258362]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 58/86 [D loss: 0.683128833770752, acc.: 57.18%] [G loss: 0.7281566858291626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 59/86 [D loss: 0.6802269220352173, acc.: 58.98%] [G loss: 0.7233740091323853]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 60/86 [D loss: 0.683704674243927, acc.: 56.59%] [G loss: 0.7305464744567871]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 61/86 [D loss: 0.6794232428073883, acc.: 59.18%] [G loss: 0.7218431234359741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 62/86 [D loss: 0.6868047714233398, acc.: 52.78%] [G loss: 0.7212209701538086]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 63/86 [D loss: 0.6812134385108948, acc.: 57.67%] [G loss: 0.7308106422424316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 64/86 [D loss: 0.6847412288188934, acc.: 54.74%] [G loss: 0.7197021842002869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 65/86 [D loss: 0.686046689748764, acc.: 54.25%] [G loss: 0.7281724214553833]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 66/86 [D loss: 0.6821995079517365, acc.: 55.27%] [G loss: 0.7204905152320862]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 67/86 [D loss: 0.6864316761493683, acc.: 54.79%] [G loss: 0.7248337864875793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 68/86 [D loss: 0.6790257096290588, acc.: 59.42%] [G loss: 0.7311570644378662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 69/86 [D loss: 0.683119922876358, acc.: 56.45%] [G loss: 0.7214338779449463]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 70/86 [D loss: 0.6830922961235046, acc.: 57.03%] [G loss: 0.7318189144134521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 71/86 [D loss: 0.6815416216850281, acc.: 57.28%] [G loss: 0.7222427725791931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 72/86 [D loss: 0.6857781708240509, acc.: 53.96%] [G loss: 0.7285835146903992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 73/86 [D loss: 0.6811664998531342, acc.: 58.64%] [G loss: 0.7202115654945374]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 74/86 [D loss: 0.686542421579361, acc.: 56.10%] [G loss: 0.7257736921310425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 75/86 [D loss: 0.6759947240352631, acc.: 61.43%] [G loss: 0.7303934097290039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 76/86 [D loss: 0.6815807521343231, acc.: 57.08%] [G loss: 0.725321888923645]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 77/86 [D loss: 0.6822050213813782, acc.: 56.74%] [G loss: 0.7274777293205261]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 78/86 [D loss: 0.6806701719760895, acc.: 58.01%] [G loss: 0.7274628281593323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 79/86 [D loss: 0.6801457703113556, acc.: 57.67%] [G loss: 0.7303000688552856]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 80/86 [D loss: 0.6808101832866669, acc.: 57.81%] [G loss: 0.7299767732620239]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 81/86 [D loss: 0.6808655560016632, acc.: 56.49%] [G loss: 0.7274404764175415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 82/86 [D loss: 0.6813444197177887, acc.: 57.13%] [G loss: 0.730742335319519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 83/86 [D loss: 0.6801727414131165, acc.: 58.06%] [G loss: 0.7282050848007202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 84/86 [D loss: 0.6839218437671661, acc.: 56.64%] [G loss: 0.729235053062439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 98/200, Batch 85/86 [D loss: 0.684654712677002, acc.: 55.47%] [G loss: 0.7281558513641357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 98/200, Batch 86/86 [D loss: 0.6825055480003357, acc.: 56.84%] [G loss: 0.7267112731933594]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 1/86 [D loss: 0.6804924309253693, acc.: 57.23%] [G loss: 0.7238911390304565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 2/86 [D loss: 0.6816032230854034, acc.: 56.93%] [G loss: 0.7267098426818848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 3/86 [D loss: 0.6812615096569061, acc.: 58.11%] [G loss: 0.7308444976806641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 4/86 [D loss: 0.6823968291282654, acc.: 57.62%] [G loss: 0.7247632741928101]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 5/86 [D loss: 0.6810785830020905, acc.: 57.96%] [G loss: 0.7221208810806274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 6/86 [D loss: 0.6821708083152771, acc.: 56.64%] [G loss: 0.7264703512191772]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 7/86 [D loss: 0.6821753978729248, acc.: 56.98%] [G loss: 0.7247648239135742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 8/86 [D loss: 0.6831568777561188, acc.: 56.74%] [G loss: 0.7255079746246338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 9/86 [D loss: 0.6820803880691528, acc.: 56.88%] [G loss: 0.7247710824012756]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 10/86 [D loss: 0.6830641329288483, acc.: 56.30%] [G loss: 0.7258076667785645]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 11/86 [D loss: 0.6829506158828735, acc.: 54.98%] [G loss: 0.7255654335021973]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 12/86 [D loss: 0.6817189455032349, acc.: 56.88%] [G loss: 0.7297344207763672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 13/86 [D loss: 0.6819973886013031, acc.: 56.79%] [G loss: 0.727541446685791]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 14/86 [D loss: 0.6808191537857056, acc.: 58.30%] [G loss: 0.7299545407295227]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 15/86 [D loss: 0.6832421123981476, acc.: 56.79%] [G loss: 0.7254155874252319]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 16/86 [D loss: 0.6823813021183014, acc.: 56.30%] [G loss: 0.7310396432876587]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 17/86 [D loss: 0.679957240819931, acc.: 58.59%] [G loss: 0.7294919490814209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 18/86 [D loss: 0.6831651926040649, acc.: 55.66%] [G loss: 0.7258233428001404]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 19/86 [D loss: 0.679572731256485, acc.: 58.40%] [G loss: 0.7270097732543945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 20/86 [D loss: 0.6835362017154694, acc.: 56.40%] [G loss: 0.7266582250595093]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 21/86 [D loss: 0.6806290149688721, acc.: 57.91%] [G loss: 0.7272213101387024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 22/86 [D loss: 0.6817223429679871, acc.: 57.13%] [G loss: 0.7275137305259705]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 23/86 [D loss: 0.6820855140686035, acc.: 56.64%] [G loss: 0.7339776754379272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 24/86 [D loss: 0.6782994270324707, acc.: 58.74%] [G loss: 0.727933406829834]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 25/86 [D loss: 0.6815569698810577, acc.: 57.86%] [G loss: 0.7286088466644287]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 26/86 [D loss: 0.6822069585323334, acc.: 55.96%] [G loss: 0.7273292541503906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 27/86 [D loss: 0.6843223571777344, acc.: 55.22%] [G loss: 0.7243965864181519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 28/86 [D loss: 0.68326535820961, acc.: 56.88%] [G loss: 0.724727213382721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 29/86 [D loss: 0.6802675724029541, acc.: 57.62%] [G loss: 0.7291662693023682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 30/86 [D loss: 0.6816235482692719, acc.: 57.18%] [G loss: 0.7271020412445068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 31/86 [D loss: 0.6834514439105988, acc.: 57.32%] [G loss: 0.7267529964447021]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 32/86 [D loss: 0.6810190379619598, acc.: 58.06%] [G loss: 0.7265079021453857]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 33/86 [D loss: 0.6812417507171631, acc.: 56.84%] [G loss: 0.7301191091537476]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 34/86 [D loss: 0.6845596134662628, acc.: 56.15%] [G loss: 0.7303136587142944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 35/86 [D loss: 0.6803208291530609, acc.: 59.62%] [G loss: 0.7301324605941772]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 36/86 [D loss: 0.6827105581760406, acc.: 57.08%] [G loss: 0.7306430339813232]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 37/86 [D loss: 0.6829738914966583, acc.: 56.35%] [G loss: 0.7307389974594116]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 38/86 [D loss: 0.680801510810852, acc.: 57.42%] [G loss: 0.7289575338363647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 39/86 [D loss: 0.6791777312755585, acc.: 59.18%] [G loss: 0.7286840081214905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 40/86 [D loss: 0.6840598285198212, acc.: 54.88%] [G loss: 0.730250358581543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 41/86 [D loss: 0.6799821257591248, acc.: 58.89%] [G loss: 0.7293710112571716]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 42/86 [D loss: 0.6817519664764404, acc.: 56.79%] [G loss: 0.7275397777557373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 43/86 [D loss: 0.6796602606773376, acc.: 58.64%] [G loss: 0.726226806640625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 44/86 [D loss: 0.6823957860469818, acc.: 57.96%] [G loss: 0.728310227394104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 45/86 [D loss: 0.6780744194984436, acc.: 58.89%] [G loss: 0.7308476567268372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 46/86 [D loss: 0.6791509687900543, acc.: 58.89%] [G loss: 0.7276278138160706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 47/86 [D loss: 0.6830309927463531, acc.: 55.91%] [G loss: 0.7265052795410156]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 48/86 [D loss: 0.6809051632881165, acc.: 57.57%] [G loss: 0.7216413617134094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 49/86 [D loss: 0.682701051235199, acc.: 57.32%] [G loss: 0.7293715476989746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 50/86 [D loss: 0.682560920715332, acc.: 55.86%] [G loss: 0.7313517928123474]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 51/86 [D loss: 0.6820489168167114, acc.: 56.93%] [G loss: 0.72611403465271]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 52/86 [D loss: 0.6811298429965973, acc.: 57.37%] [G loss: 0.7267796397209167]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 99/200, Batch 53/86 [D loss: 0.6807052493095398, acc.: 58.25%] [G loss: 0.7262479066848755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 54/86 [D loss: 0.6808636486530304, acc.: 58.06%] [G loss: 0.7305235266685486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 55/86 [D loss: 0.6797426640987396, acc.: 57.42%] [G loss: 0.7298243045806885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 56/86 [D loss: 0.6822437942028046, acc.: 56.59%] [G loss: 0.7303011417388916]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 57/86 [D loss: 0.6825494766235352, acc.: 57.37%] [G loss: 0.7252744436264038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 58/86 [D loss: 0.6813361942768097, acc.: 56.10%] [G loss: 0.7264156341552734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 59/86 [D loss: 0.6826936900615692, acc.: 56.54%] [G loss: 0.7336149215698242]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 60/86 [D loss: 0.6822319328784943, acc.: 56.01%] [G loss: 0.7275071144104004]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 61/86 [D loss: 0.6801214516162872, acc.: 57.28%] [G loss: 0.7289424538612366]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 62/86 [D loss: 0.682497650384903, acc.: 56.49%] [G loss: 0.7263908982276917]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 99/200, Batch 63/86 [D loss: 0.681782454252243, acc.: 56.69%] [G loss: 0.7287964820861816]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 64/86 [D loss: 0.6815762519836426, acc.: 58.06%] [G loss: 0.7289221286773682]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 65/86 [D loss: 0.6794449090957642, acc.: 57.71%] [G loss: 0.7342767119407654]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 66/86 [D loss: 0.6843186616897583, acc.: 54.93%] [G loss: 0.7266998291015625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 67/86 [D loss: 0.681911051273346, acc.: 58.01%] [G loss: 0.7308313250541687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 68/86 [D loss: 0.6808024644851685, acc.: 57.28%] [G loss: 0.7249192595481873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 69/86 [D loss: 0.6811188459396362, acc.: 57.67%] [G loss: 0.7282140254974365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 70/86 [D loss: 0.6802360117435455, acc.: 59.33%] [G loss: 0.7264896035194397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 71/86 [D loss: 0.6835517585277557, acc.: 57.76%] [G loss: 0.7285780906677246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 72/86 [D loss: 0.6804600358009338, acc.: 57.76%] [G loss: 0.723583459854126]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 73/86 [D loss: 0.6839827001094818, acc.: 55.18%] [G loss: 0.7233981490135193]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 74/86 [D loss: 0.6823036372661591, acc.: 56.98%] [G loss: 0.7257198095321655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 75/86 [D loss: 0.681140661239624, acc.: 57.52%] [G loss: 0.7229938507080078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 76/86 [D loss: 0.6843245029449463, acc.: 55.32%] [G loss: 0.7279289960861206]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 77/86 [D loss: 0.6824827790260315, acc.: 56.20%] [G loss: 0.7264938950538635]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 78/86 [D loss: 0.682405948638916, acc.: 54.54%] [G loss: 0.727268397808075]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 79/86 [D loss: 0.6830901205539703, acc.: 57.28%] [G loss: 0.7268877029418945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 80/86 [D loss: 0.6823602616786957, acc.: 56.49%] [G loss: 0.7256309986114502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 81/86 [D loss: 0.6819417476654053, acc.: 57.47%] [G loss: 0.7352315187454224]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 82/86 [D loss: 0.681626945734024, acc.: 55.76%] [G loss: 0.7274895906448364]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 83/86 [D loss: 0.6818410158157349, acc.: 56.59%] [G loss: 0.7295889854431152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 99/200, Batch 84/86 [D loss: 0.6808463037014008, acc.: 57.62%] [G loss: 0.7257973551750183]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 85/86 [D loss: 0.6822921335697174, acc.: 56.88%] [G loss: 0.7312837243080139]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 99/200, Batch 86/86 [D loss: 0.6801376938819885, acc.: 58.94%] [G loss: 0.7281574010848999]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 1/86 [D loss: 0.6800652742385864, acc.: 58.50%] [G loss: 0.7299870848655701]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 2/86 [D loss: 0.6825236678123474, acc.: 57.52%] [G loss: 0.7307307124137878]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 3/86 [D loss: 0.6799598336219788, acc.: 59.47%] [G loss: 0.728711724281311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 4/86 [D loss: 0.6807591915130615, acc.: 58.54%] [G loss: 0.7282571792602539]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 5/86 [D loss: 0.6824018657207489, acc.: 56.45%] [G loss: 0.7251691222190857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 6/86 [D loss: 0.6819308400154114, acc.: 56.45%] [G loss: 0.7296149730682373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 7/86 [D loss: 0.6800002157688141, acc.: 58.35%] [G loss: 0.7275065183639526]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 100/200, Batch 8/86 [D loss: 0.681678831577301, acc.: 56.69%] [G loss: 0.731704831123352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 9/86 [D loss: 0.6835307478904724, acc.: 56.84%] [G loss: 0.7291477918624878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 10/86 [D loss: 0.6791377663612366, acc.: 59.18%] [G loss: 0.7258538603782654]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 11/86 [D loss: 0.6837552785873413, acc.: 55.81%] [G loss: 0.7250553369522095]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 12/86 [D loss: 0.679755687713623, acc.: 58.98%] [G loss: 0.7299235463142395]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 13/86 [D loss: 0.6833885312080383, acc.: 58.11%] [G loss: 0.7281262874603271]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 14/86 [D loss: 0.6808016300201416, acc.: 58.01%] [G loss: 0.7260494828224182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 15/86 [D loss: 0.6815982758998871, acc.: 56.79%] [G loss: 0.7248076796531677]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 16/86 [D loss: 0.6817847788333893, acc.: 56.93%] [G loss: 0.7264689803123474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 17/86 [D loss: 0.6812525987625122, acc.: 56.59%] [G loss: 0.7338053584098816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 18/86 [D loss: 0.6837352514266968, acc.: 56.88%] [G loss: 0.7267880439758301]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 19/86 [D loss: 0.6826474964618683, acc.: 56.01%] [G loss: 0.7311015129089355]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 20/86 [D loss: 0.679144561290741, acc.: 57.86%] [G loss: 0.7319046258926392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 21/86 [D loss: 0.6789973080158234, acc.: 58.11%] [G loss: 0.729708194732666]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 22/86 [D loss: 0.6816961169242859, acc.: 56.20%] [G loss: 0.7281507253646851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 23/86 [D loss: 0.6807231605052948, acc.: 57.47%] [G loss: 0.7331817150115967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 24/86 [D loss: 0.6809944212436676, acc.: 57.32%] [G loss: 0.7288067936897278]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 25/86 [D loss: 0.6828323900699615, acc.: 56.25%] [G loss: 0.7294972538948059]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 26/86 [D loss: 0.6792722046375275, acc.: 58.54%] [G loss: 0.7265728116035461]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 27/86 [D loss: 0.6809167563915253, acc.: 57.96%] [G loss: 0.725504994392395]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 28/86 [D loss: 0.6816396415233612, acc.: 57.42%] [G loss: 0.7298598289489746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 29/86 [D loss: 0.6798869371414185, acc.: 58.30%] [G loss: 0.724807620048523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 30/86 [D loss: 0.6820309460163116, acc.: 56.40%] [G loss: 0.7303087115287781]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 31/86 [D loss: 0.6829472184181213, acc.: 56.10%] [G loss: 0.7266272306442261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 32/86 [D loss: 0.6826134920120239, acc.: 55.96%] [G loss: 0.7308716177940369]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 33/86 [D loss: 0.6835652589797974, acc.: 56.20%] [G loss: 0.7310347557067871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 34/86 [D loss: 0.6823950111865997, acc.: 56.45%] [G loss: 0.7304404377937317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 35/86 [D loss: 0.6803219020366669, acc.: 57.42%] [G loss: 0.7323074340820312]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 36/86 [D loss: 0.6800313293933868, acc.: 58.50%] [G loss: 0.7259458303451538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 37/86 [D loss: 0.6842272579669952, acc.: 56.35%] [G loss: 0.7321778535842896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 38/86 [D loss: 0.6817202866077423, acc.: 56.88%] [G loss: 0.7261022329330444]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 39/86 [D loss: 0.6797462999820709, acc.: 57.32%] [G loss: 0.7279822826385498]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 40/86 [D loss: 0.6822007596492767, acc.: 55.62%] [G loss: 0.722830057144165]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 41/86 [D loss: 0.6842688620090485, acc.: 54.88%] [G loss: 0.7277650833129883]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 42/86 [D loss: 0.6838931739330292, acc.: 55.47%] [G loss: 0.7297812104225159]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 43/86 [D loss: 0.6800982356071472, acc.: 58.01%] [G loss: 0.7275661826133728]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 44/86 [D loss: 0.6817721724510193, acc.: 55.96%] [G loss: 0.7300961017608643]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 45/86 [D loss: 0.6804186403751373, acc.: 59.13%] [G loss: 0.7291438579559326]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 46/86 [D loss: 0.681035190820694, acc.: 58.40%] [G loss: 0.7311508655548096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 47/86 [D loss: 0.6811490058898926, acc.: 58.59%] [G loss: 0.7311933040618896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 48/86 [D loss: 0.6811697483062744, acc.: 57.52%] [G loss: 0.7309232950210571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 49/86 [D loss: 0.6812085211277008, acc.: 58.79%] [G loss: 0.730150580406189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 50/86 [D loss: 0.6815291941165924, acc.: 56.20%] [G loss: 0.7252312898635864]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 51/86 [D loss: 0.6828943192958832, acc.: 57.28%] [G loss: 0.7286583185195923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 52/86 [D loss: 0.6788716912269592, acc.: 58.98%] [G loss: 0.732779860496521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 53/86 [D loss: 0.6798180341720581, acc.: 57.52%] [G loss: 0.7275906801223755]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 54/86 [D loss: 0.6805468201637268, acc.: 57.13%] [G loss: 0.7292279601097107]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 55/86 [D loss: 0.6807799339294434, acc.: 57.37%] [G loss: 0.7315411567687988]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 56/86 [D loss: 0.6801797151565552, acc.: 57.42%] [G loss: 0.7278913855552673]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 57/86 [D loss: 0.6823599934577942, acc.: 56.30%] [G loss: 0.72934889793396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 58/86 [D loss: 0.6800220012664795, acc.: 58.64%] [G loss: 0.7274142503738403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 59/86 [D loss: 0.6829479932785034, acc.: 56.01%] [G loss: 0.7277824878692627]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 60/86 [D loss: 0.6844444274902344, acc.: 54.98%] [G loss: 0.7266340851783752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 61/86 [D loss: 0.6795451939105988, acc.: 57.76%] [G loss: 0.7240601181983948]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 62/86 [D loss: 0.6811438500881195, acc.: 58.35%] [G loss: 0.726976752281189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 63/86 [D loss: 0.6818040311336517, acc.: 56.45%] [G loss: 0.7285388708114624]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 64/86 [D loss: 0.6823315024375916, acc.: 56.59%] [G loss: 0.727406919002533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 65/86 [D loss: 0.6827095746994019, acc.: 56.45%] [G loss: 0.7332434058189392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 66/86 [D loss: 0.6826582252979279, acc.: 56.69%] [G loss: 0.7301539778709412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 67/86 [D loss: 0.6797612309455872, acc.: 57.52%] [G loss: 0.7334464192390442]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 68/86 [D loss: 0.6835424602031708, acc.: 56.40%] [G loss: 0.7297782301902771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 69/86 [D loss: 0.6795744597911835, acc.: 56.79%] [G loss: 0.7286621332168579]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 70/86 [D loss: 0.6823030114173889, acc.: 56.40%] [G loss: 0.7305325865745544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 71/86 [D loss: 0.6791712641716003, acc.: 59.77%] [G loss: 0.7305468320846558]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 72/86 [D loss: 0.6838779449462891, acc.: 55.57%] [G loss: 0.7352840304374695]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 100/200, Batch 73/86 [D loss: 0.6791139245033264, acc.: 59.08%] [G loss: 0.7338212132453918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 74/86 [D loss: 0.6828858554363251, acc.: 56.01%] [G loss: 0.7294392585754395]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 75/86 [D loss: 0.6817367970943451, acc.: 56.35%] [G loss: 0.7294809222221375]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 76/86 [D loss: 0.6849798560142517, acc.: 56.45%] [G loss: 0.7331061959266663]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 77/86 [D loss: 0.6819864511489868, acc.: 56.59%] [G loss: 0.7320426106452942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 78/86 [D loss: 0.6800053715705872, acc.: 57.81%] [G loss: 0.7314290404319763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 79/86 [D loss: 0.683643102645874, acc.: 54.93%] [G loss: 0.731957733631134]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 80/86 [D loss: 0.6811936795711517, acc.: 57.08%] [G loss: 0.7353799939155579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 81/86 [D loss: 0.6829082071781158, acc.: 56.40%] [G loss: 0.7288837432861328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 82/86 [D loss: 0.6822216808795929, acc.: 57.37%] [G loss: 0.7361605167388916]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 83/86 [D loss: 0.6823285818099976, acc.: 56.20%] [G loss: 0.7288373112678528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 100/200, Batch 84/86 [D loss: 0.6851465404033661, acc.: 54.93%] [G loss: 0.7304515838623047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 85/86 [D loss: 0.6804619431495667, acc.: 58.74%] [G loss: 0.7296342849731445]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 100/200, Batch 86/86 [D loss: 0.6854730546474457, acc.: 54.74%] [G loss: 0.7290804982185364]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 1/86 [D loss: 0.6791301965713501, acc.: 56.84%] [G loss: 0.7285463809967041]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 2/86 [D loss: 0.6824808716773987, acc.: 56.05%] [G loss: 0.7285361289978027]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 101/200, Batch 3/86 [D loss: 0.6786975860595703, acc.: 56.64%] [G loss: 0.7258187532424927]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 4/86 [D loss: 0.6835572123527527, acc.: 56.10%] [G loss: 0.7316905856132507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 5/86 [D loss: 0.6774641871452332, acc.: 59.13%] [G loss: 0.7308011054992676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 6/86 [D loss: 0.6785007417201996, acc.: 58.79%] [G loss: 0.7272000908851624]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 7/86 [D loss: 0.6823069751262665, acc.: 54.88%] [G loss: 0.7320515513420105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 8/86 [D loss: 0.6815350651741028, acc.: 57.57%] [G loss: 0.7317744493484497]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 9/86 [D loss: 0.681247353553772, acc.: 56.98%] [G loss: 0.7277927994728088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 10/86 [D loss: 0.6809793710708618, acc.: 56.40%] [G loss: 0.7277215719223022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 11/86 [D loss: 0.6791339218616486, acc.: 59.08%] [G loss: 0.7263268828392029]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 12/86 [D loss: 0.6798297762870789, acc.: 58.15%] [G loss: 0.7307760715484619]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 13/86 [D loss: 0.680385947227478, acc.: 58.50%] [G loss: 0.7336389422416687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 14/86 [D loss: 0.6850047707557678, acc.: 55.47%] [G loss: 0.7273178100585938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 15/86 [D loss: 0.6825379133224487, acc.: 55.66%] [G loss: 0.7276703119277954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 16/86 [D loss: 0.6826503872871399, acc.: 56.59%] [G loss: 0.7259869575500488]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 17/86 [D loss: 0.6837033033370972, acc.: 57.08%] [G loss: 0.7326673865318298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 18/86 [D loss: 0.6819839179515839, acc.: 56.54%] [G loss: 0.7341988682746887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 19/86 [D loss: 0.6801082789897919, acc.: 58.15%] [G loss: 0.7322160005569458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 20/86 [D loss: 0.6852898001670837, acc.: 53.52%] [G loss: 0.7302873134613037]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 21/86 [D loss: 0.6808429956436157, acc.: 57.42%] [G loss: 0.7306398153305054]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 22/86 [D loss: 0.6819002032279968, acc.: 58.35%] [G loss: 0.7242996692657471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 23/86 [D loss: 0.6814372539520264, acc.: 56.05%] [G loss: 0.7268655300140381]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 24/86 [D loss: 0.6793125867843628, acc.: 57.62%] [G loss: 0.7336959838867188]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 25/86 [D loss: 0.6800296008586884, acc.: 57.23%] [G loss: 0.7261881828308105]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 26/86 [D loss: 0.6799991428852081, acc.: 58.59%] [G loss: 0.7260850667953491]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 27/86 [D loss: 0.6794305145740509, acc.: 58.74%] [G loss: 0.7293024659156799]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 28/86 [D loss: 0.6817401945590973, acc.: 58.25%] [G loss: 0.7235094308853149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 29/86 [D loss: 0.6833082437515259, acc.: 55.96%] [G loss: 0.7296962738037109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 30/86 [D loss: 0.6796019673347473, acc.: 58.01%] [G loss: 0.7294799089431763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 31/86 [D loss: 0.6853229105472565, acc.: 55.08%] [G loss: 0.7326804399490356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 32/86 [D loss: 0.6817931532859802, acc.: 56.69%] [G loss: 0.7287492156028748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 33/86 [D loss: 0.6849038004875183, acc.: 55.08%] [G loss: 0.7271550297737122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 34/86 [D loss: 0.682593047618866, acc.: 56.59%] [G loss: 0.7317243814468384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 35/86 [D loss: 0.6830637753009796, acc.: 56.40%] [G loss: 0.7247992157936096]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 101/200, Batch 36/86 [D loss: 0.6826316714286804, acc.: 55.91%] [G loss: 0.7383556365966797]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 37/86 [D loss: 0.6774466037750244, acc.: 58.69%] [G loss: 0.7288439869880676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 38/86 [D loss: 0.6841919422149658, acc.: 54.79%] [G loss: 0.7317100763320923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 39/86 [D loss: 0.6799968481063843, acc.: 56.84%] [G loss: 0.7301885485649109]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 40/86 [D loss: 0.6842852830886841, acc.: 56.01%] [G loss: 0.7445846796035767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 41/86 [D loss: 0.6778775453567505, acc.: 57.96%] [G loss: 0.7323087453842163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 42/86 [D loss: 0.6807298064231873, acc.: 56.64%] [G loss: 0.7282471060752869]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 43/86 [D loss: 0.6809383034706116, acc.: 57.32%] [G loss: 0.7306631803512573]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 44/86 [D loss: 0.6841611266136169, acc.: 54.88%] [G loss: 0.7321816682815552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 45/86 [D loss: 0.6801901161670685, acc.: 57.47%] [G loss: 0.7306458353996277]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 46/86 [D loss: 0.679739236831665, acc.: 57.96%] [G loss: 0.727728545665741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 47/86 [D loss: 0.6843503713607788, acc.: 54.93%] [G loss: 0.7308910489082336]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 48/86 [D loss: 0.682901918888092, acc.: 56.30%] [G loss: 0.730666995048523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 49/86 [D loss: 0.6871026754379272, acc.: 53.81%] [G loss: 0.7263691425323486]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 50/86 [D loss: 0.6818541586399078, acc.: 57.57%] [G loss: 0.7308443188667297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 51/86 [D loss: 0.6818835735321045, acc.: 56.45%] [G loss: 0.7229030728340149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 52/86 [D loss: 0.6831271052360535, acc.: 56.35%] [G loss: 0.7297348380088806]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 53/86 [D loss: 0.6798620223999023, acc.: 58.30%] [G loss: 0.7222318053245544]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 54/86 [D loss: 0.6853824853897095, acc.: 55.37%] [G loss: 0.7301185131072998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 55/86 [D loss: 0.680225282907486, acc.: 58.59%] [G loss: 0.724755048751831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 56/86 [D loss: 0.6833891868591309, acc.: 54.93%] [G loss: 0.728620171546936]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 57/86 [D loss: 0.6773474216461182, acc.: 59.72%] [G loss: 0.7291876673698425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 58/86 [D loss: 0.6834172904491425, acc.: 56.79%] [G loss: 0.7225094437599182]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 59/86 [D loss: 0.6838391423225403, acc.: 54.88%] [G loss: 0.7271151542663574]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 60/86 [D loss: 0.6860869228839874, acc.: 52.93%] [G loss: 0.718928873538971]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 61/86 [D loss: 0.683916300535202, acc.: 55.57%] [G loss: 0.7308112978935242]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 62/86 [D loss: 0.6788808107376099, acc.: 57.52%] [G loss: 0.726267397403717]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 63/86 [D loss: 0.6885997354984283, acc.: 53.08%] [G loss: 0.7367333769798279]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 64/86 [D loss: 0.6833955943584442, acc.: 54.20%] [G loss: 0.7192076444625854]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 65/86 [D loss: 0.6835123896598816, acc.: 54.83%] [G loss: 0.7331193685531616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 66/86 [D loss: 0.6799404323101044, acc.: 58.06%] [G loss: 0.7324751615524292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 67/86 [D loss: 0.6829889118671417, acc.: 55.76%] [G loss: 0.7286387085914612]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 68/86 [D loss: 0.683912068605423, acc.: 55.96%] [G loss: 0.7291072607040405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 69/86 [D loss: 0.679080992937088, acc.: 57.91%] [G loss: 0.7231416702270508]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 70/86 [D loss: 0.6862327456474304, acc.: 54.44%] [G loss: 0.7276902794837952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 71/86 [D loss: 0.6805648803710938, acc.: 56.69%] [G loss: 0.723800003528595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 72/86 [D loss: 0.6898910105228424, acc.: 52.98%] [G loss: 0.72612464427948]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 73/86 [D loss: 0.6799967288970947, acc.: 58.59%] [G loss: 0.7247391939163208]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 74/86 [D loss: 0.6864523589611053, acc.: 53.81%] [G loss: 0.7280296087265015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 75/86 [D loss: 0.6815273761749268, acc.: 57.42%] [G loss: 0.7337551712989807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 76/86 [D loss: 0.6800525784492493, acc.: 59.23%] [G loss: 0.7211103439331055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 77/86 [D loss: 0.688386857509613, acc.: 53.71%] [G loss: 0.7346707582473755]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 78/86 [D loss: 0.6838525533676147, acc.: 55.08%] [G loss: 0.7253795862197876]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 79/86 [D loss: 0.6874976754188538, acc.: 52.39%] [G loss: 0.7268524169921875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 80/86 [D loss: 0.6773211359977722, acc.: 59.28%] [G loss: 0.7282462120056152]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 81/86 [D loss: 0.6829765141010284, acc.: 56.54%] [G loss: 0.7290804982185364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 82/86 [D loss: 0.6812817454338074, acc.: 56.64%] [G loss: 0.7308282852172852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 83/86 [D loss: 0.6836054027080536, acc.: 54.64%] [G loss: 0.7264209389686584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 84/86 [D loss: 0.6817175447940826, acc.: 56.45%] [G loss: 0.733532726764679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 101/200, Batch 85/86 [D loss: 0.6767019331455231, acc.: 59.28%] [G loss: 0.7271239161491394]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 101/200, Batch 86/86 [D loss: 0.6823189854621887, acc.: 55.66%] [G loss: 0.7292370796203613]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 1/86 [D loss: 0.6820304989814758, acc.: 57.18%] [G loss: 0.7326381206512451]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 2/86 [D loss: 0.6797556579113007, acc.: 57.32%] [G loss: 0.7256289720535278]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 3/86 [D loss: 0.6823687851428986, acc.: 56.93%] [G loss: 0.7353938817977905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 4/86 [D loss: 0.6798258125782013, acc.: 58.01%] [G loss: 0.7342360019683838]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 5/86 [D loss: 0.6834506988525391, acc.: 56.05%] [G loss: 0.7327741384506226]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 6/86 [D loss: 0.6830021440982819, acc.: 56.74%] [G loss: 0.734368085861206]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 7/86 [D loss: 0.6784233748912811, acc.: 59.77%] [G loss: 0.7290709018707275]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 8/86 [D loss: 0.6814886033535004, acc.: 56.20%] [G loss: 0.7301529049873352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 9/86 [D loss: 0.6797935366630554, acc.: 57.71%] [G loss: 0.7319579124450684]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 10/86 [D loss: 0.6850762963294983, acc.: 55.81%] [G loss: 0.7280738949775696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 11/86 [D loss: 0.6771762371063232, acc.: 58.50%] [G loss: 0.7318301796913147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 12/86 [D loss: 0.6822112500667572, acc.: 55.86%] [G loss: 0.725968062877655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 13/86 [D loss: 0.6805458664894104, acc.: 56.45%] [G loss: 0.7338666915893555]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 14/86 [D loss: 0.6851269602775574, acc.: 54.98%] [G loss: 0.728436291217804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 15/86 [D loss: 0.6803277730941772, acc.: 57.57%] [G loss: 0.7331993579864502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 16/86 [D loss: 0.6825901865959167, acc.: 56.10%] [G loss: 0.7314624786376953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 17/86 [D loss: 0.6816039681434631, acc.: 57.28%] [G loss: 0.7287014722824097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 18/86 [D loss: 0.6853635907173157, acc.: 55.76%] [G loss: 0.7294572591781616]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 19/86 [D loss: 0.6805936098098755, acc.: 58.79%] [G loss: 0.7348693013191223]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 20/86 [D loss: 0.6783673465251923, acc.: 57.52%] [G loss: 0.7370676398277283]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 21/86 [D loss: 0.6830039620399475, acc.: 56.10%] [G loss: 0.730243980884552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 22/86 [D loss: 0.6818827390670776, acc.: 56.64%] [G loss: 0.7331534624099731]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 23/86 [D loss: 0.6822863221168518, acc.: 56.45%] [G loss: 0.7294113636016846]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 24/86 [D loss: 0.6815694272518158, acc.: 57.32%] [G loss: 0.7329937219619751]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 25/86 [D loss: 0.6814664602279663, acc.: 56.84%] [G loss: 0.7282976508140564]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 26/86 [D loss: 0.682001143693924, acc.: 57.52%] [G loss: 0.7263443470001221]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 27/86 [D loss: 0.6798899471759796, acc.: 57.62%] [G loss: 0.7296675443649292]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 28/86 [D loss: 0.6798860728740692, acc.: 57.23%] [G loss: 0.7339907884597778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 29/86 [D loss: 0.6831033825874329, acc.: 57.08%] [G loss: 0.7291248440742493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 30/86 [D loss: 0.6802000105381012, acc.: 58.50%] [G loss: 0.7295082807540894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 31/86 [D loss: 0.6787796318531036, acc.: 57.81%] [G loss: 0.7303978204727173]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 32/86 [D loss: 0.6837315559387207, acc.: 55.91%] [G loss: 0.7280693650245667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 102/200, Batch 33/86 [D loss: 0.679559051990509, acc.: 57.86%] [G loss: 0.7305278778076172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 34/86 [D loss: 0.6780750453472137, acc.: 58.89%] [G loss: 0.7279784083366394]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 35/86 [D loss: 0.6825279295444489, acc.: 57.18%] [G loss: 0.7313976883888245]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 36/86 [D loss: 0.6798182129859924, acc.: 57.18%] [G loss: 0.731342613697052]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 37/86 [D loss: 0.6832189559936523, acc.: 55.96%] [G loss: 0.7356939315795898]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 38/86 [D loss: 0.6803726255893707, acc.: 58.15%] [G loss: 0.7360457181930542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 39/86 [D loss: 0.678954690694809, acc.: 59.08%] [G loss: 0.728848397731781]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 40/86 [D loss: 0.6814431846141815, acc.: 57.08%] [G loss: 0.7319797277450562]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 41/86 [D loss: 0.6812939047813416, acc.: 56.49%] [G loss: 0.7313895225524902]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 42/86 [D loss: 0.6830157339572906, acc.: 55.71%] [G loss: 0.7289232015609741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 43/86 [D loss: 0.6837026476860046, acc.: 54.93%] [G loss: 0.7322967052459717]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 44/86 [D loss: 0.6802647113800049, acc.: 57.08%] [G loss: 0.7351363897323608]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 45/86 [D loss: 0.682860791683197, acc.: 55.76%] [G loss: 0.7314372658729553]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 46/86 [D loss: 0.6829783022403717, acc.: 56.74%] [G loss: 0.7282142043113708]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 47/86 [D loss: 0.6819973886013031, acc.: 56.20%] [G loss: 0.7336320877075195]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 48/86 [D loss: 0.6799154579639435, acc.: 58.25%] [G loss: 0.7262213230133057]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 49/86 [D loss: 0.6807994544506073, acc.: 58.89%] [G loss: 0.7280944585800171]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 50/86 [D loss: 0.680034726858139, acc.: 58.01%] [G loss: 0.7301149368286133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 51/86 [D loss: 0.6823373734951019, acc.: 57.03%] [G loss: 0.7251791954040527]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 52/86 [D loss: 0.6819241642951965, acc.: 56.59%] [G loss: 0.7281165719032288]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 53/86 [D loss: 0.6838131248950958, acc.: 56.01%] [G loss: 0.7289440631866455]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 54/86 [D loss: 0.6840972006320953, acc.: 55.22%] [G loss: 0.7310624122619629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 55/86 [D loss: 0.6790491938591003, acc.: 58.11%] [G loss: 0.7270238399505615]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 56/86 [D loss: 0.6823618710041046, acc.: 55.37%] [G loss: 0.7315442562103271]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 57/86 [D loss: 0.6784743666648865, acc.: 58.94%] [G loss: 0.7329042553901672]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 58/86 [D loss: 0.6826994717121124, acc.: 55.96%] [G loss: 0.7304652333259583]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 59/86 [D loss: 0.681826263666153, acc.: 57.91%] [G loss: 0.7291046380996704]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 60/86 [D loss: 0.6856129169464111, acc.: 54.15%] [G loss: 0.7258572578430176]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 61/86 [D loss: 0.6809124052524567, acc.: 57.62%] [G loss: 0.7333348393440247]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 62/86 [D loss: 0.6800269782543182, acc.: 57.03%] [G loss: 0.7311273217201233]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 63/86 [D loss: 0.6817150712013245, acc.: 57.71%] [G loss: 0.7269235253334045]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 102/200, Batch 64/86 [D loss: 0.6807534098625183, acc.: 57.08%] [G loss: 0.7305428385734558]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 65/86 [D loss: 0.686061441898346, acc.: 54.00%] [G loss: 0.7246052622795105]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 66/86 [D loss: 0.683119148015976, acc.: 55.08%] [G loss: 0.7329095602035522]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 67/86 [D loss: 0.681723415851593, acc.: 56.98%] [G loss: 0.7193368077278137]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 68/86 [D loss: 0.6904693841934204, acc.: 52.98%] [G loss: 0.7428052425384521]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 69/86 [D loss: 0.6829249560832977, acc.: 57.28%] [G loss: 0.7216312289237976]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 70/86 [D loss: 0.6853636801242828, acc.: 54.00%] [G loss: 0.7343065738677979]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 71/86 [D loss: 0.6814604997634888, acc.: 57.57%] [G loss: 0.7235206365585327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 72/86 [D loss: 0.6903029382228851, acc.: 52.39%] [G loss: 0.7376848459243774]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 73/86 [D loss: 0.676918238401413, acc.: 58.94%] [G loss: 0.7226446270942688]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 74/86 [D loss: 0.6865508854389191, acc.: 54.39%] [G loss: 0.7251560091972351]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 75/86 [D loss: 0.6806877553462982, acc.: 57.67%] [G loss: 0.7333948612213135]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 76/86 [D loss: 0.68160280585289, acc.: 57.67%] [G loss: 0.7280527353286743]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 77/86 [D loss: 0.6837977766990662, acc.: 56.45%] [G loss: 0.7381008863449097]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 102/200, Batch 78/86 [D loss: 0.679864764213562, acc.: 58.79%] [G loss: 0.7286866903305054]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 79/86 [D loss: 0.6803175508975983, acc.: 57.42%] [G loss: 0.7361201643943787]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 80/86 [D loss: 0.681512713432312, acc.: 56.49%] [G loss: 0.7307740449905396]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 81/86 [D loss: 0.682736873626709, acc.: 56.54%] [G loss: 0.7303885817527771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 82/86 [D loss: 0.6803211271762848, acc.: 56.20%] [G loss: 0.7360332608222961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 102/200, Batch 83/86 [D loss: 0.6815194487571716, acc.: 57.13%] [G loss: 0.7249268293380737]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 84/86 [D loss: 0.6795667707920074, acc.: 57.08%] [G loss: 0.7286363840103149]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 85/86 [D loss: 0.6815696060657501, acc.: 56.49%] [G loss: 0.7255969047546387]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 102/200, Batch 86/86 [D loss: 0.683837890625, acc.: 55.91%] [G loss: 0.728268027305603]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 1/86 [D loss: 0.6812741160392761, acc.: 58.11%] [G loss: 0.7344284653663635]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 2/86 [D loss: 0.6808975338935852, acc.: 57.23%] [G loss: 0.7298697233200073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 3/86 [D loss: 0.6839606761932373, acc.: 55.08%] [G loss: 0.7296721935272217]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 4/86 [D loss: 0.6755105257034302, acc.: 59.42%] [G loss: 0.728806734085083]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 5/86 [D loss: 0.677981972694397, acc.: 59.28%] [G loss: 0.7281662225723267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 6/86 [D loss: 0.6820285618305206, acc.: 56.49%] [G loss: 0.7289323806762695]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 7/86 [D loss: 0.6832090616226196, acc.: 56.20%] [G loss: 0.7273032665252686]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 8/86 [D loss: 0.6813209354877472, acc.: 55.66%] [G loss: 0.7267392873764038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 9/86 [D loss: 0.6792265474796295, acc.: 57.62%] [G loss: 0.7262920141220093]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 10/86 [D loss: 0.6810792684555054, acc.: 57.37%] [G loss: 0.7309028506278992]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 11/86 [D loss: 0.680965393781662, acc.: 56.59%] [G loss: 0.7335705757141113]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 12/86 [D loss: 0.6792723536491394, acc.: 58.40%] [G loss: 0.7304903268814087]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 13/86 [D loss: 0.6807703375816345, acc.: 57.62%] [G loss: 0.7285440564155579]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 14/86 [D loss: 0.6803021132946014, acc.: 57.76%] [G loss: 0.7294056415557861]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 15/86 [D loss: 0.6821530163288116, acc.: 56.93%] [G loss: 0.7281659841537476]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 16/86 [D loss: 0.6825351715087891, acc.: 55.57%] [G loss: 0.7329997420310974]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 17/86 [D loss: 0.6800439059734344, acc.: 56.64%] [G loss: 0.7288805842399597]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 103/200, Batch 18/86 [D loss: 0.6793183982372284, acc.: 57.52%] [G loss: 0.7300084233283997]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 19/86 [D loss: 0.6816412508487701, acc.: 57.47%] [G loss: 0.7316766977310181]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 20/86 [D loss: 0.6815623641014099, acc.: 56.15%] [G loss: 0.7283444404602051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 21/86 [D loss: 0.6809968054294586, acc.: 58.01%] [G loss: 0.7321393489837646]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 22/86 [D loss: 0.6808395087718964, acc.: 56.69%] [G loss: 0.7243619561195374]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 23/86 [D loss: 0.683055579662323, acc.: 55.66%] [G loss: 0.7289873957633972]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 103/200, Batch 24/86 [D loss: 0.6796126663684845, acc.: 57.32%] [G loss: 0.729945182800293]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 25/86 [D loss: 0.6789448857307434, acc.: 59.42%] [G loss: 0.7307417392730713]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 26/86 [D loss: 0.6801507771015167, acc.: 57.91%] [G loss: 0.736605703830719]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 27/86 [D loss: 0.6815443933010101, acc.: 57.32%] [G loss: 0.7311272621154785]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 28/86 [D loss: 0.6810774803161621, acc.: 56.05%] [G loss: 0.7324768900871277]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 29/86 [D loss: 0.6816021502017975, acc.: 56.40%] [G loss: 0.7281365394592285]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 30/86 [D loss: 0.6826136708259583, acc.: 56.15%] [G loss: 0.7308177947998047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 31/86 [D loss: 0.6808615624904633, acc.: 56.54%] [G loss: 0.7286882400512695]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 32/86 [D loss: 0.6847548484802246, acc.: 55.71%] [G loss: 0.7335766553878784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 33/86 [D loss: 0.6806043386459351, acc.: 55.76%] [G loss: 0.7340226769447327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 34/86 [D loss: 0.6813850104808807, acc.: 57.67%] [G loss: 0.7258085012435913]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 35/86 [D loss: 0.6847961843013763, acc.: 55.42%] [G loss: 0.7343852519989014]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 36/86 [D loss: 0.6817828118801117, acc.: 56.30%] [G loss: 0.7244725823402405]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 37/86 [D loss: 0.6831998825073242, acc.: 56.69%] [G loss: 0.7366163730621338]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 38/86 [D loss: 0.6774881482124329, acc.: 58.69%] [G loss: 0.729729950428009]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 39/86 [D loss: 0.683128148317337, acc.: 55.76%] [G loss: 0.7306593656539917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 40/86 [D loss: 0.6781711578369141, acc.: 60.01%] [G loss: 0.735254168510437]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 41/86 [D loss: 0.6801567375659943, acc.: 57.71%] [G loss: 0.7242794036865234]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 42/86 [D loss: 0.6833608746528625, acc.: 56.20%] [G loss: 0.7354341745376587]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 43/86 [D loss: 0.677340567111969, acc.: 58.98%] [G loss: 0.7276070713996887]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 44/86 [D loss: 0.6910459995269775, acc.: 50.78%] [G loss: 0.733885645866394]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 45/86 [D loss: 0.6802310049533844, acc.: 56.93%] [G loss: 0.7302307486534119]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 46/86 [D loss: 0.6826512217521667, acc.: 55.27%] [G loss: 0.7275211215019226]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 47/86 [D loss: 0.6791405379772186, acc.: 57.32%] [G loss: 0.7317193150520325]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 48/86 [D loss: 0.6826632022857666, acc.: 56.25%] [G loss: 0.7254112958908081]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 49/86 [D loss: 0.6876313388347626, acc.: 54.00%] [G loss: 0.733254075050354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 50/86 [D loss: 0.6801464557647705, acc.: 57.81%] [G loss: 0.722368597984314]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 51/86 [D loss: 0.686161071062088, acc.: 54.79%] [G loss: 0.733869731426239]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 52/86 [D loss: 0.6780028641223907, acc.: 58.45%] [G loss: 0.7302254438400269]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 53/86 [D loss: 0.6820956468582153, acc.: 57.52%] [G loss: 0.728082537651062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 54/86 [D loss: 0.6834425926208496, acc.: 56.45%] [G loss: 0.7373965978622437]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 55/86 [D loss: 0.6814002394676208, acc.: 55.81%] [G loss: 0.7288758754730225]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 56/86 [D loss: 0.6848390102386475, acc.: 54.20%] [G loss: 0.7372888922691345]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 57/86 [D loss: 0.680928647518158, acc.: 56.98%] [G loss: 0.7322185635566711]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 58/86 [D loss: 0.6857388019561768, acc.: 55.47%] [G loss: 0.734001636505127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 59/86 [D loss: 0.6785003542900085, acc.: 59.28%] [G loss: 0.7380919456481934]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 60/86 [D loss: 0.6826079189777374, acc.: 56.35%] [G loss: 0.7238650321960449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 103/200, Batch 61/86 [D loss: 0.6807516813278198, acc.: 57.47%] [G loss: 0.7336395978927612]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 62/86 [D loss: 0.679652601480484, acc.: 58.15%] [G loss: 0.729142427444458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 63/86 [D loss: 0.6859928071498871, acc.: 54.39%] [G loss: 0.7365455627441406]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 64/86 [D loss: 0.6775746047496796, acc.: 59.38%] [G loss: 0.7336105108261108]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 65/86 [D loss: 0.6848461627960205, acc.: 53.86%] [G loss: 0.7270114421844482]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 66/86 [D loss: 0.6817662417888641, acc.: 58.01%] [G loss: 0.7358798384666443]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 67/86 [D loss: 0.6759936809539795, acc.: 59.62%] [G loss: 0.7308908700942993]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 68/86 [D loss: 0.6856384873390198, acc.: 55.47%] [G loss: 0.730959415435791]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 69/86 [D loss: 0.6793981194496155, acc.: 57.37%] [G loss: 0.7296373248100281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 70/86 [D loss: 0.6834816336631775, acc.: 55.71%] [G loss: 0.7267419099807739]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 71/86 [D loss: 0.6826241612434387, acc.: 54.98%] [G loss: 0.7386418581008911]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 72/86 [D loss: 0.680366724729538, acc.: 58.74%] [G loss: 0.7305275201797485]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 73/86 [D loss: 0.6883316934108734, acc.: 52.00%] [G loss: 0.7366877198219299]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 74/86 [D loss: 0.6796005964279175, acc.: 58.30%] [G loss: 0.7294937372207642]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 75/86 [D loss: 0.6827037036418915, acc.: 56.93%] [G loss: 0.7282983660697937]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 76/86 [D loss: 0.6797471642494202, acc.: 57.23%] [G loss: 0.7375657558441162]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 77/86 [D loss: 0.6844372749328613, acc.: 54.88%] [G loss: 0.7283250093460083]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 78/86 [D loss: 0.6801920533180237, acc.: 57.52%] [G loss: 0.7308381199836731]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 79/86 [D loss: 0.6811588704586029, acc.: 56.88%] [G loss: 0.7247411608695984]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 80/86 [D loss: 0.6840292513370514, acc.: 56.01%] [G loss: 0.7312314510345459]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 81/86 [D loss: 0.678809255361557, acc.: 58.35%] [G loss: 0.7302361726760864]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 82/86 [D loss: 0.6859578788280487, acc.: 55.08%] [G loss: 0.7284716963768005]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 83/86 [D loss: 0.68035489320755, acc.: 58.54%] [G loss: 0.729803204536438]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 84/86 [D loss: 0.6842686235904694, acc.: 54.15%] [G loss: 0.7316545248031616]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 103/200, Batch 85/86 [D loss: 0.6828262805938721, acc.: 56.64%] [G loss: 0.7379245758056641]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 103/200, Batch 86/86 [D loss: 0.6811564862728119, acc.: 56.45%] [G loss: 0.7324701547622681]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 1/86 [D loss: 0.6848792433738708, acc.: 55.32%] [G loss: 0.7304626703262329]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 2/86 [D loss: 0.6791220009326935, acc.: 57.32%] [G loss: 0.7326308488845825]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 3/86 [D loss: 0.6849888265132904, acc.: 55.22%] [G loss: 0.7262053489685059]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 4/86 [D loss: 0.6812549531459808, acc.: 57.32%] [G loss: 0.7332806587219238]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 5/86 [D loss: 0.6785035729408264, acc.: 59.57%] [G loss: 0.7245959043502808]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 6/86 [D loss: 0.6849948465824127, acc.: 55.22%] [G loss: 0.7350801825523376]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 7/86 [D loss: 0.6798518300056458, acc.: 58.54%] [G loss: 0.7306804656982422]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 8/86 [D loss: 0.6804095804691315, acc.: 56.69%] [G loss: 0.7264016270637512]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 9/86 [D loss: 0.6804234385490417, acc.: 57.32%] [G loss: 0.735458254814148]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 10/86 [D loss: 0.6825687289237976, acc.: 56.30%] [G loss: 0.7312598824501038]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 11/86 [D loss: 0.6839696764945984, acc.: 56.01%] [G loss: 0.7331727147102356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 12/86 [D loss: 0.6772681176662445, acc.: 58.54%] [G loss: 0.7333185076713562]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 13/86 [D loss: 0.6822717189788818, acc.: 55.81%] [G loss: 0.7220782041549683]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 14/86 [D loss: 0.6812466084957123, acc.: 57.13%] [G loss: 0.7334011197090149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 15/86 [D loss: 0.6786708235740662, acc.: 57.62%] [G loss: 0.7300570011138916]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 16/86 [D loss: 0.6810078322887421, acc.: 58.06%] [G loss: 0.7295548915863037]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 17/86 [D loss: 0.6823498606681824, acc.: 56.25%] [G loss: 0.7326747179031372]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 18/86 [D loss: 0.6813085675239563, acc.: 57.47%] [G loss: 0.7331214547157288]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 19/86 [D loss: 0.6834065616130829, acc.: 55.57%] [G loss: 0.7340424060821533]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 20/86 [D loss: 0.6776265203952789, acc.: 59.52%] [G loss: 0.7338094115257263]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 21/86 [D loss: 0.683959424495697, acc.: 55.76%] [G loss: 0.7315713167190552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 22/86 [D loss: 0.6799390017986298, acc.: 57.47%] [G loss: 0.7309218645095825]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 23/86 [D loss: 0.6800763010978699, acc.: 57.13%] [G loss: 0.7279069423675537]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 24/86 [D loss: 0.6810675859451294, acc.: 57.18%] [G loss: 0.7329593896865845]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 25/86 [D loss: 0.6814951300621033, acc.: 56.79%] [G loss: 0.7295261025428772]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 26/86 [D loss: 0.6810873448848724, acc.: 57.23%] [G loss: 0.7259604930877686]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 27/86 [D loss: 0.6816121935844421, acc.: 56.84%] [G loss: 0.7270041704177856]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 28/86 [D loss: 0.6809409260749817, acc.: 56.74%] [G loss: 0.7304797768592834]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 29/86 [D loss: 0.676521509885788, acc.: 59.13%] [G loss: 0.7274689674377441]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 30/86 [D loss: 0.6845383048057556, acc.: 57.42%] [G loss: 0.733981728553772]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 31/86 [D loss: 0.6779865622520447, acc.: 58.74%] [G loss: 0.7284612059593201]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 32/86 [D loss: 0.6798394322395325, acc.: 57.37%] [G loss: 0.7285467982292175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 33/86 [D loss: 0.6813935041427612, acc.: 56.30%] [G loss: 0.7340478897094727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 34/86 [D loss: 0.6807767748832703, acc.: 58.06%] [G loss: 0.7333160042762756]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 35/86 [D loss: 0.6795715987682343, acc.: 58.74%] [G loss: 0.7348603010177612]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 36/86 [D loss: 0.6769560873508453, acc.: 59.67%] [G loss: 0.7283200025558472]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 37/86 [D loss: 0.6796121299266815, acc.: 57.18%] [G loss: 0.7328837513923645]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 38/86 [D loss: 0.680862694978714, acc.: 56.25%] [G loss: 0.7346320152282715]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 39/86 [D loss: 0.6808555722236633, acc.: 56.64%] [G loss: 0.7367007732391357]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 40/86 [D loss: 0.6854657530784607, acc.: 54.35%] [G loss: 0.738419234752655]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 41/86 [D loss: 0.6773628890514374, acc.: 59.03%] [G loss: 0.732732892036438]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 42/86 [D loss: 0.6833933293819427, acc.: 57.28%] [G loss: 0.7305468916893005]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 43/86 [D loss: 0.6811579763889313, acc.: 57.18%] [G loss: 0.7361211180686951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 44/86 [D loss: 0.6780287623405457, acc.: 59.81%] [G loss: 0.728866696357727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 45/86 [D loss: 0.6823617815971375, acc.: 56.05%] [G loss: 0.7376290559768677]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 46/86 [D loss: 0.6803851127624512, acc.: 57.62%] [G loss: 0.7297117114067078]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 47/86 [D loss: 0.6828992068767548, acc.: 56.15%] [G loss: 0.7317938208580017]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 48/86 [D loss: 0.6805595755577087, acc.: 57.62%] [G loss: 0.7373838424682617]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 49/86 [D loss: 0.6789935231208801, acc.: 58.98%] [G loss: 0.7266863584518433]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 50/86 [D loss: 0.6795835793018341, acc.: 58.25%] [G loss: 0.7358880043029785]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 51/86 [D loss: 0.6770573854446411, acc.: 59.28%] [G loss: 0.7284183502197266]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 52/86 [D loss: 0.6845658719539642, acc.: 55.13%] [G loss: 0.7317926287651062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 53/86 [D loss: 0.6805174350738525, acc.: 56.30%] [G loss: 0.7345010638237]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 54/86 [D loss: 0.6833515763282776, acc.: 55.91%] [G loss: 0.7282406687736511]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 55/86 [D loss: 0.6808499097824097, acc.: 58.30%] [G loss: 0.7318311929702759]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 56/86 [D loss: 0.6810119450092316, acc.: 58.01%] [G loss: 0.7261941432952881]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 104/200, Batch 57/86 [D loss: 0.682824432849884, acc.: 56.20%] [G loss: 0.7357234954833984]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 58/86 [D loss: 0.6798241436481476, acc.: 57.23%] [G loss: 0.7357872128486633]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 59/86 [D loss: 0.6807169318199158, acc.: 56.74%] [G loss: 0.7277123332023621]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 60/86 [D loss: 0.6790372431278229, acc.: 58.45%] [G loss: 0.7320430278778076]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 61/86 [D loss: 0.683336466550827, acc.: 55.91%] [G loss: 0.7314782738685608]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 62/86 [D loss: 0.6807903945446014, acc.: 56.30%] [G loss: 0.7353634238243103]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 63/86 [D loss: 0.679628849029541, acc.: 57.42%] [G loss: 0.7333207130432129]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 64/86 [D loss: 0.6812021136283875, acc.: 58.01%] [G loss: 0.7314901947975159]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 65/86 [D loss: 0.6819126307964325, acc.: 55.66%] [G loss: 0.7297191619873047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 66/86 [D loss: 0.6805362105369568, acc.: 57.03%] [G loss: 0.7327876091003418]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 67/86 [D loss: 0.6794454753398895, acc.: 57.96%] [G loss: 0.7310724854469299]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 68/86 [D loss: 0.6812257468700409, acc.: 56.54%] [G loss: 0.7306610345840454]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 69/86 [D loss: 0.6773904263973236, acc.: 59.33%] [G loss: 0.734875500202179]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 70/86 [D loss: 0.681471198797226, acc.: 56.25%] [G loss: 0.7293108701705933]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 71/86 [D loss: 0.6805083751678467, acc.: 56.59%] [G loss: 0.7358996272087097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 72/86 [D loss: 0.6793024837970734, acc.: 57.08%] [G loss: 0.7370869517326355]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 73/86 [D loss: 0.6785325407981873, acc.: 58.35%] [G loss: 0.7323073744773865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 74/86 [D loss: 0.6816529035568237, acc.: 57.76%] [G loss: 0.7369657754898071]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 75/86 [D loss: 0.6761983931064606, acc.: 60.60%] [G loss: 0.7332968711853027]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 76/86 [D loss: 0.6824771165847778, acc.: 56.88%] [G loss: 0.7338793873786926]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 77/86 [D loss: 0.6813226044178009, acc.: 55.81%] [G loss: 0.7332441806793213]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 78/86 [D loss: 0.6811800599098206, acc.: 55.42%] [G loss: 0.7352441549301147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 79/86 [D loss: 0.6801744997501373, acc.: 57.42%] [G loss: 0.7356950044631958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 80/86 [D loss: 0.678310751914978, acc.: 58.11%] [G loss: 0.7341834306716919]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 81/86 [D loss: 0.6814074516296387, acc.: 56.64%] [G loss: 0.7308578491210938]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 82/86 [D loss: 0.6830995678901672, acc.: 55.71%] [G loss: 0.7359250783920288]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 83/86 [D loss: 0.681950032711029, acc.: 56.15%] [G loss: 0.7318958044052124]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 104/200, Batch 84/86 [D loss: 0.6779356598854065, acc.: 58.45%] [G loss: 0.733335554599762]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 85/86 [D loss: 0.6820492446422577, acc.: 57.03%] [G loss: 0.7355158925056458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 104/200, Batch 86/86 [D loss: 0.680003434419632, acc.: 56.79%] [G loss: 0.7325506210327148]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 105/200, Batch 1/86 [D loss: 0.6765369772911072, acc.: 60.64%] [G loss: 0.7322537302970886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 2/86 [D loss: 0.6795907020568848, acc.: 57.23%] [G loss: 0.7321598529815674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 3/86 [D loss: 0.6809432208538055, acc.: 56.69%] [G loss: 0.7336937189102173]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 4/86 [D loss: 0.6815269589424133, acc.: 57.81%] [G loss: 0.7300735116004944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 5/86 [D loss: 0.6821124851703644, acc.: 56.01%] [G loss: 0.7361593246459961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 6/86 [D loss: 0.6802399158477783, acc.: 57.13%] [G loss: 0.7335695624351501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 7/86 [D loss: 0.6789677441120148, acc.: 58.01%] [G loss: 0.7336549162864685]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 8/86 [D loss: 0.6821634471416473, acc.: 56.64%] [G loss: 0.7358435392379761]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 9/86 [D loss: 0.6825486719608307, acc.: 55.13%] [G loss: 0.7291334271430969]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 10/86 [D loss: 0.6784313321113586, acc.: 58.06%] [G loss: 0.7362700700759888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 11/86 [D loss: 0.6794696748256683, acc.: 57.37%] [G loss: 0.7317537069320679]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 12/86 [D loss: 0.677655428647995, acc.: 58.98%] [G loss: 0.7325332760810852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 13/86 [D loss: 0.6825290620326996, acc.: 55.62%] [G loss: 0.7334001660346985]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 14/86 [D loss: 0.6824251711368561, acc.: 55.62%] [G loss: 0.7300745844841003]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 15/86 [D loss: 0.6791573762893677, acc.: 58.30%] [G loss: 0.7304625511169434]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 16/86 [D loss: 0.6806043386459351, acc.: 57.57%] [G loss: 0.7296736240386963]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 17/86 [D loss: 0.6796117722988129, acc.: 57.76%] [G loss: 0.7303459048271179]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 18/86 [D loss: 0.675160825252533, acc.: 59.38%] [G loss: 0.7297927141189575]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 19/86 [D loss: 0.683686226606369, acc.: 56.01%] [G loss: 0.7323315143585205]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 20/86 [D loss: 0.6801149249076843, acc.: 56.69%] [G loss: 0.731766939163208]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 21/86 [D loss: 0.6797304153442383, acc.: 58.15%] [G loss: 0.7332007884979248]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 22/86 [D loss: 0.6794580221176147, acc.: 57.23%] [G loss: 0.7381308674812317]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 23/86 [D loss: 0.6804572343826294, acc.: 56.40%] [G loss: 0.7330443263053894]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 24/86 [D loss: 0.6814680397510529, acc.: 57.03%] [G loss: 0.7339972257614136]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 25/86 [D loss: 0.6808520555496216, acc.: 56.98%] [G loss: 0.7310118079185486]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 26/86 [D loss: 0.6821039915084839, acc.: 57.42%] [G loss: 0.7302504777908325]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 27/86 [D loss: 0.679427981376648, acc.: 59.03%] [G loss: 0.7345305681228638]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 28/86 [D loss: 0.6801762580871582, acc.: 57.76%] [G loss: 0.734402596950531]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 29/86 [D loss: 0.6783560216426849, acc.: 57.71%] [G loss: 0.729947030544281]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 30/86 [D loss: 0.6815593242645264, acc.: 56.40%] [G loss: 0.736836850643158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 105/200, Batch 31/86 [D loss: 0.681064248085022, acc.: 57.13%] [G loss: 0.7327749133110046]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 32/86 [D loss: 0.6829368472099304, acc.: 56.40%] [G loss: 0.732732355594635]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 33/86 [D loss: 0.6822113394737244, acc.: 54.98%] [G loss: 0.729952871799469]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 34/86 [D loss: 0.6803812384605408, acc.: 57.71%] [G loss: 0.734545111656189]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 35/86 [D loss: 0.6846624314785004, acc.: 55.32%] [G loss: 0.7352113723754883]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 36/86 [D loss: 0.6780587732791901, acc.: 57.18%] [G loss: 0.7348067760467529]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 37/86 [D loss: 0.6825940310955048, acc.: 56.30%] [G loss: 0.737541139125824]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 38/86 [D loss: 0.6812137961387634, acc.: 55.42%] [G loss: 0.7333624362945557]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 39/86 [D loss: 0.6830951273441315, acc.: 56.25%] [G loss: 0.7317246794700623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 40/86 [D loss: 0.6797688901424408, acc.: 57.47%] [G loss: 0.7316948175430298]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 41/86 [D loss: 0.6807309687137604, acc.: 56.64%] [G loss: 0.7326720952987671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 42/86 [D loss: 0.6806353330612183, acc.: 57.96%] [G loss: 0.7367377281188965]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 43/86 [D loss: 0.6809494495391846, acc.: 56.84%] [G loss: 0.7316163182258606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 44/86 [D loss: 0.6797490119934082, acc.: 58.11%] [G loss: 0.7317707538604736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 45/86 [D loss: 0.6803272366523743, acc.: 58.69%] [G loss: 0.7335072159767151]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 46/86 [D loss: 0.6769779622554779, acc.: 59.72%] [G loss: 0.7316347360610962]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 47/86 [D loss: 0.6803569495677948, acc.: 57.57%] [G loss: 0.7346839308738708]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 48/86 [D loss: 0.6811339557170868, acc.: 57.57%] [G loss: 0.7321275472640991]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 49/86 [D loss: 0.6825944781303406, acc.: 57.13%] [G loss: 0.730394184589386]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 50/86 [D loss: 0.6834556460380554, acc.: 54.05%] [G loss: 0.7290246486663818]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 51/86 [D loss: 0.6844358742237091, acc.: 55.37%] [G loss: 0.7323363423347473]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 52/86 [D loss: 0.6788292825222015, acc.: 57.71%] [G loss: 0.7292157411575317]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 53/86 [D loss: 0.6805016994476318, acc.: 56.93%] [G loss: 0.740746259689331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 54/86 [D loss: 0.6779883503913879, acc.: 57.18%] [G loss: 0.733807384967804]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 55/86 [D loss: 0.6841160655021667, acc.: 55.81%] [G loss: 0.7322074770927429]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 56/86 [D loss: 0.6836273074150085, acc.: 55.03%] [G loss: 0.7342369556427002]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 57/86 [D loss: 0.6821137070655823, acc.: 55.47%] [G loss: 0.7316147089004517]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 58/86 [D loss: 0.6821085810661316, acc.: 56.69%] [G loss: 0.7347952127456665]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 59/86 [D loss: 0.6813282668590546, acc.: 57.37%] [G loss: 0.7316287755966187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 60/86 [D loss: 0.678290456533432, acc.: 57.81%] [G loss: 0.733596920967102]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 61/86 [D loss: 0.6810747385025024, acc.: 56.45%] [G loss: 0.7324503064155579]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 62/86 [D loss: 0.6798624992370605, acc.: 58.84%] [G loss: 0.7321597337722778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 63/86 [D loss: 0.6758856475353241, acc.: 60.11%] [G loss: 0.7333624958992004]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 105/200, Batch 64/86 [D loss: 0.6798247694969177, acc.: 57.13%] [G loss: 0.7316107749938965]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 65/86 [D loss: 0.6804280281066895, acc.: 57.03%] [G loss: 0.7365319728851318]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 66/86 [D loss: 0.6784043610095978, acc.: 58.06%] [G loss: 0.7318894267082214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 67/86 [D loss: 0.6836585998535156, acc.: 55.76%] [G loss: 0.7364572286605835]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 68/86 [D loss: 0.6803851127624512, acc.: 56.25%] [G loss: 0.7285805344581604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 69/86 [D loss: 0.6801427006721497, acc.: 57.62%] [G loss: 0.7353726625442505]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 70/86 [D loss: 0.6781134605407715, acc.: 57.37%] [G loss: 0.7316197752952576]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 71/86 [D loss: 0.6861109733581543, acc.: 55.13%] [G loss: 0.7359731793403625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 72/86 [D loss: 0.6784263849258423, acc.: 58.15%] [G loss: 0.7304588556289673]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 73/86 [D loss: 0.6845110952854156, acc.: 53.47%] [G loss: 0.7291015386581421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 74/86 [D loss: 0.6811683773994446, acc.: 56.84%] [G loss: 0.7418320178985596]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 75/86 [D loss: 0.6812990307807922, acc.: 57.23%] [G loss: 0.7207244634628296]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 76/86 [D loss: 0.6904129087924957, acc.: 51.86%] [G loss: 0.7495144605636597]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 77/86 [D loss: 0.679307609796524, acc.: 56.45%] [G loss: 0.7201429009437561]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 78/86 [D loss: 0.685557097196579, acc.: 54.10%] [G loss: 0.7375295162200928]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 105/200, Batch 79/86 [D loss: 0.6730397939682007, acc.: 61.28%] [G loss: 0.7230947017669678]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 80/86 [D loss: 0.6998850703239441, acc.: 47.75%] [G loss: 0.743516206741333]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 105/200, Batch 81/86 [D loss: 0.6749539077281952, acc.: 59.62%] [G loss: 0.7203311920166016]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 82/86 [D loss: 0.6930299997329712, acc.: 49.37%] [G loss: 0.72478848695755]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 83/86 [D loss: 0.6784171760082245, acc.: 57.37%] [G loss: 0.7398777604103088]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 105/200, Batch 84/86 [D loss: 0.6795931458473206, acc.: 58.25%] [G loss: 0.7205103039741516]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 85/86 [D loss: 0.6870952546596527, acc.: 52.73%] [G loss: 0.7375801205635071]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 105/200, Batch 86/86 [D loss: 0.6767091155052185, acc.: 58.45%] [G loss: 0.7215603590011597]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 1/86 [D loss: 0.684491366147995, acc.: 54.93%] [G loss: 0.734067440032959]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 2/86 [D loss: 0.6800134181976318, acc.: 56.93%] [G loss: 0.7331546545028687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 3/86 [D loss: 0.6825090944766998, acc.: 57.13%] [G loss: 0.7274684309959412]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 4/86 [D loss: 0.6789749264717102, acc.: 58.69%] [G loss: 0.7427043914794922]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 5/86 [D loss: 0.6798089742660522, acc.: 57.47%] [G loss: 0.7301117777824402]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 6/86 [D loss: 0.6854678094387054, acc.: 53.61%] [G loss: 0.7301422357559204]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 7/86 [D loss: 0.6809337437152863, acc.: 56.10%] [G loss: 0.7341718673706055]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 8/86 [D loss: 0.68403160572052, acc.: 55.96%] [G loss: 0.7260778546333313]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 9/86 [D loss: 0.679831475019455, acc.: 57.32%] [G loss: 0.7329087257385254]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 10/86 [D loss: 0.6769547760486603, acc.: 59.57%] [G loss: 0.7316672801971436]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 11/86 [D loss: 0.6818533837795258, acc.: 57.47%] [G loss: 0.7293722629547119]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 12/86 [D loss: 0.6771389245986938, acc.: 57.47%] [G loss: 0.732874870300293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 13/86 [D loss: 0.6824855208396912, acc.: 56.64%] [G loss: 0.7290345430374146]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 106/200, Batch 14/86 [D loss: 0.681572824716568, acc.: 56.64%] [G loss: 0.7351264953613281]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 15/86 [D loss: 0.6806026697158813, acc.: 57.03%] [G loss: 0.7313838005065918]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 16/86 [D loss: 0.6835122108459473, acc.: 55.52%] [G loss: 0.7307074069976807]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 17/86 [D loss: 0.6774477064609528, acc.: 59.13%] [G loss: 0.7339123487472534]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 18/86 [D loss: 0.681083470582962, acc.: 57.86%] [G loss: 0.732340395450592]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 19/86 [D loss: 0.6821597218513489, acc.: 57.08%] [G loss: 0.7331337928771973]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 20/86 [D loss: 0.6795580089092255, acc.: 57.52%] [G loss: 0.7391035556793213]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 21/86 [D loss: 0.6823922991752625, acc.: 56.20%] [G loss: 0.733235239982605]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 22/86 [D loss: 0.6783743500709534, acc.: 58.25%] [G loss: 0.7347455024719238]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 23/86 [D loss: 0.6787159442901611, acc.: 58.79%] [G loss: 0.7352687120437622]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 24/86 [D loss: 0.6818096935749054, acc.: 56.88%] [G loss: 0.735929548740387]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 25/86 [D loss: 0.6794290542602539, acc.: 58.06%] [G loss: 0.7308626174926758]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 26/86 [D loss: 0.681636393070221, acc.: 54.98%] [G loss: 0.7327930331230164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 27/86 [D loss: 0.6788867712020874, acc.: 56.88%] [G loss: 0.732935905456543]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 28/86 [D loss: 0.6794015765190125, acc.: 58.25%] [G loss: 0.7302581667900085]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 29/86 [D loss: 0.6824442148208618, acc.: 55.86%] [G loss: 0.734643816947937]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 30/86 [D loss: 0.6828276515007019, acc.: 55.96%] [G loss: 0.7324175834655762]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 31/86 [D loss: 0.6812238991260529, acc.: 55.81%] [G loss: 0.7338290810585022]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 32/86 [D loss: 0.6780007183551788, acc.: 58.20%] [G loss: 0.7343689203262329]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 33/86 [D loss: 0.6819401383399963, acc.: 56.64%] [G loss: 0.7350144982337952]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 34/86 [D loss: 0.682822197675705, acc.: 55.71%] [G loss: 0.7345994114875793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 35/86 [D loss: 0.6818752884864807, acc.: 55.86%] [G loss: 0.7308269143104553]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 36/86 [D loss: 0.6827251613140106, acc.: 55.86%] [G loss: 0.7336175441741943]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 37/86 [D loss: 0.6823024153709412, acc.: 55.37%] [G loss: 0.7318215370178223]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 38/86 [D loss: 0.6848537921905518, acc.: 55.18%] [G loss: 0.7315675020217896]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 39/86 [D loss: 0.6807858347892761, acc.: 56.98%] [G loss: 0.7312003374099731]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 40/86 [D loss: 0.6785063147544861, acc.: 57.71%] [G loss: 0.7358888387680054]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 41/86 [D loss: 0.6792749166488647, acc.: 56.98%] [G loss: 0.7338195443153381]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 42/86 [D loss: 0.6779585480690002, acc.: 57.91%] [G loss: 0.7356128692626953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 43/86 [D loss: 0.6805867254734039, acc.: 56.35%] [G loss: 0.737272322177887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 44/86 [D loss: 0.6819107830524445, acc.: 56.79%] [G loss: 0.7326158285140991]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 45/86 [D loss: 0.6781384348869324, acc.: 59.38%] [G loss: 0.7347294688224792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 46/86 [D loss: 0.6822036802768707, acc.: 55.71%] [G loss: 0.7360408306121826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 47/86 [D loss: 0.6796244084835052, acc.: 57.67%] [G loss: 0.7322149276733398]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 48/86 [D loss: 0.6800756454467773, acc.: 56.64%] [G loss: 0.7317343950271606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 49/86 [D loss: 0.6798486113548279, acc.: 56.49%] [G loss: 0.7340954542160034]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 106/200, Batch 50/86 [D loss: 0.6816159784793854, acc.: 56.74%] [G loss: 0.7344059348106384]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 51/86 [D loss: 0.6804946959018707, acc.: 56.69%] [G loss: 0.7359943389892578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 52/86 [D loss: 0.6779221296310425, acc.: 58.64%] [G loss: 0.7304313778877258]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 53/86 [D loss: 0.6790728867053986, acc.: 57.86%] [G loss: 0.7380335927009583]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 54/86 [D loss: 0.681058794260025, acc.: 56.40%] [G loss: 0.7353033423423767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 55/86 [D loss: 0.6785973906517029, acc.: 59.38%] [G loss: 0.7316315174102783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 56/86 [D loss: 0.6763979196548462, acc.: 58.89%] [G loss: 0.7348053455352783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 57/86 [D loss: 0.6756923794746399, acc.: 59.38%] [G loss: 0.7360551357269287]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 58/86 [D loss: 0.682070255279541, acc.: 55.71%] [G loss: 0.7290955781936646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 59/86 [D loss: 0.6837443113327026, acc.: 54.83%] [G loss: 0.7352716326713562]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 60/86 [D loss: 0.6823825538158417, acc.: 56.05%] [G loss: 0.7310077548027039]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 61/86 [D loss: 0.6810945868492126, acc.: 57.28%] [G loss: 0.7291179895401001]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 62/86 [D loss: 0.6802941858768463, acc.: 57.13%] [G loss: 0.7325726747512817]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 63/86 [D loss: 0.6768395006656647, acc.: 59.81%] [G loss: 0.7329356074333191]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 64/86 [D loss: 0.6786848604679108, acc.: 57.71%] [G loss: 0.7337950468063354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 65/86 [D loss: 0.6792310476303101, acc.: 56.59%] [G loss: 0.7347408533096313]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 66/86 [D loss: 0.678922176361084, acc.: 58.15%] [G loss: 0.7322291731834412]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 67/86 [D loss: 0.6816824972629547, acc.: 55.76%] [G loss: 0.7281247973442078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 68/86 [D loss: 0.6808283627033234, acc.: 57.08%] [G loss: 0.7342426776885986]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 69/86 [D loss: 0.6810775995254517, acc.: 57.47%] [G loss: 0.7333231568336487]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 70/86 [D loss: 0.6784118711948395, acc.: 57.47%] [G loss: 0.7333910465240479]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 71/86 [D loss: 0.679152101278305, acc.: 58.06%] [G loss: 0.7322767972946167]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 72/86 [D loss: 0.6793434619903564, acc.: 57.57%] [G loss: 0.7327841520309448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 73/86 [D loss: 0.6784299612045288, acc.: 58.40%] [G loss: 0.7304145097732544]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 74/86 [D loss: 0.6782881617546082, acc.: 58.35%] [G loss: 0.7372247576713562]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 106/200, Batch 75/86 [D loss: 0.6797637343406677, acc.: 58.30%] [G loss: 0.7327921986579895]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 76/86 [D loss: 0.6795269548892975, acc.: 56.15%] [G loss: 0.7347041368484497]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 77/86 [D loss: 0.6809298992156982, acc.: 56.35%] [G loss: 0.7260543704032898]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 78/86 [D loss: 0.678512841463089, acc.: 58.74%] [G loss: 0.7338353395462036]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 79/86 [D loss: 0.6804671585559845, acc.: 56.49%] [G loss: 0.7332861423492432]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 80/86 [D loss: 0.6767575442790985, acc.: 58.94%] [G loss: 0.7374615669250488]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 106/200, Batch 81/86 [D loss: 0.6788308024406433, acc.: 58.54%] [G loss: 0.7383139133453369]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 82/86 [D loss: 0.6792419850826263, acc.: 58.54%] [G loss: 0.7327938675880432]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 83/86 [D loss: 0.6782642304897308, acc.: 58.69%] [G loss: 0.7347682118415833]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 84/86 [D loss: 0.6790009140968323, acc.: 58.30%] [G loss: 0.7374062538146973]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 106/200, Batch 85/86 [D loss: 0.683090478181839, acc.: 55.91%] [G loss: 0.7370071411132812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 106/200, Batch 86/86 [D loss: 0.6808091104030609, acc.: 57.08%] [G loss: 0.7365013360977173]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 1/86 [D loss: 0.6775288581848145, acc.: 58.35%] [G loss: 0.7334837317466736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 2/86 [D loss: 0.678268700838089, acc.: 57.67%] [G loss: 0.7314475178718567]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 3/86 [D loss: 0.6805989146232605, acc.: 57.13%] [G loss: 0.737313985824585]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 4/86 [D loss: 0.6765046119689941, acc.: 60.01%] [G loss: 0.7378235459327698]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 5/86 [D loss: 0.6805622577667236, acc.: 55.81%] [G loss: 0.7376156449317932]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 6/86 [D loss: 0.6767075657844543, acc.: 58.98%] [G loss: 0.7309809923171997]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 7/86 [D loss: 0.68280890583992, acc.: 56.35%] [G loss: 0.7352983355522156]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 8/86 [D loss: 0.6789530813694, acc.: 58.30%] [G loss: 0.7360630035400391]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 9/86 [D loss: 0.679171234369278, acc.: 58.06%] [G loss: 0.7350336313247681]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 10/86 [D loss: 0.6817838549613953, acc.: 56.79%] [G loss: 0.7357780337333679]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 11/86 [D loss: 0.6777572333812714, acc.: 59.33%] [G loss: 0.7377446293830872]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 12/86 [D loss: 0.6810125708580017, acc.: 56.45%] [G loss: 0.7353248596191406]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 13/86 [D loss: 0.6778429746627808, acc.: 58.45%] [G loss: 0.7334083914756775]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 14/86 [D loss: 0.6818079352378845, acc.: 54.98%] [G loss: 0.7314545512199402]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 15/86 [D loss: 0.6798244416713715, acc.: 58.11%] [G loss: 0.7376620769500732]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 16/86 [D loss: 0.6799629926681519, acc.: 55.96%] [G loss: 0.7345166802406311]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 17/86 [D loss: 0.6798552572727203, acc.: 57.71%] [G loss: 0.7378743886947632]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 18/86 [D loss: 0.6789579093456268, acc.: 57.76%] [G loss: 0.7386780977249146]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 19/86 [D loss: 0.6806811094284058, acc.: 56.74%] [G loss: 0.7334231734275818]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 20/86 [D loss: 0.6802239418029785, acc.: 57.71%] [G loss: 0.7334624528884888]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 21/86 [D loss: 0.683416098356247, acc.: 55.32%] [G loss: 0.7309780120849609]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 22/86 [D loss: 0.6787951588630676, acc.: 57.76%] [G loss: 0.7327737212181091]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 23/86 [D loss: 0.6803006827831268, acc.: 56.05%] [G loss: 0.7378475666046143]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 107/200, Batch 24/86 [D loss: 0.6759444177150726, acc.: 60.16%] [G loss: 0.7371845841407776]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 25/86 [D loss: 0.6811523139476776, acc.: 56.79%] [G loss: 0.7322799563407898]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 26/86 [D loss: 0.6797007024288177, acc.: 56.79%] [G loss: 0.7364433407783508]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 27/86 [D loss: 0.6791142225265503, acc.: 58.20%] [G loss: 0.7338895201683044]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 28/86 [D loss: 0.6791590452194214, acc.: 57.18%] [G loss: 0.7321697473526001]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 29/86 [D loss: 0.6801231801509857, acc.: 58.30%] [G loss: 0.7315366268157959]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 30/86 [D loss: 0.6793910562992096, acc.: 57.18%] [G loss: 0.7344537973403931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 31/86 [D loss: 0.6794678866863251, acc.: 58.06%] [G loss: 0.7282949686050415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 32/86 [D loss: 0.6781323254108429, acc.: 57.71%] [G loss: 0.7375023365020752]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 33/86 [D loss: 0.6781671643257141, acc.: 57.42%] [G loss: 0.7389534711837769]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 34/86 [D loss: 0.6756322383880615, acc.: 59.28%] [G loss: 0.739337682723999]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 35/86 [D loss: 0.6810756027698517, acc.: 58.01%] [G loss: 0.7330339550971985]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 36/86 [D loss: 0.6832048296928406, acc.: 56.45%] [G loss: 0.7365639209747314]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 37/86 [D loss: 0.6801537573337555, acc.: 56.93%] [G loss: 0.7355625629425049]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 38/86 [D loss: 0.6795938909053802, acc.: 57.03%] [G loss: 0.7388936281204224]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 39/86 [D loss: 0.6796451210975647, acc.: 58.06%] [G loss: 0.7347216606140137]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 107/200, Batch 40/86 [D loss: 0.6781955063343048, acc.: 58.94%] [G loss: 0.7376787662506104]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 41/86 [D loss: 0.6801506280899048, acc.: 57.18%] [G loss: 0.7370165586471558]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 42/86 [D loss: 0.6761215031147003, acc.: 59.86%] [G loss: 0.7360086441040039]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 107/200, Batch 43/86 [D loss: 0.6770759522914886, acc.: 58.40%] [G loss: 0.7331154942512512]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 44/86 [D loss: 0.6784317493438721, acc.: 57.76%] [G loss: 0.7335693836212158]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 45/86 [D loss: 0.6811642646789551, acc.: 56.10%] [G loss: 0.734904944896698]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 46/86 [D loss: 0.6783254444599152, acc.: 58.54%] [G loss: 0.7337555289268494]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 47/86 [D loss: 0.6753291487693787, acc.: 59.62%] [G loss: 0.7351414561271667]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 48/86 [D loss: 0.6822575926780701, acc.: 56.54%] [G loss: 0.734347403049469]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 49/86 [D loss: 0.6816830635070801, acc.: 57.08%] [G loss: 0.7325034141540527]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 50/86 [D loss: 0.6756615340709686, acc.: 59.03%] [G loss: 0.7335672974586487]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 51/86 [D loss: 0.6785735785961151, acc.: 57.42%] [G loss: 0.7363236546516418]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 52/86 [D loss: 0.6797893047332764, acc.: 56.40%] [G loss: 0.7353469133377075]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 53/86 [D loss: 0.6824746131896973, acc.: 56.84%] [G loss: 0.7303173542022705]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 54/86 [D loss: 0.6773692071437836, acc.: 58.11%] [G loss: 0.7360938191413879]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 55/86 [D loss: 0.6814567744731903, acc.: 56.59%] [G loss: 0.7313228249549866]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 56/86 [D loss: 0.6784533560276031, acc.: 57.76%] [G loss: 0.7370222210884094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 57/86 [D loss: 0.6779176890850067, acc.: 58.50%] [G loss: 0.7348171472549438]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 58/86 [D loss: 0.6796700358390808, acc.: 56.54%] [G loss: 0.7362037897109985]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 59/86 [D loss: 0.6783961355686188, acc.: 58.25%] [G loss: 0.7373338937759399]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 60/86 [D loss: 0.6796282231807709, acc.: 57.81%] [G loss: 0.7325286865234375]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 61/86 [D loss: 0.6803330183029175, acc.: 56.93%] [G loss: 0.7325077652931213]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 62/86 [D loss: 0.6790644824504852, acc.: 57.42%] [G loss: 0.7357751727104187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 63/86 [D loss: 0.6825555264949799, acc.: 56.93%] [G loss: 0.741766095161438]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 64/86 [D loss: 0.6796291172504425, acc.: 58.01%] [G loss: 0.733157753944397]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 65/86 [D loss: 0.6803820729255676, acc.: 56.98%] [G loss: 0.742511510848999]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 66/86 [D loss: 0.6778603494167328, acc.: 57.96%] [G loss: 0.7358205318450928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 67/86 [D loss: 0.6835946440696716, acc.: 55.76%] [G loss: 0.7345321774482727]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 68/86 [D loss: 0.6830328106880188, acc.: 55.52%] [G loss: 0.7305951118469238]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 69/86 [D loss: 0.6860429346561432, acc.: 54.35%] [G loss: 0.7428167462348938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 70/86 [D loss: 0.6779680252075195, acc.: 58.98%] [G loss: 0.7393661141395569]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 71/86 [D loss: 0.6846187710762024, acc.: 55.13%] [G loss: 0.734325647354126]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 72/86 [D loss: 0.6786036193370819, acc.: 57.42%] [G loss: 0.7427448630332947]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 73/86 [D loss: 0.6765764653682709, acc.: 60.16%] [G loss: 0.7315975427627563]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 74/86 [D loss: 0.6827059984207153, acc.: 55.86%] [G loss: 0.7417514324188232]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 75/86 [D loss: 0.676288366317749, acc.: 57.76%] [G loss: 0.7241119742393494]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 76/86 [D loss: 0.6854225993156433, acc.: 54.49%] [G loss: 0.738835334777832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 77/86 [D loss: 0.6780320405960083, acc.: 57.71%] [G loss: 0.7325478792190552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 78/86 [D loss: 0.6882700026035309, acc.: 51.86%] [G loss: 0.7532995939254761]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 79/86 [D loss: 0.6783531606197357, acc.: 58.40%] [G loss: 0.7287970185279846]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 80/86 [D loss: 0.6814498007297516, acc.: 56.84%] [G loss: 0.7290234565734863]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 81/86 [D loss: 0.6790638566017151, acc.: 57.28%] [G loss: 0.7312712669372559]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 107/200, Batch 82/86 [D loss: 0.6806465983390808, acc.: 57.32%] [G loss: 0.7317955493927002]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 83/86 [D loss: 0.677550345659256, acc.: 57.76%] [G loss: 0.7355071306228638]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 84/86 [D loss: 0.6820537745952606, acc.: 55.71%] [G loss: 0.7297475337982178]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 85/86 [D loss: 0.6810222864151001, acc.: 55.22%] [G loss: 0.7308382987976074]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 107/200, Batch 86/86 [D loss: 0.676025778055191, acc.: 58.74%] [G loss: 0.7373133897781372]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 108/200, Batch 1/86 [D loss: 0.682282954454422, acc.: 56.45%] [G loss: 0.7453232407569885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 2/86 [D loss: 0.6751560270786285, acc.: 60.06%] [G loss: 0.731694221496582]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 3/86 [D loss: 0.6820043623447418, acc.: 56.69%] [G loss: 0.7347699403762817]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 4/86 [D loss: 0.6820081770420074, acc.: 55.91%] [G loss: 0.732738196849823]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 5/86 [D loss: 0.6819008886814117, acc.: 56.49%] [G loss: 0.7360392808914185]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 6/86 [D loss: 0.6777588427066803, acc.: 57.96%] [G loss: 0.7383428812026978]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 7/86 [D loss: 0.6802505850791931, acc.: 56.01%] [G loss: 0.7358740568161011]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 8/86 [D loss: 0.6793171465396881, acc.: 57.96%] [G loss: 0.7373560070991516]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 9/86 [D loss: 0.6828915178775787, acc.: 55.03%] [G loss: 0.7385389804840088]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 10/86 [D loss: 0.6821791231632233, acc.: 56.20%] [G loss: 0.7307597398757935]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 11/86 [D loss: 0.6753207445144653, acc.: 58.50%] [G loss: 0.736681342124939]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 12/86 [D loss: 0.6789486408233643, acc.: 57.57%] [G loss: 0.7324928045272827]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 13/86 [D loss: 0.6769426465034485, acc.: 58.89%] [G loss: 0.7356252670288086]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 14/86 [D loss: 0.6805649101734161, acc.: 56.88%] [G loss: 0.7325299382209778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 15/86 [D loss: 0.6796883344650269, acc.: 58.25%] [G loss: 0.7360039949417114]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 108/200, Batch 16/86 [D loss: 0.6787774562835693, acc.: 57.96%] [G loss: 0.7357317805290222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 17/86 [D loss: 0.6784243881702423, acc.: 58.01%] [G loss: 0.7354739904403687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 18/86 [D loss: 0.6776334643363953, acc.: 56.30%] [G loss: 0.7382959723472595]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 19/86 [D loss: 0.6801705956459045, acc.: 57.23%] [G loss: 0.7330603003501892]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 20/86 [D loss: 0.6785589158535004, acc.: 58.45%] [G loss: 0.7411237955093384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 21/86 [D loss: 0.677951991558075, acc.: 59.13%] [G loss: 0.7342702746391296]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 22/86 [D loss: 0.6802980899810791, acc.: 56.45%] [G loss: 0.7347638010978699]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 23/86 [D loss: 0.6800865232944489, acc.: 56.79%] [G loss: 0.7361371517181396]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 24/86 [D loss: 0.6816747486591339, acc.: 55.76%] [G loss: 0.7365833520889282]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 25/86 [D loss: 0.6804514527320862, acc.: 56.74%] [G loss: 0.7366690039634705]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 26/86 [D loss: 0.6794815063476562, acc.: 56.88%] [G loss: 0.737829327583313]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 27/86 [D loss: 0.6836979389190674, acc.: 54.30%] [G loss: 0.73697829246521]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 28/86 [D loss: 0.6784041821956635, acc.: 57.86%] [G loss: 0.7330051064491272]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 29/86 [D loss: 0.6850394308567047, acc.: 55.86%] [G loss: 0.7325859665870667]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 30/86 [D loss: 0.6802265048027039, acc.: 57.52%] [G loss: 0.7292173504829407]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 31/86 [D loss: 0.6825830936431885, acc.: 55.62%] [G loss: 0.7307425141334534]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 32/86 [D loss: 0.6788357794284821, acc.: 56.15%] [G loss: 0.7365700006484985]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 33/86 [D loss: 0.678682804107666, acc.: 56.74%] [G loss: 0.7294579148292542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 34/86 [D loss: 0.6814048290252686, acc.: 55.27%] [G loss: 0.7372941970825195]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 35/86 [D loss: 0.6800222098827362, acc.: 57.67%] [G loss: 0.72993004322052]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 36/86 [D loss: 0.6842365264892578, acc.: 54.20%] [G loss: 0.7388295531272888]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 37/86 [D loss: 0.6785764992237091, acc.: 58.25%] [G loss: 0.7297005653381348]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 38/86 [D loss: 0.6834524869918823, acc.: 56.49%] [G loss: 0.7350926399230957]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 39/86 [D loss: 0.6793983280658722, acc.: 57.52%] [G loss: 0.73447585105896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 40/86 [D loss: 0.6818736791610718, acc.: 56.59%] [G loss: 0.7312648296356201]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 41/86 [D loss: 0.6827942728996277, acc.: 54.64%] [G loss: 0.7365735173225403]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 42/86 [D loss: 0.680409848690033, acc.: 56.79%] [G loss: 0.7319995164871216]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 43/86 [D loss: 0.6794136166572571, acc.: 58.84%] [G loss: 0.7393075227737427]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 44/86 [D loss: 0.6798904240131378, acc.: 57.47%] [G loss: 0.7306948304176331]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 45/86 [D loss: 0.6789971590042114, acc.: 56.98%] [G loss: 0.7369820475578308]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 46/86 [D loss: 0.6779472231864929, acc.: 58.20%] [G loss: 0.734250545501709]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 47/86 [D loss: 0.680745929479599, acc.: 57.52%] [G loss: 0.7316945791244507]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 48/86 [D loss: 0.6811819672584534, acc.: 56.64%] [G loss: 0.737694501876831]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 49/86 [D loss: 0.6760352253913879, acc.: 58.20%] [G loss: 0.7398022413253784]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 50/86 [D loss: 0.6817217767238617, acc.: 55.76%] [G loss: 0.7405433654785156]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 51/86 [D loss: 0.6800725758075714, acc.: 57.13%] [G loss: 0.7328770160675049]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 52/86 [D loss: 0.6815263628959656, acc.: 56.20%] [G loss: 0.733250617980957]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 53/86 [D loss: 0.6828201413154602, acc.: 55.27%] [G loss: 0.7376958131790161]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 54/86 [D loss: 0.6867096424102783, acc.: 53.12%] [G loss: 0.7397575378417969]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 55/86 [D loss: 0.6763945817947388, acc.: 58.59%] [G loss: 0.7356359958648682]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 56/86 [D loss: 0.6801981627941132, acc.: 55.81%] [G loss: 0.7365099191665649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 57/86 [D loss: 0.6804655492305756, acc.: 57.52%] [G loss: 0.7380156517028809]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 58/86 [D loss: 0.6781843304634094, acc.: 58.59%] [G loss: 0.7372637391090393]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 59/86 [D loss: 0.6829947829246521, acc.: 54.98%] [G loss: 0.7339569926261902]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 60/86 [D loss: 0.6785399317741394, acc.: 56.49%] [G loss: 0.7354861497879028]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 61/86 [D loss: 0.6771746277809143, acc.: 57.76%] [G loss: 0.7369782328605652]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 62/86 [D loss: 0.6792869567871094, acc.: 58.79%] [G loss: 0.7368367910385132]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 63/86 [D loss: 0.6759249269962311, acc.: 58.45%] [G loss: 0.7333540916442871]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 64/86 [D loss: 0.6784723401069641, acc.: 57.71%] [G loss: 0.7383977174758911]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 65/86 [D loss: 0.6784582436084747, acc.: 57.76%] [G loss: 0.7361443042755127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 66/86 [D loss: 0.679788738489151, acc.: 58.45%] [G loss: 0.7317719459533691]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 67/86 [D loss: 0.6791191101074219, acc.: 57.03%] [G loss: 0.7372530102729797]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 68/86 [D loss: 0.6778611838817596, acc.: 58.01%] [G loss: 0.7381023168563843]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 69/86 [D loss: 0.6817126870155334, acc.: 56.15%] [G loss: 0.7344456315040588]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 70/86 [D loss: 0.6774519681930542, acc.: 58.40%] [G loss: 0.7359327077865601]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 71/86 [D loss: 0.6775265634059906, acc.: 57.47%] [G loss: 0.7395234704017639]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 72/86 [D loss: 0.6771937310695648, acc.: 58.50%] [G loss: 0.7354717254638672]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 73/86 [D loss: 0.677612692117691, acc.: 57.81%] [G loss: 0.7349957823753357]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 74/86 [D loss: 0.6785542666912079, acc.: 57.42%] [G loss: 0.7378281950950623]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 75/86 [D loss: 0.6784251034259796, acc.: 58.54%] [G loss: 0.7398943305015564]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 76/86 [D loss: 0.679297536611557, acc.: 57.57%] [G loss: 0.7369032502174377]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 77/86 [D loss: 0.6808183193206787, acc.: 57.37%] [G loss: 0.7388453483581543]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 78/86 [D loss: 0.68013134598732, acc.: 57.86%] [G loss: 0.7381779551506042]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 79/86 [D loss: 0.6775502562522888, acc.: 58.69%] [G loss: 0.73867267370224]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 80/86 [D loss: 0.6806771755218506, acc.: 57.96%] [G loss: 0.7385463118553162]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 81/86 [D loss: 0.6794683039188385, acc.: 56.59%] [G loss: 0.7367496490478516]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 82/86 [D loss: 0.681211918592453, acc.: 57.62%] [G loss: 0.7365021705627441]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 108/200, Batch 83/86 [D loss: 0.6784350872039795, acc.: 56.98%] [G loss: 0.7365808486938477]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 84/86 [D loss: 0.676242470741272, acc.: 59.03%] [G loss: 0.7320243120193481]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 85/86 [D loss: 0.6799698770046234, acc.: 57.47%] [G loss: 0.7385644316673279]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 108/200, Batch 86/86 [D loss: 0.6810465157032013, acc.: 56.98%] [G loss: 0.7335049510002136]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 109/200, Batch 1/86 [D loss: 0.680711030960083, acc.: 56.98%] [G loss: 0.7386837005615234]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 2/86 [D loss: 0.6787473857402802, acc.: 58.84%] [G loss: 0.738109290599823]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 3/86 [D loss: 0.6820997595787048, acc.: 56.93%] [G loss: 0.7417920231819153]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 4/86 [D loss: 0.6745834052562714, acc.: 59.77%] [G loss: 0.7394133806228638]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 5/86 [D loss: 0.6806147694587708, acc.: 55.91%] [G loss: 0.7368941307067871]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 6/86 [D loss: 0.6792044043540955, acc.: 58.06%] [G loss: 0.7412556409835815]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 7/86 [D loss: 0.6798159182071686, acc.: 57.76%] [G loss: 0.7340760231018066]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 8/86 [D loss: 0.6813974976539612, acc.: 57.08%] [G loss: 0.7394425272941589]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 9/86 [D loss: 0.6766868233680725, acc.: 59.72%] [G loss: 0.7312271595001221]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 10/86 [D loss: 0.6824252903461456, acc.: 55.66%] [G loss: 0.733083963394165]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 11/86 [D loss: 0.6778405606746674, acc.: 57.76%] [G loss: 0.7388097643852234]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 12/86 [D loss: 0.6817604303359985, acc.: 56.98%] [G loss: 0.733350396156311]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 13/86 [D loss: 0.6772359609603882, acc.: 59.52%] [G loss: 0.7354534268379211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 14/86 [D loss: 0.6827397346496582, acc.: 56.54%] [G loss: 0.7319618463516235]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 15/86 [D loss: 0.6771389842033386, acc.: 57.71%] [G loss: 0.7352606058120728]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 16/86 [D loss: 0.6772825419902802, acc.: 58.40%] [G loss: 0.7350620031356812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 17/86 [D loss: 0.6855981945991516, acc.: 54.98%] [G loss: 0.7300503253936768]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 18/86 [D loss: 0.6779346764087677, acc.: 57.96%] [G loss: 0.7340675592422485]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 19/86 [D loss: 0.6807109117507935, acc.: 56.40%] [G loss: 0.7339966893196106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 20/86 [D loss: 0.684558629989624, acc.: 55.27%] [G loss: 0.7377002239227295]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 21/86 [D loss: 0.6764450967311859, acc.: 59.03%] [G loss: 0.7327486276626587]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 22/86 [D loss: 0.6830677390098572, acc.: 56.35%] [G loss: 0.7417699694633484]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 23/86 [D loss: 0.675035685300827, acc.: 60.06%] [G loss: 0.72794109582901]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 24/86 [D loss: 0.6775214970111847, acc.: 57.03%] [G loss: 0.7356104254722595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 25/86 [D loss: 0.6761806309223175, acc.: 59.52%] [G loss: 0.7323023080825806]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 26/86 [D loss: 0.6832163333892822, acc.: 55.27%] [G loss: 0.7321701049804688]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 27/86 [D loss: 0.681305855512619, acc.: 56.74%] [G loss: 0.7395742535591125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 28/86 [D loss: 0.6821394860744476, acc.: 56.30%] [G loss: 0.7327669858932495]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 29/86 [D loss: 0.6806871592998505, acc.: 57.57%] [G loss: 0.7394089102745056]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 30/86 [D loss: 0.6770597696304321, acc.: 57.13%] [G loss: 0.7294308543205261]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 31/86 [D loss: 0.6849198937416077, acc.: 55.57%] [G loss: 0.737238347530365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 32/86 [D loss: 0.6754736602306366, acc.: 59.18%] [G loss: 0.7334336638450623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 33/86 [D loss: 0.6849832832813263, acc.: 53.91%] [G loss: 0.7330259084701538]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 34/86 [D loss: 0.6775061786174774, acc.: 57.57%] [G loss: 0.7409943342208862]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 35/86 [D loss: 0.6773314774036407, acc.: 57.37%] [G loss: 0.7296096086502075]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 36/86 [D loss: 0.679189920425415, acc.: 57.23%] [G loss: 0.7407113313674927]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 37/86 [D loss: 0.6757106184959412, acc.: 58.45%] [G loss: 0.7342237234115601]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 38/86 [D loss: 0.6815564632415771, acc.: 56.69%] [G loss: 0.7348536849021912]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 39/86 [D loss: 0.6770989298820496, acc.: 57.47%] [G loss: 0.7356536388397217]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 40/86 [D loss: 0.6796455979347229, acc.: 57.23%] [G loss: 0.7406710386276245]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 41/86 [D loss: 0.6786591112613678, acc.: 57.28%] [G loss: 0.7346993684768677]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 42/86 [D loss: 0.6799236238002777, acc.: 57.32%] [G loss: 0.7394905090332031]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 43/86 [D loss: 0.6809322834014893, acc.: 56.74%] [G loss: 0.7428929805755615]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 44/86 [D loss: 0.6790779232978821, acc.: 56.69%] [G loss: 0.7341775298118591]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 45/86 [D loss: 0.6831828355789185, acc.: 56.40%] [G loss: 0.7432191371917725]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 46/86 [D loss: 0.6797813177108765, acc.: 56.88%] [G loss: 0.7362279891967773]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 47/86 [D loss: 0.6842310726642609, acc.: 55.08%] [G loss: 0.7397840023040771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 48/86 [D loss: 0.6763442754745483, acc.: 58.69%] [G loss: 0.7363602519035339]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 49/86 [D loss: 0.679931640625, acc.: 57.62%] [G loss: 0.7337973117828369]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 50/86 [D loss: 0.6819509863853455, acc.: 57.18%] [G loss: 0.7370115518569946]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 51/86 [D loss: 0.6822504997253418, acc.: 56.05%] [G loss: 0.738519012928009]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 52/86 [D loss: 0.6787844300270081, acc.: 57.91%] [G loss: 0.7342483997344971]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 53/86 [D loss: 0.6776756346225739, acc.: 58.06%] [G loss: 0.7371914386749268]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 54/86 [D loss: 0.680400162935257, acc.: 55.62%] [G loss: 0.734416127204895]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 55/86 [D loss: 0.6792899370193481, acc.: 58.30%] [G loss: 0.7386646270751953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 56/86 [D loss: 0.6776959896087646, acc.: 58.06%] [G loss: 0.7340167760848999]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 57/86 [D loss: 0.6802982687950134, acc.: 57.62%] [G loss: 0.7396384477615356]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 58/86 [D loss: 0.6780429482460022, acc.: 57.81%] [G loss: 0.7350220084190369]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 59/86 [D loss: 0.6818118393421173, acc.: 56.20%] [G loss: 0.7373195290565491]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 60/86 [D loss: 0.6775864958763123, acc.: 58.98%] [G loss: 0.7385105490684509]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 61/86 [D loss: 0.678366094827652, acc.: 56.01%] [G loss: 0.7348799109458923]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 62/86 [D loss: 0.6775714159011841, acc.: 58.59%] [G loss: 0.7391565442085266]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 63/86 [D loss: 0.6802359819412231, acc.: 56.93%] [G loss: 0.7316964268684387]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 64/86 [D loss: 0.6788911819458008, acc.: 56.74%] [G loss: 0.7398526072502136]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 65/86 [D loss: 0.6805071830749512, acc.: 56.15%] [G loss: 0.7344303131103516]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 66/86 [D loss: 0.6766093969345093, acc.: 58.94%] [G loss: 0.7321285009384155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 67/86 [D loss: 0.67871955037117, acc.: 58.50%] [G loss: 0.7333703637123108]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 68/86 [D loss: 0.6775212585926056, acc.: 58.15%] [G loss: 0.7355505228042603]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 69/86 [D loss: 0.6772108376026154, acc.: 58.06%] [G loss: 0.7406864166259766]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 70/86 [D loss: 0.6742818355560303, acc.: 59.91%] [G loss: 0.7349414825439453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 71/86 [D loss: 0.6776467561721802, acc.: 57.47%] [G loss: 0.7361566424369812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 72/86 [D loss: 0.6796747446060181, acc.: 57.67%] [G loss: 0.7402013540267944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 73/86 [D loss: 0.6780990362167358, acc.: 58.98%] [G loss: 0.7378880381584167]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 74/86 [D loss: 0.6779185235500336, acc.: 60.01%] [G loss: 0.7400588393211365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 75/86 [D loss: 0.6790069937705994, acc.: 56.59%] [G loss: 0.7382601499557495]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 76/86 [D loss: 0.6809199154376984, acc.: 56.05%] [G loss: 0.737375795841217]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 77/86 [D loss: 0.6771740317344666, acc.: 57.62%] [G loss: 0.738956868648529]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 78/86 [D loss: 0.6776524186134338, acc.: 57.67%] [G loss: 0.7364307045936584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 79/86 [D loss: 0.6798535287380219, acc.: 57.03%] [G loss: 0.7440077662467957]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 80/86 [D loss: 0.6778891682624817, acc.: 57.62%] [G loss: 0.7349275946617126]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 81/86 [D loss: 0.6796422898769379, acc.: 57.28%] [G loss: 0.7375431060791016]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 82/86 [D loss: 0.6811979711055756, acc.: 57.23%] [G loss: 0.7456172704696655]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 109/200, Batch 83/86 [D loss: 0.6816286742687225, acc.: 56.20%] [G loss: 0.7354605197906494]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 84/86 [D loss: 0.6810056269168854, acc.: 55.71%] [G loss: 0.7373918890953064]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 85/86 [D loss: 0.6767958998680115, acc.: 58.84%] [G loss: 0.7359794974327087]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 109/200, Batch 86/86 [D loss: 0.679805725812912, acc.: 56.79%] [G loss: 0.7353485822677612]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 1/86 [D loss: 0.6799276769161224, acc.: 56.35%] [G loss: 0.7349152565002441]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 2/86 [D loss: 0.6771264672279358, acc.: 56.93%] [G loss: 0.733117401599884]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 3/86 [D loss: 0.6789533197879791, acc.: 58.50%] [G loss: 0.73642897605896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 4/86 [D loss: 0.6801177859306335, acc.: 56.05%] [G loss: 0.7306283712387085]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 5/86 [D loss: 0.6797226071357727, acc.: 57.23%] [G loss: 0.7378995418548584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 6/86 [D loss: 0.6789110004901886, acc.: 57.71%] [G loss: 0.7350106239318848]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 7/86 [D loss: 0.6801342666149139, acc.: 56.74%] [G loss: 0.7390555739402771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 8/86 [D loss: 0.6765953898429871, acc.: 59.86%] [G loss: 0.7384242415428162]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 9/86 [D loss: 0.6803370714187622, acc.: 57.28%] [G loss: 0.7326010465621948]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 10/86 [D loss: 0.6799622178077698, acc.: 57.32%] [G loss: 0.7322102785110474]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 11/86 [D loss: 0.6829037368297577, acc.: 55.47%] [G loss: 0.7354989647865295]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 12/86 [D loss: 0.6800420582294464, acc.: 56.74%] [G loss: 0.7430417537689209]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 13/86 [D loss: 0.6800018846988678, acc.: 56.93%] [G loss: 0.7347914576530457]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 14/86 [D loss: 0.6817287504673004, acc.: 56.10%] [G loss: 0.7369682192802429]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 15/86 [D loss: 0.6777551770210266, acc.: 58.94%] [G loss: 0.7408491373062134]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 16/86 [D loss: 0.6825562715530396, acc.: 56.35%] [G loss: 0.731332004070282]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 17/86 [D loss: 0.6755292415618896, acc.: 58.94%] [G loss: 0.7351508140563965]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 110/200, Batch 18/86 [D loss: 0.6818109452724457, acc.: 56.05%] [G loss: 0.734877347946167]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 19/86 [D loss: 0.6767010688781738, acc.: 59.08%] [G loss: 0.7364372611045837]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 20/86 [D loss: 0.6768075227737427, acc.: 58.15%] [G loss: 0.7378828525543213]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 21/86 [D loss: 0.6775562465190887, acc.: 58.30%] [G loss: 0.741654098033905]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 22/86 [D loss: 0.6808444857597351, acc.: 55.91%] [G loss: 0.738533616065979]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 23/86 [D loss: 0.6788562536239624, acc.: 57.03%] [G loss: 0.7370887398719788]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 24/86 [D loss: 0.6805198192596436, acc.: 55.91%] [G loss: 0.7410010695457458]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 25/86 [D loss: 0.6788680851459503, acc.: 56.88%] [G loss: 0.7379324436187744]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 26/86 [D loss: 0.6799606084823608, acc.: 56.54%] [G loss: 0.7326518297195435]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 27/86 [D loss: 0.6763874590396881, acc.: 58.30%] [G loss: 0.737468957901001]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 28/86 [D loss: 0.6822891235351562, acc.: 55.71%] [G loss: 0.7424464225769043]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 29/86 [D loss: 0.6817417740821838, acc.: 55.62%] [G loss: 0.7334187030792236]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 30/86 [D loss: 0.6759663224220276, acc.: 59.08%] [G loss: 0.7286574840545654]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 31/86 [D loss: 0.6809140145778656, acc.: 56.88%] [G loss: 0.7342299222946167]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 32/86 [D loss: 0.6767165958881378, acc.: 59.91%] [G loss: 0.7343517541885376]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 33/86 [D loss: 0.6801754832267761, acc.: 58.20%] [G loss: 0.7378511428833008]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 34/86 [D loss: 0.6789172291755676, acc.: 56.88%] [G loss: 0.7352254390716553]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 35/86 [D loss: 0.6789939999580383, acc.: 57.28%] [G loss: 0.7357931733131409]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 36/86 [D loss: 0.6803463697433472, acc.: 57.42%] [G loss: 0.7368848919868469]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 37/86 [D loss: 0.6776641309261322, acc.: 57.57%] [G loss: 0.7353261709213257]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 38/86 [D loss: 0.6752418577671051, acc.: 58.89%] [G loss: 0.7409270405769348]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 39/86 [D loss: 0.6749792397022247, acc.: 59.96%] [G loss: 0.7365673780441284]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 40/86 [D loss: 0.6772395074367523, acc.: 57.81%] [G loss: 0.7393994331359863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 41/86 [D loss: 0.6761711239814758, acc.: 59.03%] [G loss: 0.7420803308486938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 42/86 [D loss: 0.6775345206260681, acc.: 57.67%] [G loss: 0.7361574769020081]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 43/86 [D loss: 0.6780818700790405, acc.: 57.71%] [G loss: 0.7390084862709045]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 44/86 [D loss: 0.6801955699920654, acc.: 56.93%] [G loss: 0.7333780527114868]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 45/86 [D loss: 0.6788041889667511, acc.: 56.40%] [G loss: 0.7396174669265747]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 46/86 [D loss: 0.6785896718502045, acc.: 57.28%] [G loss: 0.7382530570030212]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 47/86 [D loss: 0.6796653270721436, acc.: 56.79%] [G loss: 0.7422711849212646]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 110/200, Batch 48/86 [D loss: 0.6798778176307678, acc.: 56.25%] [G loss: 0.7375832796096802]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 49/86 [D loss: 0.6794801652431488, acc.: 57.71%] [G loss: 0.7375650405883789]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 50/86 [D loss: 0.6802676320075989, acc.: 56.54%] [G loss: 0.7396684885025024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 51/86 [D loss: 0.681151956319809, acc.: 56.10%] [G loss: 0.7440441846847534]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 52/86 [D loss: 0.6817633211612701, acc.: 55.47%] [G loss: 0.7420799732208252]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 53/86 [D loss: 0.6748527884483337, acc.: 59.08%] [G loss: 0.7366308569908142]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 110/200, Batch 54/86 [D loss: 0.6769990622997284, acc.: 58.30%] [G loss: 0.7431334257125854]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 55/86 [D loss: 0.6795403361320496, acc.: 56.64%] [G loss: 0.7396801114082336]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 56/86 [D loss: 0.6809598207473755, acc.: 56.54%] [G loss: 0.7378107309341431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 57/86 [D loss: 0.6789783239364624, acc.: 58.45%] [G loss: 0.7370351552963257]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 58/86 [D loss: 0.6771118938922882, acc.: 58.11%] [G loss: 0.7333567142486572]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 59/86 [D loss: 0.6799314916133881, acc.: 56.88%] [G loss: 0.7391501665115356]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 110/200, Batch 60/86 [D loss: 0.6817398369312286, acc.: 57.86%] [G loss: 0.7343581914901733]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 61/86 [D loss: 0.6775332689285278, acc.: 57.96%] [G loss: 0.7382024526596069]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 62/86 [D loss: 0.6790205538272858, acc.: 57.96%] [G loss: 0.734512209892273]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 110/200, Batch 63/86 [D loss: 0.6777932047843933, acc.: 57.57%] [G loss: 0.7377890944480896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 64/86 [D loss: 0.6797215938568115, acc.: 57.18%] [G loss: 0.7402404546737671]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 65/86 [D loss: 0.676941841840744, acc.: 58.11%] [G loss: 0.7339280843734741]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 66/86 [D loss: 0.6816476583480835, acc.: 56.15%] [G loss: 0.7414848804473877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 67/86 [D loss: 0.6797360479831696, acc.: 58.35%] [G loss: 0.7323848009109497]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 68/86 [D loss: 0.6780886948108673, acc.: 57.96%] [G loss: 0.7405598163604736]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 69/86 [D loss: 0.6755927503108978, acc.: 59.28%] [G loss: 0.737071692943573]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 70/86 [D loss: 0.678510308265686, acc.: 58.25%] [G loss: 0.7306849360466003]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 71/86 [D loss: 0.6804143190383911, acc.: 56.35%] [G loss: 0.7330024242401123]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 72/86 [D loss: 0.6832782626152039, acc.: 55.71%] [G loss: 0.733559250831604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 73/86 [D loss: 0.6810720562934875, acc.: 56.15%] [G loss: 0.7377018928527832]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 74/86 [D loss: 0.6794711649417877, acc.: 56.88%] [G loss: 0.7327876091003418]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 75/86 [D loss: 0.6767667531967163, acc.: 58.59%] [G loss: 0.7409789562225342]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 76/86 [D loss: 0.6804398000240326, acc.: 57.23%] [G loss: 0.7323729991912842]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 77/86 [D loss: 0.6812639832496643, acc.: 56.64%] [G loss: 0.7396960854530334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 78/86 [D loss: 0.6760978400707245, acc.: 59.23%] [G loss: 0.7379292249679565]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 79/86 [D loss: 0.6779280304908752, acc.: 57.23%] [G loss: 0.7382617592811584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 80/86 [D loss: 0.6760077774524689, acc.: 58.30%] [G loss: 0.7347568273544312]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 81/86 [D loss: 0.6801793575286865, acc.: 56.79%] [G loss: 0.7331599593162537]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 82/86 [D loss: 0.6754752099514008, acc.: 58.74%] [G loss: 0.7353968620300293]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 83/86 [D loss: 0.6850126087665558, acc.: 54.69%] [G loss: 0.7348160743713379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 84/86 [D loss: 0.6792771816253662, acc.: 57.37%] [G loss: 0.7479588985443115]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 110/200, Batch 85/86 [D loss: 0.6783347725868225, acc.: 57.42%] [G loss: 0.7328375577926636]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 110/200, Batch 86/86 [D loss: 0.680780291557312, acc.: 57.52%] [G loss: 0.7404525279998779]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 1/86 [D loss: 0.6779904961585999, acc.: 57.08%] [G loss: 0.7324618101119995]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 2/86 [D loss: 0.6842774748802185, acc.: 54.35%] [G loss: 0.7373690605163574]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 3/86 [D loss: 0.6807149648666382, acc.: 57.32%] [G loss: 0.72768235206604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 4/86 [D loss: 0.6839175522327423, acc.: 54.15%] [G loss: 0.7347241044044495]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 5/86 [D loss: 0.678561806678772, acc.: 58.45%] [G loss: 0.7369982600212097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 6/86 [D loss: 0.6868880391120911, acc.: 54.98%] [G loss: 0.7333070039749146]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 7/86 [D loss: 0.6773202419281006, acc.: 57.91%] [G loss: 0.7387548685073853]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 8/86 [D loss: 0.6804872453212738, acc.: 55.71%] [G loss: 0.7338787913322449]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 9/86 [D loss: 0.6773969531059265, acc.: 57.86%] [G loss: 0.7380862832069397]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 10/86 [D loss: 0.6774259507656097, acc.: 58.50%] [G loss: 0.7382970452308655]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 11/86 [D loss: 0.6797765493392944, acc.: 56.40%] [G loss: 0.7448751926422119]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 12/86 [D loss: 0.675909161567688, acc.: 59.47%] [G loss: 0.735141396522522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 13/86 [D loss: 0.6774489283561707, acc.: 57.57%] [G loss: 0.738746702671051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 14/86 [D loss: 0.6778351068496704, acc.: 57.62%] [G loss: 0.7387226819992065]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 15/86 [D loss: 0.6814062893390656, acc.: 55.37%] [G loss: 0.7374250292778015]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 16/86 [D loss: 0.6797625720500946, acc.: 58.01%] [G loss: 0.7380324602127075]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 111/200, Batch 17/86 [D loss: 0.6780773997306824, acc.: 57.86%] [G loss: 0.7352472543716431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 18/86 [D loss: 0.6783033609390259, acc.: 57.57%] [G loss: 0.7379240989685059]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 19/86 [D loss: 0.6776204109191895, acc.: 59.08%] [G loss: 0.7374324798583984]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 20/86 [D loss: 0.6804952621459961, acc.: 56.74%] [G loss: 0.7391406297683716]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 21/86 [D loss: 0.6786055266857147, acc.: 57.42%] [G loss: 0.740923285484314]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 22/86 [D loss: 0.6766217350959778, acc.: 59.86%] [G loss: 0.7390165328979492]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 23/86 [D loss: 0.6754018068313599, acc.: 58.64%] [G loss: 0.7411006689071655]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 24/86 [D loss: 0.6802352964878082, acc.: 57.76%] [G loss: 0.7388217449188232]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 25/86 [D loss: 0.6793034374713898, acc.: 57.67%] [G loss: 0.738801896572113]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 26/86 [D loss: 0.6745132505893707, acc.: 59.62%] [G loss: 0.7402586936950684]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 27/86 [D loss: 0.6784870028495789, acc.: 57.23%] [G loss: 0.7323551177978516]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 111/200, Batch 28/86 [D loss: 0.6791875958442688, acc.: 57.08%] [G loss: 0.7403625249862671]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 29/86 [D loss: 0.6803075075149536, acc.: 56.20%] [G loss: 0.7379007935523987]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 30/86 [D loss: 0.6815550327301025, acc.: 56.10%] [G loss: 0.7412371635437012]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 31/86 [D loss: 0.680535614490509, acc.: 56.59%] [G loss: 0.7384762763977051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 32/86 [D loss: 0.677685558795929, acc.: 57.18%] [G loss: 0.7375200390815735]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 33/86 [D loss: 0.6785649061203003, acc.: 56.69%] [G loss: 0.736254870891571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 34/86 [D loss: 0.6788054406642914, acc.: 57.08%] [G loss: 0.7325502634048462]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 35/86 [D loss: 0.6830881834030151, acc.: 54.74%] [G loss: 0.738795816898346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 36/86 [D loss: 0.6781881153583527, acc.: 58.40%] [G loss: 0.7365486025810242]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 37/86 [D loss: 0.6789333820343018, acc.: 56.45%] [G loss: 0.7360966801643372]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 38/86 [D loss: 0.6817174851894379, acc.: 55.86%] [G loss: 0.7415066957473755]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 39/86 [D loss: 0.6800786554813385, acc.: 55.42%] [G loss: 0.7350050806999207]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 40/86 [D loss: 0.6778209507465363, acc.: 58.11%] [G loss: 0.7390650510787964]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 41/86 [D loss: 0.6788069605827332, acc.: 56.20%] [G loss: 0.7324913144111633]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 42/86 [D loss: 0.677467554807663, acc.: 57.96%] [G loss: 0.7337110042572021]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 111/200, Batch 43/86 [D loss: 0.6774377822875977, acc.: 57.52%] [G loss: 0.736459493637085]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 44/86 [D loss: 0.6832373738288879, acc.: 55.57%] [G loss: 0.7389642596244812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 45/86 [D loss: 0.6723404228687286, acc.: 60.55%] [G loss: 0.7340482473373413]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 46/86 [D loss: 0.6846737861633301, acc.: 54.54%] [G loss: 0.7424805760383606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 47/86 [D loss: 0.6765289306640625, acc.: 58.30%] [G loss: 0.7402064204216003]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 48/86 [D loss: 0.6792598366737366, acc.: 57.47%] [G loss: 0.7450188994407654]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 49/86 [D loss: 0.6798455715179443, acc.: 57.23%] [G loss: 0.7426467537879944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 50/86 [D loss: 0.6789336502552032, acc.: 56.74%] [G loss: 0.7298866510391235]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 51/86 [D loss: 0.6782525479793549, acc.: 57.23%] [G loss: 0.7409232258796692]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 52/86 [D loss: 0.6804395914077759, acc.: 57.86%] [G loss: 0.731226921081543]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 53/86 [D loss: 0.6847871541976929, acc.: 54.54%] [G loss: 0.7401326298713684]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 54/86 [D loss: 0.6829245388507843, acc.: 56.54%] [G loss: 0.7332764863967896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 55/86 [D loss: 0.6790725588798523, acc.: 56.93%] [G loss: 0.7328993082046509]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 56/86 [D loss: 0.6751629710197449, acc.: 59.86%] [G loss: 0.733974039554596]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 57/86 [D loss: 0.6777705550193787, acc.: 58.64%] [G loss: 0.7331808805465698]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 111/200, Batch 58/86 [D loss: 0.6783511936664581, acc.: 56.79%] [G loss: 0.7384169101715088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 59/86 [D loss: 0.6807030141353607, acc.: 55.62%] [G loss: 0.7349667549133301]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 60/86 [D loss: 0.6806078553199768, acc.: 56.45%] [G loss: 0.7401466369628906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 61/86 [D loss: 0.6779687106609344, acc.: 56.64%] [G loss: 0.7393259406089783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 62/86 [D loss: 0.6820754706859589, acc.: 56.20%] [G loss: 0.7370261549949646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 63/86 [D loss: 0.6767768263816833, acc.: 58.50%] [G loss: 0.7351945638656616]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 64/86 [D loss: 0.6813062727451324, acc.: 57.08%] [G loss: 0.7395887970924377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 65/86 [D loss: 0.6786797642707825, acc.: 56.69%] [G loss: 0.7329323291778564]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 111/200, Batch 66/86 [D loss: 0.6781879663467407, acc.: 57.18%] [G loss: 0.7409104704856873]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 67/86 [D loss: 0.675944596529007, acc.: 58.94%] [G loss: 0.7432473301887512]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 68/86 [D loss: 0.6787716746330261, acc.: 56.74%] [G loss: 0.7377538681030273]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 69/86 [D loss: 0.6796559691429138, acc.: 56.49%] [G loss: 0.7416578531265259]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 111/200, Batch 70/86 [D loss: 0.6812097728252411, acc.: 56.10%] [G loss: 0.7342109084129333]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 71/86 [D loss: 0.6831836998462677, acc.: 55.57%] [G loss: 0.7426944375038147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 72/86 [D loss: 0.6756828725337982, acc.: 59.77%] [G loss: 0.7393395304679871]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 73/86 [D loss: 0.6800764501094818, acc.: 56.15%] [G loss: 0.7406216263771057]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 74/86 [D loss: 0.6758451461791992, acc.: 58.84%] [G loss: 0.7382274866104126]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 75/86 [D loss: 0.6817046403884888, acc.: 55.76%] [G loss: 0.7442697286605835]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 76/86 [D loss: 0.6766843795776367, acc.: 59.33%] [G loss: 0.7369062900543213]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 111/200, Batch 77/86 [D loss: 0.6767647862434387, acc.: 58.84%] [G loss: 0.7343302369117737]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 78/86 [D loss: 0.682645320892334, acc.: 55.42%] [G loss: 0.7455828785896301]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 79/86 [D loss: 0.6809028089046478, acc.: 57.42%] [G loss: 0.7419389486312866]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 80/86 [D loss: 0.678528755903244, acc.: 56.40%] [G loss: 0.7315859794616699]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 81/86 [D loss: 0.6789611876010895, acc.: 56.84%] [G loss: 0.7376354932785034]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 82/86 [D loss: 0.6779526770114899, acc.: 56.54%] [G loss: 0.7377532124519348]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 111/200, Batch 83/86 [D loss: 0.6775691211223602, acc.: 57.57%] [G loss: 0.7363293766975403]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 84/86 [D loss: 0.6773906350135803, acc.: 57.96%] [G loss: 0.7443016767501831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 85/86 [D loss: 0.6791144907474518, acc.: 58.64%] [G loss: 0.7423030138015747]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 111/200, Batch 86/86 [D loss: 0.6794896423816681, acc.: 57.62%] [G loss: 0.7373776435852051]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 112/200, Batch 1/86 [D loss: 0.6773339509963989, acc.: 58.15%] [G loss: 0.7432318329811096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 2/86 [D loss: 0.6791056096553802, acc.: 57.08%] [G loss: 0.7373222708702087]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 3/86 [D loss: 0.6789960861206055, acc.: 57.86%] [G loss: 0.7413557767868042]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 4/86 [D loss: 0.6806922852993011, acc.: 56.05%] [G loss: 0.734298050403595]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 5/86 [D loss: 0.6786401569843292, acc.: 57.13%] [G loss: 0.7348854541778564]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 6/86 [D loss: 0.6782435178756714, acc.: 58.15%] [G loss: 0.7366452217102051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 7/86 [D loss: 0.6764713525772095, acc.: 58.54%] [G loss: 0.7348070740699768]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 8/86 [D loss: 0.675961434841156, acc.: 58.64%] [G loss: 0.7382403612136841]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 9/86 [D loss: 0.6771980226039886, acc.: 58.79%] [G loss: 0.7402665019035339]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 10/86 [D loss: 0.6800121068954468, acc.: 57.42%] [G loss: 0.7421329617500305]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 11/86 [D loss: 0.6798368692398071, acc.: 56.79%] [G loss: 0.7374042868614197]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 12/86 [D loss: 0.6794039309024811, acc.: 57.23%] [G loss: 0.7372257709503174]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 13/86 [D loss: 0.6731075644493103, acc.: 60.30%] [G loss: 0.7336059808731079]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 14/86 [D loss: 0.6779127418994904, acc.: 56.59%] [G loss: 0.7353240251541138]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 15/86 [D loss: 0.6809950768947601, acc.: 56.05%] [G loss: 0.7376178503036499]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 16/86 [D loss: 0.6774314045906067, acc.: 57.57%] [G loss: 0.7311896681785583]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 17/86 [D loss: 0.6783609688282013, acc.: 57.28%] [G loss: 0.7349088191986084]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 18/86 [D loss: 0.6777387261390686, acc.: 58.74%] [G loss: 0.7407219409942627]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 19/86 [D loss: 0.6774358749389648, acc.: 56.45%] [G loss: 0.7370350956916809]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 20/86 [D loss: 0.6775715351104736, acc.: 58.98%] [G loss: 0.7406411170959473]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 21/86 [D loss: 0.6784490346908569, acc.: 56.64%] [G loss: 0.7402071952819824]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 22/86 [D loss: 0.6777875125408173, acc.: 57.71%] [G loss: 0.7353550791740417]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 23/86 [D loss: 0.6786522269248962, acc.: 56.54%] [G loss: 0.7402665019035339]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 24/86 [D loss: 0.676918089389801, acc.: 58.25%] [G loss: 0.7350930571556091]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 25/86 [D loss: 0.6733335256576538, acc.: 59.67%] [G loss: 0.7384076714515686]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 26/86 [D loss: 0.6774220168590546, acc.: 57.91%] [G loss: 0.7388505935668945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 27/86 [D loss: 0.6783328950405121, acc.: 58.54%] [G loss: 0.739344596862793]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 28/86 [D loss: 0.6792022883892059, acc.: 58.11%] [G loss: 0.7403030395507812]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 29/86 [D loss: 0.6804437339305878, acc.: 56.01%] [G loss: 0.7358946204185486]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 30/86 [D loss: 0.6770495772361755, acc.: 59.62%] [G loss: 0.7399115562438965]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 31/86 [D loss: 0.6783999800682068, acc.: 58.01%] [G loss: 0.7378915548324585]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 32/86 [D loss: 0.6779481470584869, acc.: 58.30%] [G loss: 0.7387546300888062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 33/86 [D loss: 0.6793154180049896, acc.: 57.86%] [G loss: 0.7447719573974609]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 34/86 [D loss: 0.6771446466445923, acc.: 58.64%] [G loss: 0.7354006767272949]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 35/86 [D loss: 0.6788484156131744, acc.: 56.88%] [G loss: 0.7397997975349426]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 36/86 [D loss: 0.6794920265674591, acc.: 58.11%] [G loss: 0.7412459254264832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 37/86 [D loss: 0.6771969497203827, acc.: 57.47%] [G loss: 0.7498066425323486]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 38/86 [D loss: 0.6761796176433563, acc.: 58.40%] [G loss: 0.7435896396636963]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 39/86 [D loss: 0.6801067292690277, acc.: 56.01%] [G loss: 0.7410746216773987]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 40/86 [D loss: 0.6773780584335327, acc.: 58.98%] [G loss: 0.7367158532142639]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 41/86 [D loss: 0.677304595708847, acc.: 57.91%] [G loss: 0.7421296238899231]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 112/200, Batch 42/86 [D loss: 0.6775951385498047, acc.: 58.15%] [G loss: 0.7433921694755554]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 43/86 [D loss: 0.680279552936554, acc.: 55.62%] [G loss: 0.746364951133728]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 44/86 [D loss: 0.6771852970123291, acc.: 57.91%] [G loss: 0.7399278283119202]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 45/86 [D loss: 0.6780897378921509, acc.: 58.15%] [G loss: 0.7438271641731262]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 46/86 [D loss: 0.6742296516895294, acc.: 59.52%] [G loss: 0.7425774335861206]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 47/86 [D loss: 0.6792531311511993, acc.: 56.74%] [G loss: 0.7413914799690247]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 48/86 [D loss: 0.6767434179782867, acc.: 59.13%] [G loss: 0.7409039735794067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 49/86 [D loss: 0.6795597076416016, acc.: 57.47%] [G loss: 0.7392520904541016]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 50/86 [D loss: 0.6789811551570892, acc.: 56.40%] [G loss: 0.7418397068977356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 51/86 [D loss: 0.679098516702652, acc.: 57.08%] [G loss: 0.7395281791687012]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 52/86 [D loss: 0.6794677078723907, acc.: 56.40%] [G loss: 0.7399241328239441]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 53/86 [D loss: 0.676322728395462, acc.: 57.71%] [G loss: 0.7390133142471313]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 54/86 [D loss: 0.6763315796852112, acc.: 58.64%] [G loss: 0.7377446889877319]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 55/86 [D loss: 0.6798104047775269, acc.: 56.98%] [G loss: 0.7410397529602051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 56/86 [D loss: 0.6745902895927429, acc.: 58.94%] [G loss: 0.7390601634979248]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 57/86 [D loss: 0.6804575324058533, acc.: 56.93%] [G loss: 0.7412549257278442]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 58/86 [D loss: 0.6779459714889526, acc.: 58.40%] [G loss: 0.7398315072059631]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 59/86 [D loss: 0.677231639623642, acc.: 57.47%] [G loss: 0.7394883632659912]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 60/86 [D loss: 0.6774928867816925, acc.: 57.67%] [G loss: 0.7377182245254517]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 61/86 [D loss: 0.6773937046527863, acc.: 57.86%] [G loss: 0.7396209836006165]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 62/86 [D loss: 0.6774157285690308, acc.: 59.42%] [G loss: 0.7406935691833496]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 63/86 [D loss: 0.6771216988563538, acc.: 58.20%] [G loss: 0.7374390363693237]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 64/86 [D loss: 0.6792415380477905, acc.: 56.54%] [G loss: 0.7393653392791748]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 65/86 [D loss: 0.6751250326633453, acc.: 58.54%] [G loss: 0.739115297794342]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 66/86 [D loss: 0.6812849938869476, acc.: 56.64%] [G loss: 0.7403252720832825]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 67/86 [D loss: 0.6783812642097473, acc.: 57.91%] [G loss: 0.7377734780311584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 68/86 [D loss: 0.6831226348876953, acc.: 54.83%] [G loss: 0.7356595396995544]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 69/86 [D loss: 0.6760096848011017, acc.: 58.35%] [G loss: 0.7436341047286987]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 70/86 [D loss: 0.677268773317337, acc.: 57.03%] [G loss: 0.7456470727920532]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 71/86 [D loss: 0.6760059893131256, acc.: 58.59%] [G loss: 0.7456411123275757]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 72/86 [D loss: 0.6768490970134735, acc.: 58.20%] [G loss: 0.7421187162399292]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 112/200, Batch 73/86 [D loss: 0.6741686761379242, acc.: 58.25%] [G loss: 0.7423418760299683]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 74/86 [D loss: 0.6767912209033966, acc.: 59.13%] [G loss: 0.7420791387557983]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 75/86 [D loss: 0.6762921214103699, acc.: 58.40%] [G loss: 0.7389681339263916]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 76/86 [D loss: 0.6776316165924072, acc.: 56.49%] [G loss: 0.7455036640167236]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 77/86 [D loss: 0.6788077652454376, acc.: 58.15%] [G loss: 0.7408972382545471]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 78/86 [D loss: 0.6811211109161377, acc.: 55.76%] [G loss: 0.7383936643600464]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 79/86 [D loss: 0.6782768666744232, acc.: 56.69%] [G loss: 0.7346022129058838]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 112/200, Batch 80/86 [D loss: 0.6767536997795105, acc.: 58.54%] [G loss: 0.740607500076294]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 112/200, Batch 81/86 [D loss: 0.6833429336547852, acc.: 56.35%] [G loss: 0.7423191666603088]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 82/86 [D loss: 0.6802341043949127, acc.: 56.64%] [G loss: 0.7375031113624573]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 83/86 [D loss: 0.6752260327339172, acc.: 59.57%] [G loss: 0.7398093938827515]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 84/86 [D loss: 0.6754233241081238, acc.: 58.79%] [G loss: 0.7383065223693848]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 85/86 [D loss: 0.6789076328277588, acc.: 58.11%] [G loss: 0.7405599355697632]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 112/200, Batch 86/86 [D loss: 0.6745370924472809, acc.: 60.11%] [G loss: 0.7329350709915161]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 113/200, Batch 1/86 [D loss: 0.6774922013282776, acc.: 58.15%] [G loss: 0.7392838001251221]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 2/86 [D loss: 0.677771270275116, acc.: 56.88%] [G loss: 0.736449658870697]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 3/86 [D loss: 0.6804625988006592, acc.: 56.15%] [G loss: 0.74428790807724]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 4/86 [D loss: 0.6770699322223663, acc.: 58.50%] [G loss: 0.7360397577285767]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 5/86 [D loss: 0.6806804239749908, acc.: 57.47%] [G loss: 0.7400456070899963]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 6/86 [D loss: 0.6791646182537079, acc.: 56.20%] [G loss: 0.7376278042793274]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 7/86 [D loss: 0.6767994165420532, acc.: 56.54%] [G loss: 0.7426865696907043]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 8/86 [D loss: 0.6790409982204437, acc.: 56.54%] [G loss: 0.7366337776184082]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 9/86 [D loss: 0.6754410564899445, acc.: 58.89%] [G loss: 0.7354693412780762]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 10/86 [D loss: 0.6754112541675568, acc.: 59.18%] [G loss: 0.7383241653442383]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 11/86 [D loss: 0.6804526150226593, acc.: 56.45%] [G loss: 0.7379138469696045]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 12/86 [D loss: 0.679429680109024, acc.: 57.13%] [G loss: 0.7395845055580139]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 13/86 [D loss: 0.6790856719017029, acc.: 56.88%] [G loss: 0.7395987510681152]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 14/86 [D loss: 0.6758306622505188, acc.: 58.20%] [G loss: 0.7427043914794922]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 15/86 [D loss: 0.6787740886211395, acc.: 56.93%] [G loss: 0.7442787289619446]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 16/86 [D loss: 0.67794269323349, acc.: 58.25%] [G loss: 0.737049400806427]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 17/86 [D loss: 0.6787241101264954, acc.: 57.28%] [G loss: 0.7404727339744568]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 18/86 [D loss: 0.6786097884178162, acc.: 58.15%] [G loss: 0.7409781813621521]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 19/86 [D loss: 0.6793097257614136, acc.: 57.96%] [G loss: 0.7442667484283447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 113/200, Batch 20/86 [D loss: 0.6777657270431519, acc.: 57.42%] [G loss: 0.738963782787323]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 21/86 [D loss: 0.6781146824359894, acc.: 56.84%] [G loss: 0.7420394420623779]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 22/86 [D loss: 0.6754670441150665, acc.: 58.06%] [G loss: 0.7375960946083069]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 23/86 [D loss: 0.677786648273468, acc.: 57.47%] [G loss: 0.7437853217124939]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 24/86 [D loss: 0.6782056093215942, acc.: 57.57%] [G loss: 0.7383647561073303]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 25/86 [D loss: 0.676382303237915, acc.: 59.13%] [G loss: 0.7377238869667053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 26/86 [D loss: 0.678523451089859, acc.: 57.86%] [G loss: 0.7393602728843689]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 27/86 [D loss: 0.6764895617961884, acc.: 57.96%] [G loss: 0.7408424019813538]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 28/86 [D loss: 0.6791185140609741, acc.: 56.69%] [G loss: 0.7364582419395447]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 29/86 [D loss: 0.6784573793411255, acc.: 58.54%] [G loss: 0.7361983060836792]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 30/86 [D loss: 0.679133951663971, acc.: 57.86%] [G loss: 0.7430543303489685]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 31/86 [D loss: 0.6773994863033295, acc.: 57.13%] [G loss: 0.7371402978897095]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 32/86 [D loss: 0.681888997554779, acc.: 56.35%] [G loss: 0.738060474395752]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 33/86 [D loss: 0.6764360666275024, acc.: 57.23%] [G loss: 0.7417051792144775]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 34/86 [D loss: 0.6744660437107086, acc.: 59.13%] [G loss: 0.7413898706436157]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 35/86 [D loss: 0.6765319406986237, acc.: 57.96%] [G loss: 0.7418872117996216]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 36/86 [D loss: 0.6782898902893066, acc.: 56.84%] [G loss: 0.741470456123352]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 37/86 [D loss: 0.6785287857055664, acc.: 56.54%] [G loss: 0.7397705912590027]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 38/86 [D loss: 0.6755320131778717, acc.: 59.77%] [G loss: 0.7500353455543518]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 39/86 [D loss: 0.6761724352836609, acc.: 58.06%] [G loss: 0.7410603165626526]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 40/86 [D loss: 0.6801633536815643, acc.: 57.32%] [G loss: 0.7377654314041138]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 41/86 [D loss: 0.679836094379425, acc.: 57.32%] [G loss: 0.742908239364624]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 42/86 [D loss: 0.6751879751682281, acc.: 59.52%] [G loss: 0.743294358253479]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 43/86 [D loss: 0.6769387423992157, acc.: 58.01%] [G loss: 0.7358691096305847]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 44/86 [D loss: 0.6785117387771606, acc.: 57.71%] [G loss: 0.7467935681343079]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 45/86 [D loss: 0.6782136559486389, acc.: 57.81%] [G loss: 0.7399544715881348]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 46/86 [D loss: 0.6777335405349731, acc.: 57.42%] [G loss: 0.7374310493469238]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 113/200, Batch 47/86 [D loss: 0.6793107986450195, acc.: 57.03%] [G loss: 0.7434426546096802]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 48/86 [D loss: 0.6757311224937439, acc.: 58.06%] [G loss: 0.747315526008606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 49/86 [D loss: 0.6784728169441223, acc.: 58.59%] [G loss: 0.7428230047225952]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 50/86 [D loss: 0.6816558241844177, acc.: 56.59%] [G loss: 0.7356557846069336]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 51/86 [D loss: 0.6770505905151367, acc.: 59.08%] [G loss: 0.7446319460868835]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 52/86 [D loss: 0.6772641241550446, acc.: 58.45%] [G loss: 0.7492141723632812]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 53/86 [D loss: 0.6806225180625916, acc.: 55.71%] [G loss: 0.7449050545692444]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 54/86 [D loss: 0.6783604025840759, acc.: 57.96%] [G loss: 0.7460530996322632]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 55/86 [D loss: 0.6749363243579865, acc.: 58.98%] [G loss: 0.7377119660377502]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 56/86 [D loss: 0.6789111495018005, acc.: 57.47%] [G loss: 0.7380374670028687]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 113/200, Batch 57/86 [D loss: 0.6798910796642303, acc.: 56.98%] [G loss: 0.741412878036499]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 113/200, Batch 58/86 [D loss: 0.6755287647247314, acc.: 58.64%] [G loss: 0.7377131581306458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 59/86 [D loss: 0.6791956126689911, acc.: 57.18%] [G loss: 0.7380836009979248]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 113/200, Batch 60/86 [D loss: 0.6798476576805115, acc.: 57.62%] [G loss: 0.7456462383270264]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 61/86 [D loss: 0.6765032708644867, acc.: 58.45%] [G loss: 0.7277834415435791]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 113/200, Batch 62/86 [D loss: 0.6828992366790771, acc.: 55.81%] [G loss: 0.7433429956436157]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 63/86 [D loss: 0.6780539751052856, acc.: 57.76%] [G loss: 0.7343125343322754]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 113/200, Batch 64/86 [D loss: 0.6799761652946472, acc.: 56.35%] [G loss: 0.7441900968551636]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 65/86 [D loss: 0.6793354451656342, acc.: 56.30%] [G loss: 0.7325621843338013]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 66/86 [D loss: 0.680916428565979, acc.: 55.18%] [G loss: 0.7440248131752014]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 113/200, Batch 67/86 [D loss: 0.6777586936950684, acc.: 57.96%] [G loss: 0.7347764372825623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 68/86 [D loss: 0.6824787855148315, acc.: 57.23%] [G loss: 0.7446877360343933]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 69/86 [D loss: 0.6792677044868469, acc.: 57.47%] [G loss: 0.7315675616264343]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 70/86 [D loss: 0.6840353608131409, acc.: 52.78%] [G loss: 0.7401004433631897]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 71/86 [D loss: 0.6746907532215118, acc.: 59.33%] [G loss: 0.732737123966217]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 72/86 [D loss: 0.6870642900466919, acc.: 52.05%] [G loss: 0.748083770275116]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 73/86 [D loss: 0.6718897819519043, acc.: 61.77%] [G loss: 0.7319414615631104]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 113/200, Batch 74/86 [D loss: 0.6827395260334015, acc.: 54.49%] [G loss: 0.7346667647361755]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 113/200, Batch 75/86 [D loss: 0.6776567101478577, acc.: 57.47%] [G loss: 0.7400273084640503]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 76/86 [D loss: 0.6812559068202972, acc.: 57.57%] [G loss: 0.7411034107208252]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 77/86 [D loss: 0.6787449717521667, acc.: 57.62%] [G loss: 0.7377623915672302]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 113/200, Batch 78/86 [D loss: 0.6861887574195862, acc.: 54.00%] [G loss: 0.7368819117546082]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 79/86 [D loss: 0.6790375113487244, acc.: 55.42%] [G loss: 0.7441136837005615]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 80/86 [D loss: 0.6742814481258392, acc.: 59.18%] [G loss: 0.7390734553337097]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 81/86 [D loss: 0.6819469332695007, acc.: 55.18%] [G loss: 0.7434799671173096]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 82/86 [D loss: 0.675322949886322, acc.: 58.25%] [G loss: 0.7357715368270874]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 113/200, Batch 83/86 [D loss: 0.6811151802539825, acc.: 55.81%] [G loss: 0.7393754124641418]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 113/200, Batch 84/86 [D loss: 0.6787821054458618, acc.: 58.45%] [G loss: 0.7422019243240356]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 113/200, Batch 85/86 [D loss: 0.6811563968658447, acc.: 55.96%] [G loss: 0.7445948123931885]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 113/200, Batch 86/86 [D loss: 0.6744537055492401, acc.: 59.13%] [G loss: 0.7386655211448669]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 1/86 [D loss: 0.6829861700534821, acc.: 55.81%] [G loss: 0.7377657294273376]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 2/86 [D loss: 0.678147941827774, acc.: 56.79%] [G loss: 0.7388566732406616]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 114/200, Batch 3/86 [D loss: 0.6791236698627472, acc.: 56.49%] [G loss: 0.7359640598297119]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 4/86 [D loss: 0.6786616146564484, acc.: 57.76%] [G loss: 0.7410844564437866]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 5/86 [D loss: 0.6757131814956665, acc.: 57.32%] [G loss: 0.7419467568397522]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 114/200, Batch 6/86 [D loss: 0.6774026155471802, acc.: 57.67%] [G loss: 0.747626543045044]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 7/86 [D loss: 0.6757795214653015, acc.: 57.13%] [G loss: 0.7380281090736389]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 114/200, Batch 8/86 [D loss: 0.6783375442028046, acc.: 57.62%] [G loss: 0.7404432892799377]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 9/86 [D loss: 0.6778716742992401, acc.: 58.54%] [G loss: 0.7450536489486694]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 114/200, Batch 10/86 [D loss: 0.6765274703502655, acc.: 57.42%] [G loss: 0.7373001575469971]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 114/200, Batch 11/86 [D loss: 0.681573897600174, acc.: 55.76%] [G loss: 0.7443283796310425]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 12/86 [D loss: 0.6787583231925964, acc.: 58.40%] [G loss: 0.7417309284210205]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 114/200, Batch 13/86 [D loss: 0.6766446828842163, acc.: 58.40%] [G loss: 0.7401118874549866]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 114/200, Batch 14/86 [D loss: 0.6782662868499756, acc.: 57.37%] [G loss: 0.7438119053840637]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 15/86 [D loss: 0.6795958578586578, acc.: 56.59%] [G loss: 0.7426168918609619]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 16/86 [D loss: 0.6768996715545654, acc.: 58.11%] [G loss: 0.7402833700180054]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 114/200, Batch 17/86 [D loss: 0.6774002909660339, acc.: 57.57%] [G loss: 0.736242949962616]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 114/200, Batch 18/86 [D loss: 0.6753745675086975, acc.: 59.08%] [G loss: 0.7385234236717224]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 114/200, Batch 19/86 [D loss: 0.6778265237808228, acc.: 57.57%] [G loss: 0.7441359162330627]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 114/200, Batch 20/86 [D loss: 0.6766337156295776, acc.: 58.30%] [G loss: 0.7411002516746521]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 21/86 [D loss: 0.6779178380966187, acc.: 57.57%] [G loss: 0.7392258644104004]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 114/200, Batch 22/86 [D loss: 0.6793020665645599, acc.: 56.40%] [G loss: 0.7428045272827148]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 23/86 [D loss: 0.6793813109397888, acc.: 57.71%] [G loss: 0.7394887804985046]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 114/200, Batch 24/86 [D loss: 0.6799773871898651, acc.: 57.18%] [G loss: 0.7362235188484192]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 114/200, Batch 25/86 [D loss: 0.6775787770748138, acc.: 56.64%] [G loss: 0.7440741658210754]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 114/200, Batch 26/86 [D loss: 0.6793586015701294, acc.: 56.93%] [G loss: 0.7437126636505127]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 114/200, Batch 27/86 [D loss: 0.6792478263378143, acc.: 57.23%] [G loss: 0.7439390420913696]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 28/86 [D loss: 0.678863912820816, acc.: 56.35%] [G loss: 0.7371056079864502]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 29/86 [D loss: 0.6811968684196472, acc.: 55.47%] [G loss: 0.7419943809509277]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 30/86 [D loss: 0.6784009635448456, acc.: 57.57%] [G loss: 0.7427685260772705]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 31/86 [D loss: 0.6801394522190094, acc.: 56.30%] [G loss: 0.7443661689758301]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 32/86 [D loss: 0.6761921644210815, acc.: 57.52%] [G loss: 0.7453192472457886]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 33/86 [D loss: 0.6774056851863861, acc.: 58.59%] [G loss: 0.7424476742744446]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 34/86 [D loss: 0.679108053445816, acc.: 57.18%] [G loss: 0.7421836853027344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 35/86 [D loss: 0.6794494986534119, acc.: 55.03%] [G loss: 0.7408134937286377]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 114/200, Batch 36/86 [D loss: 0.6775864064693451, acc.: 59.28%] [G loss: 0.7388107776641846]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 37/86 [D loss: 0.6749430000782013, acc.: 59.23%] [G loss: 0.74156653881073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 38/86 [D loss: 0.6799120604991913, acc.: 56.49%] [G loss: 0.7399160265922546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 39/86 [D loss: 0.6773921251296997, acc.: 57.03%] [G loss: 0.7397745847702026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 40/86 [D loss: 0.6783237457275391, acc.: 58.45%] [G loss: 0.7433320879936218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 41/86 [D loss: 0.6779681742191315, acc.: 58.15%] [G loss: 0.7463613152503967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 42/86 [D loss: 0.6773221492767334, acc.: 58.06%] [G loss: 0.7403587698936462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 43/86 [D loss: 0.6791034936904907, acc.: 56.93%] [G loss: 0.7364702224731445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 44/86 [D loss: 0.6788913607597351, acc.: 58.35%] [G loss: 0.7445370554924011]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 45/86 [D loss: 0.6797818541526794, acc.: 55.96%] [G loss: 0.7386181354522705]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 46/86 [D loss: 0.6795268654823303, acc.: 56.49%] [G loss: 0.735858142375946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 47/86 [D loss: 0.6809239983558655, acc.: 56.69%] [G loss: 0.7472603917121887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 48/86 [D loss: 0.6772119700908661, acc.: 59.08%] [G loss: 0.736246645450592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 49/86 [D loss: 0.6849863231182098, acc.: 54.39%] [G loss: 0.7483341693878174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 50/86 [D loss: 0.6759608089923859, acc.: 59.96%] [G loss: 0.7378411293029785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 51/86 [D loss: 0.67902672290802, acc.: 56.20%] [G loss: 0.7397525310516357]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 114/200, Batch 52/86 [D loss: 0.6787616014480591, acc.: 57.71%] [G loss: 0.7420534491539001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 53/86 [D loss: 0.6806059777736664, acc.: 56.05%] [G loss: 0.7481964826583862]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 54/86 [D loss: 0.6776362955570221, acc.: 56.64%] [G loss: 0.7407721281051636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 55/86 [D loss: 0.6772941052913666, acc.: 57.71%] [G loss: 0.7428074479103088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 56/86 [D loss: 0.6786206960678101, acc.: 57.71%] [G loss: 0.7422668933868408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 57/86 [D loss: 0.6809351444244385, acc.: 55.62%] [G loss: 0.7518579959869385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 58/86 [D loss: 0.6783732175827026, acc.: 57.42%] [G loss: 0.7349523305892944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 59/86 [D loss: 0.6844625473022461, acc.: 54.15%] [G loss: 0.7392163276672363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 60/86 [D loss: 0.679304450750351, acc.: 56.25%] [G loss: 0.741537868976593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 61/86 [D loss: 0.679417759180069, acc.: 57.52%] [G loss: 0.7473012208938599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 62/86 [D loss: 0.6787101328372955, acc.: 57.86%] [G loss: 0.7403379678726196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 63/86 [D loss: 0.6834412813186646, acc.: 54.39%] [G loss: 0.7373325228691101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 64/86 [D loss: 0.6737124025821686, acc.: 59.08%] [G loss: 0.740308403968811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 65/86 [D loss: 0.6804853677749634, acc.: 57.08%] [G loss: 0.7411335706710815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 66/86 [D loss: 0.674677312374115, acc.: 58.69%] [G loss: 0.7406172752380371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 67/86 [D loss: 0.6836091876029968, acc.: 55.03%] [G loss: 0.7384463548660278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 68/86 [D loss: 0.6751536726951599, acc.: 57.52%] [G loss: 0.7469407320022583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 69/86 [D loss: 0.6814588904380798, acc.: 56.45%] [G loss: 0.738969087600708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 70/86 [D loss: 0.6765158176422119, acc.: 59.13%] [G loss: 0.7429043650627136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 71/86 [D loss: 0.6765808761119843, acc.: 57.47%] [G loss: 0.7361478805541992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 72/86 [D loss: 0.677905797958374, acc.: 58.59%] [G loss: 0.7476975917816162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 73/86 [D loss: 0.6784734129905701, acc.: 57.76%] [G loss: 0.7351852059364319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 74/86 [D loss: 0.6817648112773895, acc.: 57.08%] [G loss: 0.7439907789230347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 75/86 [D loss: 0.6820060610771179, acc.: 54.74%] [G loss: 0.7347149848937988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 76/86 [D loss: 0.6808218359947205, acc.: 55.76%] [G loss: 0.7408786416053772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 77/86 [D loss: 0.6732242703437805, acc.: 59.72%] [G loss: 0.7308566570281982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 78/86 [D loss: 0.6862300932407379, acc.: 54.64%] [G loss: 0.7422301769256592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 79/86 [D loss: 0.6749736964702606, acc.: 59.72%] [G loss: 0.7372145652770996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 80/86 [D loss: 0.6817567348480225, acc.: 54.05%] [G loss: 0.7434177994728088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 81/86 [D loss: 0.6783387660980225, acc.: 56.54%] [G loss: 0.7435061931610107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 82/86 [D loss: 0.6851950287818909, acc.: 54.98%] [G loss: 0.7405021786689758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 83/86 [D loss: 0.6797997653484344, acc.: 56.88%] [G loss: 0.7419207692146301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 84/86 [D loss: 0.6811888217926025, acc.: 55.96%] [G loss: 0.7335950136184692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 85/86 [D loss: 0.6787455379962921, acc.: 56.98%] [G loss: 0.7433598637580872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 114/200, Batch 86/86 [D loss: 0.6748197674751282, acc.: 58.59%] [G loss: 0.735394299030304]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 1/86 [D loss: 0.682573139667511, acc.: 56.64%] [G loss: 0.7439475059509277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 2/86 [D loss: 0.6773540675640106, acc.: 57.81%] [G loss: 0.7418320178985596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 3/86 [D loss: 0.6808093786239624, acc.: 56.25%] [G loss: 0.7389235496520996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 4/86 [D loss: 0.6783114373683929, acc.: 57.08%] [G loss: 0.7447980642318726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 5/86 [D loss: 0.6778081953525543, acc.: 56.74%] [G loss: 0.7383700609207153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 6/86 [D loss: 0.6830355226993561, acc.: 55.52%] [G loss: 0.7459492087364197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 7/86 [D loss: 0.6753734946250916, acc.: 59.08%] [G loss: 0.7403730154037476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 8/86 [D loss: 0.6803580820560455, acc.: 56.59%] [G loss: 0.7420589327812195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 9/86 [D loss: 0.6786528527736664, acc.: 57.03%] [G loss: 0.7425734400749207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 10/86 [D loss: 0.6785211861133575, acc.: 58.35%] [G loss: 0.7471235990524292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 11/86 [D loss: 0.6801470816135406, acc.: 57.32%] [G loss: 0.7317856550216675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 12/86 [D loss: 0.6778083443641663, acc.: 56.54%] [G loss: 0.7409574389457703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 13/86 [D loss: 0.6800743341445923, acc.: 55.62%] [G loss: 0.746690034866333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 14/86 [D loss: 0.6794739663600922, acc.: 56.45%] [G loss: 0.7371648550033569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 15/86 [D loss: 0.6809895038604736, acc.: 54.74%] [G loss: 0.7486302852630615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 16/86 [D loss: 0.6752398908138275, acc.: 58.59%] [G loss: 0.7409865856170654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 17/86 [D loss: 0.6813550591468811, acc.: 57.62%] [G loss: 0.7412125468254089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 18/86 [D loss: 0.6788415312767029, acc.: 56.64%] [G loss: 0.7407253980636597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 19/86 [D loss: 0.6780099868774414, acc.: 57.23%] [G loss: 0.7352017164230347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 20/86 [D loss: 0.6807265877723694, acc.: 57.13%] [G loss: 0.7433220744132996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 21/86 [D loss: 0.6772718131542206, acc.: 57.81%] [G loss: 0.7340815663337708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 22/86 [D loss: 0.6825101673603058, acc.: 54.88%] [G loss: 0.7483041882514954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 23/86 [D loss: 0.6739653646945953, acc.: 58.30%] [G loss: 0.7404162883758545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 24/86 [D loss: 0.6845661699771881, acc.: 54.39%] [G loss: 0.7515590786933899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 25/86 [D loss: 0.6753297448158264, acc.: 59.67%] [G loss: 0.7347441911697388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 26/86 [D loss: 0.684005469083786, acc.: 54.20%] [G loss: 0.7328060269355774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 27/86 [D loss: 0.6768288314342499, acc.: 58.15%] [G loss: 0.7407968640327454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 28/86 [D loss: 0.6767557561397552, acc.: 58.40%] [G loss: 0.7316132187843323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 29/86 [D loss: 0.6801891624927521, acc.: 56.79%] [G loss: 0.7516840100288391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 30/86 [D loss: 0.6811991930007935, acc.: 55.22%] [G loss: 0.7201746106147766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 31/86 [D loss: 0.6853985786437988, acc.: 54.25%] [G loss: 0.751257061958313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 32/86 [D loss: 0.673975020647049, acc.: 59.67%] [G loss: 0.7232788801193237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 33/86 [D loss: 0.6970852911472321, acc.: 49.61%] [G loss: 0.752413809299469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 34/86 [D loss: 0.6790680289268494, acc.: 55.57%] [G loss: 0.7302069067955017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 35/86 [D loss: 0.6880958080291748, acc.: 54.30%] [G loss: 0.7456793785095215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 36/86 [D loss: 0.6746340394020081, acc.: 58.50%] [G loss: 0.7294975519180298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 37/86 [D loss: 0.693670928478241, acc.: 51.27%] [G loss: 0.7466548085212708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 38/86 [D loss: 0.6740061044692993, acc.: 58.79%] [G loss: 0.7326297760009766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 39/86 [D loss: 0.6814693212509155, acc.: 55.08%] [G loss: 0.7402361631393433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 40/86 [D loss: 0.6776389181613922, acc.: 56.45%] [G loss: 0.7406395673751831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 41/86 [D loss: 0.6773218512535095, acc.: 57.52%] [G loss: 0.7343538999557495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 42/86 [D loss: 0.6789143979549408, acc.: 56.59%] [G loss: 0.7406471967697144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 43/86 [D loss: 0.6771185100078583, acc.: 57.23%] [G loss: 0.7369875311851501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 44/86 [D loss: 0.6796938478946686, acc.: 56.79%] [G loss: 0.7397671937942505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 45/86 [D loss: 0.6771723926067352, acc.: 58.01%] [G loss: 0.7408210635185242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 46/86 [D loss: 0.6741424202919006, acc.: 59.52%] [G loss: 0.7365369200706482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 47/86 [D loss: 0.6810184121131897, acc.: 57.32%] [G loss: 0.7429410815238953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 48/86 [D loss: 0.6765818893909454, acc.: 57.47%] [G loss: 0.7351040840148926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 49/86 [D loss: 0.6823210716247559, acc.: 55.42%] [G loss: 0.7419848442077637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 50/86 [D loss: 0.6789980232715607, acc.: 56.69%] [G loss: 0.7403863668441772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 51/86 [D loss: 0.6799506843090057, acc.: 56.01%] [G loss: 0.7457959651947021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 52/86 [D loss: 0.6784875988960266, acc.: 56.93%] [G loss: 0.7386288642883301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 53/86 [D loss: 0.6786483824253082, acc.: 57.71%] [G loss: 0.7460187673568726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 54/86 [D loss: 0.6767570674419403, acc.: 57.86%] [G loss: 0.7399226427078247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 55/86 [D loss: 0.6790079176425934, acc.: 58.06%] [G loss: 0.7371761202812195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 56/86 [D loss: 0.6808373332023621, acc.: 54.98%] [G loss: 0.7373048663139343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 57/86 [D loss: 0.6740694046020508, acc.: 58.54%] [G loss: 0.7420706748962402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 58/86 [D loss: 0.678433358669281, acc.: 56.35%] [G loss: 0.7374134659767151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 59/86 [D loss: 0.6751977801322937, acc.: 58.40%] [G loss: 0.744559109210968]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 60/86 [D loss: 0.6737380921840668, acc.: 58.30%] [G loss: 0.7383348941802979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 61/86 [D loss: 0.680186003446579, acc.: 57.18%] [G loss: 0.7355201840400696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 62/86 [D loss: 0.6772395968437195, acc.: 57.37%] [G loss: 0.744375467300415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 63/86 [D loss: 0.6812861263751984, acc.: 55.47%] [G loss: 0.7378450036048889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 64/86 [D loss: 0.6780007183551788, acc.: 59.67%] [G loss: 0.7404619455337524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 65/86 [D loss: 0.6797555387020111, acc.: 57.81%] [G loss: 0.7426861524581909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 66/86 [D loss: 0.6793117225170135, acc.: 56.93%] [G loss: 0.7411504983901978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 67/86 [D loss: 0.678894430398941, acc.: 57.47%] [G loss: 0.7453761696815491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 68/86 [D loss: 0.6795289516448975, acc.: 56.35%] [G loss: 0.7397675514221191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 69/86 [D loss: 0.67628213763237, acc.: 57.91%] [G loss: 0.7398800849914551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 70/86 [D loss: 0.6758226752281189, acc.: 59.03%] [G loss: 0.7395225167274475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 71/86 [D loss: 0.675025463104248, acc.: 58.69%] [G loss: 0.7410624623298645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 72/86 [D loss: 0.6778038144111633, acc.: 58.20%] [G loss: 0.736933708190918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 73/86 [D loss: 0.6759251654148102, acc.: 58.98%] [G loss: 0.7425506711006165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 74/86 [D loss: 0.6776885390281677, acc.: 57.18%] [G loss: 0.7416322827339172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 75/86 [D loss: 0.6821151673793793, acc.: 56.15%] [G loss: 0.7403159141540527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 76/86 [D loss: 0.6770021021366119, acc.: 58.25%] [G loss: 0.7425016164779663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 77/86 [D loss: 0.6726637780666351, acc.: 59.91%] [G loss: 0.7449948787689209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 78/86 [D loss: 0.6810859143733978, acc.: 56.15%] [G loss: 0.7443735003471375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 79/86 [D loss: 0.6764100193977356, acc.: 58.01%] [G loss: 0.7390521764755249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 80/86 [D loss: 0.6741263270378113, acc.: 59.52%] [G loss: 0.7505033016204834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 81/86 [D loss: 0.6781964004039764, acc.: 56.98%] [G loss: 0.7491395473480225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 82/86 [D loss: 0.6782263815402985, acc.: 57.32%] [G loss: 0.7456359267234802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 83/86 [D loss: 0.6775200963020325, acc.: 56.49%] [G loss: 0.7473587989807129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 84/86 [D loss: 0.6751481592655182, acc.: 58.54%] [G loss: 0.7425581216812134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 85/86 [D loss: 0.6767143309116364, acc.: 57.81%] [G loss: 0.7407265901565552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 115/200, Batch 86/86 [D loss: 0.6799195408821106, acc.: 55.52%] [G loss: 0.7446788549423218]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 1/86 [D loss: 0.6788666546344757, acc.: 56.64%] [G loss: 0.7371248006820679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 2/86 [D loss: 0.6789062917232513, acc.: 55.91%] [G loss: 0.7467035055160522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 3/86 [D loss: 0.6785393357276917, acc.: 57.81%] [G loss: 0.7421483993530273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 4/86 [D loss: 0.6785468459129333, acc.: 56.98%] [G loss: 0.7456655502319336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 5/86 [D loss: 0.6758244335651398, acc.: 59.03%] [G loss: 0.7405633330345154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 6/86 [D loss: 0.6761812269687653, acc.: 59.18%] [G loss: 0.7468361258506775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 7/86 [D loss: 0.677916556596756, acc.: 57.28%] [G loss: 0.7419099807739258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 8/86 [D loss: 0.6786235868930817, acc.: 56.79%] [G loss: 0.7408177256584167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 9/86 [D loss: 0.6800469160079956, acc.: 58.40%] [G loss: 0.7429913878440857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 10/86 [D loss: 0.6758202016353607, acc.: 57.57%] [G loss: 0.7398515343666077]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 11/86 [D loss: 0.677070289850235, acc.: 57.96%] [G loss: 0.7458586692810059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 12/86 [D loss: 0.6758136749267578, acc.: 58.40%] [G loss: 0.7442504167556763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 13/86 [D loss: 0.6769258081912994, acc.: 57.23%] [G loss: 0.7414750456809998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 14/86 [D loss: 0.6778774857521057, acc.: 57.81%] [G loss: 0.7422325611114502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 15/86 [D loss: 0.6727467775344849, acc.: 59.52%] [G loss: 0.7464633584022522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 16/86 [D loss: 0.6778295636177063, acc.: 57.81%] [G loss: 0.7386376857757568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 17/86 [D loss: 0.674569696187973, acc.: 59.38%] [G loss: 0.7393512725830078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 18/86 [D loss: 0.6761164665222168, acc.: 58.64%] [G loss: 0.7432911992073059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 19/86 [D loss: 0.677459716796875, acc.: 57.81%] [G loss: 0.7374041676521301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 20/86 [D loss: 0.6785565912723541, acc.: 55.66%] [G loss: 0.7389678359031677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 21/86 [D loss: 0.6791889369487762, acc.: 56.64%] [G loss: 0.7495344281196594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 22/86 [D loss: 0.6762984693050385, acc.: 58.59%] [G loss: 0.737252950668335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 23/86 [D loss: 0.6810977160930634, acc.: 56.35%] [G loss: 0.7457830905914307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 24/86 [D loss: 0.6770408153533936, acc.: 57.28%] [G loss: 0.744522750377655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 25/86 [D loss: 0.679151177406311, acc.: 56.74%] [G loss: 0.7428327798843384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 26/86 [D loss: 0.6716903746128082, acc.: 60.50%] [G loss: 0.7400069832801819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 27/86 [D loss: 0.6837982535362244, acc.: 55.22%] [G loss: 0.7421127557754517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 28/86 [D loss: 0.6745243966579437, acc.: 58.54%] [G loss: 0.7391954064369202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 29/86 [D loss: 0.6836072206497192, acc.: 54.83%] [G loss: 0.740001916885376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 30/86 [D loss: 0.6768501102924347, acc.: 56.98%] [G loss: 0.7406944036483765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 31/86 [D loss: 0.6794834136962891, acc.: 56.93%] [G loss: 0.7347888350486755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 32/86 [D loss: 0.6843299269676208, acc.: 54.64%] [G loss: 0.749450147151947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 33/86 [D loss: 0.6782350540161133, acc.: 56.49%] [G loss: 0.73786860704422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 34/86 [D loss: 0.6784839332103729, acc.: 57.03%] [G loss: 0.743156909942627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 35/86 [D loss: 0.6777690052986145, acc.: 57.91%] [G loss: 0.7444676160812378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 36/86 [D loss: 0.6802003383636475, acc.: 56.64%] [G loss: 0.7430556416511536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 37/86 [D loss: 0.6777889430522919, acc.: 57.28%] [G loss: 0.7443701028823853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 38/86 [D loss: 0.6773450374603271, acc.: 55.62%] [G loss: 0.7359349131584167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 39/86 [D loss: 0.6792301833629608, acc.: 57.28%] [G loss: 0.7447046637535095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 40/86 [D loss: 0.6771509051322937, acc.: 57.96%] [G loss: 0.7395848035812378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 41/86 [D loss: 0.6865783333778381, acc.: 52.64%] [G loss: 0.7414342761039734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 42/86 [D loss: 0.6730340123176575, acc.: 59.28%] [G loss: 0.7424225807189941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 43/86 [D loss: 0.6784849762916565, acc.: 56.74%] [G loss: 0.7410738468170166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 44/86 [D loss: 0.6752303540706635, acc.: 57.47%] [G loss: 0.7417064309120178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 45/86 [D loss: 0.6741484999656677, acc.: 58.98%] [G loss: 0.7444343566894531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 46/86 [D loss: 0.6808182895183563, acc.: 56.93%] [G loss: 0.7468554973602295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 47/86 [D loss: 0.6783995926380157, acc.: 56.84%] [G loss: 0.737454354763031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 48/86 [D loss: 0.6782736480236053, acc.: 57.32%] [G loss: 0.7517856359481812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 49/86 [D loss: 0.6741952300071716, acc.: 59.33%] [G loss: 0.7382798790931702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 50/86 [D loss: 0.6813692450523376, acc.: 56.69%] [G loss: 0.750022292137146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 51/86 [D loss: 0.678710013628006, acc.: 57.37%] [G loss: 0.7423961162567139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 52/86 [D loss: 0.6819595098495483, acc.: 54.64%] [G loss: 0.7405903935432434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 53/86 [D loss: 0.6775498986244202, acc.: 57.47%] [G loss: 0.7445002198219299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 54/86 [D loss: 0.6817384958267212, acc.: 55.47%] [G loss: 0.7320303320884705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 55/86 [D loss: 0.6805950701236725, acc.: 55.76%] [G loss: 0.7527855634689331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 56/86 [D loss: 0.6759048700332642, acc.: 57.71%] [G loss: 0.7370469570159912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 57/86 [D loss: 0.6820079386234283, acc.: 55.52%] [G loss: 0.7484729886054993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 58/86 [D loss: 0.6750010550022125, acc.: 59.42%] [G loss: 0.7434077858924866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 59/86 [D loss: 0.6861573755741119, acc.: 54.88%] [G loss: 0.7388526201248169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 60/86 [D loss: 0.6748030185699463, acc.: 60.25%] [G loss: 0.7474688291549683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 61/86 [D loss: 0.6814471483230591, acc.: 54.35%] [G loss: 0.7400618195533752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 62/86 [D loss: 0.6797351241111755, acc.: 56.45%] [G loss: 0.7469527721405029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 63/86 [D loss: 0.6773601770401001, acc.: 57.57%] [G loss: 0.734027624130249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 64/86 [D loss: 0.6823077499866486, acc.: 56.30%] [G loss: 0.7512813806533813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 65/86 [D loss: 0.6765599250793457, acc.: 57.57%] [G loss: 0.7392767667770386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 66/86 [D loss: 0.6753947734832764, acc.: 57.32%] [G loss: 0.735040545463562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 67/86 [D loss: 0.6751567125320435, acc.: 58.25%] [G loss: 0.7495835423469543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 68/86 [D loss: 0.6762088239192963, acc.: 58.94%] [G loss: 0.7433616518974304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 69/86 [D loss: 0.6811671257019043, acc.: 56.15%] [G loss: 0.7462852597236633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 70/86 [D loss: 0.6779479086399078, acc.: 57.37%] [G loss: 0.7444531321525574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 71/86 [D loss: 0.6781912744045258, acc.: 56.05%] [G loss: 0.7420915365219116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 72/86 [D loss: 0.6785005629062653, acc.: 57.76%] [G loss: 0.7467764616012573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 73/86 [D loss: 0.6777672469615936, acc.: 57.52%] [G loss: 0.74672532081604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 74/86 [D loss: 0.6728324592113495, acc.: 60.11%] [G loss: 0.7467722296714783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 75/86 [D loss: 0.6782241761684418, acc.: 58.40%] [G loss: 0.7455637454986572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 76/86 [D loss: 0.6809056997299194, acc.: 55.76%] [G loss: 0.7444572448730469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 77/86 [D loss: 0.676713228225708, acc.: 58.64%] [G loss: 0.7400548458099365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 78/86 [D loss: 0.6789669990539551, acc.: 57.86%] [G loss: 0.7449790835380554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 79/86 [D loss: 0.6794399917125702, acc.: 57.13%] [G loss: 0.743010938167572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 80/86 [D loss: 0.6759532690048218, acc.: 59.38%] [G loss: 0.7443110942840576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 81/86 [D loss: 0.67741858959198, acc.: 56.45%] [G loss: 0.7413791418075562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 82/86 [D loss: 0.678104043006897, acc.: 57.67%] [G loss: 0.7455712556838989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 83/86 [D loss: 0.6770208179950714, acc.: 57.71%] [G loss: 0.7418738603591919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 84/86 [D loss: 0.6755896210670471, acc.: 57.28%] [G loss: 0.7436690926551819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 85/86 [D loss: 0.6784672439098358, acc.: 56.93%] [G loss: 0.7437539100646973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 116/200, Batch 86/86 [D loss: 0.6744836270809174, acc.: 58.15%] [G loss: 0.7406432032585144]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 1/86 [D loss: 0.6787222325801849, acc.: 57.42%] [G loss: 0.7394303679466248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 2/86 [D loss: 0.6800369918346405, acc.: 57.28%] [G loss: 0.7457353472709656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 3/86 [D loss: 0.6740762591362, acc.: 59.42%] [G loss: 0.7459099292755127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 4/86 [D loss: 0.6810939610004425, acc.: 55.81%] [G loss: 0.7448601722717285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 5/86 [D loss: 0.6791742444038391, acc.: 57.08%] [G loss: 0.73961341381073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 6/86 [D loss: 0.6770991683006287, acc.: 57.91%] [G loss: 0.741679310798645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 7/86 [D loss: 0.6756104230880737, acc.: 57.18%] [G loss: 0.7465428113937378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 8/86 [D loss: 0.6778481900691986, acc.: 57.13%] [G loss: 0.7404673099517822]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 9/86 [D loss: 0.6784659028053284, acc.: 57.42%] [G loss: 0.74452143907547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 10/86 [D loss: 0.67963045835495, acc.: 56.10%] [G loss: 0.7418624758720398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 11/86 [D loss: 0.677588552236557, acc.: 58.15%] [G loss: 0.7424184083938599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 12/86 [D loss: 0.679313600063324, acc.: 56.25%] [G loss: 0.7414491772651672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 13/86 [D loss: 0.6779546439647675, acc.: 57.13%] [G loss: 0.7426187992095947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 14/86 [D loss: 0.6753591597080231, acc.: 58.25%] [G loss: 0.73985356092453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 15/86 [D loss: 0.6781049370765686, acc.: 57.86%] [G loss: 0.7403746247291565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 16/86 [D loss: 0.6749010682106018, acc.: 58.54%] [G loss: 0.7426186800003052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 17/86 [D loss: 0.6802600920200348, acc.: 56.74%] [G loss: 0.7407172918319702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 18/86 [D loss: 0.6776432394981384, acc.: 56.79%] [G loss: 0.7473424673080444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 19/86 [D loss: 0.6776354908943176, acc.: 57.96%] [G loss: 0.747901439666748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 20/86 [D loss: 0.6779953241348267, acc.: 59.13%] [G loss: 0.748477041721344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 21/86 [D loss: 0.6789657473564148, acc.: 56.98%] [G loss: 0.7335168123245239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 22/86 [D loss: 0.6809784471988678, acc.: 55.18%] [G loss: 0.7414962649345398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 23/86 [D loss: 0.6762543022632599, acc.: 58.01%] [G loss: 0.7465128898620605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 24/86 [D loss: 0.6809324026107788, acc.: 56.54%] [G loss: 0.7445149421691895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 25/86 [D loss: 0.6734222769737244, acc.: 59.38%] [G loss: 0.741853654384613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 26/86 [D loss: 0.6787043511867523, acc.: 55.76%] [G loss: 0.7388275861740112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 27/86 [D loss: 0.6748646199703217, acc.: 58.79%] [G loss: 0.7428679466247559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 28/86 [D loss: 0.6753323078155518, acc.: 59.42%] [G loss: 0.7385158538818359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 29/86 [D loss: 0.6817175447940826, acc.: 56.54%] [G loss: 0.7496061325073242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 30/86 [D loss: 0.6763635277748108, acc.: 58.11%] [G loss: 0.742481529712677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 31/86 [D loss: 0.6839627921581268, acc.: 55.22%] [G loss: 0.7436001896858215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 32/86 [D loss: 0.6723507046699524, acc.: 59.72%] [G loss: 0.7498225569725037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 33/86 [D loss: 0.6814558506011963, acc.: 54.93%] [G loss: 0.7379329800605774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 34/86 [D loss: 0.6734160482883453, acc.: 59.81%] [G loss: 0.7510129809379578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 35/86 [D loss: 0.6809513866901398, acc.: 56.05%] [G loss: 0.7323704957962036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 36/86 [D loss: 0.6811182498931885, acc.: 56.30%] [G loss: 0.748790979385376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 37/86 [D loss: 0.6760782897472382, acc.: 58.40%] [G loss: 0.7341305017471313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 38/86 [D loss: 0.6790436506271362, acc.: 58.25%] [G loss: 0.7424030303955078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 39/86 [D loss: 0.6798916757106781, acc.: 57.62%] [G loss: 0.7409676313400269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 40/86 [D loss: 0.6791802942752838, acc.: 55.37%] [G loss: 0.7447283267974854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 41/86 [D loss: 0.6784893572330475, acc.: 57.86%] [G loss: 0.7486029863357544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 42/86 [D loss: 0.6755212545394897, acc.: 57.23%] [G loss: 0.7321771383285522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 43/86 [D loss: 0.6824582517147064, acc.: 55.27%] [G loss: 0.7482882738113403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 44/86 [D loss: 0.6757906973361969, acc.: 57.47%] [G loss: 0.734991192817688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 45/86 [D loss: 0.6798000037670135, acc.: 56.20%] [G loss: 0.7393835186958313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 46/86 [D loss: 0.6743534803390503, acc.: 59.18%] [G loss: 0.7422664165496826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 47/86 [D loss: 0.6845667362213135, acc.: 54.39%] [G loss: 0.742000937461853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 48/86 [D loss: 0.6731032729148865, acc.: 60.60%] [G loss: 0.7402018308639526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 49/86 [D loss: 0.6816721558570862, acc.: 55.03%] [G loss: 0.7329606413841248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 50/86 [D loss: 0.6814198791980743, acc.: 55.18%] [G loss: 0.7497478127479553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 51/86 [D loss: 0.6754870712757111, acc.: 58.59%] [G loss: 0.7386770248413086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 52/86 [D loss: 0.6801432073116302, acc.: 55.96%] [G loss: 0.7411422729492188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 53/86 [D loss: 0.6747356951236725, acc.: 59.28%] [G loss: 0.7444342970848083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 54/86 [D loss: 0.6825476586818695, acc.: 55.66%] [G loss: 0.7372331619262695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 55/86 [D loss: 0.6747333705425262, acc.: 58.40%] [G loss: 0.7450742721557617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 56/86 [D loss: 0.6789992153644562, acc.: 58.15%] [G loss: 0.7375065684318542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 57/86 [D loss: 0.6876023709774017, acc.: 52.34%] [G loss: 0.7457798719406128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 58/86 [D loss: 0.6737533211708069, acc.: 59.77%] [G loss: 0.7401067614555359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 59/86 [D loss: 0.6784338057041168, acc.: 56.59%] [G loss: 0.7459739446640015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 60/86 [D loss: 0.6738696396350861, acc.: 59.38%] [G loss: 0.7471073865890503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 61/86 [D loss: 0.6814224720001221, acc.: 56.49%] [G loss: 0.7353630065917969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 62/86 [D loss: 0.6763971149921417, acc.: 58.30%] [G loss: 0.7446190118789673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 63/86 [D loss: 0.6766158640384674, acc.: 58.25%] [G loss: 0.7398324608802795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 64/86 [D loss: 0.6797116994857788, acc.: 55.96%] [G loss: 0.7431179285049438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 65/86 [D loss: 0.6734854876995087, acc.: 59.42%] [G loss: 0.7451415657997131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 66/86 [D loss: 0.6807266175746918, acc.: 56.59%] [G loss: 0.7403264045715332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 67/86 [D loss: 0.6756710112094879, acc.: 58.15%] [G loss: 0.7413492798805237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 68/86 [D loss: 0.6789169609546661, acc.: 56.01%] [G loss: 0.7402471303939819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 69/86 [D loss: 0.6754636168479919, acc.: 58.45%] [G loss: 0.745635986328125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 70/86 [D loss: 0.6759940087795258, acc.: 59.18%] [G loss: 0.7450229525566101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 71/86 [D loss: 0.6784402430057526, acc.: 56.54%] [G loss: 0.7398733496665955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 72/86 [D loss: 0.6777879297733307, acc.: 56.20%] [G loss: 0.7434332370758057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 73/86 [D loss: 0.6770305037498474, acc.: 57.52%] [G loss: 0.7399362325668335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 74/86 [D loss: 0.6740046739578247, acc.: 59.86%] [G loss: 0.7440893054008484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 75/86 [D loss: 0.6778024733066559, acc.: 58.30%] [G loss: 0.7413744926452637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 76/86 [D loss: 0.6788437366485596, acc.: 56.84%] [G loss: 0.7488152980804443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 77/86 [D loss: 0.6775460243225098, acc.: 57.62%] [G loss: 0.7397221922874451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 78/86 [D loss: 0.6776595413684845, acc.: 57.37%] [G loss: 0.7435107231140137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 79/86 [D loss: 0.6774040162563324, acc.: 56.74%] [G loss: 0.742725133895874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 80/86 [D loss: 0.6804355382919312, acc.: 56.69%] [G loss: 0.743732213973999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 81/86 [D loss: 0.6745172441005707, acc.: 59.08%] [G loss: 0.7461349964141846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 82/86 [D loss: 0.6759952306747437, acc.: 57.28%] [G loss: 0.7423841953277588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 83/86 [D loss: 0.6806061863899231, acc.: 54.93%] [G loss: 0.7457184195518494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 84/86 [D loss: 0.6779261529445648, acc.: 56.49%] [G loss: 0.7446247935295105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 85/86 [D loss: 0.6817701756954193, acc.: 55.96%] [G loss: 0.7478632926940918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 117/200, Batch 86/86 [D loss: 0.6746532022953033, acc.: 59.03%] [G loss: 0.7465471029281616]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 1/86 [D loss: 0.6749371886253357, acc.: 58.11%] [G loss: 0.7405388355255127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 2/86 [D loss: 0.6765151023864746, acc.: 58.15%] [G loss: 0.746558666229248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 3/86 [D loss: 0.6748938262462616, acc.: 57.08%] [G loss: 0.7459514141082764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 4/86 [D loss: 0.6776423752307892, acc.: 57.47%] [G loss: 0.7456096410751343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 5/86 [D loss: 0.6774955987930298, acc.: 57.37%] [G loss: 0.742733359336853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 6/86 [D loss: 0.6750451922416687, acc.: 59.62%] [G loss: 0.7353394627571106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 7/86 [D loss: 0.6774083077907562, acc.: 57.67%] [G loss: 0.7443265318870544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 8/86 [D loss: 0.6779248416423798, acc.: 56.98%] [G loss: 0.7374482154846191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 9/86 [D loss: 0.682530015707016, acc.: 55.71%] [G loss: 0.7440467476844788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 10/86 [D loss: 0.6742126941680908, acc.: 58.59%] [G loss: 0.7485321164131165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 11/86 [D loss: 0.6795093417167664, acc.: 57.96%] [G loss: 0.7474368214607239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 12/86 [D loss: 0.6796773076057434, acc.: 56.20%] [G loss: 0.749679446220398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 13/86 [D loss: 0.6766667068004608, acc.: 58.50%] [G loss: 0.7387060523033142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 14/86 [D loss: 0.6771286129951477, acc.: 57.42%] [G loss: 0.7422603368759155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 15/86 [D loss: 0.6798548102378845, acc.: 57.42%] [G loss: 0.7418110966682434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 16/86 [D loss: 0.677564263343811, acc.: 56.84%] [G loss: 0.7414860129356384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 17/86 [D loss: 0.678438663482666, acc.: 56.54%] [G loss: 0.7428539395332336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 18/86 [D loss: 0.6786918044090271, acc.: 57.62%] [G loss: 0.7411709427833557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 19/86 [D loss: 0.6753527522087097, acc.: 58.94%] [G loss: 0.7406378984451294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 20/86 [D loss: 0.6778696179389954, acc.: 56.20%] [G loss: 0.7343793511390686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 21/86 [D loss: 0.677284836769104, acc.: 57.62%] [G loss: 0.7488187551498413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 22/86 [D loss: 0.67771115899086, acc.: 56.69%] [G loss: 0.737017035484314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 23/86 [D loss: 0.6766790151596069, acc.: 57.76%] [G loss: 0.7434606552124023]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 118/200, Batch 24/86 [D loss: 0.6781464517116547, acc.: 56.93%] [G loss: 0.7480185031890869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 25/86 [D loss: 0.6760980784893036, acc.: 58.06%] [G loss: 0.7431210279464722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 26/86 [D loss: 0.6786705255508423, acc.: 56.05%] [G loss: 0.7408853769302368]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 27/86 [D loss: 0.6770290732383728, acc.: 57.76%] [G loss: 0.7372990846633911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 28/86 [D loss: 0.68008953332901, acc.: 56.01%] [G loss: 0.7398040294647217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 29/86 [D loss: 0.6765457987785339, acc.: 59.23%] [G loss: 0.7430155277252197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 30/86 [D loss: 0.6734947860240936, acc.: 59.28%] [G loss: 0.7427960634231567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 31/86 [D loss: 0.6770029962062836, acc.: 57.96%] [G loss: 0.7396284937858582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 32/86 [D loss: 0.6751850843429565, acc.: 59.42%] [G loss: 0.7466903328895569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 33/86 [D loss: 0.6774282455444336, acc.: 57.67%] [G loss: 0.7415657043457031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 34/86 [D loss: 0.670356035232544, acc.: 59.81%] [G loss: 0.7357991933822632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 35/86 [D loss: 0.6733938753604889, acc.: 59.62%] [G loss: 0.7361936569213867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 36/86 [D loss: 0.6778527498245239, acc.: 57.76%] [G loss: 0.7453429102897644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 37/86 [D loss: 0.6753571331501007, acc.: 58.01%] [G loss: 0.7436053156852722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 38/86 [D loss: 0.6738213300704956, acc.: 59.67%] [G loss: 0.7471689581871033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 39/86 [D loss: 0.675834596157074, acc.: 58.30%] [G loss: 0.7377638816833496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 40/86 [D loss: 0.6810866296291351, acc.: 56.20%] [G loss: 0.7467104196548462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 41/86 [D loss: 0.675993025302887, acc.: 58.94%] [G loss: 0.74085533618927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 42/86 [D loss: 0.6764689385890961, acc.: 58.01%] [G loss: 0.7444511651992798]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 43/86 [D loss: 0.6733109652996063, acc.: 59.67%] [G loss: 0.7424505352973938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 44/86 [D loss: 0.6729589700698853, acc.: 58.84%] [G loss: 0.7436542510986328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 45/86 [D loss: 0.6776081025600433, acc.: 56.20%] [G loss: 0.7436846494674683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 46/86 [D loss: 0.6785947978496552, acc.: 58.01%] [G loss: 0.741438627243042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 47/86 [D loss: 0.6810089349746704, acc.: 55.27%] [G loss: 0.7404870986938477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 48/86 [D loss: 0.6785238981246948, acc.: 56.49%] [G loss: 0.7379024028778076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 49/86 [D loss: 0.6765145063400269, acc.: 57.67%] [G loss: 0.7414981126785278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 50/86 [D loss: 0.6779886186122894, acc.: 57.03%] [G loss: 0.7398120164871216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 51/86 [D loss: 0.6773093640804291, acc.: 57.08%] [G loss: 0.7426829934120178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 52/86 [D loss: 0.6770175397396088, acc.: 56.64%] [G loss: 0.747808575630188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 53/86 [D loss: 0.6711046993732452, acc.: 61.04%] [G loss: 0.7378048896789551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 54/86 [D loss: 0.6788785457611084, acc.: 57.08%] [G loss: 0.7432483434677124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 55/86 [D loss: 0.6755110025405884, acc.: 58.98%] [G loss: 0.7376492619514465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 56/86 [D loss: 0.6858582496643066, acc.: 54.59%] [G loss: 0.7502652406692505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 57/86 [D loss: 0.6726598143577576, acc.: 60.11%] [G loss: 0.7390235066413879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 58/86 [D loss: 0.6830380856990814, acc.: 53.56%] [G loss: 0.7423574328422546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 59/86 [D loss: 0.6784827411174774, acc.: 57.71%] [G loss: 0.7471509575843811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 60/86 [D loss: 0.6766227185726166, acc.: 58.45%] [G loss: 0.734551727771759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 61/86 [D loss: 0.6812537014484406, acc.: 57.28%] [G loss: 0.7500868439674377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 62/86 [D loss: 0.6731283068656921, acc.: 58.84%] [G loss: 0.732903003692627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 63/86 [D loss: 0.6867322325706482, acc.: 52.64%] [G loss: 0.7500565052032471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 64/86 [D loss: 0.6760624647140503, acc.: 58.40%] [G loss: 0.7356641292572021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 65/86 [D loss: 0.6854275166988373, acc.: 53.76%] [G loss: 0.745093822479248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 66/86 [D loss: 0.6786230206489563, acc.: 57.96%] [G loss: 0.7370229363441467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 67/86 [D loss: 0.6800245940685272, acc.: 56.84%] [G loss: 0.7340161800384521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 68/86 [D loss: 0.680995911359787, acc.: 55.18%] [G loss: 0.7535886764526367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 69/86 [D loss: 0.6747736036777496, acc.: 60.11%] [G loss: 0.7407991290092468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 70/86 [D loss: 0.6856066584587097, acc.: 53.61%] [G loss: 0.747369647026062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 71/86 [D loss: 0.6750132739543915, acc.: 58.79%] [G loss: 0.7418221235275269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 72/86 [D loss: 0.6833289563655853, acc.: 54.05%] [G loss: 0.7435880899429321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 73/86 [D loss: 0.6752845942974091, acc.: 58.40%] [G loss: 0.7454015016555786]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 74/86 [D loss: 0.6744189858436584, acc.: 58.59%] [G loss: 0.7410594820976257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 75/86 [D loss: 0.6789655983448029, acc.: 57.37%] [G loss: 0.7483619451522827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 76/86 [D loss: 0.6718189418315887, acc.: 60.40%] [G loss: 0.7399641871452332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 77/86 [D loss: 0.6783448755741119, acc.: 56.25%] [G loss: 0.7434883117675781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 78/86 [D loss: 0.6797769069671631, acc.: 56.40%] [G loss: 0.7401641011238098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 79/86 [D loss: 0.6792458295822144, acc.: 57.03%] [G loss: 0.7401106357574463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 80/86 [D loss: 0.674883633852005, acc.: 58.59%] [G loss: 0.7411660552024841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 81/86 [D loss: 0.6791666150093079, acc.: 56.30%] [G loss: 0.7393093705177307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 82/86 [D loss: 0.6802961528301239, acc.: 56.64%] [G loss: 0.7465682625770569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 83/86 [D loss: 0.6758565604686737, acc.: 57.47%] [G loss: 0.7450866103172302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 84/86 [D loss: 0.6788955926895142, acc.: 56.93%] [G loss: 0.7443544268608093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 85/86 [D loss: 0.6749260127544403, acc.: 58.25%] [G loss: 0.7450115084648132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 118/200, Batch 86/86 [D loss: 0.6766327321529388, acc.: 58.11%] [G loss: 0.738692045211792]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 1/86 [D loss: 0.6775036752223969, acc.: 57.57%] [G loss: 0.7458820939064026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 2/86 [D loss: 0.6738186776638031, acc.: 59.42%] [G loss: 0.7417277097702026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 3/86 [D loss: 0.6744748950004578, acc.: 59.72%] [G loss: 0.7445709705352783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 4/86 [D loss: 0.6778119504451752, acc.: 57.32%] [G loss: 0.7438305616378784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 5/86 [D loss: 0.675687313079834, acc.: 58.74%] [G loss: 0.7397383451461792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 6/86 [D loss: 0.6749871373176575, acc.: 57.91%] [G loss: 0.7458343505859375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 7/86 [D loss: 0.6762491464614868, acc.: 58.74%] [G loss: 0.7483522295951843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 8/86 [D loss: 0.6776122152805328, acc.: 57.32%] [G loss: 0.7469808459281921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 9/86 [D loss: 0.6796346306800842, acc.: 57.23%] [G loss: 0.7501218914985657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 10/86 [D loss: 0.6754341125488281, acc.: 58.40%] [G loss: 0.7424622774124146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 11/86 [D loss: 0.672490805387497, acc.: 60.30%] [G loss: 0.741904616355896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 12/86 [D loss: 0.670351654291153, acc.: 59.86%] [G loss: 0.7409271597862244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 13/86 [D loss: 0.6762546896934509, acc.: 57.42%] [G loss: 0.7457689642906189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 14/86 [D loss: 0.6776114404201508, acc.: 57.08%] [G loss: 0.7467553019523621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 15/86 [D loss: 0.6770927608013153, acc.: 58.20%] [G loss: 0.7445532083511353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 16/86 [D loss: 0.6780678033828735, acc.: 57.18%] [G loss: 0.7432798147201538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 17/86 [D loss: 0.6812424659729004, acc.: 56.69%] [G loss: 0.7451725602149963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 18/86 [D loss: 0.6779939234256744, acc.: 56.88%] [G loss: 0.7458067536354065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 19/86 [D loss: 0.6808223724365234, acc.: 56.69%] [G loss: 0.750525176525116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 20/86 [D loss: 0.6755311787128448, acc.: 58.20%] [G loss: 0.7402597665786743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 21/86 [D loss: 0.6794625520706177, acc.: 57.03%] [G loss: 0.7392981648445129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 22/86 [D loss: 0.6780566573143005, acc.: 58.84%] [G loss: 0.7441366314888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 23/86 [D loss: 0.6750069856643677, acc.: 57.18%] [G loss: 0.749244749546051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 24/86 [D loss: 0.6780764758586884, acc.: 56.98%] [G loss: 0.7408823370933533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 25/86 [D loss: 0.676317423582077, acc.: 57.86%] [G loss: 0.7457826137542725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 26/86 [D loss: 0.6828382015228271, acc.: 56.20%] [G loss: 0.7444223761558533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 27/86 [D loss: 0.6759399473667145, acc.: 57.96%] [G loss: 0.7433946132659912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 28/86 [D loss: 0.676412433385849, acc.: 56.74%] [G loss: 0.7457209229469299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 29/86 [D loss: 0.6788592040538788, acc.: 57.37%] [G loss: 0.7434617877006531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 30/86 [D loss: 0.6778032183647156, acc.: 57.18%] [G loss: 0.7458509206771851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 31/86 [D loss: 0.677889496088028, acc.: 57.52%] [G loss: 0.7429882287979126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 32/86 [D loss: 0.6765320301055908, acc.: 57.03%] [G loss: 0.7457817792892456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 33/86 [D loss: 0.6732000410556793, acc.: 58.89%] [G loss: 0.7500048875808716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 34/86 [D loss: 0.6746041774749756, acc.: 58.40%] [G loss: 0.745195746421814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 35/86 [D loss: 0.6763371825218201, acc.: 58.74%] [G loss: 0.7397361397743225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 36/86 [D loss: 0.6729560494422913, acc.: 59.38%] [G loss: 0.7430537343025208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 37/86 [D loss: 0.6763892769813538, acc.: 56.88%] [G loss: 0.7432268261909485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 38/86 [D loss: 0.6775891184806824, acc.: 56.54%] [G loss: 0.7457414865493774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 39/86 [D loss: 0.6737172305583954, acc.: 57.71%] [G loss: 0.742726743221283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 40/86 [D loss: 0.6760684251785278, acc.: 58.30%] [G loss: 0.7420638799667358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 41/86 [D loss: 0.676263839006424, acc.: 57.47%] [G loss: 0.7501025795936584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 42/86 [D loss: 0.6796861886978149, acc.: 56.49%] [G loss: 0.7481252551078796]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 43/86 [D loss: 0.6750709116458893, acc.: 57.37%] [G loss: 0.7435352206230164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 44/86 [D loss: 0.6775704920291901, acc.: 58.50%] [G loss: 0.7425667643547058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 45/86 [D loss: 0.674942284822464, acc.: 57.28%] [G loss: 0.7443365454673767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 46/86 [D loss: 0.6764812469482422, acc.: 58.15%] [G loss: 0.7482573390007019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 47/86 [D loss: 0.6755846738815308, acc.: 59.03%] [G loss: 0.7436689138412476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 48/86 [D loss: 0.6761146187782288, acc.: 58.59%] [G loss: 0.7467454671859741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 49/86 [D loss: 0.6787146925926208, acc.: 56.93%] [G loss: 0.7522063255310059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 50/86 [D loss: 0.6756171584129333, acc.: 59.72%] [G loss: 0.7420129776000977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 51/86 [D loss: 0.6771050691604614, acc.: 57.52%] [G loss: 0.7429184317588806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 52/86 [D loss: 0.6790731549263, acc.: 56.98%] [G loss: 0.7419195175170898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 53/86 [D loss: 0.6785961091518402, acc.: 57.28%] [G loss: 0.7438946962356567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 54/86 [D loss: 0.6735675036907196, acc.: 58.69%] [G loss: 0.7443758845329285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 55/86 [D loss: 0.6755528450012207, acc.: 59.33%] [G loss: 0.750696063041687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 56/86 [D loss: 0.6756769716739655, acc.: 58.11%] [G loss: 0.741948127746582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 57/86 [D loss: 0.6799441277980804, acc.: 56.54%] [G loss: 0.7488932609558105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 58/86 [D loss: 0.676531195640564, acc.: 57.23%] [G loss: 0.7476352453231812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 59/86 [D loss: 0.6736786365509033, acc.: 57.96%] [G loss: 0.7427132725715637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 60/86 [D loss: 0.6777755618095398, acc.: 55.86%] [G loss: 0.740653932094574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 61/86 [D loss: 0.6784623861312866, acc.: 56.40%] [G loss: 0.7430922389030457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 62/86 [D loss: 0.6764185428619385, acc.: 57.86%] [G loss: 0.7484816908836365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 63/86 [D loss: 0.6772984862327576, acc.: 57.57%] [G loss: 0.7370423674583435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 64/86 [D loss: 0.6753170490264893, acc.: 58.40%] [G loss: 0.7456478476524353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 65/86 [D loss: 0.6779187619686127, acc.: 56.79%] [G loss: 0.7355927228927612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 66/86 [D loss: 0.6794570982456207, acc.: 56.05%] [G loss: 0.7459642291069031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 67/86 [D loss: 0.6762435734272003, acc.: 56.64%] [G loss: 0.7472121715545654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 68/86 [D loss: 0.6732943654060364, acc.: 59.18%] [G loss: 0.7470413446426392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 69/86 [D loss: 0.6752556264400482, acc.: 58.59%] [G loss: 0.7517023086547852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 70/86 [D loss: 0.6776120960712433, acc.: 57.62%] [G loss: 0.7471966743469238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 71/86 [D loss: 0.6764047741889954, acc.: 58.74%] [G loss: 0.7505311369895935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 72/86 [D loss: 0.6777815520763397, acc.: 58.20%] [G loss: 0.7511798143386841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 73/86 [D loss: 0.6744360327720642, acc.: 59.57%] [G loss: 0.744267463684082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 74/86 [D loss: 0.6761461794376373, acc.: 58.01%] [G loss: 0.7478134036064148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 75/86 [D loss: 0.6751125156879425, acc.: 57.86%] [G loss: 0.7503261566162109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 76/86 [D loss: 0.6742875277996063, acc.: 58.11%] [G loss: 0.7516816854476929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 77/86 [D loss: 0.6767611801624298, acc.: 56.98%] [G loss: 0.7462320327758789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 78/86 [D loss: 0.6735456585884094, acc.: 58.64%] [G loss: 0.7430236339569092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 79/86 [D loss: 0.6754046976566315, acc.: 58.74%] [G loss: 0.7531008124351501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 80/86 [D loss: 0.6789703965187073, acc.: 56.30%] [G loss: 0.7421958446502686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 81/86 [D loss: 0.6801500916481018, acc.: 57.18%] [G loss: 0.7489796280860901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 82/86 [D loss: 0.6746783256530762, acc.: 58.79%] [G loss: 0.7426024675369263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 83/86 [D loss: 0.6762107312679291, acc.: 58.89%] [G loss: 0.7421364784240723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 84/86 [D loss: 0.6746529638767242, acc.: 59.42%] [G loss: 0.7419731020927429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 85/86 [D loss: 0.6720840334892273, acc.: 59.08%] [G loss: 0.7473137974739075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 119/200, Batch 86/86 [D loss: 0.6754611134529114, acc.: 57.47%] [G loss: 0.7483816742897034]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 1/86 [D loss: 0.6767246723175049, acc.: 57.23%] [G loss: 0.7476754188537598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 2/86 [D loss: 0.6765986084938049, acc.: 57.57%] [G loss: 0.7437618970870972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 3/86 [D loss: 0.6757504045963287, acc.: 58.74%] [G loss: 0.7438900470733643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 4/86 [D loss: 0.6805521547794342, acc.: 56.40%] [G loss: 0.7432340979576111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 5/86 [D loss: 0.675246387720108, acc.: 58.54%] [G loss: 0.743416428565979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 6/86 [D loss: 0.6776880621910095, acc.: 56.98%] [G loss: 0.747204065322876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 7/86 [D loss: 0.6732364892959595, acc.: 59.28%] [G loss: 0.7440205812454224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 8/86 [D loss: 0.6810826063156128, acc.: 54.64%] [G loss: 0.7462071180343628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 9/86 [D loss: 0.6770992875099182, acc.: 58.01%] [G loss: 0.745180606842041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 10/86 [D loss: 0.6779901087284088, acc.: 57.03%] [G loss: 0.7488516569137573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 11/86 [D loss: 0.676770955324173, acc.: 57.42%] [G loss: 0.747904896736145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 12/86 [D loss: 0.6762612760066986, acc.: 58.79%] [G loss: 0.74503093957901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 13/86 [D loss: 0.676806777715683, acc.: 57.67%] [G loss: 0.7428730130195618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 14/86 [D loss: 0.6786493360996246, acc.: 56.59%] [G loss: 0.7493569254875183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 15/86 [D loss: 0.6729510426521301, acc.: 59.03%] [G loss: 0.7539788484573364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 16/86 [D loss: 0.6773492395877838, acc.: 57.32%] [G loss: 0.7430850267410278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 17/86 [D loss: 0.6715352833271027, acc.: 60.30%] [G loss: 0.7514896988868713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 18/86 [D loss: 0.6735886931419373, acc.: 58.40%] [G loss: 0.7387454509735107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 19/86 [D loss: 0.6740153133869171, acc.: 57.67%] [G loss: 0.7501357793807983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 20/86 [D loss: 0.675687700510025, acc.: 57.76%] [G loss: 0.7422968149185181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 21/86 [D loss: 0.6729682385921478, acc.: 59.72%] [G loss: 0.7533270120620728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 22/86 [D loss: 0.6792582273483276, acc.: 57.37%] [G loss: 0.7427847385406494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 23/86 [D loss: 0.6733504235744476, acc.: 58.11%] [G loss: 0.7455500364303589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 24/86 [D loss: 0.6765041053295135, acc.: 58.01%] [G loss: 0.7373250722885132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 25/86 [D loss: 0.6757085621356964, acc.: 57.71%] [G loss: 0.7405973672866821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 26/86 [D loss: 0.6767252683639526, acc.: 58.01%] [G loss: 0.7376002073287964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 27/86 [D loss: 0.6757200062274933, acc.: 58.25%] [G loss: 0.7471520900726318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 28/86 [D loss: 0.6777997612953186, acc.: 56.98%] [G loss: 0.7384862899780273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 29/86 [D loss: 0.6756066679954529, acc.: 56.54%] [G loss: 0.7460578680038452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 30/86 [D loss: 0.676034688949585, acc.: 58.20%] [G loss: 0.746753454208374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 31/86 [D loss: 0.674671471118927, acc.: 59.28%] [G loss: 0.7482266426086426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 32/86 [D loss: 0.6765834391117096, acc.: 57.67%] [G loss: 0.7495858073234558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 33/86 [D loss: 0.6756822466850281, acc.: 58.40%] [G loss: 0.7457031011581421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 34/86 [D loss: 0.6749314367771149, acc.: 58.40%] [G loss: 0.745976984500885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 35/86 [D loss: 0.6772797107696533, acc.: 57.42%] [G loss: 0.7475152015686035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 36/86 [D loss: 0.672092854976654, acc.: 59.08%] [G loss: 0.7404292821884155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 37/86 [D loss: 0.6770688891410828, acc.: 57.47%] [G loss: 0.7486399412155151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 38/86 [D loss: 0.6745503544807434, acc.: 58.45%] [G loss: 0.7453176975250244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 39/86 [D loss: 0.6780023276805878, acc.: 57.52%] [G loss: 0.7512989044189453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 40/86 [D loss: 0.6736931800842285, acc.: 58.64%] [G loss: 0.748440682888031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 41/86 [D loss: 0.678376704454422, acc.: 56.93%] [G loss: 0.7463045120239258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 42/86 [D loss: 0.6786126494407654, acc.: 56.15%] [G loss: 0.747188150882721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 43/86 [D loss: 0.678246945142746, acc.: 58.40%] [G loss: 0.7446491718292236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 44/86 [D loss: 0.6751879155635834, acc.: 57.62%] [G loss: 0.7502083778381348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 45/86 [D loss: 0.6772370338439941, acc.: 57.08%] [G loss: 0.7501773238182068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 46/86 [D loss: 0.6775580644607544, acc.: 59.23%] [G loss: 0.7474482655525208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 47/86 [D loss: 0.6762014925479889, acc.: 58.69%] [G loss: 0.7448285818099976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 48/86 [D loss: 0.6787078380584717, acc.: 57.42%] [G loss: 0.7473488450050354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 49/86 [D loss: 0.6802602708339691, acc.: 55.27%] [G loss: 0.7422541379928589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 50/86 [D loss: 0.6796784996986389, acc.: 56.01%] [G loss: 0.744038462638855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 51/86 [D loss: 0.6745957434177399, acc.: 58.40%] [G loss: 0.749017596244812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 52/86 [D loss: 0.6759034693241119, acc.: 56.84%] [G loss: 0.7456721067428589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 53/86 [D loss: 0.6775982975959778, acc.: 56.98%] [G loss: 0.7515836358070374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 54/86 [D loss: 0.6787078380584717, acc.: 56.98%] [G loss: 0.7436675429344177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 55/86 [D loss: 0.6788618564605713, acc.: 58.35%] [G loss: 0.7488372325897217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 56/86 [D loss: 0.6751165688037872, acc.: 58.40%] [G loss: 0.7492517232894897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 57/86 [D loss: 0.6775203943252563, acc.: 57.76%] [G loss: 0.7486698627471924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 58/86 [D loss: 0.6775205433368683, acc.: 58.25%] [G loss: 0.7472105622291565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 59/86 [D loss: 0.6748605072498322, acc.: 56.59%] [G loss: 0.7490424513816833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 60/86 [D loss: 0.6741029024124146, acc.: 56.88%] [G loss: 0.7496259212493896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 61/86 [D loss: 0.6793276071548462, acc.: 57.37%] [G loss: 0.741815447807312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 62/86 [D loss: 0.6815923452377319, acc.: 56.88%] [G loss: 0.7439372539520264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 63/86 [D loss: 0.679918497800827, acc.: 55.66%] [G loss: 0.7508571743965149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 64/86 [D loss: 0.6755924820899963, acc.: 57.71%] [G loss: 0.7480173110961914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 65/86 [D loss: 0.6760276257991791, acc.: 57.81%] [G loss: 0.7447463274002075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 66/86 [D loss: 0.6748344004154205, acc.: 57.96%] [G loss: 0.7457166910171509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 67/86 [D loss: 0.6767144501209259, acc.: 57.67%] [G loss: 0.7468858361244202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 68/86 [D loss: 0.6791273951530457, acc.: 57.81%] [G loss: 0.7476584911346436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 69/86 [D loss: 0.6759630739688873, acc.: 57.91%] [G loss: 0.7502279281616211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 70/86 [D loss: 0.6782170236110687, acc.: 55.86%] [G loss: 0.748186469078064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 71/86 [D loss: 0.675156444311142, acc.: 59.28%] [G loss: 0.7425973415374756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 72/86 [D loss: 0.6788871586322784, acc.: 57.08%] [G loss: 0.7492311596870422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 73/86 [D loss: 0.6768651008605957, acc.: 58.50%] [G loss: 0.7475345134735107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 74/86 [D loss: 0.6787420213222504, acc.: 56.93%] [G loss: 0.7403730750083923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 75/86 [D loss: 0.6777432560920715, acc.: 57.08%] [G loss: 0.7386206388473511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 76/86 [D loss: 0.6766433417797089, acc.: 58.74%] [G loss: 0.7469828128814697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 77/86 [D loss: 0.6767618060112, acc.: 57.96%] [G loss: 0.7470517158508301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 78/86 [D loss: 0.6720257103443146, acc.: 59.62%] [G loss: 0.7462774515151978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 79/86 [D loss: 0.6762388944625854, acc.: 57.71%] [G loss: 0.7438627481460571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 80/86 [D loss: 0.677394300699234, acc.: 57.08%] [G loss: 0.7497288584709167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 81/86 [D loss: 0.6740659475326538, acc.: 58.11%] [G loss: 0.7444583773612976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 82/86 [D loss: 0.6733703017234802, acc.: 58.59%] [G loss: 0.7431550621986389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 83/86 [D loss: 0.6749734878540039, acc.: 58.01%] [G loss: 0.7398775815963745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 84/86 [D loss: 0.676857203245163, acc.: 57.08%] [G loss: 0.7442065477371216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 85/86 [D loss: 0.6765609085559845, acc.: 58.11%] [G loss: 0.7509192824363708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 120/200, Batch 86/86 [D loss: 0.6761452555656433, acc.: 58.45%] [G loss: 0.7445937395095825]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 1/86 [D loss: 0.6741586327552795, acc.: 59.52%] [G loss: 0.749653697013855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 2/86 [D loss: 0.6750284433364868, acc.: 59.38%] [G loss: 0.7474533915519714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 3/86 [D loss: 0.6794091761112213, acc.: 57.18%] [G loss: 0.7521504759788513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 4/86 [D loss: 0.676927238702774, acc.: 56.88%] [G loss: 0.7484667897224426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 5/86 [D loss: 0.6753545999526978, acc.: 59.42%] [G loss: 0.7487212419509888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 6/86 [D loss: 0.677033007144928, acc.: 58.20%] [G loss: 0.745846688747406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 7/86 [D loss: 0.6791159808635712, acc.: 56.79%] [G loss: 0.7436481714248657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 8/86 [D loss: 0.6781262755393982, acc.: 56.74%] [G loss: 0.7389547824859619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 9/86 [D loss: 0.6779080331325531, acc.: 57.47%] [G loss: 0.7443351149559021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 10/86 [D loss: 0.6725518703460693, acc.: 59.47%] [G loss: 0.7438525557518005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 11/86 [D loss: 0.6761341989040375, acc.: 56.98%] [G loss: 0.747294008731842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 12/86 [D loss: 0.6736828982830048, acc.: 58.64%] [G loss: 0.7455323934555054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 13/86 [D loss: 0.6781977415084839, acc.: 57.03%] [G loss: 0.7485454678535461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 14/86 [D loss: 0.6773470938205719, acc.: 58.06%] [G loss: 0.753991425037384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 15/86 [D loss: 0.6785755157470703, acc.: 56.84%] [G loss: 0.746146559715271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 16/86 [D loss: 0.6800432205200195, acc.: 55.37%] [G loss: 0.7430897355079651]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 17/86 [D loss: 0.6769692897796631, acc.: 57.67%] [G loss: 0.7444000840187073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 18/86 [D loss: 0.6778896152973175, acc.: 57.67%] [G loss: 0.7433717250823975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 19/86 [D loss: 0.6759555637836456, acc.: 57.23%] [G loss: 0.7398503422737122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 20/86 [D loss: 0.6735800206661224, acc.: 58.74%] [G loss: 0.7467091679573059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 21/86 [D loss: 0.6710747182369232, acc.: 58.64%] [G loss: 0.7456861138343811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 22/86 [D loss: 0.6738096475601196, acc.: 58.79%] [G loss: 0.7484459280967712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 23/86 [D loss: 0.6741557717323303, acc.: 58.20%] [G loss: 0.7477306127548218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 24/86 [D loss: 0.6831305921077728, acc.: 53.91%] [G loss: 0.7416524887084961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 25/86 [D loss: 0.6719603538513184, acc.: 58.06%] [G loss: 0.7410231828689575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 26/86 [D loss: 0.6813377439975739, acc.: 56.01%] [G loss: 0.75205397605896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 27/86 [D loss: 0.6763929724693298, acc.: 57.67%] [G loss: 0.7429198026657104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 28/86 [D loss: 0.6787548065185547, acc.: 56.15%] [G loss: 0.7520151138305664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 29/86 [D loss: 0.6744970977306366, acc.: 57.52%] [G loss: 0.7452121376991272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 30/86 [D loss: 0.6797727048397064, acc.: 57.91%] [G loss: 0.7525330781936646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 31/86 [D loss: 0.6717929542064667, acc.: 58.84%] [G loss: 0.7447725534439087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 32/86 [D loss: 0.6751227378845215, acc.: 57.71%] [G loss: 0.74173903465271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 33/86 [D loss: 0.674561470746994, acc.: 59.33%] [G loss: 0.7513412237167358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 34/86 [D loss: 0.6798626184463501, acc.: 54.93%] [G loss: 0.742652177810669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 35/86 [D loss: 0.6771435737609863, acc.: 59.23%] [G loss: 0.7500787377357483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 36/86 [D loss: 0.6746508777141571, acc.: 56.88%] [G loss: 0.7394795417785645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 37/86 [D loss: 0.6792076230049133, acc.: 55.27%] [G loss: 0.7499761581420898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 38/86 [D loss: 0.6763527095317841, acc.: 57.67%] [G loss: 0.7412527799606323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 39/86 [D loss: 0.6792673468589783, acc.: 56.40%] [G loss: 0.7463129758834839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 40/86 [D loss: 0.6695650815963745, acc.: 61.08%] [G loss: 0.7450359463691711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 41/86 [D loss: 0.6813634037971497, acc.: 54.00%] [G loss: 0.7472079992294312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 42/86 [D loss: 0.6767907440662384, acc.: 58.15%] [G loss: 0.7503573894500732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 43/86 [D loss: 0.6777909696102142, acc.: 57.91%] [G loss: 0.7488275766372681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 44/86 [D loss: 0.6753667593002319, acc.: 57.71%] [G loss: 0.748482346534729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 45/86 [D loss: 0.6769604086875916, acc.: 56.74%] [G loss: 0.7443945407867432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 46/86 [D loss: 0.677235871553421, acc.: 58.06%] [G loss: 0.7599854469299316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 47/86 [D loss: 0.6801156997680664, acc.: 55.66%] [G loss: 0.7490084767341614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 48/86 [D loss: 0.6790189743041992, acc.: 55.66%] [G loss: 0.7508479952812195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 49/86 [D loss: 0.6804239749908447, acc.: 57.18%] [G loss: 0.7446138262748718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 50/86 [D loss: 0.6791644990444183, acc.: 56.25%] [G loss: 0.7434922456741333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 51/86 [D loss: 0.6752404868602753, acc.: 58.45%] [G loss: 0.7489153742790222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 52/86 [D loss: 0.6783495545387268, acc.: 56.79%] [G loss: 0.7468733787536621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 53/86 [D loss: 0.6765190958976746, acc.: 56.49%] [G loss: 0.7525498867034912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 54/86 [D loss: 0.6723453402519226, acc.: 59.57%] [G loss: 0.7496000528335571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 55/86 [D loss: 0.6714647710323334, acc.: 60.21%] [G loss: 0.7498959898948669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 56/86 [D loss: 0.6770306527614594, acc.: 57.57%] [G loss: 0.7495030164718628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 57/86 [D loss: 0.6734535694122314, acc.: 58.89%] [G loss: 0.746623694896698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 58/86 [D loss: 0.6782757043838501, acc.: 57.52%] [G loss: 0.7473915815353394]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 59/86 [D loss: 0.6791267395019531, acc.: 56.59%] [G loss: 0.7454597353935242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 60/86 [D loss: 0.6784665584564209, acc.: 56.20%] [G loss: 0.7416050434112549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 61/86 [D loss: 0.6807208359241486, acc.: 56.79%] [G loss: 0.7515859007835388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 62/86 [D loss: 0.6731404960155487, acc.: 59.62%] [G loss: 0.7479981184005737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 63/86 [D loss: 0.6798581480979919, acc.: 57.13%] [G loss: 0.7453831434249878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 64/86 [D loss: 0.6792815923690796, acc.: 56.35%] [G loss: 0.7469503283500671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 65/86 [D loss: 0.6740041673183441, acc.: 58.01%] [G loss: 0.7457989454269409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 66/86 [D loss: 0.6794735193252563, acc.: 55.96%] [G loss: 0.7478724718093872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 67/86 [D loss: 0.6769877672195435, acc.: 57.86%] [G loss: 0.7460148334503174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 68/86 [D loss: 0.6773613393306732, acc.: 56.98%] [G loss: 0.7479076385498047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 69/86 [D loss: 0.6726978421211243, acc.: 59.67%] [G loss: 0.731164276599884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 70/86 [D loss: 0.677094429731369, acc.: 58.54%] [G loss: 0.7492259740829468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 71/86 [D loss: 0.6796145141124725, acc.: 56.10%] [G loss: 0.7419983744621277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 72/86 [D loss: 0.6784982979297638, acc.: 54.98%] [G loss: 0.7490273714065552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 73/86 [D loss: 0.6749424636363983, acc.: 58.06%] [G loss: 0.7458030581474304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 74/86 [D loss: 0.6752659678459167, acc.: 59.47%] [G loss: 0.7454912066459656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 75/86 [D loss: 0.6809850335121155, acc.: 56.15%] [G loss: 0.747121274471283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 76/86 [D loss: 0.6773549318313599, acc.: 56.69%] [G loss: 0.7464166283607483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 77/86 [D loss: 0.6780696213245392, acc.: 56.98%] [G loss: 0.7538060545921326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 78/86 [D loss: 0.6793777644634247, acc.: 55.62%] [G loss: 0.7408967018127441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 79/86 [D loss: 0.6759232878684998, acc.: 59.18%] [G loss: 0.7497746348381042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 80/86 [D loss: 0.6725715398788452, acc.: 58.40%] [G loss: 0.7423327565193176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 81/86 [D loss: 0.6783294379711151, acc.: 56.30%] [G loss: 0.7449032664299011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 82/86 [D loss: 0.6761055886745453, acc.: 58.79%] [G loss: 0.7479692697525024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 83/86 [D loss: 0.6755657494068146, acc.: 59.18%] [G loss: 0.7491635680198669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 84/86 [D loss: 0.6747928559780121, acc.: 58.30%] [G loss: 0.7390564680099487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 85/86 [D loss: 0.675395131111145, acc.: 58.50%] [G loss: 0.7478640079498291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 121/200, Batch 86/86 [D loss: 0.675829142332077, acc.: 58.01%] [G loss: 0.7463107705116272]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 1/86 [D loss: 0.6811626255512238, acc.: 55.57%] [G loss: 0.7482273578643799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 2/86 [D loss: 0.6757583916187286, acc.: 57.86%] [G loss: 0.7523543834686279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 3/86 [D loss: 0.6753097176551819, acc.: 57.52%] [G loss: 0.746248185634613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 4/86 [D loss: 0.6785085201263428, acc.: 57.28%] [G loss: 0.7464752793312073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 5/86 [D loss: 0.6751084327697754, acc.: 57.37%] [G loss: 0.743538498878479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 6/86 [D loss: 0.6716355681419373, acc.: 59.77%] [G loss: 0.7485100626945496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 7/86 [D loss: 0.6745011806488037, acc.: 58.40%] [G loss: 0.7462832927703857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 8/86 [D loss: 0.6721697151660919, acc.: 58.59%] [G loss: 0.7497326731681824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 9/86 [D loss: 0.6778457760810852, acc.: 55.27%] [G loss: 0.7479016184806824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 10/86 [D loss: 0.6767634451389313, acc.: 58.40%] [G loss: 0.7446864247322083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 11/86 [D loss: 0.6776549816131592, acc.: 56.84%] [G loss: 0.7497806549072266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 12/86 [D loss: 0.6754643619060516, acc.: 59.28%] [G loss: 0.7401134967803955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 13/86 [D loss: 0.6770713925361633, acc.: 56.54%] [G loss: 0.7515853643417358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 14/86 [D loss: 0.6766228973865509, acc.: 57.23%] [G loss: 0.754011869430542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 15/86 [D loss: 0.6742419302463531, acc.: 59.47%] [G loss: 0.7534564733505249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 16/86 [D loss: 0.6735786497592926, acc.: 58.30%] [G loss: 0.7483053207397461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 17/86 [D loss: 0.6732004582881927, acc.: 58.25%] [G loss: 0.7434571385383606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 18/86 [D loss: 0.6770130395889282, acc.: 56.79%] [G loss: 0.7517198324203491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 19/86 [D loss: 0.6735860109329224, acc.: 59.42%] [G loss: 0.7446624040603638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 20/86 [D loss: 0.6765267848968506, acc.: 57.52%] [G loss: 0.7506839036941528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 21/86 [D loss: 0.6781550645828247, acc.: 58.84%] [G loss: 0.7435623407363892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 22/86 [D loss: 0.6747686862945557, acc.: 58.89%] [G loss: 0.7452827095985413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 23/86 [D loss: 0.673019289970398, acc.: 58.59%] [G loss: 0.7514271140098572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 24/86 [D loss: 0.6765559911727905, acc.: 57.23%] [G loss: 0.7513450980186462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 25/86 [D loss: 0.6742289364337921, acc.: 59.96%] [G loss: 0.7508671283721924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 26/86 [D loss: 0.6793014705181122, acc.: 55.52%] [G loss: 0.7470015287399292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 27/86 [D loss: 0.6715625524520874, acc.: 59.57%] [G loss: 0.7477264404296875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 28/86 [D loss: 0.6762165427207947, acc.: 58.15%] [G loss: 0.7480893135070801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 29/86 [D loss: 0.6774529814720154, acc.: 58.45%] [G loss: 0.7550967931747437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 30/86 [D loss: 0.6765211820602417, acc.: 58.01%] [G loss: 0.7455872297286987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 31/86 [D loss: 0.6769372820854187, acc.: 57.03%] [G loss: 0.7448445558547974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 32/86 [D loss: 0.6779661774635315, acc.: 57.23%] [G loss: 0.7428432703018188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 33/86 [D loss: 0.6805742383003235, acc.: 56.49%] [G loss: 0.7534163594245911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 34/86 [D loss: 0.6742592751979828, acc.: 59.38%] [G loss: 0.7405692934989929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 35/86 [D loss: 0.6772845089435577, acc.: 57.96%] [G loss: 0.7384296655654907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 36/86 [D loss: 0.6711506843566895, acc.: 59.23%] [G loss: 0.7404799461364746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 37/86 [D loss: 0.6771220862865448, acc.: 58.94%] [G loss: 0.7536051869392395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 38/86 [D loss: 0.6777384579181671, acc.: 58.15%] [G loss: 0.746097207069397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 39/86 [D loss: 0.6800507009029388, acc.: 56.35%] [G loss: 0.7427725791931152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 40/86 [D loss: 0.6783665716648102, acc.: 57.32%] [G loss: 0.7478718757629395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 41/86 [D loss: 0.6779281497001648, acc.: 56.45%] [G loss: 0.7399570941925049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 42/86 [D loss: 0.6770434379577637, acc.: 57.03%] [G loss: 0.7474831342697144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 43/86 [D loss: 0.6830040514469147, acc.: 54.93%] [G loss: 0.7419716119766235]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 44/86 [D loss: 0.6746469736099243, acc.: 59.13%] [G loss: 0.7432126998901367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 45/86 [D loss: 0.6823358237743378, acc.: 55.18%] [G loss: 0.7488623261451721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 46/86 [D loss: 0.6728983223438263, acc.: 59.13%] [G loss: 0.7499483227729797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 47/86 [D loss: 0.6783206164836884, acc.: 56.88%] [G loss: 0.748479425907135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 48/86 [D loss: 0.6775241196155548, acc.: 57.42%] [G loss: 0.7469514012336731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 49/86 [D loss: 0.6761038303375244, acc.: 58.35%] [G loss: 0.7415738105773926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 50/86 [D loss: 0.6795262396335602, acc.: 57.42%] [G loss: 0.7448301315307617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 51/86 [D loss: 0.6781457364559174, acc.: 58.15%] [G loss: 0.7484517693519592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 52/86 [D loss: 0.6784794330596924, acc.: 55.81%] [G loss: 0.7511926889419556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 53/86 [D loss: 0.6705393195152283, acc.: 59.77%] [G loss: 0.7455151081085205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 54/86 [D loss: 0.6752334237098694, acc.: 58.20%] [G loss: 0.7420220971107483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 55/86 [D loss: 0.6794414818286896, acc.: 56.45%] [G loss: 0.743027925491333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 56/86 [D loss: 0.6801907122135162, acc.: 55.76%] [G loss: 0.7438517212867737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 57/86 [D loss: 0.6795528531074524, acc.: 57.42%] [G loss: 0.7447683811187744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 58/86 [D loss: 0.6755867600440979, acc.: 57.86%] [G loss: 0.7511386871337891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 59/86 [D loss: 0.673372894525528, acc.: 58.45%] [G loss: 0.7395428419113159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 60/86 [D loss: 0.6749620139598846, acc.: 57.71%] [G loss: 0.7469898462295532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 61/86 [D loss: 0.6762040555477142, acc.: 58.94%] [G loss: 0.7421717643737793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 62/86 [D loss: 0.6736954152584076, acc.: 58.11%] [G loss: 0.7481485605239868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 63/86 [D loss: 0.6759873032569885, acc.: 57.96%] [G loss: 0.7463900446891785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 64/86 [D loss: 0.6778272986412048, acc.: 56.45%] [G loss: 0.7492416501045227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 65/86 [D loss: 0.6763760447502136, acc.: 57.57%] [G loss: 0.742370069026947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 66/86 [D loss: 0.6721033751964569, acc.: 57.81%] [G loss: 0.7455295324325562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 67/86 [D loss: 0.6751889884471893, acc.: 57.52%] [G loss: 0.7486318349838257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 68/86 [D loss: 0.6759740114212036, acc.: 57.71%] [G loss: 0.7499187588691711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 69/86 [D loss: 0.6713122725486755, acc.: 59.77%] [G loss: 0.7455611228942871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 70/86 [D loss: 0.6740952730178833, acc.: 58.30%] [G loss: 0.7475327253341675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 71/86 [D loss: 0.6754975914955139, acc.: 58.94%] [G loss: 0.7459251284599304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 72/86 [D loss: 0.6755634546279907, acc.: 58.25%] [G loss: 0.753294825553894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 73/86 [D loss: 0.673940896987915, acc.: 59.13%] [G loss: 0.7494305372238159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 74/86 [D loss: 0.6748667657375336, acc.: 57.57%] [G loss: 0.7431641221046448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 75/86 [D loss: 0.6768777966499329, acc.: 57.71%] [G loss: 0.7451942563056946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 76/86 [D loss: 0.6778558492660522, acc.: 57.71%] [G loss: 0.7451581954956055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 77/86 [D loss: 0.6737489998340607, acc.: 58.79%] [G loss: 0.749735951423645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 78/86 [D loss: 0.6744772493839264, acc.: 58.11%] [G loss: 0.7431402802467346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 79/86 [D loss: 0.6787534654140472, acc.: 56.45%] [G loss: 0.7482194304466248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 80/86 [D loss: 0.6716840863227844, acc.: 58.98%] [G loss: 0.7454620599746704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 81/86 [D loss: 0.6756580770015717, acc.: 58.50%] [G loss: 0.7485946416854858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 82/86 [D loss: 0.6794876754283905, acc.: 56.45%] [G loss: 0.7406868934631348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 83/86 [D loss: 0.678225964307785, acc.: 57.67%] [G loss: 0.7478206157684326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 84/86 [D loss: 0.6731868088245392, acc.: 57.91%] [G loss: 0.7465147972106934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 85/86 [D loss: 0.6732447147369385, acc.: 57.96%] [G loss: 0.7493492364883423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 122/200, Batch 86/86 [D loss: 0.6733677089214325, acc.: 58.79%] [G loss: 0.7415659427642822]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 1/86 [D loss: 0.6756178736686707, acc.: 58.94%] [G loss: 0.7480291128158569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 2/86 [D loss: 0.6791394650936127, acc.: 56.93%] [G loss: 0.7452145218849182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 3/86 [D loss: 0.6752651631832123, acc.: 58.30%] [G loss: 0.750238835811615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 4/86 [D loss: 0.6751362085342407, acc.: 58.79%] [G loss: 0.7476837635040283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 5/86 [D loss: 0.6758523285388947, acc.: 58.74%] [G loss: 0.7430689930915833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 6/86 [D loss: 0.6813653111457825, acc.: 55.37%] [G loss: 0.7489142417907715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 7/86 [D loss: 0.6746290624141693, acc.: 58.50%] [G loss: 0.7422289252281189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 8/86 [D loss: 0.6769920885562897, acc.: 57.71%] [G loss: 0.7436782717704773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 9/86 [D loss: 0.6777724027633667, acc.: 58.30%] [G loss: 0.7461010813713074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 10/86 [D loss: 0.6757717132568359, acc.: 58.54%] [G loss: 0.7493967413902283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 11/86 [D loss: 0.6750261783599854, acc.: 57.57%] [G loss: 0.7558489441871643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 12/86 [D loss: 0.6767748296260834, acc.: 57.03%] [G loss: 0.7411247491836548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 13/86 [D loss: 0.6751412451267242, acc.: 57.71%] [G loss: 0.7500541806221008]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 14/86 [D loss: 0.6765718460083008, acc.: 57.67%] [G loss: 0.7505635619163513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 15/86 [D loss: 0.6766892075538635, acc.: 56.30%] [G loss: 0.7490838170051575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 16/86 [D loss: 0.6715502738952637, acc.: 59.96%] [G loss: 0.748292088508606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 17/86 [D loss: 0.6758061349391937, acc.: 58.89%] [G loss: 0.7533766627311707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 18/86 [D loss: 0.6740557551383972, acc.: 58.54%] [G loss: 0.7496482133865356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 19/86 [D loss: 0.6810621321201324, acc.: 56.30%] [G loss: 0.7457732558250427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 20/86 [D loss: 0.6721817851066589, acc.: 59.91%] [G loss: 0.7463402152061462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 21/86 [D loss: 0.6758596897125244, acc.: 56.88%] [G loss: 0.7461078763008118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 22/86 [D loss: 0.6772896945476532, acc.: 57.32%] [G loss: 0.7509148120880127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 23/86 [D loss: 0.6731680333614349, acc.: 57.81%] [G loss: 0.7443802356719971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 24/86 [D loss: 0.6832036375999451, acc.: 54.93%] [G loss: 0.7587746381759644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 25/86 [D loss: 0.6741089224815369, acc.: 57.52%] [G loss: 0.7482772469520569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 26/86 [D loss: 0.6780251264572144, acc.: 57.67%] [G loss: 0.7583433389663696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 27/86 [D loss: 0.6780916154384613, acc.: 57.03%] [G loss: 0.7532082796096802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 28/86 [D loss: 0.6768893301486969, acc.: 57.23%] [G loss: 0.7524495720863342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 29/86 [D loss: 0.6711484789848328, acc.: 59.81%] [G loss: 0.745476245880127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 30/86 [D loss: 0.6803426146507263, acc.: 56.20%] [G loss: 0.7480131387710571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 31/86 [D loss: 0.6786323487758636, acc.: 55.13%] [G loss: 0.7480568885803223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 32/86 [D loss: 0.6799361705780029, acc.: 54.74%] [G loss: 0.7467522621154785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 33/86 [D loss: 0.6748344004154205, acc.: 57.32%] [G loss: 0.748356282711029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 34/86 [D loss: 0.6752319931983948, acc.: 58.98%] [G loss: 0.7484896183013916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 35/86 [D loss: 0.6771933734416962, acc.: 57.71%] [G loss: 0.7483959197998047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 36/86 [D loss: 0.6720674633979797, acc.: 58.84%] [G loss: 0.7448194026947021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 37/86 [D loss: 0.6759399771690369, acc.: 56.93%] [G loss: 0.7536979913711548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 38/86 [D loss: 0.675743967294693, acc.: 58.79%] [G loss: 0.7479755282402039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 39/86 [D loss: 0.6770735085010529, acc.: 57.28%] [G loss: 0.7492626905441284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 40/86 [D loss: 0.678542971611023, acc.: 56.79%] [G loss: 0.7452667355537415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 41/86 [D loss: 0.6801230907440186, acc.: 54.98%] [G loss: 0.7502834796905518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 42/86 [D loss: 0.6744202375411987, acc.: 57.67%] [G loss: 0.7490341663360596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 43/86 [D loss: 0.6748457849025726, acc.: 58.64%] [G loss: 0.7517005801200867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 44/86 [D loss: 0.6725912392139435, acc.: 59.77%] [G loss: 0.7524852752685547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 45/86 [D loss: 0.6832892894744873, acc.: 53.86%] [G loss: 0.746066153049469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 46/86 [D loss: 0.6721858084201813, acc.: 60.11%] [G loss: 0.7520339488983154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 47/86 [D loss: 0.6796675622463226, acc.: 56.35%] [G loss: 0.7381092309951782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 48/86 [D loss: 0.680663138628006, acc.: 56.01%] [G loss: 0.7582705616950989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 49/86 [D loss: 0.6810473203659058, acc.: 55.37%] [G loss: 0.737666666507721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 50/86 [D loss: 0.6807551085948944, acc.: 56.45%] [G loss: 0.7529228329658508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 51/86 [D loss: 0.6744182705879211, acc.: 58.64%] [G loss: 0.7396323084831238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 52/86 [D loss: 0.6825068593025208, acc.: 55.47%] [G loss: 0.7605113983154297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 53/86 [D loss: 0.678555428981781, acc.: 57.47%] [G loss: 0.7398737668991089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 54/86 [D loss: 0.6869941353797913, acc.: 53.86%] [G loss: 0.7471531629562378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 55/86 [D loss: 0.6771923303604126, acc.: 58.11%] [G loss: 0.7422908544540405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 56/86 [D loss: 0.6847417056560516, acc.: 52.39%] [G loss: 0.7543853521347046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 57/86 [D loss: 0.6685000658035278, acc.: 60.16%] [G loss: 0.7401875257492065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 58/86 [D loss: 0.6756786406040192, acc.: 57.08%] [G loss: 0.7481957674026489]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 59/86 [D loss: 0.6767712533473969, acc.: 57.18%] [G loss: 0.7471696138381958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 60/86 [D loss: 0.6727824509143829, acc.: 57.62%] [G loss: 0.7443404197692871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 61/86 [D loss: 0.676740825176239, acc.: 57.08%] [G loss: 0.7546744346618652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 62/86 [D loss: 0.6767823398113251, acc.: 57.03%] [G loss: 0.7418293356895447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 63/86 [D loss: 0.6804375946521759, acc.: 54.44%] [G loss: 0.7461273670196533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 64/86 [D loss: 0.6750706732273102, acc.: 58.50%] [G loss: 0.7505573630332947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 65/86 [D loss: 0.6773519814014435, acc.: 57.18%] [G loss: 0.7507680058479309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 66/86 [D loss: 0.6728796362876892, acc.: 58.89%] [G loss: 0.7523090839385986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 67/86 [D loss: 0.6727197170257568, acc.: 58.64%] [G loss: 0.7505295872688293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 68/86 [D loss: 0.6745324730873108, acc.: 56.40%] [G loss: 0.7488644123077393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 69/86 [D loss: 0.6777153313159943, acc.: 57.67%] [G loss: 0.7425904870033264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 70/86 [D loss: 0.6787167191505432, acc.: 56.88%] [G loss: 0.7552188634872437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 71/86 [D loss: 0.6763588786125183, acc.: 58.98%] [G loss: 0.7490783333778381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 72/86 [D loss: 0.6777748465538025, acc.: 57.47%] [G loss: 0.7531810998916626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 73/86 [D loss: 0.6759469211101532, acc.: 58.25%] [G loss: 0.7547076344490051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 74/86 [D loss: 0.6776267886161804, acc.: 57.57%] [G loss: 0.7497987747192383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 75/86 [D loss: 0.6761358082294464, acc.: 58.84%] [G loss: 0.7480798363685608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 76/86 [D loss: 0.6753644049167633, acc.: 57.03%] [G loss: 0.747748613357544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 77/86 [D loss: 0.6745187044143677, acc.: 58.30%] [G loss: 0.7462475895881653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 78/86 [D loss: 0.676068902015686, acc.: 57.18%] [G loss: 0.7462303042411804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 79/86 [D loss: 0.6748657524585724, acc.: 59.47%] [G loss: 0.7503353953361511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 80/86 [D loss: 0.6786534786224365, acc.: 57.57%] [G loss: 0.7465048432350159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 81/86 [D loss: 0.6769048571586609, acc.: 57.03%] [G loss: 0.7461103200912476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 82/86 [D loss: 0.6754755675792694, acc.: 58.35%] [G loss: 0.7499774098396301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 83/86 [D loss: 0.6728118062019348, acc.: 59.03%] [G loss: 0.7500787377357483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 84/86 [D loss: 0.6762773394584656, acc.: 58.89%] [G loss: 0.7438923120498657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 85/86 [D loss: 0.6785373389720917, acc.: 56.40%] [G loss: 0.7500255107879639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 123/200, Batch 86/86 [D loss: 0.6772377789020538, acc.: 56.40%] [G loss: 0.7460042238235474]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 1/86 [D loss: 0.6776243150234222, acc.: 57.13%] [G loss: 0.7492156028747559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 2/86 [D loss: 0.6772880256175995, acc.: 57.13%] [G loss: 0.7486462593078613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 3/86 [D loss: 0.6733617782592773, acc.: 58.79%] [G loss: 0.7425127029418945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 4/86 [D loss: 0.6709181368350983, acc.: 58.74%] [G loss: 0.7444576025009155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 5/86 [D loss: 0.675239235162735, acc.: 57.91%] [G loss: 0.7550905346870422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 6/86 [D loss: 0.6719337105751038, acc.: 59.28%] [G loss: 0.7500215768814087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 7/86 [D loss: 0.6711050868034363, acc.: 58.94%] [G loss: 0.7513450384140015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 8/86 [D loss: 0.6755824387073517, acc.: 58.11%] [G loss: 0.7494190335273743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 9/86 [D loss: 0.6757001578807831, acc.: 58.01%] [G loss: 0.7443481087684631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 10/86 [D loss: 0.6770413517951965, acc.: 57.32%] [G loss: 0.7490834593772888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 11/86 [D loss: 0.6718427836894989, acc.: 58.98%] [G loss: 0.7504581212997437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 12/86 [D loss: 0.6808085739612579, acc.: 54.88%] [G loss: 0.7484421730041504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 13/86 [D loss: 0.675632119178772, acc.: 56.98%] [G loss: 0.7515636682510376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 14/86 [D loss: 0.676945298910141, acc.: 56.59%] [G loss: 0.7430243492126465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 15/86 [D loss: 0.6761389672756195, acc.: 57.47%] [G loss: 0.7536569833755493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 16/86 [D loss: 0.6708511710166931, acc.: 59.03%] [G loss: 0.7480538487434387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 17/86 [D loss: 0.6756205260753632, acc.: 58.54%] [G loss: 0.7502976655960083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 18/86 [D loss: 0.6721327304840088, acc.: 58.98%] [G loss: 0.7465789914131165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 19/86 [D loss: 0.6761290431022644, acc.: 56.40%] [G loss: 0.7489356994628906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 20/86 [D loss: 0.6742321252822876, acc.: 58.50%] [G loss: 0.7492471933364868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 21/86 [D loss: 0.67646723985672, acc.: 56.25%] [G loss: 0.7466281652450562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 22/86 [D loss: 0.6767304241657257, acc.: 58.15%] [G loss: 0.7493169903755188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 23/86 [D loss: 0.6740360260009766, acc.: 58.84%] [G loss: 0.7458096146583557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 24/86 [D loss: 0.6739530265331268, acc.: 58.45%] [G loss: 0.749655544757843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 25/86 [D loss: 0.6715701222419739, acc.: 58.94%] [G loss: 0.7513800263404846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 26/86 [D loss: 0.6771557033061981, acc.: 56.79%] [G loss: 0.7555080652236938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 27/86 [D loss: 0.6766173541545868, acc.: 56.35%] [G loss: 0.7532346248626709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 28/86 [D loss: 0.67414590716362, acc.: 58.30%] [G loss: 0.7421610355377197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 29/86 [D loss: 0.6796722710132599, acc.: 57.03%] [G loss: 0.7454484105110168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 30/86 [D loss: 0.6762766540050507, acc.: 57.62%] [G loss: 0.7463604211807251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 31/86 [D loss: 0.676679790019989, acc.: 57.52%] [G loss: 0.7555512189865112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 32/86 [D loss: 0.6734240055084229, acc.: 59.42%] [G loss: 0.7440904974937439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 33/86 [D loss: 0.6793041825294495, acc.: 55.71%] [G loss: 0.7481013536453247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 34/86 [D loss: 0.6737973988056183, acc.: 57.81%] [G loss: 0.7466955184936523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 35/86 [D loss: 0.6763823330402374, acc.: 56.98%] [G loss: 0.751227855682373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 36/86 [D loss: 0.6761675477027893, acc.: 59.08%] [G loss: 0.7439720630645752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 37/86 [D loss: 0.6763086020946503, acc.: 57.71%] [G loss: 0.74806809425354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 38/86 [D loss: 0.6737134754657745, acc.: 59.08%] [G loss: 0.75001060962677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 39/86 [D loss: 0.6800387501716614, acc.: 56.54%] [G loss: 0.7420153617858887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 40/86 [D loss: 0.6748417913913727, acc.: 58.59%] [G loss: 0.7556682825088501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 41/86 [D loss: 0.6764732897281647, acc.: 56.20%] [G loss: 0.7464768886566162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 42/86 [D loss: 0.6799483299255371, acc.: 55.22%] [G loss: 0.7489667534828186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 43/86 [D loss: 0.6728312373161316, acc.: 60.35%] [G loss: 0.7441194653511047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 44/86 [D loss: 0.6802802681922913, acc.: 56.98%] [G loss: 0.7474141120910645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 45/86 [D loss: 0.6738974750041962, acc.: 59.33%] [G loss: 0.7436105012893677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 46/86 [D loss: 0.677418977022171, acc.: 56.79%] [G loss: 0.753227710723877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 47/86 [D loss: 0.6742486357688904, acc.: 58.40%] [G loss: 0.7522566914558411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 48/86 [D loss: 0.6792824566364288, acc.: 57.18%] [G loss: 0.7491515874862671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 49/86 [D loss: 0.6747240722179413, acc.: 56.49%] [G loss: 0.7468603253364563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 50/86 [D loss: 0.6779964566230774, acc.: 56.45%] [G loss: 0.7450323104858398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 51/86 [D loss: 0.6763734817504883, acc.: 57.18%] [G loss: 0.7498171329498291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 52/86 [D loss: 0.6773958504199982, acc.: 57.37%] [G loss: 0.7454450130462646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 53/86 [D loss: 0.6766668260097504, acc.: 57.32%] [G loss: 0.7478552460670471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 54/86 [D loss: 0.6754146814346313, acc.: 57.81%] [G loss: 0.7420308589935303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 55/86 [D loss: 0.6755289733409882, acc.: 56.79%] [G loss: 0.7531836032867432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 56/86 [D loss: 0.6759046614170074, acc.: 57.37%] [G loss: 0.7433543801307678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 57/86 [D loss: 0.6806372702121735, acc.: 54.98%] [G loss: 0.7508130073547363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 58/86 [D loss: 0.6757531464099884, acc.: 58.40%] [G loss: 0.7417828440666199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 59/86 [D loss: 0.6778576374053955, acc.: 56.54%] [G loss: 0.7453022003173828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 60/86 [D loss: 0.673015832901001, acc.: 57.37%] [G loss: 0.7516233921051025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 61/86 [D loss: 0.6747087836265564, acc.: 57.57%] [G loss: 0.7443021535873413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 62/86 [D loss: 0.6751136481761932, acc.: 58.45%] [G loss: 0.7482668161392212]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 63/86 [D loss: 0.6798163652420044, acc.: 54.88%] [G loss: 0.7418049573898315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 64/86 [D loss: 0.6781162023544312, acc.: 54.93%] [G loss: 0.7516973614692688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 65/86 [D loss: 0.6731170117855072, acc.: 58.89%] [G loss: 0.7424432635307312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 66/86 [D loss: 0.681225597858429, acc.: 56.15%] [G loss: 0.7505881190299988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 67/86 [D loss: 0.6720502078533173, acc.: 58.98%] [G loss: 0.7457170486450195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 68/86 [D loss: 0.6782099306583405, acc.: 56.20%] [G loss: 0.7365683913230896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 69/86 [D loss: 0.6761723756790161, acc.: 57.08%] [G loss: 0.7489421367645264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 70/86 [D loss: 0.676894873380661, acc.: 57.03%] [G loss: 0.7460435032844543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 71/86 [D loss: 0.6790163516998291, acc.: 57.52%] [G loss: 0.754909098148346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 72/86 [D loss: 0.6755632758140564, acc.: 57.71%] [G loss: 0.7434080243110657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 73/86 [D loss: 0.6757887601852417, acc.: 57.08%] [G loss: 0.7449179887771606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 74/86 [D loss: 0.6730300784111023, acc.: 58.40%] [G loss: 0.7470250129699707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 75/86 [D loss: 0.6761776804924011, acc.: 57.71%] [G loss: 0.7474876046180725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 76/86 [D loss: 0.6730013191699982, acc.: 57.71%] [G loss: 0.7417846918106079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 77/86 [D loss: 0.6777517199516296, acc.: 57.23%] [G loss: 0.7540756464004517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 78/86 [D loss: 0.6767223179340363, acc.: 57.13%] [G loss: 0.752079427242279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 79/86 [D loss: 0.6724803447723389, acc.: 58.74%] [G loss: 0.7497087717056274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 80/86 [D loss: 0.6727903485298157, acc.: 59.33%] [G loss: 0.7477110028266907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 81/86 [D loss: 0.6758125126361847, acc.: 57.42%] [G loss: 0.7575584650039673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 82/86 [D loss: 0.6711626648902893, acc.: 58.94%] [G loss: 0.7559120059013367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 83/86 [D loss: 0.6742330193519592, acc.: 58.54%] [G loss: 0.7503882646560669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 84/86 [D loss: 0.674992561340332, acc.: 57.13%] [G loss: 0.748906135559082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 85/86 [D loss: 0.6763139367103577, acc.: 58.06%] [G loss: 0.7498992085456848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 124/200, Batch 86/86 [D loss: 0.6750529408454895, acc.: 58.64%] [G loss: 0.7482244968414307]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 1/86 [D loss: 0.6721717119216919, acc.: 59.62%] [G loss: 0.749848484992981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 2/86 [D loss: 0.6737472116947174, acc.: 59.23%] [G loss: 0.7524334192276001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 3/86 [D loss: 0.675277829170227, acc.: 57.57%] [G loss: 0.7543808221817017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 4/86 [D loss: 0.6744502782821655, acc.: 58.06%] [G loss: 0.7526865005493164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 5/86 [D loss: 0.6762986481189728, acc.: 57.57%] [G loss: 0.7454186081886292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 6/86 [D loss: 0.6717558205127716, acc.: 60.16%] [G loss: 0.7500642538070679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 7/86 [D loss: 0.6732709407806396, acc.: 58.98%] [G loss: 0.7524282932281494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 8/86 [D loss: 0.6727765202522278, acc.: 60.40%] [G loss: 0.7554096579551697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 9/86 [D loss: 0.6734404563903809, acc.: 58.11%] [G loss: 0.7484021186828613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 10/86 [D loss: 0.6742520034313202, acc.: 57.91%] [G loss: 0.7502815127372742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 11/86 [D loss: 0.6724240481853485, acc.: 58.94%] [G loss: 0.75101637840271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 12/86 [D loss: 0.681477814912796, acc.: 56.59%] [G loss: 0.7514141201972961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 13/86 [D loss: 0.6729921102523804, acc.: 59.18%] [G loss: 0.7502151131629944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 14/86 [D loss: 0.6788905560970306, acc.: 56.74%] [G loss: 0.7498778700828552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 15/86 [D loss: 0.6722960472106934, acc.: 59.28%] [G loss: 0.7402628064155579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 16/86 [D loss: 0.6758614778518677, acc.: 57.42%] [G loss: 0.7500393390655518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 17/86 [D loss: 0.6729528605937958, acc.: 59.18%] [G loss: 0.7545484304428101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 18/86 [D loss: 0.6748926937580109, acc.: 58.45%] [G loss: 0.7512623071670532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 19/86 [D loss: 0.6740364730358124, acc.: 58.64%] [G loss: 0.7505667209625244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 20/86 [D loss: 0.6695646345615387, acc.: 60.11%] [G loss: 0.7479844093322754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 21/86 [D loss: 0.6783566176891327, acc.: 57.18%] [G loss: 0.7511739134788513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 22/86 [D loss: 0.6759308576583862, acc.: 58.15%] [G loss: 0.7466449737548828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 23/86 [D loss: 0.677380234003067, acc.: 57.42%] [G loss: 0.7489678263664246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 24/86 [D loss: 0.6721550226211548, acc.: 58.25%] [G loss: 0.747356653213501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 25/86 [D loss: 0.6738880574703217, acc.: 58.06%] [G loss: 0.7553465962409973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 26/86 [D loss: 0.6706522405147552, acc.: 59.91%] [G loss: 0.7412154674530029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 27/86 [D loss: 0.6827486455440521, acc.: 55.18%] [G loss: 0.7586954832077026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 28/86 [D loss: 0.6753615736961365, acc.: 58.25%] [G loss: 0.742049515247345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 29/86 [D loss: 0.6797534227371216, acc.: 56.30%] [G loss: 0.7528371214866638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 30/86 [D loss: 0.6729143559932709, acc.: 59.08%] [G loss: 0.7407876253128052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 31/86 [D loss: 0.6826199591159821, acc.: 54.74%] [G loss: 0.7551742196083069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 32/86 [D loss: 0.6744244694709778, acc.: 57.91%] [G loss: 0.741708517074585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 33/86 [D loss: 0.6803085505962372, acc.: 55.57%] [G loss: 0.7369973659515381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 34/86 [D loss: 0.6792699098587036, acc.: 57.57%] [G loss: 0.7543730735778809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 35/86 [D loss: 0.6727574467658997, acc.: 59.18%] [G loss: 0.738625705242157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 36/86 [D loss: 0.6785452663898468, acc.: 56.69%] [G loss: 0.7564316391944885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 37/86 [D loss: 0.6715775728225708, acc.: 59.47%] [G loss: 0.7430877089500427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 38/86 [D loss: 0.6785262227058411, acc.: 54.83%] [G loss: 0.7547110319137573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 39/86 [D loss: 0.6728319525718689, acc.: 59.18%] [G loss: 0.7407731413841248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 40/86 [D loss: 0.6747485399246216, acc.: 58.84%] [G loss: 0.7547003030776978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 41/86 [D loss: 0.6760807633399963, acc.: 57.81%] [G loss: 0.7476077079772949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 42/86 [D loss: 0.6743253469467163, acc.: 58.01%] [G loss: 0.7458387017250061]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 43/86 [D loss: 0.6757658123970032, acc.: 58.79%] [G loss: 0.7472680807113647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 44/86 [D loss: 0.6734769344329834, acc.: 58.30%] [G loss: 0.7445501089096069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 45/86 [D loss: 0.6746171414852142, acc.: 58.25%] [G loss: 0.7512937188148499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 46/86 [D loss: 0.6792699992656708, acc.: 56.01%] [G loss: 0.748525083065033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 47/86 [D loss: 0.6781772077083588, acc.: 56.01%] [G loss: 0.7498661279678345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 48/86 [D loss: 0.6753831803798676, acc.: 57.71%] [G loss: 0.745972752571106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 49/86 [D loss: 0.6775731444358826, acc.: 56.79%] [G loss: 0.7471616864204407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 50/86 [D loss: 0.6775515377521515, acc.: 58.01%] [G loss: 0.7464842796325684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 51/86 [D loss: 0.6749737560749054, acc.: 57.86%] [G loss: 0.7538051605224609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 52/86 [D loss: 0.6742547452449799, acc.: 57.76%] [G loss: 0.7499523162841797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 53/86 [D loss: 0.6766104400157928, acc.: 57.67%] [G loss: 0.7523391246795654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 54/86 [D loss: 0.6763211488723755, acc.: 57.62%] [G loss: 0.7468059062957764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 55/86 [D loss: 0.6728291511535645, acc.: 58.50%] [G loss: 0.7522661089897156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 56/86 [D loss: 0.6755692660808563, acc.: 58.25%] [G loss: 0.7432896494865417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 57/86 [D loss: 0.6771271824836731, acc.: 55.76%] [G loss: 0.7515616416931152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 58/86 [D loss: 0.6783562004566193, acc.: 56.15%] [G loss: 0.7479900121688843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 59/86 [D loss: 0.6719825863838196, acc.: 58.40%] [G loss: 0.7506684064865112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 60/86 [D loss: 0.6755916178226471, acc.: 58.25%] [G loss: 0.7446610927581787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 61/86 [D loss: 0.6793809831142426, acc.: 54.93%] [G loss: 0.7479429841041565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 62/86 [D loss: 0.680927574634552, acc.: 57.37%] [G loss: 0.7482125759124756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 63/86 [D loss: 0.6771135330200195, acc.: 56.64%] [G loss: 0.7445365190505981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 64/86 [D loss: 0.6736426055431366, acc.: 60.60%] [G loss: 0.74640953540802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 65/86 [D loss: 0.6813395917415619, acc.: 55.03%] [G loss: 0.7425744533538818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 66/86 [D loss: 0.6740754246711731, acc.: 57.32%] [G loss: 0.7528674006462097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 67/86 [D loss: 0.678473562002182, acc.: 57.32%] [G loss: 0.7510315775871277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 68/86 [D loss: 0.6729786992073059, acc.: 58.94%] [G loss: 0.7518184781074524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 69/86 [D loss: 0.6763418316841125, acc.: 56.69%] [G loss: 0.7500849962234497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 70/86 [D loss: 0.6730880439281464, acc.: 58.69%] [G loss: 0.7428978681564331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 71/86 [D loss: 0.6763895153999329, acc.: 57.32%] [G loss: 0.7458552122116089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 72/86 [D loss: 0.6754432618618011, acc.: 58.40%] [G loss: 0.7436297535896301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 73/86 [D loss: 0.675047755241394, acc.: 57.76%] [G loss: 0.7514654994010925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 74/86 [D loss: 0.6743309795856476, acc.: 58.06%] [G loss: 0.7512356042861938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 75/86 [D loss: 0.672809511423111, acc.: 58.74%] [G loss: 0.7539473176002502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 76/86 [D loss: 0.6732155382633209, acc.: 58.54%] [G loss: 0.7545609474182129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 77/86 [D loss: 0.6752144694328308, acc.: 57.76%] [G loss: 0.7484923601150513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 78/86 [D loss: 0.6739902198314667, acc.: 57.37%] [G loss: 0.7516061663627625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 79/86 [D loss: 0.6737175583839417, acc.: 57.91%] [G loss: 0.7531492710113525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 80/86 [D loss: 0.6732278168201447, acc.: 58.11%] [G loss: 0.7526178359985352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 81/86 [D loss: 0.6725510358810425, acc.: 57.91%] [G loss: 0.7480921149253845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 82/86 [D loss: 0.6792031824588776, acc.: 57.18%] [G loss: 0.7519693970680237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 83/86 [D loss: 0.6778005957603455, acc.: 56.93%] [G loss: 0.7474676370620728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 84/86 [D loss: 0.6752911508083344, acc.: 57.32%] [G loss: 0.7479564547538757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 85/86 [D loss: 0.6691296994686127, acc.: 59.47%] [G loss: 0.7510521411895752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 125/200, Batch 86/86 [D loss: 0.6759518980979919, acc.: 57.71%] [G loss: 0.7517223954200745]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 1/86 [D loss: 0.6725946068763733, acc.: 59.33%] [G loss: 0.7499919533729553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 2/86 [D loss: 0.6729964911937714, acc.: 59.08%] [G loss: 0.7505664825439453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 3/86 [D loss: 0.670359194278717, acc.: 59.52%] [G loss: 0.7505242824554443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 4/86 [D loss: 0.6729637384414673, acc.: 59.23%] [G loss: 0.749052882194519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 5/86 [D loss: 0.6736115217208862, acc.: 59.67%] [G loss: 0.7487064003944397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 6/86 [D loss: 0.6729129254817963, acc.: 59.03%] [G loss: 0.7519273161888123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 7/86 [D loss: 0.6723592877388, acc.: 59.08%] [G loss: 0.7501532435417175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 8/86 [D loss: 0.6780698597431183, acc.: 56.10%] [G loss: 0.746016263961792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 9/86 [D loss: 0.673864483833313, acc.: 58.40%] [G loss: 0.754717230796814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 10/86 [D loss: 0.6724951863288879, acc.: 58.94%] [G loss: 0.74949049949646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 11/86 [D loss: 0.6728791296482086, acc.: 58.84%] [G loss: 0.7530801296234131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 12/86 [D loss: 0.6765989363193512, acc.: 56.25%] [G loss: 0.7512819170951843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 13/86 [D loss: 0.6733949780464172, acc.: 58.40%] [G loss: 0.7460393905639648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 14/86 [D loss: 0.6779637336730957, acc.: 56.54%] [G loss: 0.7495144605636597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 15/86 [D loss: 0.6738870441913605, acc.: 58.20%] [G loss: 0.7538362741470337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 16/86 [D loss: 0.6783202886581421, acc.: 56.35%] [G loss: 0.759716272354126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 17/86 [D loss: 0.6738123893737793, acc.: 58.11%] [G loss: 0.7538781762123108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 18/86 [D loss: 0.6787758767604828, acc.: 55.62%] [G loss: 0.7569716572761536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 19/86 [D loss: 0.6723597347736359, acc.: 60.89%] [G loss: 0.7508206367492676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 20/86 [D loss: 0.6736361682415009, acc.: 59.08%] [G loss: 0.7546554803848267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 21/86 [D loss: 0.6797864437103271, acc.: 56.54%] [G loss: 0.747039794921875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 22/86 [D loss: 0.6743794679641724, acc.: 58.45%] [G loss: 0.7484328150749207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 23/86 [D loss: 0.6788788437843323, acc.: 57.28%] [G loss: 0.7407459020614624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 24/86 [D loss: 0.6747904121875763, acc.: 58.15%] [G loss: 0.7600975036621094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 25/86 [D loss: 0.6725117862224579, acc.: 58.84%] [G loss: 0.747957170009613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 26/86 [D loss: 0.6737974286079407, acc.: 57.67%] [G loss: 0.7497791051864624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 27/86 [D loss: 0.6768857836723328, acc.: 57.52%] [G loss: 0.7526553869247437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 28/86 [D loss: 0.6763127148151398, acc.: 58.59%] [G loss: 0.7556197047233582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 29/86 [D loss: 0.6771189868450165, acc.: 56.79%] [G loss: 0.7510728240013123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 30/86 [D loss: 0.6722201704978943, acc.: 58.11%] [G loss: 0.7500112652778625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 31/86 [D loss: 0.6755475699901581, acc.: 57.86%] [G loss: 0.7474493980407715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 32/86 [D loss: 0.675346314907074, acc.: 57.76%] [G loss: 0.7472506165504456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 33/86 [D loss: 0.6744785010814667, acc.: 59.03%] [G loss: 0.7516435980796814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 34/86 [D loss: 0.6727516353130341, acc.: 59.23%] [G loss: 0.7516132593154907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 35/86 [D loss: 0.6755218803882599, acc.: 56.69%] [G loss: 0.7529107928276062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 36/86 [D loss: 0.6728960275650024, acc.: 58.45%] [G loss: 0.753404974937439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 37/86 [D loss: 0.6740363538265228, acc.: 58.50%] [G loss: 0.75182044506073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 38/86 [D loss: 0.6753581464290619, acc.: 58.11%] [G loss: 0.7480370998382568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 39/86 [D loss: 0.6744300723075867, acc.: 58.84%] [G loss: 0.7519630193710327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 40/86 [D loss: 0.6771933436393738, acc.: 56.93%] [G loss: 0.7519745230674744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 41/86 [D loss: 0.6772233247756958, acc.: 56.88%] [G loss: 0.7560718655586243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 42/86 [D loss: 0.6772860884666443, acc.: 56.84%] [G loss: 0.7509610652923584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 43/86 [D loss: 0.6753483414649963, acc.: 58.35%] [G loss: 0.7461341619491577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 44/86 [D loss: 0.6791278421878815, acc.: 56.35%] [G loss: 0.7485415935516357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 45/86 [D loss: 0.6759600043296814, acc.: 58.15%] [G loss: 0.7519781589508057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 46/86 [D loss: 0.6729499101638794, acc.: 58.59%] [G loss: 0.7479822635650635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 47/86 [D loss: 0.6753678023815155, acc.: 57.67%] [G loss: 0.7452710270881653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 48/86 [D loss: 0.6753895878791809, acc.: 56.45%] [G loss: 0.7513327598571777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 49/86 [D loss: 0.6785624921321869, acc.: 56.35%] [G loss: 0.754822850227356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 50/86 [D loss: 0.6739811897277832, acc.: 58.35%] [G loss: 0.7513379454612732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 51/86 [D loss: 0.6751620173454285, acc.: 57.52%] [G loss: 0.7446010112762451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 52/86 [D loss: 0.6753993630409241, acc.: 58.74%] [G loss: 0.7497387528419495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 53/86 [D loss: 0.6750407516956329, acc.: 57.57%] [G loss: 0.7506338953971863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 54/86 [D loss: 0.6731023788452148, acc.: 58.54%] [G loss: 0.7501575350761414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 55/86 [D loss: 0.6762987673282623, acc.: 56.64%] [G loss: 0.7516504526138306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 56/86 [D loss: 0.6731540560722351, acc.: 58.20%] [G loss: 0.7546676993370056]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 57/86 [D loss: 0.6750858128070831, acc.: 58.40%] [G loss: 0.7507839798927307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 58/86 [D loss: 0.6729409098625183, acc.: 58.35%] [G loss: 0.7539675831794739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 59/86 [D loss: 0.667663186788559, acc.: 61.72%] [G loss: 0.7517067193984985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 60/86 [D loss: 0.6796042025089264, acc.: 56.15%] [G loss: 0.7491775155067444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 61/86 [D loss: 0.6720034778118134, acc.: 58.54%] [G loss: 0.7466768622398376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 62/86 [D loss: 0.6745401918888092, acc.: 59.67%] [G loss: 0.7501588463783264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 63/86 [D loss: 0.6791353523731232, acc.: 56.35%] [G loss: 0.7581210732460022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 64/86 [D loss: 0.6793848276138306, acc.: 56.88%] [G loss: 0.7497044801712036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 65/86 [D loss: 0.6753729283809662, acc.: 58.30%] [G loss: 0.7481751441955566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 66/86 [D loss: 0.6759913563728333, acc.: 56.79%] [G loss: 0.7492608428001404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 67/86 [D loss: 0.6797581613063812, acc.: 56.45%] [G loss: 0.7561693787574768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 68/86 [D loss: 0.6704629957675934, acc.: 60.94%] [G loss: 0.7560811638832092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 69/86 [D loss: 0.6749486327171326, acc.: 58.45%] [G loss: 0.7472661733627319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 70/86 [D loss: 0.68014857172966, acc.: 54.93%] [G loss: 0.7526590824127197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 71/86 [D loss: 0.6762091815471649, acc.: 57.86%] [G loss: 0.7400490045547485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 72/86 [D loss: 0.6770569980144501, acc.: 56.64%] [G loss: 0.7608447074890137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 73/86 [D loss: 0.6777080595493317, acc.: 56.98%] [G loss: 0.7508591413497925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 74/86 [D loss: 0.6787400841712952, acc.: 55.86%] [G loss: 0.7444029450416565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 75/86 [D loss: 0.6736747026443481, acc.: 58.94%] [G loss: 0.7536481618881226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 76/86 [D loss: 0.6781483888626099, acc.: 57.08%] [G loss: 0.7513659596443176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 77/86 [D loss: 0.6738959550857544, acc.: 57.81%] [G loss: 0.7479797601699829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 78/86 [D loss: 0.6726535260677338, acc.: 59.33%] [G loss: 0.746596097946167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 79/86 [D loss: 0.673381507396698, acc.: 58.84%] [G loss: 0.7465595602989197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 80/86 [D loss: 0.6768855452537537, acc.: 56.20%] [G loss: 0.763325572013855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 81/86 [D loss: 0.6741113364696503, acc.: 58.35%] [G loss: 0.7451064586639404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 82/86 [D loss: 0.6716228723526001, acc.: 59.13%] [G loss: 0.754130482673645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 83/86 [D loss: 0.6753916144371033, acc.: 57.62%] [G loss: 0.7539768218994141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 84/86 [D loss: 0.677795946598053, acc.: 56.79%] [G loss: 0.7560921311378479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 85/86 [D loss: 0.6721582412719727, acc.: 58.30%] [G loss: 0.7527434825897217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 126/200, Batch 86/86 [D loss: 0.6755419373512268, acc.: 57.67%] [G loss: 0.7452363967895508]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 1/86 [D loss: 0.674114316701889, acc.: 58.20%] [G loss: 0.7530155181884766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 2/86 [D loss: 0.6778451800346375, acc.: 57.18%] [G loss: 0.7541615962982178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 3/86 [D loss: 0.6757737994194031, acc.: 56.98%] [G loss: 0.7560840845108032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 4/86 [D loss: 0.6777432262897491, acc.: 57.67%] [G loss: 0.7544946074485779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 5/86 [D loss: 0.6727415919303894, acc.: 59.47%] [G loss: 0.7497295141220093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 6/86 [D loss: 0.6765829920768738, acc.: 56.59%] [G loss: 0.752449095249176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 7/86 [D loss: 0.6742800772190094, acc.: 58.79%] [G loss: 0.7495903968811035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 8/86 [D loss: 0.6755498349666595, acc.: 57.62%] [G loss: 0.7564471960067749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 9/86 [D loss: 0.6758760511875153, acc.: 58.40%] [G loss: 0.7436795234680176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 10/86 [D loss: 0.6726607978343964, acc.: 59.08%] [G loss: 0.7537525296211243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 11/86 [D loss: 0.6726024448871613, acc.: 59.03%] [G loss: 0.7552363276481628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 12/86 [D loss: 0.6720245778560638, acc.: 58.35%] [G loss: 0.7462273240089417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 13/86 [D loss: 0.6765851676464081, acc.: 57.62%] [G loss: 0.7511605620384216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 14/86 [D loss: 0.6758994460105896, acc.: 58.98%] [G loss: 0.7450981736183167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 15/86 [D loss: 0.676399290561676, acc.: 56.74%] [G loss: 0.7548443078994751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 16/86 [D loss: 0.6728240251541138, acc.: 59.38%] [G loss: 0.7508822083473206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 17/86 [D loss: 0.6755885481834412, acc.: 57.47%] [G loss: 0.7446084022521973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 18/86 [D loss: 0.6737416684627533, acc.: 58.69%] [G loss: 0.7466376423835754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 19/86 [D loss: 0.6756402552127838, acc.: 58.06%] [G loss: 0.7545051574707031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 20/86 [D loss: 0.6778997778892517, acc.: 57.47%] [G loss: 0.758050799369812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 21/86 [D loss: 0.6709350049495697, acc.: 59.81%] [G loss: 0.7434865236282349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 22/86 [D loss: 0.6751854717731476, acc.: 56.74%] [G loss: 0.7521798014640808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 23/86 [D loss: 0.6777076423168182, acc.: 56.05%] [G loss: 0.7516024112701416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 24/86 [D loss: 0.6784425377845764, acc.: 57.37%] [G loss: 0.7505966424942017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 25/86 [D loss: 0.6734937727451324, acc.: 58.98%] [G loss: 0.7552452683448792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 26/86 [D loss: 0.6747140288352966, acc.: 57.32%] [G loss: 0.7483851313591003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 27/86 [D loss: 0.6795190572738647, acc.: 56.30%] [G loss: 0.7555120587348938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 28/86 [D loss: 0.6792344450950623, acc.: 56.49%] [G loss: 0.7469232678413391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 29/86 [D loss: 0.6755479276180267, acc.: 57.86%] [G loss: 0.761809229850769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 30/86 [D loss: 0.671276718378067, acc.: 59.28%] [G loss: 0.7558024525642395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 31/86 [D loss: 0.6766261756420135, acc.: 57.13%] [G loss: 0.7529801726341248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 32/86 [D loss: 0.6732591986656189, acc.: 59.72%] [G loss: 0.7495442628860474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 33/86 [D loss: 0.6767915487289429, acc.: 56.93%] [G loss: 0.7549697756767273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 34/86 [D loss: 0.674470990896225, acc.: 58.01%] [G loss: 0.7504733800888062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 35/86 [D loss: 0.6771148145198822, acc.: 56.45%] [G loss: 0.7541521787643433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 36/86 [D loss: 0.6736810505390167, acc.: 59.13%] [G loss: 0.7530431747436523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 37/86 [D loss: 0.6767800152301788, acc.: 56.54%] [G loss: 0.7490471005439758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 38/86 [D loss: 0.6719449162483215, acc.: 59.28%] [G loss: 0.7474624514579773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 39/86 [D loss: 0.6742018163204193, acc.: 58.20%] [G loss: 0.7507388591766357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 40/86 [D loss: 0.6754359006881714, acc.: 57.71%] [G loss: 0.7541996240615845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 41/86 [D loss: 0.6749031245708466, acc.: 58.54%] [G loss: 0.7534535527229309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 42/86 [D loss: 0.6711105108261108, acc.: 60.25%] [G loss: 0.7536525130271912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 43/86 [D loss: 0.6737994849681854, acc.: 58.59%] [G loss: 0.7522832751274109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 44/86 [D loss: 0.6757243573665619, acc.: 56.88%] [G loss: 0.7468174695968628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 45/86 [D loss: 0.6737034618854523, acc.: 58.20%] [G loss: 0.7463715076446533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 46/86 [D loss: 0.6743153631687164, acc.: 58.84%] [G loss: 0.750158429145813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 47/86 [D loss: 0.6723340749740601, acc.: 58.84%] [G loss: 0.756196916103363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 48/86 [D loss: 0.6756156384944916, acc.: 57.32%] [G loss: 0.7488881349563599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 49/86 [D loss: 0.6707634329795837, acc.: 59.57%] [G loss: 0.7517080903053284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 50/86 [D loss: 0.6746669709682465, acc.: 58.89%] [G loss: 0.7532814741134644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 51/86 [D loss: 0.6719791889190674, acc.: 59.08%] [G loss: 0.7598738074302673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 52/86 [D loss: 0.6766864359378815, acc.: 57.08%] [G loss: 0.7503941059112549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 53/86 [D loss: 0.6751493215560913, acc.: 57.96%] [G loss: 0.7574092149734497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 54/86 [D loss: 0.6744821965694427, acc.: 57.86%] [G loss: 0.7515416741371155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 55/86 [D loss: 0.6794746518135071, acc.: 55.47%] [G loss: 0.754612922668457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 56/86 [D loss: 0.6761918067932129, acc.: 57.47%] [G loss: 0.7548345923423767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 57/86 [D loss: 0.6767429411411285, acc.: 56.88%] [G loss: 0.7501122355461121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 58/86 [D loss: 0.6755942702293396, acc.: 57.86%] [G loss: 0.7527283430099487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 59/86 [D loss: 0.6776419281959534, acc.: 56.49%] [G loss: 0.7486884593963623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 60/86 [D loss: 0.6719757318496704, acc.: 58.94%] [G loss: 0.7520092725753784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 61/86 [D loss: 0.6740413308143616, acc.: 58.11%] [G loss: 0.7479512691497803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 62/86 [D loss: 0.6791221499443054, acc.: 56.74%] [G loss: 0.7539290189743042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 63/86 [D loss: 0.6755779981613159, acc.: 57.62%] [G loss: 0.7481590509414673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 64/86 [D loss: 0.6698130965232849, acc.: 58.59%] [G loss: 0.7551364898681641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 65/86 [D loss: 0.6778639853000641, acc.: 58.30%] [G loss: 0.7577748894691467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 66/86 [D loss: 0.674881100654602, acc.: 57.13%] [G loss: 0.7487232685089111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 67/86 [D loss: 0.6728972792625427, acc.: 59.62%] [G loss: 0.7603210210800171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 68/86 [D loss: 0.6705955862998962, acc.: 60.60%] [G loss: 0.7543537020683289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 69/86 [D loss: 0.6725212037563324, acc.: 58.94%] [G loss: 0.7558125853538513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 70/86 [D loss: 0.6764896214008331, acc.: 56.35%] [G loss: 0.7592800259590149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 71/86 [D loss: 0.6710233688354492, acc.: 60.64%] [G loss: 0.7564209699630737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 72/86 [D loss: 0.6776846647262573, acc.: 57.03%] [G loss: 0.7471621036529541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 73/86 [D loss: 0.6739801168441772, acc.: 56.84%] [G loss: 0.7609454989433289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 74/86 [D loss: 0.6707179546356201, acc.: 59.81%] [G loss: 0.753092348575592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 75/86 [D loss: 0.6756172776222229, acc.: 57.76%] [G loss: 0.7534242868423462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 76/86 [D loss: 0.6733105480670929, acc.: 58.84%] [G loss: 0.755495548248291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 77/86 [D loss: 0.67372265458107, acc.: 58.89%] [G loss: 0.7549832463264465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 78/86 [D loss: 0.6702641248703003, acc.: 59.23%] [G loss: 0.7577672004699707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 79/86 [D loss: 0.6775906980037689, acc.: 56.84%] [G loss: 0.7457561492919922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 80/86 [D loss: 0.6745134890079498, acc.: 57.71%] [G loss: 0.7534844279289246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 81/86 [D loss: 0.6750785410404205, acc.: 59.08%] [G loss: 0.7600784301757812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 82/86 [D loss: 0.6733372509479523, acc.: 59.08%] [G loss: 0.7566606402397156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 83/86 [D loss: 0.6755772829055786, acc.: 57.96%] [G loss: 0.7572261691093445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 84/86 [D loss: 0.6728747189044952, acc.: 58.89%] [G loss: 0.750201940536499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 85/86 [D loss: 0.6783004403114319, acc.: 56.49%] [G loss: 0.7557983994483948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 127/200, Batch 86/86 [D loss: 0.676864504814148, acc.: 57.96%] [G loss: 0.7498547434806824]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 1/86 [D loss: 0.6758973598480225, acc.: 56.45%] [G loss: 0.763640284538269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 2/86 [D loss: 0.6750951707363129, acc.: 57.91%] [G loss: 0.7511273622512817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 3/86 [D loss: 0.6774756908416748, acc.: 56.93%] [G loss: 0.7544057369232178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 4/86 [D loss: 0.6718064248561859, acc.: 58.54%] [G loss: 0.7529723644256592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 5/86 [D loss: 0.6772708892822266, acc.: 56.79%] [G loss: 0.7571488618850708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 6/86 [D loss: 0.6724120080471039, acc.: 59.72%] [G loss: 0.7599630355834961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 7/86 [D loss: 0.6716217398643494, acc.: 60.45%] [G loss: 0.7510879635810852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 8/86 [D loss: 0.6736072599887848, acc.: 59.28%] [G loss: 0.7510474324226379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 9/86 [D loss: 0.6731787621974945, acc.: 57.62%] [G loss: 0.7499621510505676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 10/86 [D loss: 0.6751822531223297, acc.: 56.98%] [G loss: 0.7516045570373535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 11/86 [D loss: 0.6760073900222778, acc.: 57.13%] [G loss: 0.7509235739707947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 12/86 [D loss: 0.6753599047660828, acc.: 57.37%] [G loss: 0.7558484077453613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 13/86 [D loss: 0.6789723634719849, acc.: 56.79%] [G loss: 0.7562873363494873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 14/86 [D loss: 0.6796179413795471, acc.: 55.08%] [G loss: 0.7478950619697571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 15/86 [D loss: 0.6716643273830414, acc.: 59.23%] [G loss: 0.7553102374076843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 16/86 [D loss: 0.6706914007663727, acc.: 59.96%] [G loss: 0.7534439563751221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 17/86 [D loss: 0.6749081313610077, acc.: 58.01%] [G loss: 0.750877320766449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 18/86 [D loss: 0.6790414750576019, acc.: 57.28%] [G loss: 0.7520462870597839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 19/86 [D loss: 0.6801351010799408, acc.: 54.88%] [G loss: 0.7539178133010864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 20/86 [D loss: 0.6717742681503296, acc.: 58.45%] [G loss: 0.7480642795562744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 21/86 [D loss: 0.6746574342250824, acc.: 57.57%] [G loss: 0.751396894454956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 22/86 [D loss: 0.67636638879776, acc.: 59.13%] [G loss: 0.7568563222885132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 23/86 [D loss: 0.6734616458415985, acc.: 57.32%] [G loss: 0.7497640252113342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 24/86 [D loss: 0.6713761687278748, acc.: 58.50%] [G loss: 0.7535383105278015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 25/86 [D loss: 0.675887405872345, acc.: 58.25%] [G loss: 0.7562599182128906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 26/86 [D loss: 0.6735679507255554, acc.: 57.76%] [G loss: 0.7517977356910706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 27/86 [D loss: 0.6753721535205841, acc.: 57.18%] [G loss: 0.7565498352050781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 28/86 [D loss: 0.6761415302753448, acc.: 56.93%] [G loss: 0.7527412176132202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 29/86 [D loss: 0.6747523546218872, acc.: 58.54%] [G loss: 0.752103865146637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 30/86 [D loss: 0.6772748231887817, acc.: 57.76%] [G loss: 0.7525603175163269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 31/86 [D loss: 0.6732982397079468, acc.: 58.50%] [G loss: 0.7524306774139404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 32/86 [D loss: 0.675064355134964, acc.: 57.23%] [G loss: 0.7581849694252014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 33/86 [D loss: 0.6699451506137848, acc.: 59.57%] [G loss: 0.7562887072563171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 34/86 [D loss: 0.6748544871807098, acc.: 58.35%] [G loss: 0.755276620388031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 35/86 [D loss: 0.6770415306091309, acc.: 57.52%] [G loss: 0.7528673410415649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 36/86 [D loss: 0.677285373210907, acc.: 57.08%] [G loss: 0.7521383166313171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 37/86 [D loss: 0.6722920835018158, acc.: 58.74%] [G loss: 0.7505407929420471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 38/86 [D loss: 0.6740438044071198, acc.: 58.79%] [G loss: 0.7516117095947266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 39/86 [D loss: 0.6772290170192719, acc.: 55.66%] [G loss: 0.7540051341056824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 40/86 [D loss: 0.6679613292217255, acc.: 60.11%] [G loss: 0.7462032437324524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 41/86 [D loss: 0.6761894226074219, acc.: 57.28%] [G loss: 0.7517625093460083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 42/86 [D loss: 0.6759462058544159, acc.: 56.35%] [G loss: 0.7484338283538818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 43/86 [D loss: 0.6758855581283569, acc.: 57.08%] [G loss: 0.757576584815979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 44/86 [D loss: 0.6754200160503387, acc.: 58.11%] [G loss: 0.7567738890647888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 45/86 [D loss: 0.6753934621810913, acc.: 58.11%] [G loss: 0.7519750595092773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 46/86 [D loss: 0.6714386343955994, acc.: 58.11%] [G loss: 0.7492387294769287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 47/86 [D loss: 0.6794871687889099, acc.: 55.91%] [G loss: 0.7503907084465027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 48/86 [D loss: 0.6715837121009827, acc.: 58.69%] [G loss: 0.7580151557922363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 49/86 [D loss: 0.6714555621147156, acc.: 59.42%] [G loss: 0.7504435181617737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 50/86 [D loss: 0.6740283966064453, acc.: 58.54%] [G loss: 0.7491322159767151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 51/86 [D loss: 0.6798687279224396, acc.: 55.32%] [G loss: 0.7442759275436401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 52/86 [D loss: 0.6705498993396759, acc.: 58.94%] [G loss: 0.7627416849136353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 53/86 [D loss: 0.6822798550128937, acc.: 56.10%] [G loss: 0.7488500475883484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 54/86 [D loss: 0.6729266047477722, acc.: 58.74%] [G loss: 0.75726318359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 55/86 [D loss: 0.6765022873878479, acc.: 56.69%] [G loss: 0.7476577758789062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 56/86 [D loss: 0.676476776599884, acc.: 58.45%] [G loss: 0.7559332847595215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 57/86 [D loss: 0.6776256263256073, acc.: 57.37%] [G loss: 0.7410790920257568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 58/86 [D loss: 0.6845334768295288, acc.: 55.47%] [G loss: 0.7662314772605896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 59/86 [D loss: 0.6738776862621307, acc.: 59.08%] [G loss: 0.7406714558601379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 60/86 [D loss: 0.6784119307994843, acc.: 55.22%] [G loss: 0.7573270201683044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 61/86 [D loss: 0.6754110455513, acc.: 57.62%] [G loss: 0.7400619983673096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 62/86 [D loss: 0.6821884512901306, acc.: 54.98%] [G loss: 0.7660719752311707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 63/86 [D loss: 0.6702177226543427, acc.: 59.52%] [G loss: 0.743407130241394]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 64/86 [D loss: 0.6782788634300232, acc.: 55.66%] [G loss: 0.7590848207473755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 65/86 [D loss: 0.671656996011734, acc.: 59.28%] [G loss: 0.753495454788208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 66/86 [D loss: 0.6802155077457428, acc.: 56.35%] [G loss: 0.7606481313705444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 67/86 [D loss: 0.6698926985263824, acc.: 59.62%] [G loss: 0.7519227266311646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 68/86 [D loss: 0.6789368093013763, acc.: 56.84%] [G loss: 0.7498511672019958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 69/86 [D loss: 0.6731446981430054, acc.: 57.71%] [G loss: 0.7553537487983704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 70/86 [D loss: 0.6729623079299927, acc.: 58.45%] [G loss: 0.7517939209938049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 71/86 [D loss: 0.6787583231925964, acc.: 57.18%] [G loss: 0.7623814344406128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 72/86 [D loss: 0.6792735457420349, acc.: 57.28%] [G loss: 0.7528883814811707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 73/86 [D loss: 0.6798131167888641, acc.: 57.91%] [G loss: 0.753753662109375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 74/86 [D loss: 0.6686836779117584, acc.: 60.79%] [G loss: 0.7518321871757507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 75/86 [D loss: 0.6735580265522003, acc.: 58.25%] [G loss: 0.756314754486084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 76/86 [D loss: 0.672749936580658, acc.: 58.59%] [G loss: 0.7502389550209045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 77/86 [D loss: 0.6726655066013336, acc.: 59.23%] [G loss: 0.7552279233932495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 78/86 [D loss: 0.671640545129776, acc.: 58.01%] [G loss: 0.7549532055854797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 79/86 [D loss: 0.6728069186210632, acc.: 59.03%] [G loss: 0.7476849555969238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 80/86 [D loss: 0.6744285225868225, acc.: 58.40%] [G loss: 0.7546453475952148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 81/86 [D loss: 0.6711001992225647, acc.: 59.03%] [G loss: 0.7613145112991333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 82/86 [D loss: 0.6784338355064392, acc.: 56.20%] [G loss: 0.7535099387168884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 83/86 [D loss: 0.6744179129600525, acc.: 57.76%] [G loss: 0.750592052936554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 84/86 [D loss: 0.6716938018798828, acc.: 58.40%] [G loss: 0.7593667507171631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 85/86 [D loss: 0.6772350668907166, acc.: 54.93%] [G loss: 0.7499001026153564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 128/200, Batch 86/86 [D loss: 0.6733228862285614, acc.: 58.94%] [G loss: 0.7503738403320312]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 1/86 [D loss: 0.673122227191925, acc.: 58.94%] [G loss: 0.748427152633667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 2/86 [D loss: 0.6723081469535828, acc.: 57.86%] [G loss: 0.7503035664558411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 3/86 [D loss: 0.6734675765037537, acc.: 58.45%] [G loss: 0.753086507320404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 4/86 [D loss: 0.6720502078533173, acc.: 58.45%] [G loss: 0.7520957589149475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 5/86 [D loss: 0.6763777136802673, acc.: 56.25%] [G loss: 0.7595134973526001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 6/86 [D loss: 0.6737135350704193, acc.: 58.06%] [G loss: 0.7569618225097656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 7/86 [D loss: 0.6770942509174347, acc.: 58.01%] [G loss: 0.7510601282119751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 8/86 [D loss: 0.6712965071201324, acc.: 60.30%] [G loss: 0.7461503744125366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 9/86 [D loss: 0.6754068434238434, acc.: 56.45%] [G loss: 0.7525205016136169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 10/86 [D loss: 0.673862636089325, acc.: 57.96%] [G loss: 0.7518171072006226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 11/86 [D loss: 0.6800431311130524, acc.: 55.57%] [G loss: 0.7576460242271423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 12/86 [D loss: 0.6768254935741425, acc.: 57.76%] [G loss: 0.7544367909431458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 13/86 [D loss: 0.6767992675304413, acc.: 56.79%] [G loss: 0.750800371170044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 14/86 [D loss: 0.6780133843421936, acc.: 57.42%] [G loss: 0.7510507702827454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 15/86 [D loss: 0.6755876839160919, acc.: 57.81%] [G loss: 0.7531353831291199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 16/86 [D loss: 0.6723435819149017, acc.: 59.42%] [G loss: 0.757413387298584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 17/86 [D loss: 0.6801152229309082, acc.: 55.66%] [G loss: 0.754883885383606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 18/86 [D loss: 0.6787688732147217, acc.: 56.35%] [G loss: 0.7565284967422485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 19/86 [D loss: 0.6751071512699127, acc.: 57.91%] [G loss: 0.7540462017059326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 20/86 [D loss: 0.6754123866558075, acc.: 56.54%] [G loss: 0.7529305219650269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 21/86 [D loss: 0.6730409264564514, acc.: 56.93%] [G loss: 0.7548491954803467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 22/86 [D loss: 0.6784187257289886, acc.: 55.81%] [G loss: 0.757670521736145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 23/86 [D loss: 0.6713321208953857, acc.: 59.67%] [G loss: 0.7548406720161438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 24/86 [D loss: 0.6816965341567993, acc.: 55.91%] [G loss: 0.7514254450798035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 25/86 [D loss: 0.6735653877258301, acc.: 58.64%] [G loss: 0.7491024136543274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 26/86 [D loss: 0.6782798767089844, acc.: 56.49%] [G loss: 0.752942681312561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 27/86 [D loss: 0.6735520660877228, acc.: 58.11%] [G loss: 0.7581870555877686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 28/86 [D loss: 0.6754567325115204, acc.: 57.42%] [G loss: 0.7476006746292114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 29/86 [D loss: 0.675561934709549, acc.: 58.59%] [G loss: 0.7505849003791809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 30/86 [D loss: 0.673095703125, acc.: 59.03%] [G loss: 0.7525485754013062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 31/86 [D loss: 0.6745752096176147, acc.: 56.45%] [G loss: 0.7522924542427063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 32/86 [D loss: 0.6726439893245697, acc.: 58.74%] [G loss: 0.7487295269966125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 33/86 [D loss: 0.6753978431224823, acc.: 57.47%] [G loss: 0.7540053725242615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 34/86 [D loss: 0.6731235682964325, acc.: 57.57%] [G loss: 0.7561957240104675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 35/86 [D loss: 0.6761687994003296, acc.: 57.03%] [G loss: 0.7473545074462891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 36/86 [D loss: 0.6738333106040955, acc.: 56.93%] [G loss: 0.7484323978424072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 37/86 [D loss: 0.6810573041439056, acc.: 55.81%] [G loss: 0.7620588541030884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 38/86 [D loss: 0.6711205840110779, acc.: 58.98%] [G loss: 0.7577375173568726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 39/86 [D loss: 0.6760698556900024, acc.: 55.81%] [G loss: 0.7497544288635254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 40/86 [D loss: 0.6727417409420013, acc.: 59.72%] [G loss: 0.7542219161987305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 41/86 [D loss: 0.6767255663871765, acc.: 56.79%] [G loss: 0.7559198141098022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 42/86 [D loss: 0.671430379152298, acc.: 59.96%] [G loss: 0.7476059198379517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 43/86 [D loss: 0.6766317486763, acc.: 56.10%] [G loss: 0.7580434083938599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 44/86 [D loss: 0.6714736521244049, acc.: 58.54%] [G loss: 0.7443252205848694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 45/86 [D loss: 0.6751937866210938, acc.: 58.64%] [G loss: 0.7577954530715942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 46/86 [D loss: 0.6699779331684113, acc.: 60.55%] [G loss: 0.754244863986969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 47/86 [D loss: 0.6751932799816132, acc.: 56.40%] [G loss: 0.7513186931610107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 48/86 [D loss: 0.6761009097099304, acc.: 57.52%] [G loss: 0.7480108141899109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 49/86 [D loss: 0.6760224103927612, acc.: 57.18%] [G loss: 0.7549692392349243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 50/86 [D loss: 0.6712426543235779, acc.: 58.59%] [G loss: 0.7488679885864258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 51/86 [D loss: 0.6747372150421143, acc.: 57.91%] [G loss: 0.749800980091095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 52/86 [D loss: 0.6738815903663635, acc.: 57.37%] [G loss: 0.7590495347976685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 53/86 [D loss: 0.6742563545703888, acc.: 58.98%] [G loss: 0.744351863861084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 54/86 [D loss: 0.6767934262752533, acc.: 56.40%] [G loss: 0.758984386920929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 55/86 [D loss: 0.672276496887207, acc.: 59.23%] [G loss: 0.7515072822570801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 56/86 [D loss: 0.6743293106555939, acc.: 58.20%] [G loss: 0.7565807700157166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 57/86 [D loss: 0.670671284198761, acc.: 58.74%] [G loss: 0.7498205304145813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 58/86 [D loss: 0.6733717322349548, acc.: 58.11%] [G loss: 0.7555458545684814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 59/86 [D loss: 0.6744278371334076, acc.: 57.91%] [G loss: 0.756697416305542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 60/86 [D loss: 0.6763164401054382, acc.: 57.32%] [G loss: 0.753566324710846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 61/86 [D loss: 0.6701731383800507, acc.: 60.11%] [G loss: 0.7475157976150513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 62/86 [D loss: 0.678034633398056, acc.: 56.69%] [G loss: 0.7465565204620361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 63/86 [D loss: 0.6718746423721313, acc.: 59.91%] [G loss: 0.7470558881759644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 64/86 [D loss: 0.6802449822425842, acc.: 54.79%] [G loss: 0.7537233829498291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 65/86 [D loss: 0.6748469173908234, acc.: 57.13%] [G loss: 0.7577097415924072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 66/86 [D loss: 0.6779136955738068, acc.: 56.64%] [G loss: 0.7436907887458801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 67/86 [D loss: 0.6681332886219025, acc.: 61.38%] [G loss: 0.7423353791236877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 68/86 [D loss: 0.6749471724033356, acc.: 57.76%] [G loss: 0.7469518184661865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 69/86 [D loss: 0.6756775081157684, acc.: 57.76%] [G loss: 0.7571614384651184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 70/86 [D loss: 0.6703661382198334, acc.: 59.38%] [G loss: 0.7425752878189087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 71/86 [D loss: 0.6783507168292999, acc.: 56.15%] [G loss: 0.7622163891792297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 72/86 [D loss: 0.6758016049861908, acc.: 57.42%] [G loss: 0.7455745935440063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 73/86 [D loss: 0.67908975481987, acc.: 56.25%] [G loss: 0.7525582909584045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 74/86 [D loss: 0.6749854385852814, acc.: 57.03%] [G loss: 0.7495205998420715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 75/86 [D loss: 0.671970009803772, acc.: 57.71%] [G loss: 0.7479900121688843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 76/86 [D loss: 0.669838935136795, acc.: 58.25%] [G loss: 0.7571280598640442]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 77/86 [D loss: 0.6720010936260223, acc.: 59.23%] [G loss: 0.7549101114273071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 78/86 [D loss: 0.672310084104538, acc.: 57.81%] [G loss: 0.7537549734115601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 79/86 [D loss: 0.6735052466392517, acc.: 57.62%] [G loss: 0.7529611587524414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 80/86 [D loss: 0.6731621623039246, acc.: 59.62%] [G loss: 0.7552207112312317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 81/86 [D loss: 0.6765213310718536, acc.: 57.37%] [G loss: 0.7536965012550354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 82/86 [D loss: 0.6775475740432739, acc.: 57.08%] [G loss: 0.7527713179588318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 83/86 [D loss: 0.6773004531860352, acc.: 56.84%] [G loss: 0.748944878578186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 84/86 [D loss: 0.6768210828304291, acc.: 56.84%] [G loss: 0.7587956190109253]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 85/86 [D loss: 0.6767164170742035, acc.: 56.93%] [G loss: 0.7493515014648438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 129/200, Batch 86/86 [D loss: 0.6738489270210266, acc.: 57.52%] [G loss: 0.759678840637207]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 1/86 [D loss: 0.6748770773410797, acc.: 57.81%] [G loss: 0.7494159936904907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 2/86 [D loss: 0.6775218844413757, acc.: 56.69%] [G loss: 0.7612000703811646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 3/86 [D loss: 0.6749542951583862, acc.: 58.06%] [G loss: 0.7524303793907166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 4/86 [D loss: 0.6802291870117188, acc.: 56.10%] [G loss: 0.7494040131568909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 5/86 [D loss: 0.6770965456962585, acc.: 57.18%] [G loss: 0.7534211874008179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 6/86 [D loss: 0.6800151467323303, acc.: 55.47%] [G loss: 0.7534949779510498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 7/86 [D loss: 0.6717045605182648, acc.: 57.67%] [G loss: 0.7500676512718201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 8/86 [D loss: 0.6769512593746185, acc.: 58.69%] [G loss: 0.7537513375282288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 9/86 [D loss: 0.6756744980812073, acc.: 56.69%] [G loss: 0.7530357241630554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 10/86 [D loss: 0.6760049164295197, acc.: 56.45%] [G loss: 0.7619245052337646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 11/86 [D loss: 0.6770915985107422, acc.: 57.96%] [G loss: 0.7525172829627991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 12/86 [D loss: 0.6704392433166504, acc.: 59.81%] [G loss: 0.7549159526824951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 13/86 [D loss: 0.672682911157608, acc.: 58.35%] [G loss: 0.7573875188827515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 14/86 [D loss: 0.6749929189682007, acc.: 58.45%] [G loss: 0.7617673873901367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 15/86 [D loss: 0.670259565114975, acc.: 59.67%] [G loss: 0.7576025724411011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 16/86 [D loss: 0.6751724481582642, acc.: 57.18%] [G loss: 0.7541618347167969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 17/86 [D loss: 0.6681388914585114, acc.: 60.25%] [G loss: 0.7439001798629761]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 18/86 [D loss: 0.6744928956031799, acc.: 58.64%] [G loss: 0.7497513294219971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 19/86 [D loss: 0.6726828813552856, acc.: 58.84%] [G loss: 0.7563770413398743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 20/86 [D loss: 0.6750674247741699, acc.: 56.35%] [G loss: 0.7557042241096497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 21/86 [D loss: 0.6705256998538971, acc.: 58.94%] [G loss: 0.7585971355438232]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 22/86 [D loss: 0.6702553927898407, acc.: 59.91%] [G loss: 0.7590608596801758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 23/86 [D loss: 0.6735873818397522, acc.: 57.76%] [G loss: 0.7515535354614258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 24/86 [D loss: 0.6748737096786499, acc.: 57.81%] [G loss: 0.7524734735488892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 25/86 [D loss: 0.6727936565876007, acc.: 58.40%] [G loss: 0.7553441524505615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 26/86 [D loss: 0.6726099252700806, acc.: 58.45%] [G loss: 0.7470370531082153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 27/86 [D loss: 0.6752999424934387, acc.: 57.32%] [G loss: 0.7579482793807983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 28/86 [D loss: 0.6743777394294739, acc.: 58.79%] [G loss: 0.7588069438934326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 29/86 [D loss: 0.6740841567516327, acc.: 57.23%] [G loss: 0.7486734390258789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 30/86 [D loss: 0.6728816628456116, acc.: 58.40%] [G loss: 0.7469418048858643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 31/86 [D loss: 0.6788518130779266, acc.: 56.69%] [G loss: 0.753347635269165]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 130/200, Batch 32/86 [D loss: 0.6717269420623779, acc.: 59.96%] [G loss: 0.7492202520370483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 33/86 [D loss: 0.6735159158706665, acc.: 60.30%] [G loss: 0.7521262764930725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 34/86 [D loss: 0.6730103194713593, acc.: 58.89%] [G loss: 0.7521367073059082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 35/86 [D loss: 0.677177906036377, acc.: 56.40%] [G loss: 0.7467432022094727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 36/86 [D loss: 0.6689996421337128, acc.: 59.77%] [G loss: 0.7515989542007446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 37/86 [D loss: 0.6716570854187012, acc.: 58.94%] [G loss: 0.7529159188270569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 38/86 [D loss: 0.6692777574062347, acc.: 61.23%] [G loss: 0.7596002817153931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 39/86 [D loss: 0.6722216308116913, acc.: 58.69%] [G loss: 0.7544837594032288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 40/86 [D loss: 0.675496518611908, acc.: 57.03%] [G loss: 0.7505574822425842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 41/86 [D loss: 0.6756739318370819, acc.: 58.40%] [G loss: 0.7493892908096313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 42/86 [D loss: 0.6748749017715454, acc.: 57.32%] [G loss: 0.753470778465271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 43/86 [D loss: 0.6742669939994812, acc.: 58.01%] [G loss: 0.7557511329650879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 44/86 [D loss: 0.6775496006011963, acc.: 58.59%] [G loss: 0.7524822950363159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 45/86 [D loss: 0.6705260574817657, acc.: 59.47%] [G loss: 0.7507771849632263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 46/86 [D loss: 0.6746867001056671, acc.: 56.54%] [G loss: 0.7499170899391174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 47/86 [D loss: 0.6718296408653259, acc.: 59.47%] [G loss: 0.7559642195701599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 48/86 [D loss: 0.6751015186309814, acc.: 57.37%] [G loss: 0.7511036396026611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 49/86 [D loss: 0.6780772805213928, acc.: 56.30%] [G loss: 0.7493991255760193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 50/86 [D loss: 0.6702809631824493, acc.: 60.25%] [G loss: 0.748558759689331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 51/86 [D loss: 0.6792788207530975, acc.: 56.20%] [G loss: 0.7625827789306641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 52/86 [D loss: 0.6692431271076202, acc.: 60.35%] [G loss: 0.7541475296020508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 53/86 [D loss: 0.6746184825897217, acc.: 59.18%] [G loss: 0.7523984313011169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 54/86 [D loss: 0.6770414113998413, acc.: 56.30%] [G loss: 0.7532761096954346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 55/86 [D loss: 0.6763084530830383, acc.: 58.15%] [G loss: 0.75107741355896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 56/86 [D loss: 0.6758241057395935, acc.: 58.30%] [G loss: 0.7559512257575989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 57/86 [D loss: 0.6773333251476288, acc.: 56.35%] [G loss: 0.7593377828598022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 58/86 [D loss: 0.6695501506328583, acc.: 59.57%] [G loss: 0.7693297266960144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 59/86 [D loss: 0.6758660078048706, acc.: 56.74%] [G loss: 0.7535435557365417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 60/86 [D loss: 0.6722348630428314, acc.: 58.59%] [G loss: 0.7571103572845459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 61/86 [D loss: 0.6720305979251862, acc.: 59.42%] [G loss: 0.7448359131813049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 62/86 [D loss: 0.6779812574386597, acc.: 57.67%] [G loss: 0.7665235996246338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 63/86 [D loss: 0.6733720302581787, acc.: 59.47%] [G loss: 0.7460871934890747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 64/86 [D loss: 0.6747635304927826, acc.: 56.88%] [G loss: 0.7577672004699707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 65/86 [D loss: 0.676246851682663, acc.: 57.08%] [G loss: 0.7432364821434021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 66/86 [D loss: 0.6815859079360962, acc.: 56.35%] [G loss: 0.7636100649833679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 67/86 [D loss: 0.6730551719665527, acc.: 58.84%] [G loss: 0.7444276809692383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 68/86 [D loss: 0.6767222583293915, acc.: 56.74%] [G loss: 0.7591652274131775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 69/86 [D loss: 0.670419842004776, acc.: 59.62%] [G loss: 0.7564152479171753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 70/86 [D loss: 0.6806793808937073, acc.: 56.15%] [G loss: 0.7624919414520264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 71/86 [D loss: 0.6709142923355103, acc.: 60.60%] [G loss: 0.7499885559082031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 72/86 [D loss: 0.6798371374607086, acc.: 56.49%] [G loss: 0.7430920004844666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 73/86 [D loss: 0.6752797663211823, acc.: 56.40%] [G loss: 0.757306694984436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 74/86 [D loss: 0.6693480014801025, acc.: 59.47%] [G loss: 0.7323926687240601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 75/86 [D loss: 0.6793785393238068, acc.: 56.35%] [G loss: 0.775344729423523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 76/86 [D loss: 0.6777807772159576, acc.: 56.79%] [G loss: 0.7377341985702515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 77/86 [D loss: 0.6785862445831299, acc.: 55.71%] [G loss: 0.7558190822601318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 78/86 [D loss: 0.6680877506732941, acc.: 59.91%] [G loss: 0.7414735555648804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 79/86 [D loss: 0.6863458156585693, acc.: 53.66%] [G loss: 0.7664680480957031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 80/86 [D loss: 0.6674488186836243, acc.: 59.72%] [G loss: 0.7387153506278992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 81/86 [D loss: 0.6824265718460083, acc.: 54.69%] [G loss: 0.7632868885993958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 82/86 [D loss: 0.6745314598083496, acc.: 57.32%] [G loss: 0.7576246857643127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 83/86 [D loss: 0.677985817193985, acc.: 56.74%] [G loss: 0.7605329155921936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 84/86 [D loss: 0.6739842593669891, acc.: 59.08%] [G loss: 0.7619496583938599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 85/86 [D loss: 0.6773723661899567, acc.: 56.54%] [G loss: 0.7430630922317505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 130/200, Batch 86/86 [D loss: 0.6739217042922974, acc.: 58.35%] [G loss: 0.7614489197731018]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 131/200, Batch 1/86 [D loss: 0.6713667511940002, acc.: 58.54%] [G loss: 0.7522059679031372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 2/86 [D loss: 0.6769802272319794, acc.: 57.03%] [G loss: 0.7577754855155945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 3/86 [D loss: 0.6707514822483063, acc.: 60.06%] [G loss: 0.7561731338500977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 4/86 [D loss: 0.6738688945770264, acc.: 59.28%] [G loss: 0.7561408281326294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 5/86 [D loss: 0.6742773652076721, acc.: 58.11%] [G loss: 0.75510573387146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 6/86 [D loss: 0.6737308502197266, acc.: 58.64%] [G loss: 0.755607545375824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 7/86 [D loss: 0.6697509586811066, acc.: 59.81%] [G loss: 0.7642393708229065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 8/86 [D loss: 0.6755691766738892, acc.: 57.86%] [G loss: 0.747147262096405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 9/86 [D loss: 0.6747286021709442, acc.: 57.67%] [G loss: 0.7494971752166748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 10/86 [D loss: 0.6720999479293823, acc.: 57.76%] [G loss: 0.7554047703742981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 11/86 [D loss: 0.6746579110622406, acc.: 58.94%] [G loss: 0.7525625228881836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 12/86 [D loss: 0.6734895706176758, acc.: 58.40%] [G loss: 0.7586292028427124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 13/86 [D loss: 0.6794417798519135, acc.: 55.62%] [G loss: 0.7500389218330383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 14/86 [D loss: 0.6685699820518494, acc.: 59.47%] [G loss: 0.7568329572677612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 15/86 [D loss: 0.6710720658302307, acc.: 60.11%] [G loss: 0.7537761330604553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 16/86 [D loss: 0.672754555940628, acc.: 58.74%] [G loss: 0.7551751732826233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 17/86 [D loss: 0.6729380190372467, acc.: 58.59%] [G loss: 0.7555117011070251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 18/86 [D loss: 0.6730772852897644, acc.: 57.23%] [G loss: 0.7564670443534851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 19/86 [D loss: 0.6755197048187256, acc.: 56.64%] [G loss: 0.7623746395111084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 20/86 [D loss: 0.6760953664779663, acc.: 56.88%] [G loss: 0.7646249532699585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 21/86 [D loss: 0.6685085594654083, acc.: 60.40%] [G loss: 0.7583186030387878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 22/86 [D loss: 0.6768263280391693, acc.: 57.28%] [G loss: 0.7561452984809875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 23/86 [D loss: 0.6711361408233643, acc.: 59.23%] [G loss: 0.7528027892112732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 24/86 [D loss: 0.6703717708587646, acc.: 60.35%] [G loss: 0.7480484843254089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 25/86 [D loss: 0.6725701093673706, acc.: 58.98%] [G loss: 0.7538606524467468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 26/86 [D loss: 0.673213392496109, acc.: 58.35%] [G loss: 0.7519606351852417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 27/86 [D loss: 0.6710569858551025, acc.: 58.35%] [G loss: 0.7524422407150269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 28/86 [D loss: 0.6777158677577972, acc.: 56.10%] [G loss: 0.7532296180725098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 29/86 [D loss: 0.6688176393508911, acc.: 59.86%] [G loss: 0.7531371712684631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 30/86 [D loss: 0.6758001148700714, acc.: 57.03%] [G loss: 0.7494689226150513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 31/86 [D loss: 0.6723647713661194, acc.: 59.86%] [G loss: 0.7539919018745422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 32/86 [D loss: 0.6723000407218933, acc.: 59.13%] [G loss: 0.7561203837394714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 33/86 [D loss: 0.6778593957424164, acc.: 56.54%] [G loss: 0.7556768655776978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 34/86 [D loss: 0.677079051733017, acc.: 56.79%] [G loss: 0.7499552369117737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 35/86 [D loss: 0.6739362180233002, acc.: 58.11%] [G loss: 0.7570317983627319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 36/86 [D loss: 0.6741573214530945, acc.: 57.71%] [G loss: 0.7569900751113892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 37/86 [D loss: 0.6694552004337311, acc.: 59.33%] [G loss: 0.7568089365959167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 38/86 [D loss: 0.6727499663829803, acc.: 58.06%] [G loss: 0.7515057921409607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 39/86 [D loss: 0.6732944846153259, acc.: 58.45%] [G loss: 0.7592498064041138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 40/86 [D loss: 0.675139456987381, acc.: 57.76%] [G loss: 0.7471505403518677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 41/86 [D loss: 0.6756250262260437, acc.: 58.15%] [G loss: 0.7595997452735901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 42/86 [D loss: 0.679031103849411, acc.: 56.74%] [G loss: 0.746850311756134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 43/86 [D loss: 0.6724389791488647, acc.: 58.50%] [G loss: 0.7617323398590088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 44/86 [D loss: 0.6703040599822998, acc.: 58.64%] [G loss: 0.7540044784545898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 45/86 [D loss: 0.6768768727779388, acc.: 57.13%] [G loss: 0.7558608055114746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 46/86 [D loss: 0.6727263927459717, acc.: 58.89%] [G loss: 0.7582045197486877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 47/86 [D loss: 0.6732300519943237, acc.: 59.57%] [G loss: 0.759166419506073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 48/86 [D loss: 0.6775051951408386, acc.: 57.32%] [G loss: 0.7545039653778076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 49/86 [D loss: 0.6752287447452545, acc.: 58.69%] [G loss: 0.7545596361160278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 50/86 [D loss: 0.6783100068569183, acc.: 55.76%] [G loss: 0.762427568435669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 51/86 [D loss: 0.6757578253746033, acc.: 57.08%] [G loss: 0.7526300549507141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 52/86 [D loss: 0.6745948195457458, acc.: 58.84%] [G loss: 0.7531397342681885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 53/86 [D loss: 0.680654376745224, acc.: 54.93%] [G loss: 0.7567542791366577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 54/86 [D loss: 0.6743357181549072, acc.: 58.45%] [G loss: 0.755007266998291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 55/86 [D loss: 0.6735225021839142, acc.: 58.35%] [G loss: 0.7514943480491638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 56/86 [D loss: 0.673821747303009, acc.: 57.86%] [G loss: 0.7555862069129944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 57/86 [D loss: 0.6698111593723297, acc.: 58.74%] [G loss: 0.7566571831703186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 58/86 [D loss: 0.6735612154006958, acc.: 58.40%] [G loss: 0.7574383020401001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 59/86 [D loss: 0.6716041564941406, acc.: 59.57%] [G loss: 0.756300687789917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 60/86 [D loss: 0.6703628897666931, acc.: 60.01%] [G loss: 0.7619922161102295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 61/86 [D loss: 0.6690922975540161, acc.: 59.33%] [G loss: 0.764519453048706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 62/86 [D loss: 0.6702850461006165, acc.: 59.38%] [G loss: 0.7577452659606934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 63/86 [D loss: 0.6762315034866333, acc.: 56.88%] [G loss: 0.7536653876304626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 64/86 [D loss: 0.6688182651996613, acc.: 61.38%] [G loss: 0.7555820345878601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 65/86 [D loss: 0.6790165305137634, acc.: 56.25%] [G loss: 0.7558139562606812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 66/86 [D loss: 0.669611006975174, acc.: 59.52%] [G loss: 0.7565542459487915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 67/86 [D loss: 0.6725966930389404, acc.: 58.30%] [G loss: 0.7504292130470276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 68/86 [D loss: 0.6715066730976105, acc.: 59.13%] [G loss: 0.7580743432044983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 69/86 [D loss: 0.6725887358188629, acc.: 57.86%] [G loss: 0.7535002827644348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 70/86 [D loss: 0.6677141785621643, acc.: 61.13%] [G loss: 0.7443312406539917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 71/86 [D loss: 0.6748645901679993, acc.: 57.18%] [G loss: 0.7544896006584167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 72/86 [D loss: 0.6690925657749176, acc.: 59.08%] [G loss: 0.7531296014785767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 73/86 [D loss: 0.6764829158782959, acc.: 57.71%] [G loss: 0.7643372416496277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 74/86 [D loss: 0.6794376969337463, acc.: 56.74%] [G loss: 0.7593939900398254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 75/86 [D loss: 0.6731695532798767, acc.: 57.47%] [G loss: 0.7557366490364075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 76/86 [D loss: 0.6704883575439453, acc.: 59.18%] [G loss: 0.7600952982902527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 77/86 [D loss: 0.6714359521865845, acc.: 58.45%] [G loss: 0.7538882493972778]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 78/86 [D loss: 0.6694203019142151, acc.: 60.16%] [G loss: 0.7637012004852295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 79/86 [D loss: 0.6742748916149139, acc.: 57.57%] [G loss: 0.7542847394943237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 80/86 [D loss: 0.6739689707756042, acc.: 58.30%] [G loss: 0.7605968713760376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 81/86 [D loss: 0.6749657392501831, acc.: 56.98%] [G loss: 0.7558920979499817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 82/86 [D loss: 0.6725479364395142, acc.: 58.45%] [G loss: 0.7587266564369202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 83/86 [D loss: 0.6723094880580902, acc.: 60.01%] [G loss: 0.7544427514076233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 84/86 [D loss: 0.6727121472358704, acc.: 59.03%] [G loss: 0.7567176222801208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 85/86 [D loss: 0.6712271273136139, acc.: 59.72%] [G loss: 0.7530147433280945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 131/200, Batch 86/86 [D loss: 0.6772318184375763, acc.: 56.74%] [G loss: 0.7484921813011169]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 1/86 [D loss: 0.6722183525562286, acc.: 58.15%] [G loss: 0.7468850612640381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 2/86 [D loss: 0.6697184443473816, acc.: 59.81%] [G loss: 0.762263298034668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 3/86 [D loss: 0.6742203831672668, acc.: 56.88%] [G loss: 0.7573248147964478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 4/86 [D loss: 0.6747086942195892, acc.: 58.25%] [G loss: 0.7556816339492798]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 5/86 [D loss: 0.6698629856109619, acc.: 60.01%] [G loss: 0.7555144429206848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 6/86 [D loss: 0.6692850589752197, acc.: 59.52%] [G loss: 0.7586771249771118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 7/86 [D loss: 0.6713664829730988, acc.: 59.42%] [G loss: 0.7596075534820557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 8/86 [D loss: 0.6729391813278198, acc.: 57.96%] [G loss: 0.7558935284614563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 9/86 [D loss: 0.6749504804611206, acc.: 56.88%] [G loss: 0.7575646638870239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 10/86 [D loss: 0.6756476759910583, acc.: 57.57%] [G loss: 0.7536959648132324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 11/86 [D loss: 0.6733288764953613, acc.: 58.25%] [G loss: 0.7555044293403625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 12/86 [D loss: 0.6725262999534607, acc.: 59.03%] [G loss: 0.7496373653411865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 13/86 [D loss: 0.6712621450424194, acc.: 59.67%] [G loss: 0.7567835450172424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 14/86 [D loss: 0.6730852425098419, acc.: 57.76%] [G loss: 0.7598350048065186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 15/86 [D loss: 0.6731661856174469, acc.: 57.28%] [G loss: 0.7564780712127686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 16/86 [D loss: 0.673044890165329, acc.: 58.15%] [G loss: 0.7565043568611145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 17/86 [D loss: 0.6704031825065613, acc.: 59.18%] [G loss: 0.7562394738197327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 18/86 [D loss: 0.6748239994049072, acc.: 58.25%] [G loss: 0.7554624080657959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 19/86 [D loss: 0.6733766794204712, acc.: 58.11%] [G loss: 0.7597067356109619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 20/86 [D loss: 0.6701977849006653, acc.: 59.28%] [G loss: 0.7556842565536499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 21/86 [D loss: 0.6711962819099426, acc.: 58.64%] [G loss: 0.7570695281028748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 22/86 [D loss: 0.6758151352405548, acc.: 55.37%] [G loss: 0.753947377204895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 23/86 [D loss: 0.6726933419704437, acc.: 57.57%] [G loss: 0.7599119544029236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 24/86 [D loss: 0.6745216250419617, acc.: 58.69%] [G loss: 0.7536271214485168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 25/86 [D loss: 0.6756373047828674, acc.: 57.23%] [G loss: 0.7563117146492004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 26/86 [D loss: 0.6729560196399689, acc.: 58.50%] [G loss: 0.7557450532913208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 27/86 [D loss: 0.6750518083572388, acc.: 58.35%] [G loss: 0.7602666020393372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 28/86 [D loss: 0.6753864586353302, acc.: 57.67%] [G loss: 0.762328028678894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 29/86 [D loss: 0.6737596988677979, acc.: 57.03%] [G loss: 0.7562314867973328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 30/86 [D loss: 0.6692432165145874, acc.: 60.25%] [G loss: 0.7572981119155884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 31/86 [D loss: 0.6724953353404999, acc.: 59.47%] [G loss: 0.7568142414093018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 32/86 [D loss: 0.6721785068511963, acc.: 58.89%] [G loss: 0.7632933259010315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 33/86 [D loss: 0.6739987134933472, acc.: 57.67%] [G loss: 0.7588509321212769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 34/86 [D loss: 0.6710534393787384, acc.: 59.03%] [G loss: 0.7567685842514038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 35/86 [D loss: 0.6700941920280457, acc.: 59.42%] [G loss: 0.7538239359855652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 36/86 [D loss: 0.674397200345993, acc.: 57.08%] [G loss: 0.7618295550346375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 37/86 [D loss: 0.6753165423870087, acc.: 56.45%] [G loss: 0.7546265721321106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 38/86 [D loss: 0.6703680455684662, acc.: 58.94%] [G loss: 0.7556787729263306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 39/86 [D loss: 0.6787390410900116, acc.: 56.10%] [G loss: 0.7565862536430359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 40/86 [D loss: 0.6719665825366974, acc.: 59.86%] [G loss: 0.760201632976532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 41/86 [D loss: 0.6748520731925964, acc.: 56.98%] [G loss: 0.7559961676597595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 42/86 [D loss: 0.6683643758296967, acc.: 61.04%] [G loss: 0.7573131918907166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 43/86 [D loss: 0.672770231962204, acc.: 59.23%] [G loss: 0.7543202042579651]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 132/200, Batch 44/86 [D loss: 0.6764369606971741, acc.: 57.52%] [G loss: 0.7533156871795654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 45/86 [D loss: 0.6729090213775635, acc.: 58.84%] [G loss: 0.7521430850028992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 46/86 [D loss: 0.6775599122047424, acc.: 56.45%] [G loss: 0.7539072036743164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 47/86 [D loss: 0.675135463476181, acc.: 56.49%] [G loss: 0.7580741047859192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 48/86 [D loss: 0.6812723875045776, acc.: 55.96%] [G loss: 0.7596601247787476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 49/86 [D loss: 0.6706985831260681, acc.: 60.94%] [G loss: 0.7579507827758789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 50/86 [D loss: 0.6734059751033783, acc.: 58.79%] [G loss: 0.7582406401634216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 51/86 [D loss: 0.6682145893573761, acc.: 58.94%] [G loss: 0.7561313509941101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 52/86 [D loss: 0.6760389506816864, acc.: 58.15%] [G loss: 0.7543979287147522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 53/86 [D loss: 0.6708874106407166, acc.: 58.74%] [G loss: 0.756695032119751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 54/86 [D loss: 0.6727184355258942, acc.: 57.81%] [G loss: 0.7620344758033752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 55/86 [D loss: 0.6738089919090271, acc.: 56.98%] [G loss: 0.7610042095184326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 56/86 [D loss: 0.6671663820743561, acc.: 60.94%] [G loss: 0.7514378428459167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 57/86 [D loss: 0.6714501082897186, acc.: 58.89%] [G loss: 0.7504447102546692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 58/86 [D loss: 0.6765809059143066, acc.: 57.67%] [G loss: 0.7548974752426147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 59/86 [D loss: 0.6720835566520691, acc.: 59.62%] [G loss: 0.7543479204177856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 60/86 [D loss: 0.6709586977958679, acc.: 58.94%] [G loss: 0.7615376710891724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 61/86 [D loss: 0.677629292011261, acc.: 57.62%] [G loss: 0.756808876991272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 62/86 [D loss: 0.6744853556156158, acc.: 56.40%] [G loss: 0.7563877105712891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 63/86 [D loss: 0.6721409261226654, acc.: 58.89%] [G loss: 0.7602837085723877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 64/86 [D loss: 0.669729620218277, acc.: 59.28%] [G loss: 0.7552133202552795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 65/86 [D loss: 0.6720800697803497, acc.: 57.71%] [G loss: 0.7626776695251465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 66/86 [D loss: 0.6744098663330078, acc.: 58.15%] [G loss: 0.7594954371452332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 67/86 [D loss: 0.6676062941551208, acc.: 61.04%] [G loss: 0.7513854503631592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 68/86 [D loss: 0.6713361740112305, acc.: 59.33%] [G loss: 0.7598620653152466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 69/86 [D loss: 0.6725199520587921, acc.: 57.42%] [G loss: 0.7583006620407104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 70/86 [D loss: 0.6730875670909882, acc.: 58.11%] [G loss: 0.7549338936805725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 71/86 [D loss: 0.6751466691493988, acc.: 57.71%] [G loss: 0.752348780632019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 72/86 [D loss: 0.673391729593277, acc.: 58.69%] [G loss: 0.751534640789032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 73/86 [D loss: 0.674913078546524, acc.: 56.98%] [G loss: 0.7562078237533569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 74/86 [D loss: 0.6691601574420929, acc.: 58.94%] [G loss: 0.7501819133758545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 75/86 [D loss: 0.6738073527812958, acc.: 57.71%] [G loss: 0.7569722533226013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 76/86 [D loss: 0.6713068783283234, acc.: 58.30%] [G loss: 0.7552596926689148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 77/86 [D loss: 0.6777002811431885, acc.: 55.62%] [G loss: 0.7560741901397705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 78/86 [D loss: 0.6715379059314728, acc.: 59.72%] [G loss: 0.7496123313903809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 79/86 [D loss: 0.6739334464073181, acc.: 57.76%] [G loss: 0.7587636113166809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 80/86 [D loss: 0.6716666519641876, acc.: 58.40%] [G loss: 0.7506580352783203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 81/86 [D loss: 0.6736728549003601, acc.: 58.20%] [G loss: 0.7524983882904053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 82/86 [D loss: 0.6729970574378967, acc.: 59.08%] [G loss: 0.7617335915565491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 83/86 [D loss: 0.6736025214195251, acc.: 58.64%] [G loss: 0.7530620694160461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 84/86 [D loss: 0.6752287447452545, acc.: 58.20%] [G loss: 0.7558356523513794]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 85/86 [D loss: 0.6763799786567688, acc.: 57.62%] [G loss: 0.7450912594795227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 132/200, Batch 86/86 [D loss: 0.6773760616779327, acc.: 56.01%] [G loss: 0.7586705684661865]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 1/86 [D loss: 0.6734671294689178, acc.: 57.52%] [G loss: 0.7569195628166199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 2/86 [D loss: 0.6793937087059021, acc.: 55.08%] [G loss: 0.7528457641601562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 3/86 [D loss: 0.6730935573577881, acc.: 58.50%] [G loss: 0.7587734460830688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 4/86 [D loss: 0.6760171055793762, acc.: 58.25%] [G loss: 0.7571325302124023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 5/86 [D loss: 0.6726791262626648, acc.: 57.67%] [G loss: 0.756895899772644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 6/86 [D loss: 0.6749674379825592, acc.: 57.23%] [G loss: 0.7550593018531799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 7/86 [D loss: 0.6760618984699249, acc.: 55.66%] [G loss: 0.7600457072257996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 8/86 [D loss: 0.6737253367900848, acc.: 57.47%] [G loss: 0.7552170753479004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 9/86 [D loss: 0.6678237915039062, acc.: 60.25%] [G loss: 0.7617670893669128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 10/86 [D loss: 0.6769304871559143, acc.: 58.54%] [G loss: 0.7570599317550659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 11/86 [D loss: 0.6765280067920685, acc.: 55.76%] [G loss: 0.7586546540260315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 12/86 [D loss: 0.6773955225944519, acc.: 57.37%] [G loss: 0.7479952573776245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 13/86 [D loss: 0.6771001219749451, acc.: 57.32%] [G loss: 0.7595536112785339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 14/86 [D loss: 0.6766018569469452, acc.: 58.20%] [G loss: 0.7623010873794556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 15/86 [D loss: 0.6721732020378113, acc.: 59.38%] [G loss: 0.7593815326690674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 16/86 [D loss: 0.6740187108516693, acc.: 58.64%] [G loss: 0.7518570423126221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 17/86 [D loss: 0.6689028739929199, acc.: 60.50%] [G loss: 0.7552837133407593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 18/86 [D loss: 0.6764897406101227, acc.: 56.10%] [G loss: 0.7564972043037415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 19/86 [D loss: 0.6687809526920319, acc.: 59.23%] [G loss: 0.7611519694328308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 20/86 [D loss: 0.6742145419120789, acc.: 58.98%] [G loss: 0.7542285919189453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 21/86 [D loss: 0.6745423674583435, acc.: 58.30%] [G loss: 0.7555002570152283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 22/86 [D loss: 0.6719284355640411, acc.: 58.94%] [G loss: 0.7653602361679077]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 23/86 [D loss: 0.6739262044429779, acc.: 58.50%] [G loss: 0.7513467073440552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 24/86 [D loss: 0.6784479320049286, acc.: 56.25%] [G loss: 0.7566331624984741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 25/86 [D loss: 0.6723027229309082, acc.: 58.89%] [G loss: 0.7510654926300049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 26/86 [D loss: 0.6744730770587921, acc.: 58.06%] [G loss: 0.7630484700202942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 27/86 [D loss: 0.6750394105911255, acc.: 57.37%] [G loss: 0.7589839696884155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 28/86 [D loss: 0.6732929646968842, acc.: 58.89%] [G loss: 0.7586610913276672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 29/86 [D loss: 0.6682672500610352, acc.: 61.04%] [G loss: 0.7568482160568237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 30/86 [D loss: 0.6705082356929779, acc.: 58.59%] [G loss: 0.759681761264801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 31/86 [D loss: 0.6748185157775879, acc.: 57.23%] [G loss: 0.7597988247871399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 32/86 [D loss: 0.6720403134822845, acc.: 57.42%] [G loss: 0.7532088756561279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 33/86 [D loss: 0.6706303060054779, acc.: 59.86%] [G loss: 0.7625547051429749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 34/86 [D loss: 0.6708467900753021, acc.: 59.42%] [G loss: 0.7577029466629028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 35/86 [D loss: 0.6723583042621613, acc.: 58.54%] [G loss: 0.7502641677856445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 36/86 [D loss: 0.6719112992286682, acc.: 58.15%] [G loss: 0.7625112533569336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 37/86 [D loss: 0.6737808585166931, acc.: 58.11%] [G loss: 0.7626619338989258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 38/86 [D loss: 0.6766064465045929, acc.: 57.57%] [G loss: 0.7557779550552368]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 39/86 [D loss: 0.6715591549873352, acc.: 57.32%] [G loss: 0.7676220536231995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 40/86 [D loss: 0.6687049269676208, acc.: 60.50%] [G loss: 0.7573570013046265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 41/86 [D loss: 0.6726967096328735, acc.: 58.40%] [G loss: 0.7532833814620972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 42/86 [D loss: 0.6679741442203522, acc.: 58.69%] [G loss: 0.7566996216773987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 43/86 [D loss: 0.6718363761901855, acc.: 58.94%] [G loss: 0.7566099762916565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 44/86 [D loss: 0.6742812991142273, acc.: 58.69%] [G loss: 0.7567339539527893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 45/86 [D loss: 0.6677295863628387, acc.: 59.67%] [G loss: 0.7506642937660217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 46/86 [D loss: 0.678157240152359, acc.: 57.28%] [G loss: 0.7524625658988953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 47/86 [D loss: 0.6714983284473419, acc.: 58.98%] [G loss: 0.7566862106323242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 48/86 [D loss: 0.6765369772911072, acc.: 59.52%] [G loss: 0.7555899620056152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 49/86 [D loss: 0.6757311224937439, acc.: 56.49%] [G loss: 0.7534399032592773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 50/86 [D loss: 0.6758658289909363, acc.: 56.69%] [G loss: 0.7614860534667969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 51/86 [D loss: 0.672875851392746, acc.: 57.91%] [G loss: 0.7621738314628601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 52/86 [D loss: 0.6709632277488708, acc.: 59.13%] [G loss: 0.7518024444580078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 53/86 [D loss: 0.6729130446910858, acc.: 58.45%] [G loss: 0.7610807418823242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 54/86 [D loss: 0.6715536117553711, acc.: 58.30%] [G loss: 0.7570627331733704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 55/86 [D loss: 0.6728377342224121, acc.: 58.30%] [G loss: 0.7559741735458374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 56/86 [D loss: 0.6762223839759827, acc.: 57.28%] [G loss: 0.756948709487915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 57/86 [D loss: 0.6721685230731964, acc.: 58.59%] [G loss: 0.7590104937553406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 58/86 [D loss: 0.6709442138671875, acc.: 59.47%] [G loss: 0.7619736790657043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 59/86 [D loss: 0.674560546875, acc.: 57.86%] [G loss: 0.7596863508224487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 60/86 [D loss: 0.6726707816123962, acc.: 58.74%] [G loss: 0.7548877596855164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 61/86 [D loss: 0.6758637130260468, acc.: 57.57%] [G loss: 0.7616474628448486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 62/86 [D loss: 0.6787323951721191, acc.: 57.08%] [G loss: 0.7492095828056335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 63/86 [D loss: 0.6761805415153503, acc.: 57.08%] [G loss: 0.755878210067749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 64/86 [D loss: 0.6681559383869171, acc.: 60.69%] [G loss: 0.7623898983001709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 65/86 [D loss: 0.677393913269043, acc.: 57.91%] [G loss: 0.7557695508003235]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 66/86 [D loss: 0.6724157929420471, acc.: 57.42%] [G loss: 0.765406608581543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 67/86 [D loss: 0.6786719560623169, acc.: 56.30%] [G loss: 0.7539198994636536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 68/86 [D loss: 0.6719228625297546, acc.: 57.28%] [G loss: 0.7602804899215698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 69/86 [D loss: 0.6758770644664764, acc.: 57.81%] [G loss: 0.7399807572364807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 70/86 [D loss: 0.6719934344291687, acc.: 58.30%] [G loss: 0.7642455101013184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 71/86 [D loss: 0.6719489693641663, acc.: 59.08%] [G loss: 0.7501717805862427]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 133/200, Batch 72/86 [D loss: 0.6743972599506378, acc.: 58.50%] [G loss: 0.7624150514602661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 73/86 [D loss: 0.6682727634906769, acc.: 58.64%] [G loss: 0.7508540749549866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 74/86 [D loss: 0.6771092116832733, acc.: 57.23%] [G loss: 0.7574130892753601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 75/86 [D loss: 0.6728635132312775, acc.: 58.50%] [G loss: 0.7568525075912476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 76/86 [D loss: 0.6766423285007477, acc.: 56.54%] [G loss: 0.7604818344116211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 77/86 [D loss: 0.6704823970794678, acc.: 60.11%] [G loss: 0.7530022859573364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 78/86 [D loss: 0.6752553284168243, acc.: 58.30%] [G loss: 0.7643643617630005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 79/86 [D loss: 0.6688730716705322, acc.: 59.86%] [G loss: 0.7589147090911865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 80/86 [D loss: 0.6740735769271851, acc.: 57.37%] [G loss: 0.7528061866760254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 81/86 [D loss: 0.6700267493724823, acc.: 59.28%] [G loss: 0.7529360055923462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 82/86 [D loss: 0.6761654913425446, acc.: 57.37%] [G loss: 0.7648819088935852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 83/86 [D loss: 0.6735451221466064, acc.: 56.79%] [G loss: 0.7515770196914673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 84/86 [D loss: 0.6749858260154724, acc.: 58.06%] [G loss: 0.7580757737159729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 85/86 [D loss: 0.6768782138824463, acc.: 56.30%] [G loss: 0.7586297988891602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 133/200, Batch 86/86 [D loss: 0.6747403144836426, acc.: 56.98%] [G loss: 0.7544991970062256]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 1/86 [D loss: 0.6718583106994629, acc.: 58.79%] [G loss: 0.7592675685882568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 2/86 [D loss: 0.6714954972267151, acc.: 58.54%] [G loss: 0.7539224028587341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 3/86 [D loss: 0.6758161187171936, acc.: 57.96%] [G loss: 0.764846920967102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 4/86 [D loss: 0.6704716384410858, acc.: 59.47%] [G loss: 0.7504812479019165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 5/86 [D loss: 0.6752385497093201, acc.: 59.13%] [G loss: 0.7604256272315979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 6/86 [D loss: 0.676482617855072, acc.: 56.45%] [G loss: 0.7549145221710205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 7/86 [D loss: 0.6755819022655487, acc.: 57.32%] [G loss: 0.7487477660179138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 8/86 [D loss: 0.6743679642677307, acc.: 57.71%] [G loss: 0.7593927383422852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 9/86 [D loss: 0.6727312505245209, acc.: 57.86%] [G loss: 0.7569899559020996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 10/86 [D loss: 0.6693751811981201, acc.: 59.47%] [G loss: 0.7544999122619629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 11/86 [D loss: 0.6732567548751831, acc.: 58.35%] [G loss: 0.7554138898849487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 12/86 [D loss: 0.6685643196105957, acc.: 59.86%] [G loss: 0.7596588134765625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 13/86 [D loss: 0.6716582775115967, acc.: 58.94%] [G loss: 0.7584918737411499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 14/86 [D loss: 0.6703533232212067, acc.: 58.98%] [G loss: 0.7540051341056824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 15/86 [D loss: 0.6702246367931366, acc.: 58.84%] [G loss: 0.7535197138786316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 16/86 [D loss: 0.6695225536823273, acc.: 60.30%] [G loss: 0.7600851655006409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 17/86 [D loss: 0.6728695631027222, acc.: 58.35%] [G loss: 0.7529571652412415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 18/86 [D loss: 0.6682119071483612, acc.: 60.45%] [G loss: 0.7540301084518433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 19/86 [D loss: 0.6791585683822632, acc.: 55.57%] [G loss: 0.7511186003684998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 20/86 [D loss: 0.6740713119506836, acc.: 57.42%] [G loss: 0.7604609727859497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 21/86 [D loss: 0.672195553779602, acc.: 59.42%] [G loss: 0.7620412707328796]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 22/86 [D loss: 0.6718115210533142, acc.: 60.35%] [G loss: 0.768211305141449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 23/86 [D loss: 0.6715493500232697, acc.: 58.01%] [G loss: 0.7573203444480896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 24/86 [D loss: 0.675021767616272, acc.: 56.93%] [G loss: 0.761524498462677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 25/86 [D loss: 0.6707499623298645, acc.: 58.59%] [G loss: 0.7608502507209778]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 26/86 [D loss: 0.672190397977829, acc.: 58.06%] [G loss: 0.7592693567276001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 27/86 [D loss: 0.6720578074455261, acc.: 59.18%] [G loss: 0.7564975023269653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 28/86 [D loss: 0.6736223697662354, acc.: 57.47%] [G loss: 0.7498701214790344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 29/86 [D loss: 0.6778346598148346, acc.: 56.59%] [G loss: 0.7577701807022095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 30/86 [D loss: 0.6705255508422852, acc.: 58.40%] [G loss: 0.7555676698684692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 31/86 [D loss: 0.6696965992450714, acc.: 60.06%] [G loss: 0.7627425193786621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 32/86 [D loss: 0.6743704676628113, acc.: 57.57%] [G loss: 0.7527257800102234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 33/86 [D loss: 0.6706385314464569, acc.: 59.03%] [G loss: 0.7584728002548218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 34/86 [D loss: 0.6758662164211273, acc.: 57.37%] [G loss: 0.7541830539703369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 35/86 [D loss: 0.6779979467391968, acc.: 56.88%] [G loss: 0.7563528418540955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 36/86 [D loss: 0.670584499835968, acc.: 58.74%] [G loss: 0.7486713528633118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 37/86 [D loss: 0.6737954020500183, acc.: 59.03%] [G loss: 0.7684470415115356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 38/86 [D loss: 0.6671460270881653, acc.: 60.16%] [G loss: 0.7524927258491516]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 39/86 [D loss: 0.6718439161777496, acc.: 58.64%] [G loss: 0.7523349523544312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 40/86 [D loss: 0.6741494238376617, acc.: 57.52%] [G loss: 0.7548859715461731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 41/86 [D loss: 0.6681714653968811, acc.: 59.96%] [G loss: 0.7562139630317688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 42/86 [D loss: 0.6753600835800171, acc.: 57.28%] [G loss: 0.7589625120162964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 43/86 [D loss: 0.6711680293083191, acc.: 59.08%] [G loss: 0.7587839961051941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 44/86 [D loss: 0.6751730442047119, acc.: 57.42%] [G loss: 0.7588745951652527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 45/86 [D loss: 0.6733254790306091, acc.: 58.59%] [G loss: 0.760626494884491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 46/86 [D loss: 0.6762526631355286, acc.: 57.52%] [G loss: 0.7569816708564758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 47/86 [D loss: 0.6748644709587097, acc.: 58.06%] [G loss: 0.756722092628479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 48/86 [D loss: 0.6734228730201721, acc.: 58.25%] [G loss: 0.7609726190567017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 49/86 [D loss: 0.6688657999038696, acc.: 59.57%] [G loss: 0.7537296414375305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 50/86 [D loss: 0.6740792989730835, acc.: 58.06%] [G loss: 0.7590140104293823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 51/86 [D loss: 0.6738787591457367, acc.: 58.74%] [G loss: 0.7558282017707825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 52/86 [D loss: 0.6707179546356201, acc.: 59.18%] [G loss: 0.7554539442062378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 53/86 [D loss: 0.674702525138855, acc.: 57.37%] [G loss: 0.7588152885437012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 54/86 [D loss: 0.6774235665798187, acc.: 56.74%] [G loss: 0.7614557147026062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 55/86 [D loss: 0.6703011989593506, acc.: 59.67%] [G loss: 0.764283299446106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 56/86 [D loss: 0.6708502173423767, acc.: 57.96%] [G loss: 0.7571336627006531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 57/86 [D loss: 0.6744380295276642, acc.: 57.28%] [G loss: 0.7696660757064819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 58/86 [D loss: 0.6707927286624908, acc.: 59.42%] [G loss: 0.7562745809555054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 59/86 [D loss: 0.6709869801998138, acc.: 58.64%] [G loss: 0.7629730701446533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 60/86 [D loss: 0.6717069745063782, acc.: 58.84%] [G loss: 0.7622231245040894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 61/86 [D loss: 0.6740598678588867, acc.: 57.23%] [G loss: 0.764885663986206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 62/86 [D loss: 0.6691917479038239, acc.: 59.47%] [G loss: 0.7580123543739319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 63/86 [D loss: 0.672247976064682, acc.: 58.84%] [G loss: 0.7529913187026978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 64/86 [D loss: 0.6719034612178802, acc.: 58.35%] [G loss: 0.7583787441253662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 65/86 [D loss: 0.677686333656311, acc.: 57.32%] [G loss: 0.7666641473770142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 66/86 [D loss: 0.6764043271541595, acc.: 58.01%] [G loss: 0.7575781345367432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 67/86 [D loss: 0.6714094579219818, acc.: 58.54%] [G loss: 0.7579949498176575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 68/86 [D loss: 0.6729799211025238, acc.: 58.79%] [G loss: 0.7694228887557983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 69/86 [D loss: 0.6710016429424286, acc.: 58.89%] [G loss: 0.7573415637016296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 70/86 [D loss: 0.6746046543121338, acc.: 58.25%] [G loss: 0.7555667757987976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 71/86 [D loss: 0.6710645854473114, acc.: 58.98%] [G loss: 0.74748694896698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 72/86 [D loss: 0.6669396162033081, acc.: 60.16%] [G loss: 0.7642165422439575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 73/86 [D loss: 0.6721009910106659, acc.: 58.25%] [G loss: 0.7583394050598145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 74/86 [D loss: 0.6710508167743683, acc.: 59.08%] [G loss: 0.7616825699806213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 75/86 [D loss: 0.6722478270530701, acc.: 56.93%] [G loss: 0.7596133947372437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 76/86 [D loss: 0.6737996339797974, acc.: 56.88%] [G loss: 0.7570517063140869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 77/86 [D loss: 0.6758264601230621, acc.: 58.30%] [G loss: 0.7593740224838257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 78/86 [D loss: 0.6721459031105042, acc.: 57.91%] [G loss: 0.7635067105293274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 79/86 [D loss: 0.6703353822231293, acc.: 59.72%] [G loss: 0.7590669989585876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 80/86 [D loss: 0.6746674478054047, acc.: 57.67%] [G loss: 0.7559472322463989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 81/86 [D loss: 0.670021116733551, acc.: 58.54%] [G loss: 0.7549889087677002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 82/86 [D loss: 0.671343207359314, acc.: 59.03%] [G loss: 0.7506493330001831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 83/86 [D loss: 0.6704288125038147, acc.: 58.84%] [G loss: 0.7552784085273743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 84/86 [D loss: 0.6739521622657776, acc.: 58.01%] [G loss: 0.7607457041740417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 85/86 [D loss: 0.6715764403343201, acc.: 57.96%] [G loss: 0.7582794427871704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 134/200, Batch 86/86 [D loss: 0.6755886375904083, acc.: 58.30%] [G loss: 0.7490087747573853]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 1/86 [D loss: 0.6749399602413177, acc.: 57.91%] [G loss: 0.7523795962333679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 2/86 [D loss: 0.6757911443710327, acc.: 57.52%] [G loss: 0.7564935684204102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 3/86 [D loss: 0.6698300838470459, acc.: 59.72%] [G loss: 0.7540945410728455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 4/86 [D loss: 0.6697716116905212, acc.: 59.18%] [G loss: 0.7524344325065613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 5/86 [D loss: 0.6775381565093994, acc.: 56.74%] [G loss: 0.7560281753540039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 6/86 [D loss: 0.6729409098625183, acc.: 58.25%] [G loss: 0.7535976767539978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 7/86 [D loss: 0.6739888489246368, acc.: 58.01%] [G loss: 0.7568657994270325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 8/86 [D loss: 0.6697177290916443, acc.: 59.33%] [G loss: 0.755118727684021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 9/86 [D loss: 0.6767401099205017, acc.: 57.23%] [G loss: 0.761581301689148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 10/86 [D loss: 0.6719101369380951, acc.: 58.40%] [G loss: 0.761635959148407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 11/86 [D loss: 0.6789640188217163, acc.: 56.54%] [G loss: 0.7580074071884155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 12/86 [D loss: 0.6725830137729645, acc.: 57.81%] [G loss: 0.7546793818473816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 13/86 [D loss: 0.6723269522190094, acc.: 56.98%] [G loss: 0.7630482912063599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 14/86 [D loss: 0.6684410572052002, acc.: 58.74%] [G loss: 0.7703351378440857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 15/86 [D loss: 0.6714734137058258, acc.: 59.81%] [G loss: 0.7648134231567383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 16/86 [D loss: 0.6739496290683746, acc.: 58.74%] [G loss: 0.7610379457473755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 17/86 [D loss: 0.6742213368415833, acc.: 58.54%] [G loss: 0.7547364234924316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 18/86 [D loss: 0.674123078584671, acc.: 58.59%] [G loss: 0.7534416317939758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 19/86 [D loss: 0.6810480952262878, acc.: 54.69%] [G loss: 0.7692496180534363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 20/86 [D loss: 0.6706981956958771, acc.: 59.18%] [G loss: 0.7550228238105774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 21/86 [D loss: 0.6787727177143097, acc.: 55.91%] [G loss: 0.7579018473625183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 22/86 [D loss: 0.6690030694007874, acc.: 58.25%] [G loss: 0.7634849548339844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 23/86 [D loss: 0.6769379675388336, acc.: 58.30%] [G loss: 0.7656285166740417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 24/86 [D loss: 0.6700689196586609, acc.: 60.16%] [G loss: 0.7577190399169922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 25/86 [D loss: 0.6725559830665588, acc.: 58.84%] [G loss: 0.7588269114494324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 26/86 [D loss: 0.6735870540142059, acc.: 57.47%] [G loss: 0.7565709948539734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 27/86 [D loss: 0.6748495995998383, acc.: 57.91%] [G loss: 0.753201425075531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 28/86 [D loss: 0.6734729707241058, acc.: 59.28%] [G loss: 0.7573981285095215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 29/86 [D loss: 0.676011323928833, acc.: 56.64%] [G loss: 0.7498618364334106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 30/86 [D loss: 0.6657042801380157, acc.: 61.38%] [G loss: 0.7575475573539734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 31/86 [D loss: 0.6736109852790833, acc.: 58.50%] [G loss: 0.7609075307846069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 32/86 [D loss: 0.6706500947475433, acc.: 58.79%] [G loss: 0.7566254734992981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 33/86 [D loss: 0.6770678162574768, acc.: 56.15%] [G loss: 0.7496020793914795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 34/86 [D loss: 0.6735597550868988, acc.: 57.86%] [G loss: 0.7593846321105957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 35/86 [D loss: 0.6724481582641602, acc.: 57.96%] [G loss: 0.7531256675720215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 36/86 [D loss: 0.6731898784637451, acc.: 58.30%] [G loss: 0.7659662365913391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 37/86 [D loss: 0.6728857457637787, acc.: 58.84%] [G loss: 0.7494591474533081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 38/86 [D loss: 0.68025803565979, acc.: 55.22%] [G loss: 0.7638811469078064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 39/86 [D loss: 0.6719589233398438, acc.: 58.50%] [G loss: 0.7567080855369568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 40/86 [D loss: 0.6743333041667938, acc.: 58.01%] [G loss: 0.7653019428253174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 41/86 [D loss: 0.6759238243103027, acc.: 56.79%] [G loss: 0.7522428035736084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 42/86 [D loss: 0.6790297627449036, acc.: 57.03%] [G loss: 0.7609145641326904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 43/86 [D loss: 0.6683088541030884, acc.: 58.94%] [G loss: 0.7511472702026367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 44/86 [D loss: 0.6807841658592224, acc.: 55.08%] [G loss: 0.7685398459434509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 45/86 [D loss: 0.6689513623714447, acc.: 59.52%] [G loss: 0.7482510209083557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 46/86 [D loss: 0.6779017746448517, acc.: 56.59%] [G loss: 0.7582877278327942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 47/86 [D loss: 0.6730261147022247, acc.: 57.28%] [G loss: 0.7603123188018799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 48/86 [D loss: 0.6787610352039337, acc.: 56.20%] [G loss: 0.758526623249054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 49/86 [D loss: 0.667335033416748, acc.: 60.21%] [G loss: 0.7534493207931519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 50/86 [D loss: 0.6788321137428284, acc.: 55.81%] [G loss: 0.7484880685806274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 51/86 [D loss: 0.6745277643203735, acc.: 57.86%] [G loss: 0.7642159461975098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 52/86 [D loss: 0.6735593378543854, acc.: 57.37%] [G loss: 0.7542724609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 53/86 [D loss: 0.6749419867992401, acc.: 58.01%] [G loss: 0.7783545851707458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 54/86 [D loss: 0.6682658195495605, acc.: 60.74%] [G loss: 0.7518447637557983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 55/86 [D loss: 0.6755749583244324, acc.: 56.79%] [G loss: 0.7553607225418091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 56/86 [D loss: 0.6691137552261353, acc.: 59.86%] [G loss: 0.7538980841636658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 57/86 [D loss: 0.6722950637340546, acc.: 57.96%] [G loss: 0.7603868842124939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 58/86 [D loss: 0.6716635227203369, acc.: 58.59%] [G loss: 0.7593428492546082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 59/86 [D loss: 0.6682144701480865, acc.: 60.45%] [G loss: 0.7562054395675659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 60/86 [D loss: 0.67413529753685, acc.: 58.20%] [G loss: 0.75846266746521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 61/86 [D loss: 0.6761863529682159, acc.: 57.13%] [G loss: 0.755898118019104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 62/86 [D loss: 0.6755458116531372, acc.: 57.08%] [G loss: 0.7551609873771667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 63/86 [D loss: 0.6710553169250488, acc.: 58.30%] [G loss: 0.7645403742790222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 64/86 [D loss: 0.6664987802505493, acc.: 59.47%] [G loss: 0.7576913833618164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 65/86 [D loss: 0.673795610666275, acc.: 58.11%] [G loss: 0.7635176777839661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 66/86 [D loss: 0.6744599640369415, acc.: 57.71%] [G loss: 0.7618557214736938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 67/86 [D loss: 0.6708371639251709, acc.: 59.72%] [G loss: 0.7633899450302124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 68/86 [D loss: 0.6771418154239655, acc.: 57.13%] [G loss: 0.7640100121498108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 69/86 [D loss: 0.6733432412147522, acc.: 59.28%] [G loss: 0.7665492296218872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 70/86 [D loss: 0.6670902371406555, acc.: 60.16%] [G loss: 0.7666435837745667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 71/86 [D loss: 0.6747541725635529, acc.: 57.91%] [G loss: 0.7644984126091003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 72/86 [D loss: 0.6651526987552643, acc.: 61.28%] [G loss: 0.7621161937713623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 73/86 [D loss: 0.6769945919513702, acc.: 59.13%] [G loss: 0.7565033435821533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 74/86 [D loss: 0.6723974645137787, acc.: 56.84%] [G loss: 0.7594789266586304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 75/86 [D loss: 0.6710936725139618, acc.: 59.47%] [G loss: 0.7677066326141357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 76/86 [D loss: 0.6746372580528259, acc.: 57.47%] [G loss: 0.7548525333404541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 77/86 [D loss: 0.6752134263515472, acc.: 57.47%] [G loss: 0.7551336884498596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 78/86 [D loss: 0.669584721326828, acc.: 60.01%] [G loss: 0.7676503658294678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 79/86 [D loss: 0.6771319210529327, acc.: 56.49%] [G loss: 0.7650139331817627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 80/86 [D loss: 0.6703204214572906, acc.: 60.21%] [G loss: 0.770141065120697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 81/86 [D loss: 0.6721539497375488, acc.: 59.81%] [G loss: 0.7517310380935669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 82/86 [D loss: 0.6778759658336639, acc.: 56.20%] [G loss: 0.7550540566444397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 83/86 [D loss: 0.6707333028316498, acc.: 57.37%] [G loss: 0.7593527436256409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 84/86 [D loss: 0.6775286793708801, acc.: 57.13%] [G loss: 0.7603232860565186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 85/86 [D loss: 0.6731905937194824, acc.: 58.20%] [G loss: 0.7524935007095337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 135/200, Batch 86/86 [D loss: 0.6781516075134277, acc.: 57.18%] [G loss: 0.7645715475082397]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 1/86 [D loss: 0.6703032553195953, acc.: 57.96%] [G loss: 0.7565970420837402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 2/86 [D loss: 0.6751583814620972, acc.: 57.23%] [G loss: 0.7638533115386963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 3/86 [D loss: 0.6664826571941376, acc.: 59.33%] [G loss: 0.7510932087898254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 4/86 [D loss: 0.6774016320705414, acc.: 56.40%] [G loss: 0.7540901899337769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 5/86 [D loss: 0.6755297780036926, acc.: 57.28%] [G loss: 0.7565932869911194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 6/86 [D loss: 0.6757998466491699, acc.: 57.91%] [G loss: 0.7614709138870239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 7/86 [D loss: 0.6708582639694214, acc.: 58.50%] [G loss: 0.7650976777076721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 8/86 [D loss: 0.674828052520752, acc.: 57.76%] [G loss: 0.7608887553215027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 9/86 [D loss: 0.6721900403499603, acc.: 57.67%] [G loss: 0.7633272409439087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 10/86 [D loss: 0.6754457056522369, acc.: 57.86%] [G loss: 0.7617666125297546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 11/86 [D loss: 0.6749411225318909, acc.: 57.86%] [G loss: 0.76207035779953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 12/86 [D loss: 0.6765785217285156, acc.: 57.03%] [G loss: 0.7593023777008057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 13/86 [D loss: 0.672889232635498, acc.: 57.47%] [G loss: 0.7562615871429443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 14/86 [D loss: 0.6725303530693054, acc.: 58.59%] [G loss: 0.7634178996086121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 15/86 [D loss: 0.6752653419971466, acc.: 57.81%] [G loss: 0.7609000205993652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 16/86 [D loss: 0.6715617179870605, acc.: 58.54%] [G loss: 0.7634667158126831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 17/86 [D loss: 0.6701668798923492, acc.: 58.74%] [G loss: 0.7527179718017578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 18/86 [D loss: 0.6714965403079987, acc.: 58.01%] [G loss: 0.7570330500602722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 19/86 [D loss: 0.6718342900276184, acc.: 58.69%] [G loss: 0.7571888566017151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 20/86 [D loss: 0.6702379286289215, acc.: 59.57%] [G loss: 0.7596733570098877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 21/86 [D loss: 0.6714089214801788, acc.: 58.64%] [G loss: 0.7584668397903442]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 22/86 [D loss: 0.671943873167038, acc.: 58.06%] [G loss: 0.7636027932167053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 23/86 [D loss: 0.6749219596385956, acc.: 58.69%] [G loss: 0.7533032894134521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 24/86 [D loss: 0.6786519587039948, acc.: 57.37%] [G loss: 0.76225346326828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 25/86 [D loss: 0.6706845760345459, acc.: 58.79%] [G loss: 0.7554491758346558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 26/86 [D loss: 0.6767006814479828, acc.: 57.42%] [G loss: 0.7563413381576538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 27/86 [D loss: 0.6723509728908539, acc.: 58.54%] [G loss: 0.7575423717498779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 28/86 [D loss: 0.6763605773448944, acc.: 56.64%] [G loss: 0.7610476613044739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 29/86 [D loss: 0.666913628578186, acc.: 59.67%] [G loss: 0.7499300241470337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 30/86 [D loss: 0.6761559844017029, acc.: 56.54%] [G loss: 0.7580893039703369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 31/86 [D loss: 0.675511360168457, acc.: 56.88%] [G loss: 0.7595165967941284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 32/86 [D loss: 0.6745652556419373, acc.: 57.28%] [G loss: 0.7512640953063965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 33/86 [D loss: 0.6692013442516327, acc.: 59.67%] [G loss: 0.7596710920333862]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 34/86 [D loss: 0.6777874827384949, acc.: 56.05%] [G loss: 0.7554216384887695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 35/86 [D loss: 0.673035740852356, acc.: 58.69%] [G loss: 0.7603539228439331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 36/86 [D loss: 0.6691930890083313, acc.: 58.40%] [G loss: 0.7485506534576416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 37/86 [D loss: 0.6787701547145844, acc.: 56.49%] [G loss: 0.7689300179481506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 38/86 [D loss: 0.6760926842689514, acc.: 56.10%] [G loss: 0.7470433712005615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 39/86 [D loss: 0.6862519085407257, acc.: 53.27%] [G loss: 0.7567088603973389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 40/86 [D loss: 0.6668963134288788, acc.: 60.40%] [G loss: 0.752834141254425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 41/86 [D loss: 0.6823984682559967, acc.: 54.98%] [G loss: 0.7756946682929993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 42/86 [D loss: 0.6701656877994537, acc.: 58.89%] [G loss: 0.7477637529373169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 43/86 [D loss: 0.6831705570220947, acc.: 54.88%] [G loss: 0.7603980898857117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 44/86 [D loss: 0.6693121194839478, acc.: 59.77%] [G loss: 0.756499171257019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 45/86 [D loss: 0.6897661089897156, acc.: 53.37%] [G loss: 0.766477108001709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 46/86 [D loss: 0.6678332686424255, acc.: 60.11%] [G loss: 0.7504236698150635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 47/86 [D loss: 0.6792523562908173, acc.: 55.08%] [G loss: 0.7483187317848206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 48/86 [D loss: 0.6663475632667542, acc.: 59.91%] [G loss: 0.7610913515090942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 49/86 [D loss: 0.6732929050922394, acc.: 59.38%] [G loss: 0.7544437050819397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 50/86 [D loss: 0.676763504743576, acc.: 56.54%] [G loss: 0.768576979637146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 51/86 [D loss: 0.6703681647777557, acc.: 57.62%] [G loss: 0.7506341338157654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 52/86 [D loss: 0.6761834621429443, acc.: 56.93%] [G loss: 0.7590232491493225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 53/86 [D loss: 0.6678584516048431, acc.: 59.67%] [G loss: 0.7533921599388123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 54/86 [D loss: 0.6812115609645844, acc.: 55.71%] [G loss: 0.7651493549346924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 55/86 [D loss: 0.669063538312912, acc.: 59.13%] [G loss: 0.757376492023468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 56/86 [D loss: 0.676190197467804, acc.: 56.79%] [G loss: 0.7580379247665405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 57/86 [D loss: 0.6683711409568787, acc.: 59.86%] [G loss: 0.7565128803253174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 58/86 [D loss: 0.6714220941066742, acc.: 58.11%] [G loss: 0.7666544914245605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 59/86 [D loss: 0.6682734191417694, acc.: 60.50%] [G loss: 0.7673414945602417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 60/86 [D loss: 0.6747485101222992, acc.: 58.06%] [G loss: 0.7626640796661377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 61/86 [D loss: 0.6748498976230621, acc.: 57.13%] [G loss: 0.7603865265846252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 62/86 [D loss: 0.6717880070209503, acc.: 59.33%] [G loss: 0.7586721181869507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 63/86 [D loss: 0.6738442182540894, acc.: 57.57%] [G loss: 0.7588473558425903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 64/86 [D loss: 0.6658651828765869, acc.: 59.62%] [G loss: 0.7562522292137146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 65/86 [D loss: 0.6755393445491791, acc.: 57.37%] [G loss: 0.7636386156082153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 66/86 [D loss: 0.6716557741165161, acc.: 58.59%] [G loss: 0.7548147439956665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 67/86 [D loss: 0.6699059307575226, acc.: 60.11%] [G loss: 0.7642475366592407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 68/86 [D loss: 0.6675485372543335, acc.: 59.57%] [G loss: 0.7580657005310059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 69/86 [D loss: 0.6739921569824219, acc.: 57.71%] [G loss: 0.7699296474456787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 70/86 [D loss: 0.6671239733695984, acc.: 59.86%] [G loss: 0.7664476633071899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 71/86 [D loss: 0.6720798015594482, acc.: 58.74%] [G loss: 0.7554877996444702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 72/86 [D loss: 0.6738843023777008, acc.: 57.03%] [G loss: 0.7623398900032043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 73/86 [D loss: 0.6758130788803101, acc.: 56.15%] [G loss: 0.7635475993156433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 74/86 [D loss: 0.6708769798278809, acc.: 59.62%] [G loss: 0.7627519369125366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 75/86 [D loss: 0.6692269444465637, acc.: 59.38%] [G loss: 0.7554206252098083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 76/86 [D loss: 0.672642320394516, acc.: 57.71%] [G loss: 0.7616164684295654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 77/86 [D loss: 0.6776539385318756, acc.: 57.62%] [G loss: 0.7505693435668945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 78/86 [D loss: 0.6768068671226501, acc.: 56.30%] [G loss: 0.7650973796844482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 79/86 [D loss: 0.6736717820167542, acc.: 57.67%] [G loss: 0.7560324668884277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 80/86 [D loss: 0.6731401979923248, acc.: 58.50%] [G loss: 0.7713606357574463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 81/86 [D loss: 0.6708799302577972, acc.: 59.33%] [G loss: 0.756456732749939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 82/86 [D loss: 0.6675088107585907, acc.: 59.67%] [G loss: 0.7588037252426147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 83/86 [D loss: 0.673533707857132, acc.: 58.98%] [G loss: 0.7612579464912415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 84/86 [D loss: 0.6712547838687897, acc.: 57.67%] [G loss: 0.7544155120849609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 85/86 [D loss: 0.6759557723999023, acc.: 56.84%] [G loss: 0.7565407752990723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 136/200, Batch 86/86 [D loss: 0.6723742485046387, acc.: 58.89%] [G loss: 0.7609266042709351]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 1/86 [D loss: 0.6701694428920746, acc.: 59.42%] [G loss: 0.7602012753486633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 2/86 [D loss: 0.6740770936012268, acc.: 57.52%] [G loss: 0.7618075609207153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 3/86 [D loss: 0.6751535832881927, acc.: 58.20%] [G loss: 0.7488951683044434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 4/86 [D loss: 0.6727592349052429, acc.: 57.91%] [G loss: 0.771839439868927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 5/86 [D loss: 0.6742912828922272, acc.: 58.30%] [G loss: 0.7562351226806641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 6/86 [D loss: 0.6775909066200256, acc.: 55.13%] [G loss: 0.7592648267745972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 7/86 [D loss: 0.6686468422412872, acc.: 60.35%] [G loss: 0.7589151263237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 8/86 [D loss: 0.6713107228279114, acc.: 59.18%] [G loss: 0.778256893157959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 9/86 [D loss: 0.6754520237445831, acc.: 57.08%] [G loss: 0.7602999210357666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 10/86 [D loss: 0.6762964725494385, acc.: 58.20%] [G loss: 0.7537825703620911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 11/86 [D loss: 0.6712191700935364, acc.: 59.08%] [G loss: 0.7647995948791504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 12/86 [D loss: 0.6719523668289185, acc.: 58.20%] [G loss: 0.7653771638870239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 13/86 [D loss: 0.6687853932380676, acc.: 59.08%] [G loss: 0.7613348364830017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 14/86 [D loss: 0.6727626621723175, acc.: 58.50%] [G loss: 0.7501453161239624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 15/86 [D loss: 0.6706891357898712, acc.: 58.40%] [G loss: 0.7590704560279846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 16/86 [D loss: 0.6699367165565491, acc.: 59.57%] [G loss: 0.7620068788528442]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 17/86 [D loss: 0.6703008413314819, acc.: 59.28%] [G loss: 0.7646045684814453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 18/86 [D loss: 0.6668982207775116, acc.: 59.62%] [G loss: 0.7566016316413879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 19/86 [D loss: 0.6734233796596527, acc.: 58.45%] [G loss: 0.7611549496650696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 20/86 [D loss: 0.6754968762397766, acc.: 56.54%] [G loss: 0.7606945633888245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 21/86 [D loss: 0.6731625497341156, acc.: 57.71%] [G loss: 0.7602975964546204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 22/86 [D loss: 0.6693392395973206, acc.: 59.72%] [G loss: 0.7565374970436096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 23/86 [D loss: 0.6689376831054688, acc.: 60.11%] [G loss: 0.7553566694259644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 24/86 [D loss: 0.6703192591667175, acc.: 58.30%] [G loss: 0.7614156603813171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 25/86 [D loss: 0.6707063019275665, acc.: 57.96%] [G loss: 0.7568883299827576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 26/86 [D loss: 0.6667642593383789, acc.: 60.50%] [G loss: 0.7613924145698547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 27/86 [D loss: 0.6742574572563171, acc.: 57.91%] [G loss: 0.7604149580001831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 28/86 [D loss: 0.6678763926029205, acc.: 59.33%] [G loss: 0.7580066323280334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 29/86 [D loss: 0.6688006520271301, acc.: 60.60%] [G loss: 0.7636138796806335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 30/86 [D loss: 0.6757081747055054, acc.: 58.35%] [G loss: 0.7532987594604492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 31/86 [D loss: 0.6725796163082123, acc.: 57.81%] [G loss: 0.7580610513687134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 32/86 [D loss: 0.6703292429447174, acc.: 58.89%] [G loss: 0.7641938924789429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 33/86 [D loss: 0.6757571697235107, acc.: 56.84%] [G loss: 0.7649427056312561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 34/86 [D loss: 0.6716979146003723, acc.: 59.38%] [G loss: 0.7615461945533752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 35/86 [D loss: 0.671856164932251, acc.: 57.37%] [G loss: 0.7505295276641846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 36/86 [D loss: 0.6710277199745178, acc.: 58.11%] [G loss: 0.760135293006897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 37/86 [D loss: 0.6678376495838165, acc.: 59.57%] [G loss: 0.760015606880188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 38/86 [D loss: 0.6732199490070343, acc.: 58.15%] [G loss: 0.7607221007347107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 39/86 [D loss: 0.6729885041713715, acc.: 57.47%] [G loss: 0.7589969635009766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 40/86 [D loss: 0.6729074120521545, acc.: 57.91%] [G loss: 0.7573116421699524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 41/86 [D loss: 0.6707053780555725, acc.: 60.21%] [G loss: 0.7653611898422241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 42/86 [D loss: 0.6734812557697296, acc.: 57.96%] [G loss: 0.7649482488632202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 43/86 [D loss: 0.6699850559234619, acc.: 59.38%] [G loss: 0.762948751449585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 44/86 [D loss: 0.6660871505737305, acc.: 60.16%] [G loss: 0.7614532113075256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 45/86 [D loss: 0.6716221868991852, acc.: 59.28%] [G loss: 0.7553791999816895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 46/86 [D loss: 0.6731893420219421, acc.: 58.64%] [G loss: 0.7528647184371948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 47/86 [D loss: 0.6699567139148712, acc.: 59.62%] [G loss: 0.7639511227607727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 48/86 [D loss: 0.6708867847919464, acc.: 58.84%] [G loss: 0.7593536972999573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 49/86 [D loss: 0.6747176051139832, acc.: 57.86%] [G loss: 0.7631394863128662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 50/86 [D loss: 0.667116105556488, acc.: 59.47%] [G loss: 0.7614558935165405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 51/86 [D loss: 0.6716685891151428, acc.: 58.59%] [G loss: 0.7609363794326782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 52/86 [D loss: 0.6753714084625244, acc.: 57.52%] [G loss: 0.7641492486000061]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 53/86 [D loss: 0.6706923842430115, acc.: 60.40%] [G loss: 0.7590722441673279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 54/86 [D loss: 0.669502317905426, acc.: 60.45%] [G loss: 0.7620055079460144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 55/86 [D loss: 0.671833723783493, acc.: 58.11%] [G loss: 0.7516093254089355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 56/86 [D loss: 0.6703336238861084, acc.: 58.89%] [G loss: 0.7573873996734619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 57/86 [D loss: 0.674671471118927, acc.: 56.98%] [G loss: 0.7580506205558777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 58/86 [D loss: 0.6692299842834473, acc.: 57.71%] [G loss: 0.7571332454681396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 59/86 [D loss: 0.670693039894104, acc.: 58.50%] [G loss: 0.7623862028121948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 60/86 [D loss: 0.6732839643955231, acc.: 57.76%] [G loss: 0.7550850510597229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 61/86 [D loss: 0.6678549945354462, acc.: 58.64%] [G loss: 0.764297604560852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 62/86 [D loss: 0.6689974665641785, acc.: 59.86%] [G loss: 0.7487694025039673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 63/86 [D loss: 0.6705543994903564, acc.: 58.11%] [G loss: 0.7610729336738586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 64/86 [D loss: 0.6732347011566162, acc.: 57.81%] [G loss: 0.7610204219818115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 65/86 [D loss: 0.6731752455234528, acc.: 58.50%] [G loss: 0.7478549480438232]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 66/86 [D loss: 0.6671868860721588, acc.: 59.23%] [G loss: 0.7561200261116028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 67/86 [D loss: 0.6654406487941742, acc.: 60.79%] [G loss: 0.7581396102905273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 68/86 [D loss: 0.6712464988231659, acc.: 59.23%] [G loss: 0.7625026106834412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 69/86 [D loss: 0.6725437343120575, acc.: 57.57%] [G loss: 0.7596650719642639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 70/86 [D loss: 0.6683850288391113, acc.: 59.13%] [G loss: 0.7641227841377258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 71/86 [D loss: 0.6778663098812103, acc.: 57.08%] [G loss: 0.767548680305481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 72/86 [D loss: 0.6700738668441772, acc.: 58.25%] [G loss: 0.762504518032074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 73/86 [D loss: 0.6734626293182373, acc.: 56.98%] [G loss: 0.7612056136131287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 74/86 [D loss: 0.6733064353466034, acc.: 57.18%] [G loss: 0.763139009475708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 75/86 [D loss: 0.6713636517524719, acc.: 59.33%] [G loss: 0.7592730522155762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 76/86 [D loss: 0.6721548140048981, acc.: 58.15%] [G loss: 0.7658990621566772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 77/86 [D loss: 0.668876051902771, acc.: 59.08%] [G loss: 0.7588226199150085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 78/86 [D loss: 0.667077898979187, acc.: 60.11%] [G loss: 0.762191116809845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 79/86 [D loss: 0.6723672747612, acc.: 58.50%] [G loss: 0.7572891116142273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 80/86 [D loss: 0.6732980906963348, acc.: 58.06%] [G loss: 0.7649593353271484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 81/86 [D loss: 0.6727838814258575, acc.: 59.28%] [G loss: 0.7596415877342224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 82/86 [D loss: 0.6675676703453064, acc.: 59.72%] [G loss: 0.7687960863113403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 83/86 [D loss: 0.671432375907898, acc.: 59.57%] [G loss: 0.7632113099098206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 84/86 [D loss: 0.6714046895503998, acc.: 58.59%] [G loss: 0.7612093687057495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 85/86 [D loss: 0.6676031053066254, acc.: 58.84%] [G loss: 0.7596465349197388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 137/200, Batch 86/86 [D loss: 0.6739765703678131, acc.: 57.03%] [G loss: 0.7656248807907104]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 1/86 [D loss: 0.670578807592392, acc.: 59.77%] [G loss: 0.7683199644088745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 2/86 [D loss: 0.6750291287899017, acc.: 57.23%] [G loss: 0.7585239410400391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 3/86 [D loss: 0.6723064184188843, acc.: 57.18%] [G loss: 0.762677013874054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 4/86 [D loss: 0.6721833646297455, acc.: 57.18%] [G loss: 0.7561997771263123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 5/86 [D loss: 0.6718643605709076, acc.: 59.47%] [G loss: 0.7659517526626587]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 6/86 [D loss: 0.6746978461742401, acc.: 57.08%] [G loss: 0.756899356842041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 7/86 [D loss: 0.6720579862594604, acc.: 58.11%] [G loss: 0.7602787613868713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 8/86 [D loss: 0.6665652990341187, acc.: 59.33%] [G loss: 0.7515689134597778]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 9/86 [D loss: 0.6740300953388214, acc.: 58.89%] [G loss: 0.759791910648346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 10/86 [D loss: 0.6629150807857513, acc.: 61.33%] [G loss: 0.7662908434867859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 11/86 [D loss: 0.6682378351688385, acc.: 58.79%] [G loss: 0.7582963705062866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 12/86 [D loss: 0.6734601855278015, acc.: 58.06%] [G loss: 0.7605859637260437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 13/86 [D loss: 0.6749854981899261, acc.: 57.57%] [G loss: 0.7590104341506958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 14/86 [D loss: 0.6757572889328003, acc.: 56.74%] [G loss: 0.7688639760017395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 15/86 [D loss: 0.6660388708114624, acc.: 61.77%] [G loss: 0.7534704804420471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 16/86 [D loss: 0.6752082705497742, acc.: 56.98%] [G loss: 0.7600847482681274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 17/86 [D loss: 0.6699934005737305, acc.: 60.30%] [G loss: 0.7481912970542908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 18/86 [D loss: 0.675711065530777, acc.: 56.93%] [G loss: 0.7696771025657654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 19/86 [D loss: 0.677210658788681, acc.: 56.93%] [G loss: 0.7605723738670349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 20/86 [D loss: 0.6733304262161255, acc.: 57.91%] [G loss: 0.7613306641578674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 21/86 [D loss: 0.6719691753387451, acc.: 57.91%] [G loss: 0.7610083818435669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 22/86 [D loss: 0.6766790449619293, acc.: 56.98%] [G loss: 0.7640827894210815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 23/86 [D loss: 0.6723373234272003, acc.: 59.08%] [G loss: 0.7572877407073975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 24/86 [D loss: 0.6725389063358307, acc.: 58.35%] [G loss: 0.761706531047821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 25/86 [D loss: 0.6740396916866302, acc.: 57.81%] [G loss: 0.7664911150932312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 26/86 [D loss: 0.670421838760376, acc.: 59.18%] [G loss: 0.7655701041221619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 27/86 [D loss: 0.6681366264820099, acc.: 59.13%] [G loss: 0.7622504830360413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 28/86 [D loss: 0.6770873367786407, acc.: 56.64%] [G loss: 0.7540197372436523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 29/86 [D loss: 0.6729016900062561, acc.: 57.37%] [G loss: 0.7638165354728699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 30/86 [D loss: 0.675240695476532, acc.: 56.01%] [G loss: 0.7681208848953247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 31/86 [D loss: 0.6703059673309326, acc.: 58.20%] [G loss: 0.7649440169334412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 32/86 [D loss: 0.6732361316680908, acc.: 57.81%] [G loss: 0.7574923038482666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 33/86 [D loss: 0.6696489155292511, acc.: 58.40%] [G loss: 0.7671290040016174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 34/86 [D loss: 0.6678056716918945, acc.: 59.96%] [G loss: 0.7559342384338379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 35/86 [D loss: 0.6769002676010132, acc.: 56.59%] [G loss: 0.775095522403717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 36/86 [D loss: 0.66988605260849, acc.: 59.81%] [G loss: 0.758466899394989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 37/86 [D loss: 0.6793350577354431, acc.: 55.52%] [G loss: 0.761053204536438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 38/86 [D loss: 0.6735252141952515, acc.: 58.20%] [G loss: 0.7595182061195374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 39/86 [D loss: 0.6782301366329193, acc.: 56.45%] [G loss: 0.7591624855995178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 40/86 [D loss: 0.6688224971294403, acc.: 60.35%] [G loss: 0.7559560537338257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 41/86 [D loss: 0.6754165291786194, acc.: 56.35%] [G loss: 0.7560713291168213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 42/86 [D loss: 0.6669647991657257, acc.: 58.30%] [G loss: 0.7638715505599976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 43/86 [D loss: 0.6686387658119202, acc.: 59.57%] [G loss: 0.7618776559829712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 44/86 [D loss: 0.675639808177948, acc.: 57.67%] [G loss: 0.7656842470169067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 45/86 [D loss: 0.665680468082428, acc.: 59.81%] [G loss: 0.7584364414215088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 46/86 [D loss: 0.6716988682746887, acc.: 57.86%] [G loss: 0.7611443996429443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 47/86 [D loss: 0.6729986667633057, acc.: 58.50%] [G loss: 0.7584948539733887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 48/86 [D loss: 0.6699598729610443, acc.: 58.01%] [G loss: 0.7625247240066528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 49/86 [D loss: 0.6751553118228912, acc.: 57.57%] [G loss: 0.7701202034950256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 50/86 [D loss: 0.6694759130477905, acc.: 58.06%] [G loss: 0.76495760679245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 51/86 [D loss: 0.6731429696083069, acc.: 58.59%] [G loss: 0.7665978670120239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 52/86 [D loss: 0.6716918647289276, acc.: 59.28%] [G loss: 0.7664844393730164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 53/86 [D loss: 0.6736739575862885, acc.: 59.13%] [G loss: 0.7648455500602722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 54/86 [D loss: 0.6731302738189697, acc.: 57.08%] [G loss: 0.7563490867614746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 55/86 [D loss: 0.6755395233631134, acc.: 57.23%] [G loss: 0.7647700309753418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 56/86 [D loss: 0.6662349700927734, acc.: 60.16%] [G loss: 0.7565199732780457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 57/86 [D loss: 0.6687009334564209, acc.: 57.86%] [G loss: 0.7601809501647949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 58/86 [D loss: 0.6667572259902954, acc.: 60.74%] [G loss: 0.7710734605789185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 59/86 [D loss: 0.6657092869281769, acc.: 61.28%] [G loss: 0.7563203573226929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 60/86 [D loss: 0.668836385011673, acc.: 58.98%] [G loss: 0.758898913860321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 61/86 [D loss: 0.6665744185447693, acc.: 60.84%] [G loss: 0.7635164856910706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 62/86 [D loss: 0.6712549328804016, acc.: 57.57%] [G loss: 0.7593103051185608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 63/86 [D loss: 0.6719352602958679, acc.: 58.84%] [G loss: 0.7547505497932434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 64/86 [D loss: 0.6750929057598114, acc.: 57.28%] [G loss: 0.7505180835723877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 65/86 [D loss: 0.6754456758499146, acc.: 56.93%] [G loss: 0.7574369311332703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 66/86 [D loss: 0.666705310344696, acc.: 61.28%] [G loss: 0.7596595287322998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 67/86 [D loss: 0.6747788488864899, acc.: 56.01%] [G loss: 0.7602022290229797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 68/86 [D loss: 0.6716227531433105, acc.: 58.06%] [G loss: 0.7571467757225037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 69/86 [D loss: 0.6770780682563782, acc.: 55.66%] [G loss: 0.7610355615615845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 70/86 [D loss: 0.6723468899726868, acc.: 57.57%] [G loss: 0.7672696709632874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 71/86 [D loss: 0.6736638247966766, acc.: 57.52%] [G loss: 0.7559552788734436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 72/86 [D loss: 0.6716444194316864, acc.: 58.45%] [G loss: 0.7720561027526855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 73/86 [D loss: 0.671607255935669, acc.: 59.33%] [G loss: 0.760870099067688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 74/86 [D loss: 0.6749083697795868, acc.: 56.05%] [G loss: 0.760688304901123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 75/86 [D loss: 0.669547975063324, acc.: 58.35%] [G loss: 0.7686788439750671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 76/86 [D loss: 0.6716356873512268, acc.: 59.47%] [G loss: 0.7654018402099609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 77/86 [D loss: 0.672396332025528, acc.: 58.50%] [G loss: 0.7738908529281616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 78/86 [D loss: 0.6756894886493683, acc.: 56.01%] [G loss: 0.7568973302841187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 79/86 [D loss: 0.6781294643878937, acc.: 56.35%] [G loss: 0.7676588296890259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 80/86 [D loss: 0.6718151569366455, acc.: 58.74%] [G loss: 0.7543795704841614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 81/86 [D loss: 0.6763530969619751, acc.: 56.25%] [G loss: 0.761570394039154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 82/86 [D loss: 0.671158641576767, acc.: 58.59%] [G loss: 0.7537340521812439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 83/86 [D loss: 0.6748782992362976, acc.: 56.30%] [G loss: 0.7619284391403198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 84/86 [D loss: 0.6713137328624725, acc.: 58.40%] [G loss: 0.7532077431678772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 85/86 [D loss: 0.670477569103241, acc.: 59.08%] [G loss: 0.7611973881721497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 138/200, Batch 86/86 [D loss: 0.6660002171993256, acc.: 59.91%] [G loss: 0.7649403810501099]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 1/86 [D loss: 0.6730230450630188, acc.: 58.45%] [G loss: 0.7533631324768066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 2/86 [D loss: 0.6694597601890564, acc.: 58.59%] [G loss: 0.7659496665000916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 3/86 [D loss: 0.6690963506698608, acc.: 58.79%] [G loss: 0.7609559893608093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 4/86 [D loss: 0.6699463129043579, acc.: 58.25%] [G loss: 0.7666979432106018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 5/86 [D loss: 0.6704285442829132, acc.: 57.81%] [G loss: 0.7624943256378174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 6/86 [D loss: 0.6726672053337097, acc.: 57.81%] [G loss: 0.7589776515960693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 7/86 [D loss: 0.6738893985748291, acc.: 57.91%] [G loss: 0.7627661228179932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 8/86 [D loss: 0.6704550981521606, acc.: 59.47%] [G loss: 0.7634717226028442]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 9/86 [D loss: 0.6689501702785492, acc.: 59.62%] [G loss: 0.7595902681350708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 10/86 [D loss: 0.6704301834106445, acc.: 58.79%] [G loss: 0.7554914355278015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 11/86 [D loss: 0.6711822152137756, acc.: 58.06%] [G loss: 0.7591586709022522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 12/86 [D loss: 0.6711492240428925, acc.: 59.86%] [G loss: 0.7609339356422424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 13/86 [D loss: 0.6774365603923798, acc.: 56.49%] [G loss: 0.7640873789787292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 14/86 [D loss: 0.6657498776912689, acc.: 60.64%] [G loss: 0.7667967081069946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 15/86 [D loss: 0.6776531338691711, acc.: 56.88%] [G loss: 0.7659008502960205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 16/86 [D loss: 0.672478586435318, acc.: 56.74%] [G loss: 0.7685118913650513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 17/86 [D loss: 0.6729374825954437, acc.: 58.54%] [G loss: 0.7657725811004639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 18/86 [D loss: 0.6706275939941406, acc.: 58.94%] [G loss: 0.7726627588272095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 19/86 [D loss: 0.6763283014297485, acc.: 57.13%] [G loss: 0.7555760145187378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 20/86 [D loss: 0.6719280183315277, acc.: 56.59%] [G loss: 0.76337730884552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 21/86 [D loss: 0.6714871823787689, acc.: 58.15%] [G loss: 0.7667151689529419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 22/86 [D loss: 0.6765877306461334, acc.: 57.42%] [G loss: 0.7674074172973633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 23/86 [D loss: 0.6722019016742706, acc.: 58.35%] [G loss: 0.7606063485145569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 24/86 [D loss: 0.6677347719669342, acc.: 59.42%] [G loss: 0.7585466504096985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 25/86 [D loss: 0.6726295053958893, acc.: 57.37%] [G loss: 0.7590721249580383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 26/86 [D loss: 0.6747706830501556, acc.: 57.57%] [G loss: 0.7569923400878906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 27/86 [D loss: 0.675002932548523, acc.: 57.03%] [G loss: 0.7662349939346313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 28/86 [D loss: 0.675390213727951, acc.: 57.76%] [G loss: 0.7650337815284729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 29/86 [D loss: 0.6695354282855988, acc.: 58.54%] [G loss: 0.7643442153930664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 30/86 [D loss: 0.6692643165588379, acc.: 59.23%] [G loss: 0.7642450332641602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 31/86 [D loss: 0.6707503795623779, acc.: 58.69%] [G loss: 0.7596790790557861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 32/86 [D loss: 0.6773861050605774, acc.: 57.23%] [G loss: 0.7626467943191528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 33/86 [D loss: 0.6698895990848541, acc.: 58.11%] [G loss: 0.7638914585113525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 34/86 [D loss: 0.674053430557251, acc.: 57.52%] [G loss: 0.759341835975647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 35/86 [D loss: 0.6715028285980225, acc.: 58.59%] [G loss: 0.7613359689712524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 36/86 [D loss: 0.6692079305648804, acc.: 59.13%] [G loss: 0.7568663358688354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 37/86 [D loss: 0.6737247109413147, acc.: 57.86%] [G loss: 0.7639151811599731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 38/86 [D loss: 0.6671576201915741, acc.: 59.77%] [G loss: 0.7604928016662598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 39/86 [D loss: 0.6709788143634796, acc.: 59.13%] [G loss: 0.7629581689834595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 40/86 [D loss: 0.6714571416378021, acc.: 58.20%] [G loss: 0.7558562159538269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 41/86 [D loss: 0.6699143648147583, acc.: 58.50%] [G loss: 0.7625962495803833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 42/86 [D loss: 0.6699580550193787, acc.: 58.98%] [G loss: 0.7547333240509033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 43/86 [D loss: 0.6722096800804138, acc.: 58.54%] [G loss: 0.76030433177948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 44/86 [D loss: 0.6746944189071655, acc.: 57.96%] [G loss: 0.760543704032898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 45/86 [D loss: 0.6729448735713959, acc.: 57.96%] [G loss: 0.760990560054779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 46/86 [D loss: 0.6727689802646637, acc.: 58.01%] [G loss: 0.7572821974754333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 47/86 [D loss: 0.6707274615764618, acc.: 58.94%] [G loss: 0.7809188365936279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 48/86 [D loss: 0.6705989837646484, acc.: 59.72%] [G loss: 0.7534599900245667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 49/86 [D loss: 0.6716183423995972, acc.: 59.13%] [G loss: 0.7590006589889526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 50/86 [D loss: 0.6731948256492615, acc.: 57.86%] [G loss: 0.7618470788002014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 51/86 [D loss: 0.6717711091041565, acc.: 57.91%] [G loss: 0.7670872807502747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 52/86 [D loss: 0.6628823578357697, acc.: 60.45%] [G loss: 0.7613961100578308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 53/86 [D loss: 0.6743144094944, acc.: 57.18%] [G loss: 0.7621879577636719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 54/86 [D loss: 0.6741962730884552, acc.: 56.79%] [G loss: 0.7644966840744019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 55/86 [D loss: 0.6693336665630341, acc.: 59.47%] [G loss: 0.766228437423706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 56/86 [D loss: 0.6709435880184174, acc.: 58.54%] [G loss: 0.7585757374763489]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 57/86 [D loss: 0.6726738512516022, acc.: 59.33%] [G loss: 0.7593069672584534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 58/86 [D loss: 0.6739629507064819, acc.: 58.59%] [G loss: 0.7631189823150635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 59/86 [D loss: 0.6732383966445923, acc.: 58.50%] [G loss: 0.752554714679718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 60/86 [D loss: 0.6759215891361237, acc.: 55.62%] [G loss: 0.7654685378074646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 61/86 [D loss: 0.6771613955497742, acc.: 56.64%] [G loss: 0.761368453502655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 62/86 [D loss: 0.6740135252475739, acc.: 56.79%] [G loss: 0.7684968113899231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 63/86 [D loss: 0.6678745150566101, acc.: 60.25%] [G loss: 0.7632696032524109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 64/86 [D loss: 0.6754824221134186, acc.: 57.52%] [G loss: 0.7638595104217529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 65/86 [D loss: 0.674271285533905, acc.: 57.62%] [G loss: 0.7628487348556519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 66/86 [D loss: 0.6710149645805359, acc.: 57.67%] [G loss: 0.7617458701133728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 67/86 [D loss: 0.6712203621864319, acc.: 58.50%] [G loss: 0.7661041021347046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 68/86 [D loss: 0.6700884699821472, acc.: 58.40%] [G loss: 0.7617133855819702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 69/86 [D loss: 0.6750915050506592, acc.: 57.28%] [G loss: 0.7556834816932678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 70/86 [D loss: 0.6719184815883636, acc.: 57.81%] [G loss: 0.7627362012863159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 71/86 [D loss: 0.6737466752529144, acc.: 57.71%] [G loss: 0.7599402666091919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 72/86 [D loss: 0.6752480864524841, acc.: 56.84%] [G loss: 0.767390787601471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 73/86 [D loss: 0.671859622001648, acc.: 58.79%] [G loss: 0.7580462694168091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 74/86 [D loss: 0.6764088571071625, acc.: 56.98%] [G loss: 0.7579164505004883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 75/86 [D loss: 0.673062264919281, acc.: 57.67%] [G loss: 0.7599954009056091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 76/86 [D loss: 0.6717780828475952, acc.: 57.13%] [G loss: 0.7644057273864746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 77/86 [D loss: 0.6710947751998901, acc.: 59.18%] [G loss: 0.7702912092208862]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 78/86 [D loss: 0.6702842116355896, acc.: 58.45%] [G loss: 0.7605620622634888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 79/86 [D loss: 0.671924352645874, acc.: 57.42%] [G loss: 0.7687371969223022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 80/86 [D loss: 0.6717539429664612, acc.: 57.52%] [G loss: 0.7593734860420227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 81/86 [D loss: 0.6683621108531952, acc.: 59.52%] [G loss: 0.7655833959579468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 82/86 [D loss: 0.6715395450592041, acc.: 58.35%] [G loss: 0.7510367035865784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 83/86 [D loss: 0.6679426729679108, acc.: 59.67%] [G loss: 0.7653332352638245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 84/86 [D loss: 0.6690388023853302, acc.: 58.54%] [G loss: 0.7648184299468994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 85/86 [D loss: 0.670901745557785, acc.: 59.47%] [G loss: 0.7573835849761963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 139/200, Batch 86/86 [D loss: 0.6725806593894958, acc.: 57.08%] [G loss: 0.7618772983551025]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 1/86 [D loss: 0.6685856282711029, acc.: 58.20%] [G loss: 0.7656605243682861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 2/86 [D loss: 0.6705732345581055, acc.: 58.98%] [G loss: 0.7621774673461914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 3/86 [D loss: 0.6690739393234253, acc.: 60.45%] [G loss: 0.7617146968841553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 4/86 [D loss: 0.6736482381820679, acc.: 57.47%] [G loss: 0.7720511555671692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 5/86 [D loss: 0.6721033155918121, acc.: 57.62%] [G loss: 0.7649586796760559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 6/86 [D loss: 0.6697271168231964, acc.: 60.01%] [G loss: 0.7625772953033447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 7/86 [D loss: 0.6721392869949341, acc.: 58.74%] [G loss: 0.7659713625907898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 8/86 [D loss: 0.6726912558078766, acc.: 56.69%] [G loss: 0.7655947208404541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 9/86 [D loss: 0.672030508518219, acc.: 58.98%] [G loss: 0.7594655752182007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 10/86 [D loss: 0.6683237552642822, acc.: 60.06%] [G loss: 0.7590290904045105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 11/86 [D loss: 0.6711669564247131, acc.: 58.40%] [G loss: 0.7631604671478271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 12/86 [D loss: 0.6695085763931274, acc.: 58.35%] [G loss: 0.7597953081130981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 13/86 [D loss: 0.6697088479995728, acc.: 58.35%] [G loss: 0.7607512474060059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 14/86 [D loss: 0.6706031858921051, acc.: 57.91%] [G loss: 0.7615619897842407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 15/86 [D loss: 0.6697594523429871, acc.: 58.01%] [G loss: 0.7588254809379578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 16/86 [D loss: 0.6667111217975616, acc.: 58.79%] [G loss: 0.762397050857544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 17/86 [D loss: 0.673594206571579, acc.: 58.01%] [G loss: 0.7702155113220215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 18/86 [D loss: 0.6729397475719452, acc.: 58.30%] [G loss: 0.7622637748718262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 19/86 [D loss: 0.6685634851455688, acc.: 60.94%] [G loss: 0.7631363868713379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 20/86 [D loss: 0.6718371212482452, acc.: 57.76%] [G loss: 0.7626787424087524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 21/86 [D loss: 0.675864964723587, acc.: 56.25%] [G loss: 0.762772262096405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 22/86 [D loss: 0.6742188632488251, acc.: 58.11%] [G loss: 0.7717046737670898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 23/86 [D loss: 0.6717596650123596, acc.: 58.45%] [G loss: 0.7628481388092041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 24/86 [D loss: 0.6692962348461151, acc.: 58.84%] [G loss: 0.7600995898246765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 25/86 [D loss: 0.6677987277507782, acc.: 60.74%] [G loss: 0.7676271200180054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 26/86 [D loss: 0.6682792007923126, acc.: 60.35%] [G loss: 0.7614076137542725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 27/86 [D loss: 0.6758830547332764, acc.: 56.88%] [G loss: 0.7618652582168579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 28/86 [D loss: 0.6687725186347961, acc.: 59.08%] [G loss: 0.7546157240867615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 29/86 [D loss: 0.6715402901172638, acc.: 58.01%] [G loss: 0.7614791989326477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 30/86 [D loss: 0.6684881150722504, acc.: 60.79%] [G loss: 0.7652530670166016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 31/86 [D loss: 0.6703698635101318, acc.: 58.84%] [G loss: 0.7679784893989563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 32/86 [D loss: 0.6639197468757629, acc.: 61.23%] [G loss: 0.7612743973731995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 33/86 [D loss: 0.6736411452293396, acc.: 57.18%] [G loss: 0.7651892304420471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 34/86 [D loss: 0.6696261763572693, acc.: 58.94%] [G loss: 0.7632431983947754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 35/86 [D loss: 0.671841025352478, acc.: 57.42%] [G loss: 0.7644386887550354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 36/86 [D loss: 0.6721882820129395, acc.: 58.79%] [G loss: 0.7586360573768616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 37/86 [D loss: 0.6725752651691437, acc.: 57.96%] [G loss: 0.7621166706085205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 38/86 [D loss: 0.6707526743412018, acc.: 59.38%] [G loss: 0.7601451873779297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 39/86 [D loss: 0.6708588004112244, acc.: 58.69%] [G loss: 0.761770486831665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 40/86 [D loss: 0.6728075742721558, acc.: 56.10%] [G loss: 0.7684259414672852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 41/86 [D loss: 0.6654511392116547, acc.: 59.81%] [G loss: 0.758338212966919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 42/86 [D loss: 0.6691639125347137, acc.: 59.81%] [G loss: 0.7635740637779236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 43/86 [D loss: 0.6650140583515167, acc.: 60.94%] [G loss: 0.76624995470047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 44/86 [D loss: 0.6716681122779846, acc.: 57.67%] [G loss: 0.7620182633399963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 45/86 [D loss: 0.669971764087677, acc.: 60.21%] [G loss: 0.7665141820907593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 46/86 [D loss: 0.6713564395904541, acc.: 57.37%] [G loss: 0.7581658959388733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 47/86 [D loss: 0.6673544943332672, acc.: 59.13%] [G loss: 0.7602179646492004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 48/86 [D loss: 0.6714885830879211, acc.: 58.30%] [G loss: 0.763426661491394]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 49/86 [D loss: 0.6714053750038147, acc.: 58.98%] [G loss: 0.7656664252281189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 50/86 [D loss: 0.6704933643341064, acc.: 59.57%] [G loss: 0.7613382339477539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 51/86 [D loss: 0.6679788827896118, acc.: 58.89%] [G loss: 0.7665196061134338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 52/86 [D loss: 0.6685668230056763, acc.: 59.77%] [G loss: 0.7656895518302917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 53/86 [D loss: 0.6757078170776367, acc.: 56.30%] [G loss: 0.7736323475837708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 54/86 [D loss: 0.6673788130283356, acc.: 61.28%] [G loss: 0.7612897753715515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 55/86 [D loss: 0.6751455366611481, acc.: 57.32%] [G loss: 0.7582008838653564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 56/86 [D loss: 0.6677956581115723, acc.: 58.30%] [G loss: 0.7523350715637207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 57/86 [D loss: 0.6733186542987823, acc.: 59.81%] [G loss: 0.7693166732788086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 58/86 [D loss: 0.676582932472229, acc.: 55.66%] [G loss: 0.7638200521469116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 59/86 [D loss: 0.6748937368392944, acc.: 56.69%] [G loss: 0.7668266892433167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 60/86 [D loss: 0.6730139553546906, acc.: 57.23%] [G loss: 0.7574639320373535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 61/86 [D loss: 0.6647079586982727, acc.: 59.91%] [G loss: 0.7642251253128052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 62/86 [D loss: 0.6736770868301392, acc.: 57.08%] [G loss: 0.7612890601158142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 63/86 [D loss: 0.6664902865886688, acc.: 60.94%] [G loss: 0.7599654793739319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 64/86 [D loss: 0.6727984845638275, acc.: 58.84%] [G loss: 0.7638204097747803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 65/86 [D loss: 0.6690347194671631, acc.: 59.08%] [G loss: 0.7540956139564514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 66/86 [D loss: 0.677067369222641, acc.: 55.57%] [G loss: 0.7584075331687927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 67/86 [D loss: 0.6741528511047363, acc.: 56.93%] [G loss: 0.7640994787216187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 68/86 [D loss: 0.6788148880004883, acc.: 55.76%] [G loss: 0.7627751231193542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 69/86 [D loss: 0.670112669467926, acc.: 59.86%] [G loss: 0.7756735682487488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 70/86 [D loss: 0.6719167232513428, acc.: 58.01%] [G loss: 0.7587411999702454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 71/86 [D loss: 0.6755566895008087, acc.: 56.54%] [G loss: 0.7748531103134155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 72/86 [D loss: 0.6718271374702454, acc.: 59.03%] [G loss: 0.7599275708198547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 73/86 [D loss: 0.6729960143566132, acc.: 58.20%] [G loss: 0.765474796295166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 74/86 [D loss: 0.6719264984130859, acc.: 58.35%] [G loss: 0.7625928521156311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 75/86 [D loss: 0.6680049896240234, acc.: 60.74%] [G loss: 0.7691826820373535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 76/86 [D loss: 0.6688542366027832, acc.: 58.06%] [G loss: 0.7525913715362549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 77/86 [D loss: 0.6804249286651611, acc.: 55.57%] [G loss: 0.764611005783081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 78/86 [D loss: 0.6697634756565094, acc.: 58.89%] [G loss: 0.7604305744171143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 79/86 [D loss: 0.6758813858032227, acc.: 56.15%] [G loss: 0.7610189318656921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 80/86 [D loss: 0.6692017018795013, acc.: 59.81%] [G loss: 0.7595539093017578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 81/86 [D loss: 0.6739454567432404, acc.: 57.71%] [G loss: 0.7668701410293579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 82/86 [D loss: 0.6716005206108093, acc.: 58.20%] [G loss: 0.7610283493995667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 83/86 [D loss: 0.6820009648799896, acc.: 54.83%] [G loss: 0.7605654001235962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 84/86 [D loss: 0.6687559485435486, acc.: 57.37%] [G loss: 0.7583081722259521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 85/86 [D loss: 0.6724872887134552, acc.: 58.64%] [G loss: 0.7637778520584106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 140/200, Batch 86/86 [D loss: 0.6705333888530731, acc.: 60.21%] [G loss: 0.7671926021575928]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 1/86 [D loss: 0.6704968512058258, acc.: 59.18%] [G loss: 0.766472578048706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 2/86 [D loss: 0.6707666218280792, acc.: 58.30%] [G loss: 0.7582688927650452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 3/86 [D loss: 0.6728100478649139, acc.: 58.11%] [G loss: 0.7629868984222412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 4/86 [D loss: 0.6722389757633209, acc.: 58.06%] [G loss: 0.7603373527526855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 5/86 [D loss: 0.6739713549613953, acc.: 57.18%] [G loss: 0.7618246078491211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 6/86 [D loss: 0.6692843735218048, acc.: 58.64%] [G loss: 0.7667960524559021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 7/86 [D loss: 0.6687458157539368, acc.: 59.52%] [G loss: 0.7593451142311096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 8/86 [D loss: 0.6666460335254669, acc.: 60.84%] [G loss: 0.7679624557495117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 9/86 [D loss: 0.6737436652183533, acc.: 57.62%] [G loss: 0.7615294456481934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 10/86 [D loss: 0.6731902360916138, acc.: 57.23%] [G loss: 0.7644505500793457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 11/86 [D loss: 0.671647697687149, acc.: 59.18%] [G loss: 0.7565120458602905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 12/86 [D loss: 0.6711350083351135, acc.: 57.71%] [G loss: 0.7629825472831726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 13/86 [D loss: 0.6696130037307739, acc.: 58.69%] [G loss: 0.7624629139900208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 14/86 [D loss: 0.6663149893283844, acc.: 59.77%] [G loss: 0.7696329355239868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 15/86 [D loss: 0.6744763553142548, acc.: 56.69%] [G loss: 0.7639312148094177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 16/86 [D loss: 0.6751708090305328, acc.: 57.71%] [G loss: 0.759295642375946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 17/86 [D loss: 0.667240172624588, acc.: 59.42%] [G loss: 0.7629004120826721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 18/86 [D loss: 0.6706605553627014, acc.: 57.67%] [G loss: 0.7527123689651489]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 19/86 [D loss: 0.6714677810668945, acc.: 57.86%] [G loss: 0.7616249918937683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 20/86 [D loss: 0.6706438660621643, acc.: 59.18%] [G loss: 0.7566253542900085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 21/86 [D loss: 0.6711119711399078, acc.: 58.54%] [G loss: 0.7651585936546326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 22/86 [D loss: 0.673194169998169, acc.: 58.06%] [G loss: 0.7571612000465393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 23/86 [D loss: 0.6711956858634949, acc.: 59.38%] [G loss: 0.7698004245758057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 24/86 [D loss: 0.6720827221870422, acc.: 57.71%] [G loss: 0.7577334046363831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 25/86 [D loss: 0.6729787886142731, acc.: 58.50%] [G loss: 0.7668027281761169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 26/86 [D loss: 0.6723958253860474, acc.: 59.03%] [G loss: 0.7537894248962402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 27/86 [D loss: 0.6710697412490845, acc.: 58.15%] [G loss: 0.7648191452026367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 28/86 [D loss: 0.6650528013706207, acc.: 60.64%] [G loss: 0.7598491311073303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 29/86 [D loss: 0.6751010119915009, acc.: 58.20%] [G loss: 0.7670121788978577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 30/86 [D loss: 0.6687967479228973, acc.: 60.01%] [G loss: 0.7594902515411377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 31/86 [D loss: 0.6735243797302246, acc.: 56.69%] [G loss: 0.7608983516693115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 32/86 [D loss: 0.6710851192474365, acc.: 58.74%] [G loss: 0.7594295144081116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 33/86 [D loss: 0.6778957843780518, acc.: 56.88%] [G loss: 0.7636842727661133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 34/86 [D loss: 0.6680273413658142, acc.: 60.25%] [G loss: 0.7685615420341492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 35/86 [D loss: 0.6741132736206055, acc.: 57.62%] [G loss: 0.7654330134391785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 36/86 [D loss: 0.6668819487094879, acc.: 60.11%] [G loss: 0.7622742652893066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 37/86 [D loss: 0.6679658889770508, acc.: 58.94%] [G loss: 0.7602944374084473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 38/86 [D loss: 0.6694236397743225, acc.: 59.38%] [G loss: 0.7645411491394043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 39/86 [D loss: 0.667039304971695, acc.: 59.67%] [G loss: 0.7572406530380249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 40/86 [D loss: 0.6660173535346985, acc.: 58.64%] [G loss: 0.7634595632553101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 41/86 [D loss: 0.6636585295200348, acc.: 60.50%] [G loss: 0.7569241523742676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 42/86 [D loss: 0.6757453680038452, acc.: 57.23%] [G loss: 0.7707723379135132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 43/86 [D loss: 0.6714599132537842, acc.: 58.45%] [G loss: 0.7580200433731079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 44/86 [D loss: 0.6736257672309875, acc.: 58.25%] [G loss: 0.7680660486221313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 45/86 [D loss: 0.6672528386116028, acc.: 59.33%] [G loss: 0.7618407011032104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 46/86 [D loss: 0.662463366985321, acc.: 61.33%] [G loss: 0.758243978023529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 47/86 [D loss: 0.6661161780357361, acc.: 60.99%] [G loss: 0.767876923084259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 48/86 [D loss: 0.674624502658844, acc.: 57.18%] [G loss: 0.7635840773582458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 49/86 [D loss: 0.6710579097270966, acc.: 60.01%] [G loss: 0.7659313678741455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 50/86 [D loss: 0.6708012521266937, acc.: 58.25%] [G loss: 0.7658660411834717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 51/86 [D loss: 0.6747772395610809, acc.: 57.13%] [G loss: 0.7642042636871338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 52/86 [D loss: 0.6681319177150726, acc.: 59.42%] [G loss: 0.764492392539978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 53/86 [D loss: 0.671378880739212, acc.: 58.15%] [G loss: 0.7624739408493042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 54/86 [D loss: 0.6701683402061462, acc.: 59.38%] [G loss: 0.7659997940063477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 55/86 [D loss: 0.6691018342971802, acc.: 58.79%] [G loss: 0.764187753200531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 56/86 [D loss: 0.6696502864360809, acc.: 60.55%] [G loss: 0.7545416951179504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 57/86 [D loss: 0.6726664900779724, acc.: 58.45%] [G loss: 0.763242781162262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 58/86 [D loss: 0.6721976101398468, acc.: 60.21%] [G loss: 0.7651198506355286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 59/86 [D loss: 0.6738291382789612, acc.: 58.06%] [G loss: 0.7581103444099426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 60/86 [D loss: 0.669283390045166, acc.: 57.86%] [G loss: 0.7640097737312317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 61/86 [D loss: 0.6719799637794495, acc.: 58.50%] [G loss: 0.7605790495872498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 62/86 [D loss: 0.668242871761322, acc.: 59.77%] [G loss: 0.768528938293457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 63/86 [D loss: 0.6696501672267914, acc.: 58.54%] [G loss: 0.7646694779396057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 64/86 [D loss: 0.6705337464809418, acc.: 59.67%] [G loss: 0.7672466039657593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 65/86 [D loss: 0.6630748510360718, acc.: 60.16%] [G loss: 0.7629922032356262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 66/86 [D loss: 0.6720778346061707, acc.: 59.03%] [G loss: 0.7620328068733215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 67/86 [D loss: 0.6663814187049866, acc.: 61.04%] [G loss: 0.7662340402603149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 68/86 [D loss: 0.6685169637203217, acc.: 60.40%] [G loss: 0.7665826082229614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 69/86 [D loss: 0.6731335818767548, acc.: 57.96%] [G loss: 0.7636927366256714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 70/86 [D loss: 0.6709144115447998, acc.: 59.62%] [G loss: 0.7617520093917847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 71/86 [D loss: 0.6725433170795441, acc.: 58.20%] [G loss: 0.7664779424667358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 72/86 [D loss: 0.671140044927597, acc.: 59.52%] [G loss: 0.7662591934204102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 73/86 [D loss: 0.6691845953464508, acc.: 60.40%] [G loss: 0.7634171843528748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 74/86 [D loss: 0.6741617023944855, acc.: 58.01%] [G loss: 0.7678089737892151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 75/86 [D loss: 0.6675008833408356, acc.: 58.98%] [G loss: 0.7662398815155029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 76/86 [D loss: 0.6683805584907532, acc.: 60.06%] [G loss: 0.7644378542900085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 77/86 [D loss: 0.6756815612316132, acc.: 56.25%] [G loss: 0.7669562697410583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 78/86 [D loss: 0.6664812862873077, acc.: 59.57%] [G loss: 0.7634668350219727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 79/86 [D loss: 0.673444002866745, acc.: 57.42%] [G loss: 0.7623854875564575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 80/86 [D loss: 0.6643325984477997, acc.: 60.69%] [G loss: 0.7628150582313538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 81/86 [D loss: 0.670750081539154, acc.: 59.47%] [G loss: 0.7823925018310547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 82/86 [D loss: 0.6693153977394104, acc.: 58.89%] [G loss: 0.7666064500808716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 83/86 [D loss: 0.6769967973232269, acc.: 56.74%] [G loss: 0.7668486833572388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 84/86 [D loss: 0.6725870966911316, acc.: 58.30%] [G loss: 0.7623047828674316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 85/86 [D loss: 0.6722018122673035, acc.: 58.64%] [G loss: 0.763965368270874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 141/200, Batch 86/86 [D loss: 0.6659188568592072, acc.: 59.91%] [G loss: 0.7659327983856201]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 1/86 [D loss: 0.6757967174053192, acc.: 57.28%] [G loss: 0.7634940147399902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 2/86 [D loss: 0.6680734157562256, acc.: 60.64%] [G loss: 0.7622045874595642]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 3/86 [D loss: 0.6642501652240753, acc.: 60.99%] [G loss: 0.7688847780227661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 4/86 [D loss: 0.6755557358264923, acc.: 57.62%] [G loss: 0.7677714824676514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 5/86 [D loss: 0.6714292168617249, acc.: 57.18%] [G loss: 0.7689662575721741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 6/86 [D loss: 0.6679996252059937, acc.: 59.38%] [G loss: 0.7646470665931702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 7/86 [D loss: 0.6682353019714355, acc.: 60.45%] [G loss: 0.7616991996765137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 8/86 [D loss: 0.6690368354320526, acc.: 59.86%] [G loss: 0.7718662023544312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 9/86 [D loss: 0.6689222455024719, acc.: 58.74%] [G loss: 0.7675237655639648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 10/86 [D loss: 0.6719678342342377, acc.: 58.30%] [G loss: 0.7546330690383911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 11/86 [D loss: 0.6698508560657501, acc.: 58.54%] [G loss: 0.7616338729858398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 12/86 [D loss: 0.6712993085384369, acc.: 58.30%] [G loss: 0.7554759383201599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 13/86 [D loss: 0.6710862219333649, acc.: 59.38%] [G loss: 0.7658616900444031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 14/86 [D loss: 0.6747754812240601, acc.: 56.64%] [G loss: 0.755609393119812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 15/86 [D loss: 0.6753544509410858, acc.: 56.25%] [G loss: 0.7705471515655518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 16/86 [D loss: 0.6691775321960449, acc.: 59.23%] [G loss: 0.7564480304718018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 17/86 [D loss: 0.6667739450931549, acc.: 58.69%] [G loss: 0.7633325457572937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 18/86 [D loss: 0.6686848104000092, acc.: 59.91%] [G loss: 0.7615565061569214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 19/86 [D loss: 0.6728056371212006, acc.: 57.67%] [G loss: 0.7636509537696838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 20/86 [D loss: 0.6714455485343933, acc.: 58.98%] [G loss: 0.7632082104682922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 21/86 [D loss: 0.6678278744220734, acc.: 58.94%] [G loss: 0.7640349268913269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 22/86 [D loss: 0.663643091917038, acc.: 60.21%] [G loss: 0.7612384557723999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 23/86 [D loss: 0.668570876121521, acc.: 58.98%] [G loss: 0.7577134370803833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 24/86 [D loss: 0.6673230230808258, acc.: 59.91%] [G loss: 0.7629990577697754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 25/86 [D loss: 0.6733484864234924, acc.: 58.59%] [G loss: 0.7642145156860352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 26/86 [D loss: 0.6700427532196045, acc.: 58.74%] [G loss: 0.7622172832489014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 27/86 [D loss: 0.6712332963943481, acc.: 60.11%] [G loss: 0.7667694091796875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 28/86 [D loss: 0.6729556918144226, acc.: 57.23%] [G loss: 0.767807126045227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 29/86 [D loss: 0.6700736880302429, acc.: 58.94%] [G loss: 0.7704253196716309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 30/86 [D loss: 0.6686001420021057, acc.: 59.42%] [G loss: 0.7614787220954895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 31/86 [D loss: 0.6725437641143799, acc.: 57.81%] [G loss: 0.7666686773300171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 32/86 [D loss: 0.6739653944969177, acc.: 57.52%] [G loss: 0.761591374874115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 33/86 [D loss: 0.6740374863147736, acc.: 58.40%] [G loss: 0.7678326964378357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 34/86 [D loss: 0.6665885746479034, acc.: 58.74%] [G loss: 0.7652018070220947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 35/86 [D loss: 0.6733293831348419, acc.: 58.35%] [G loss: 0.7631415724754333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 36/86 [D loss: 0.6664331555366516, acc.: 60.84%] [G loss: 0.7675188183784485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 37/86 [D loss: 0.6726194024085999, acc.: 58.11%] [G loss: 0.7612307667732239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 38/86 [D loss: 0.6736169159412384, acc.: 56.98%] [G loss: 0.7687455415725708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 39/86 [D loss: 0.6687759160995483, acc.: 59.47%] [G loss: 0.762754499912262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 40/86 [D loss: 0.6698176264762878, acc.: 58.50%] [G loss: 0.7610023021697998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 41/86 [D loss: 0.6689645051956177, acc.: 59.67%] [G loss: 0.7674899101257324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 42/86 [D loss: 0.6709959208965302, acc.: 57.91%] [G loss: 0.7616325616836548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 43/86 [D loss: 0.6701489388942719, acc.: 59.23%] [G loss: 0.7605459690093994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 44/86 [D loss: 0.6681216657161713, acc.: 59.72%] [G loss: 0.7688939571380615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 45/86 [D loss: 0.6700161099433899, acc.: 58.98%] [G loss: 0.7619463205337524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 46/86 [D loss: 0.6644251346588135, acc.: 61.04%] [G loss: 0.7625037431716919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 47/86 [D loss: 0.6667149662971497, acc.: 59.47%] [G loss: 0.7544904351234436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 48/86 [D loss: 0.6692335903644562, acc.: 60.40%] [G loss: 0.7637122273445129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 49/86 [D loss: 0.6697395145893097, acc.: 59.57%] [G loss: 0.7626121044158936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 50/86 [D loss: 0.6695269048213959, acc.: 57.62%] [G loss: 0.7688538432121277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 51/86 [D loss: 0.6751544177532196, acc.: 57.28%] [G loss: 0.7617641687393188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 52/86 [D loss: 0.6678186655044556, acc.: 59.42%] [G loss: 0.766461193561554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 53/86 [D loss: 0.6680359542369843, acc.: 60.21%] [G loss: 0.764294445514679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 54/86 [D loss: 0.6759899854660034, acc.: 57.37%] [G loss: 0.7713193893432617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 55/86 [D loss: 0.6707592010498047, acc.: 58.01%] [G loss: 0.7621880769729614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 56/86 [D loss: 0.6692866086959839, acc.: 59.91%] [G loss: 0.768316924571991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 57/86 [D loss: 0.6725896000862122, acc.: 58.94%] [G loss: 0.7680737972259521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 58/86 [D loss: 0.6703153252601624, acc.: 59.52%] [G loss: 0.7649924755096436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 59/86 [D loss: 0.6706123352050781, acc.: 60.11%] [G loss: 0.7673426866531372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 60/86 [D loss: 0.6714472472667694, acc.: 58.45%] [G loss: 0.7678695917129517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 61/86 [D loss: 0.6701698303222656, acc.: 58.25%] [G loss: 0.7694198489189148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 62/86 [D loss: 0.6724051833152771, acc.: 58.54%] [G loss: 0.7639979720115662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 63/86 [D loss: 0.6723885238170624, acc.: 58.15%] [G loss: 0.7742584347724915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 64/86 [D loss: 0.6697906851768494, acc.: 59.42%] [G loss: 0.7593679428100586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 65/86 [D loss: 0.667890340089798, acc.: 59.23%] [G loss: 0.7676973342895508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 66/86 [D loss: 0.6703340113162994, acc.: 58.98%] [G loss: 0.7745746970176697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 67/86 [D loss: 0.667728066444397, acc.: 60.50%] [G loss: 0.7769656777381897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 68/86 [D loss: 0.6657582223415375, acc.: 58.50%] [G loss: 0.7647347450256348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 69/86 [D loss: 0.6756939888000488, acc.: 56.20%] [G loss: 0.7614439725875854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 70/86 [D loss: 0.6691650748252869, acc.: 59.28%] [G loss: 0.7663448452949524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 71/86 [D loss: 0.6663073003292084, acc.: 60.69%] [G loss: 0.772325873374939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 72/86 [D loss: 0.6689353287220001, acc.: 61.62%] [G loss: 0.7609282732009888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 73/86 [D loss: 0.6717741787433624, acc.: 58.79%] [G loss: 0.7622573971748352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 74/86 [D loss: 0.6689929664134979, acc.: 58.69%] [G loss: 0.7653912901878357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 75/86 [D loss: 0.6718435883522034, acc.: 59.38%] [G loss: 0.7648953199386597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 76/86 [D loss: 0.667365163564682, acc.: 60.69%] [G loss: 0.7631179094314575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 77/86 [D loss: 0.6744242906570435, acc.: 58.01%] [G loss: 0.760296106338501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 78/86 [D loss: 0.6674996018409729, acc.: 59.52%] [G loss: 0.7656455636024475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 79/86 [D loss: 0.6669662594795227, acc.: 59.57%] [G loss: 0.7602364420890808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 80/86 [D loss: 0.667734295129776, acc.: 60.50%] [G loss: 0.7619915008544922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 81/86 [D loss: 0.6682199537754059, acc.: 59.38%] [G loss: 0.7596968412399292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 82/86 [D loss: 0.6749978065490723, acc.: 56.98%] [G loss: 0.7641530632972717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 83/86 [D loss: 0.6706830859184265, acc.: 57.67%] [G loss: 0.7651165723800659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 84/86 [D loss: 0.6723971962928772, acc.: 57.47%] [G loss: 0.7689027190208435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 85/86 [D loss: 0.6655857861042023, acc.: 61.08%] [G loss: 0.7759681344032288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 142/200, Batch 86/86 [D loss: 0.6661382019519806, acc.: 59.03%] [G loss: 0.7601640224456787]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 1/86 [D loss: 0.6624654233455658, acc.: 61.28%] [G loss: 0.7757971286773682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 2/86 [D loss: 0.6729194223880768, acc.: 58.50%] [G loss: 0.7653931379318237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 3/86 [D loss: 0.6688399314880371, acc.: 58.45%] [G loss: 0.7718693017959595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 4/86 [D loss: 0.6704755127429962, acc.: 58.20%] [G loss: 0.7626045346260071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 5/86 [D loss: 0.6747540831565857, acc.: 56.20%] [G loss: 0.7686995267868042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 6/86 [D loss: 0.6661670804023743, acc.: 60.30%] [G loss: 0.7572492361068726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 7/86 [D loss: 0.6779824197292328, acc.: 57.28%] [G loss: 0.7701845169067383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 8/86 [D loss: 0.67332524061203, acc.: 58.45%] [G loss: 0.7565112709999084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 9/86 [D loss: 0.6747848987579346, acc.: 57.37%] [G loss: 0.7627710103988647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 10/86 [D loss: 0.6730154752731323, acc.: 58.50%] [G loss: 0.7552646994590759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 11/86 [D loss: 0.6731886863708496, acc.: 57.62%] [G loss: 0.761001706123352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 12/86 [D loss: 0.6693037748336792, acc.: 59.72%] [G loss: 0.7623491287231445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 13/86 [D loss: 0.6739693284034729, acc.: 57.62%] [G loss: 0.763931930065155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 14/86 [D loss: 0.6633020639419556, acc.: 61.52%] [G loss: 0.7588937282562256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 15/86 [D loss: 0.6684543192386627, acc.: 60.06%] [G loss: 0.7685247659683228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 16/86 [D loss: 0.6700015068054199, acc.: 57.76%] [G loss: 0.7692620754241943]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 17/86 [D loss: 0.6748892664909363, acc.: 57.18%] [G loss: 0.7726901173591614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 18/86 [D loss: 0.6731874048709869, acc.: 58.54%] [G loss: 0.7740495204925537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 19/86 [D loss: 0.6684697270393372, acc.: 60.50%] [G loss: 0.7673919200897217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 20/86 [D loss: 0.6682385504245758, acc.: 58.84%] [G loss: 0.7716232538223267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 21/86 [D loss: 0.67376708984375, acc.: 58.06%] [G loss: 0.7647697329521179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 22/86 [D loss: 0.6734951734542847, acc.: 58.25%] [G loss: 0.7729325890541077]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 23/86 [D loss: 0.6745438575744629, acc.: 57.18%] [G loss: 0.7584526538848877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 24/86 [D loss: 0.6714986860752106, acc.: 58.20%] [G loss: 0.7697376012802124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 25/86 [D loss: 0.6729865670204163, acc.: 57.08%] [G loss: 0.7591047286987305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 26/86 [D loss: 0.6675572097301483, acc.: 58.79%] [G loss: 0.7715634107589722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 27/86 [D loss: 0.6706321239471436, acc.: 59.91%] [G loss: 0.7574892640113831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 28/86 [D loss: 0.6700741350650787, acc.: 59.47%] [G loss: 0.7701345682144165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 29/86 [D loss: 0.6677829921245575, acc.: 59.77%] [G loss: 0.7537884712219238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 30/86 [D loss: 0.6765202581882477, acc.: 55.57%] [G loss: 0.7737781405448914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 31/86 [D loss: 0.6655320525169373, acc.: 61.04%] [G loss: 0.7628377079963684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 32/86 [D loss: 0.675247073173523, acc.: 57.08%] [G loss: 0.7789183259010315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 33/86 [D loss: 0.6686841249465942, acc.: 58.25%] [G loss: 0.7625418305397034]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 34/86 [D loss: 0.6711907386779785, acc.: 57.67%] [G loss: 0.7695677280426025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 35/86 [D loss: 0.6659733653068542, acc.: 59.86%] [G loss: 0.7670290470123291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 36/86 [D loss: 0.6769503355026245, acc.: 56.93%] [G loss: 0.7790968418121338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 37/86 [D loss: 0.6694464683532715, acc.: 60.55%] [G loss: 0.7629184722900391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 38/86 [D loss: 0.6728270649909973, acc.: 56.93%] [G loss: 0.7666095495223999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 39/86 [D loss: 0.6698689162731171, acc.: 59.08%] [G loss: 0.7555683255195618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 40/86 [D loss: 0.6720916926860809, acc.: 58.35%] [G loss: 0.7675883173942566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 41/86 [D loss: 0.6731152534484863, acc.: 57.67%] [G loss: 0.7637588381767273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 42/86 [D loss: 0.6716137230396271, acc.: 58.06%] [G loss: 0.7686935663223267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 43/86 [D loss: 0.6680252850055695, acc.: 57.67%] [G loss: 0.7743145227432251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 44/86 [D loss: 0.6736695170402527, acc.: 57.71%] [G loss: 0.781451940536499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 45/86 [D loss: 0.6693525910377502, acc.: 58.59%] [G loss: 0.7651243805885315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 46/86 [D loss: 0.6737899780273438, acc.: 57.86%] [G loss: 0.759788453578949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 47/86 [D loss: 0.6769540011882782, acc.: 57.32%] [G loss: 0.7651602625846863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 48/86 [D loss: 0.6693015992641449, acc.: 58.59%] [G loss: 0.7618893384933472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 49/86 [D loss: 0.676879495382309, acc.: 57.03%] [G loss: 0.7765953540802002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 50/86 [D loss: 0.6711276471614838, acc.: 57.47%] [G loss: 0.7575746178627014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 51/86 [D loss: 0.6743485927581787, acc.: 55.22%] [G loss: 0.7744576334953308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 52/86 [D loss: 0.6709944307804108, acc.: 58.59%] [G loss: 0.7552976608276367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 53/86 [D loss: 0.678229421377182, acc.: 56.15%] [G loss: 0.7679955363273621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 54/86 [D loss: 0.6675974428653717, acc.: 60.64%] [G loss: 0.7656342387199402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 55/86 [D loss: 0.6750374734401703, acc.: 54.83%] [G loss: 0.7624750137329102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 56/86 [D loss: 0.6665330231189728, acc.: 58.98%] [G loss: 0.7710226774215698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 57/86 [D loss: 0.671638697385788, acc.: 57.47%] [G loss: 0.7635050415992737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 58/86 [D loss: 0.6709363758563995, acc.: 58.45%] [G loss: 0.7654367685317993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 59/86 [D loss: 0.6717917025089264, acc.: 58.25%] [G loss: 0.7622489333152771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 60/86 [D loss: 0.6729497015476227, acc.: 58.11%] [G loss: 0.7704769372940063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 61/86 [D loss: 0.669587641954422, acc.: 58.79%] [G loss: 0.7576556205749512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 62/86 [D loss: 0.6645997762680054, acc.: 59.28%] [G loss: 0.7768454551696777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 63/86 [D loss: 0.6702664792537689, acc.: 58.35%] [G loss: 0.7587602734565735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 64/86 [D loss: 0.6732387840747833, acc.: 58.15%] [G loss: 0.7689782381057739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 65/86 [D loss: 0.6671167612075806, acc.: 59.52%] [G loss: 0.7587540745735168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 66/86 [D loss: 0.6771183609962463, acc.: 57.03%] [G loss: 0.7796050310134888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 67/86 [D loss: 0.6748419404029846, acc.: 58.15%] [G loss: 0.769952118396759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 68/86 [D loss: 0.6752530634403229, acc.: 57.32%] [G loss: 0.7604600191116333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 69/86 [D loss: 0.6683050096035004, acc.: 58.94%] [G loss: 0.7594972848892212]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 70/86 [D loss: 0.6780900955200195, acc.: 56.54%] [G loss: 0.7622860074043274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 71/86 [D loss: 0.6681793928146362, acc.: 59.52%] [G loss: 0.762864887714386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 72/86 [D loss: 0.6711969971656799, acc.: 57.18%] [G loss: 0.7557575702667236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 73/86 [D loss: 0.6660650968551636, acc.: 60.06%] [G loss: 0.763259768486023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 74/86 [D loss: 0.6667440831661224, acc.: 60.30%] [G loss: 0.7771346569061279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 75/86 [D loss: 0.6614372432231903, acc.: 62.26%] [G loss: 0.7583132386207581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 76/86 [D loss: 0.672089159488678, acc.: 57.91%] [G loss: 0.7601946592330933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 77/86 [D loss: 0.6742092669010162, acc.: 57.52%] [G loss: 0.7715719938278198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 78/86 [D loss: 0.6701932549476624, acc.: 59.23%] [G loss: 0.7562544941902161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 79/86 [D loss: 0.6738883256912231, acc.: 57.08%] [G loss: 0.7629147171974182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 80/86 [D loss: 0.6680394113063812, acc.: 59.38%] [G loss: 0.7645704746246338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 81/86 [D loss: 0.6740383505821228, acc.: 57.08%] [G loss: 0.7648253440856934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 82/86 [D loss: 0.6684864163398743, acc.: 59.47%] [G loss: 0.7722402215003967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 83/86 [D loss: 0.6832047402858734, acc.: 54.00%] [G loss: 0.7679770588874817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 84/86 [D loss: 0.665309339761734, acc.: 61.18%] [G loss: 0.7554410696029663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 85/86 [D loss: 0.6729491651058197, acc.: 57.37%] [G loss: 0.768092691898346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 143/200, Batch 86/86 [D loss: 0.6670505404472351, acc.: 59.47%] [G loss: 0.7671433687210083]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 1/86 [D loss: 0.6656323373317719, acc.: 60.01%] [G loss: 0.7684778571128845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 2/86 [D loss: 0.6663582921028137, acc.: 59.33%] [G loss: 0.7743778824806213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 3/86 [D loss: 0.6714066565036774, acc.: 58.59%] [G loss: 0.7751578092575073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 4/86 [D loss: 0.6684281229972839, acc.: 59.72%] [G loss: 0.7714428305625916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 5/86 [D loss: 0.6703212857246399, acc.: 58.20%] [G loss: 0.7661368250846863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 6/86 [D loss: 0.6722195446491241, acc.: 57.76%] [G loss: 0.7727533578872681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 7/86 [D loss: 0.6751590073108673, acc.: 57.32%] [G loss: 0.7691031098365784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 8/86 [D loss: 0.6655865907669067, acc.: 60.60%] [G loss: 0.7721179723739624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 9/86 [D loss: 0.6688241064548492, acc.: 59.13%] [G loss: 0.7702521681785583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 10/86 [D loss: 0.6679921448230743, acc.: 58.98%] [G loss: 0.768346905708313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 11/86 [D loss: 0.6689779162406921, acc.: 58.94%] [G loss: 0.7701711058616638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 12/86 [D loss: 0.6725535094738007, acc.: 58.74%] [G loss: 0.763232946395874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 13/86 [D loss: 0.6712968349456787, acc.: 58.11%] [G loss: 0.7660119533538818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 14/86 [D loss: 0.6751562654972076, acc.: 58.01%] [G loss: 0.7590841054916382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 15/86 [D loss: 0.6667581796646118, acc.: 59.28%] [G loss: 0.7662071585655212]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 16/86 [D loss: 0.6652430295944214, acc.: 59.62%] [G loss: 0.7684879302978516]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 17/86 [D loss: 0.676242858171463, acc.: 56.35%] [G loss: 0.7684011459350586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 18/86 [D loss: 0.6731902956962585, acc.: 57.76%] [G loss: 0.760331928730011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 19/86 [D loss: 0.6761103570461273, acc.: 56.88%] [G loss: 0.7629267573356628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 20/86 [D loss: 0.6725892722606659, acc.: 57.67%] [G loss: 0.7696853280067444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 21/86 [D loss: 0.6778319478034973, acc.: 56.35%] [G loss: 0.7759323120117188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 22/86 [D loss: 0.6701046228408813, acc.: 59.18%] [G loss: 0.7744534611701965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 23/86 [D loss: 0.6663915514945984, acc.: 59.47%] [G loss: 0.7657958269119263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 24/86 [D loss: 0.6762365996837616, acc.: 57.13%] [G loss: 0.7663232684135437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 25/86 [D loss: 0.665753960609436, acc.: 59.47%] [G loss: 0.7703647613525391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 26/86 [D loss: 0.6712271869182587, acc.: 58.01%] [G loss: 0.7734661102294922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 27/86 [D loss: 0.6676920652389526, acc.: 59.52%] [G loss: 0.7702996730804443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 28/86 [D loss: 0.6674198508262634, acc.: 58.64%] [G loss: 0.769599437713623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 29/86 [D loss: 0.6694937646389008, acc.: 59.86%] [G loss: 0.7678824067115784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 30/86 [D loss: 0.6644521951675415, acc.: 59.52%] [G loss: 0.7710570693016052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 31/86 [D loss: 0.6717436909675598, acc.: 56.84%] [G loss: 0.7652143239974976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 32/86 [D loss: 0.6715432405471802, acc.: 58.20%] [G loss: 0.7543760538101196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 33/86 [D loss: 0.6715654730796814, acc.: 58.06%] [G loss: 0.7665709257125854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 34/86 [D loss: 0.6744267344474792, acc.: 56.69%] [G loss: 0.7647736668586731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 35/86 [D loss: 0.6699434220790863, acc.: 59.42%] [G loss: 0.7747215032577515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 36/86 [D loss: 0.669824481010437, acc.: 59.23%] [G loss: 0.76726233959198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 37/86 [D loss: 0.6726973950862885, acc.: 58.74%] [G loss: 0.7581947445869446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 38/86 [D loss: 0.6713548600673676, acc.: 57.76%] [G loss: 0.7725913524627686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 39/86 [D loss: 0.6709922552108765, acc.: 59.62%] [G loss: 0.7739905118942261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 40/86 [D loss: 0.6667517423629761, acc.: 59.47%] [G loss: 0.7739201784133911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 41/86 [D loss: 0.6655294299125671, acc.: 61.23%] [G loss: 0.7746652364730835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 42/86 [D loss: 0.6726528406143188, acc.: 58.45%] [G loss: 0.76408451795578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 43/86 [D loss: 0.6705702543258667, acc.: 57.96%] [G loss: 0.7647559642791748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 44/86 [D loss: 0.6705337762832642, acc.: 57.96%] [G loss: 0.7744730114936829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 45/86 [D loss: 0.6691825091838837, acc.: 58.64%] [G loss: 0.7641661763191223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 46/86 [D loss: 0.673784464597702, acc.: 56.79%] [G loss: 0.7681849598884583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 47/86 [D loss: 0.6675338447093964, acc.: 59.77%] [G loss: 0.7688761949539185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 48/86 [D loss: 0.6690937280654907, acc.: 59.72%] [G loss: 0.769681990146637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 49/86 [D loss: 0.6714644432067871, acc.: 58.64%] [G loss: 0.7692366242408752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 50/86 [D loss: 0.6723676025867462, acc.: 58.11%] [G loss: 0.7660955190658569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 51/86 [D loss: 0.6767370700836182, acc.: 56.98%] [G loss: 0.7676916718482971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 52/86 [D loss: 0.6765933632850647, acc.: 57.18%] [G loss: 0.7694639563560486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 53/86 [D loss: 0.6752942502498627, acc.: 58.06%] [G loss: 0.7711420655250549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 54/86 [D loss: 0.6690941452980042, acc.: 59.03%] [G loss: 0.7637335062026978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 55/86 [D loss: 0.6713118553161621, acc.: 57.86%] [G loss: 0.766608715057373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 56/86 [D loss: 0.672674685716629, acc.: 57.23%] [G loss: 0.765641450881958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 57/86 [D loss: 0.6676029562950134, acc.: 59.72%] [G loss: 0.7620167136192322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 58/86 [D loss: 0.6735705137252808, acc.: 59.33%] [G loss: 0.773356020450592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 59/86 [D loss: 0.6717186272144318, acc.: 57.96%] [G loss: 0.7698047161102295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 60/86 [D loss: 0.6662076115608215, acc.: 59.77%] [G loss: 0.7592021226882935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 61/86 [D loss: 0.6764068007469177, acc.: 57.23%] [G loss: 0.760319173336029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 62/86 [D loss: 0.6696220636367798, acc.: 58.69%] [G loss: 0.772125244140625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 63/86 [D loss: 0.6717455983161926, acc.: 58.74%] [G loss: 0.7621080875396729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 64/86 [D loss: 0.6625732183456421, acc.: 61.91%] [G loss: 0.7648061513900757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 65/86 [D loss: 0.671303778886795, acc.: 57.81%] [G loss: 0.7617031335830688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 66/86 [D loss: 0.6752278804779053, acc.: 55.86%] [G loss: 0.7702748775482178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 67/86 [D loss: 0.6721029579639435, acc.: 59.33%] [G loss: 0.747038722038269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 68/86 [D loss: 0.6721862852573395, acc.: 58.25%] [G loss: 0.7716466784477234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 69/86 [D loss: 0.6733241379261017, acc.: 56.64%] [G loss: 0.7553578615188599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 70/86 [D loss: 0.6770033538341522, acc.: 55.91%] [G loss: 0.7699971199035645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 71/86 [D loss: 0.6678551733493805, acc.: 58.25%] [G loss: 0.7512304782867432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 72/86 [D loss: 0.6752599775791168, acc.: 55.62%] [G loss: 0.7696587443351746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 73/86 [D loss: 0.6677467823028564, acc.: 60.01%] [G loss: 0.7560458779335022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 74/86 [D loss: 0.6711723506450653, acc.: 57.42%] [G loss: 0.7739354372024536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 75/86 [D loss: 0.6716659367084503, acc.: 57.67%] [G loss: 0.7643065452575684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 76/86 [D loss: 0.6640788316726685, acc.: 60.06%] [G loss: 0.7790142297744751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 77/86 [D loss: 0.6694259345531464, acc.: 59.18%] [G loss: 0.7839053869247437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 78/86 [D loss: 0.6730850040912628, acc.: 58.01%] [G loss: 0.7648665904998779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 79/86 [D loss: 0.6640105843544006, acc.: 58.98%] [G loss: 0.7619489431381226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 80/86 [D loss: 0.6679516732692719, acc.: 60.30%] [G loss: 0.7578397989273071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 81/86 [D loss: 0.6648704707622528, acc.: 59.08%] [G loss: 0.7680731415748596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 82/86 [D loss: 0.6725745499134064, acc.: 58.25%] [G loss: 0.7647854685783386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 83/86 [D loss: 0.6724728643894196, acc.: 57.76%] [G loss: 0.7716062068939209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 84/86 [D loss: 0.6687366962432861, acc.: 56.88%] [G loss: 0.7666900157928467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 85/86 [D loss: 0.6661810874938965, acc.: 59.38%] [G loss: 0.770471453666687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 144/200, Batch 86/86 [D loss: 0.6688761711120605, acc.: 57.67%] [G loss: 0.7617424726486206]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 1/86 [D loss: 0.670245885848999, acc.: 57.71%] [G loss: 0.7637917995452881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 2/86 [D loss: 0.6711902320384979, acc.: 58.79%] [G loss: 0.7731980085372925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 3/86 [D loss: 0.6707898378372192, acc.: 58.69%] [G loss: 0.7702556848526001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 4/86 [D loss: 0.6671752333641052, acc.: 59.96%] [G loss: 0.7672004699707031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 5/86 [D loss: 0.6763864159584045, acc.: 56.40%] [G loss: 0.7709164023399353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 6/86 [D loss: 0.6683312952518463, acc.: 59.96%] [G loss: 0.7636972665786743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 7/86 [D loss: 0.6728730499744415, acc.: 58.25%] [G loss: 0.7668107748031616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 8/86 [D loss: 0.6703011095523834, acc.: 59.67%] [G loss: 0.7704501152038574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 9/86 [D loss: 0.6702963709831238, acc.: 57.62%] [G loss: 0.7678236961364746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 10/86 [D loss: 0.6701242923736572, acc.: 59.47%] [G loss: 0.768222987651825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 11/86 [D loss: 0.671816885471344, acc.: 58.15%] [G loss: 0.7732700705528259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 12/86 [D loss: 0.6618528664112091, acc.: 60.60%] [G loss: 0.7674969434738159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 13/86 [D loss: 0.6698727607727051, acc.: 59.42%] [G loss: 0.7637246251106262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 14/86 [D loss: 0.6713760197162628, acc.: 57.76%] [G loss: 0.7646603584289551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 15/86 [D loss: 0.6702487766742706, acc.: 57.03%] [G loss: 0.7762526869773865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 16/86 [D loss: 0.6713666617870331, acc.: 58.35%] [G loss: 0.7711979150772095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 17/86 [D loss: 0.670012354850769, acc.: 59.47%] [G loss: 0.7715470790863037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 18/86 [D loss: 0.6700704395771027, acc.: 59.72%] [G loss: 0.7601161599159241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 19/86 [D loss: 0.6711913347244263, acc.: 58.25%] [G loss: 0.7692877650260925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 20/86 [D loss: 0.6668490469455719, acc.: 60.01%] [G loss: 0.7580738067626953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 21/86 [D loss: 0.6754898428916931, acc.: 56.35%] [G loss: 0.7707852125167847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 22/86 [D loss: 0.6724328994750977, acc.: 57.71%] [G loss: 0.7617526054382324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 23/86 [D loss: 0.6684271991252899, acc.: 58.64%] [G loss: 0.7710280418395996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 24/86 [D loss: 0.6722204685211182, acc.: 58.54%] [G loss: 0.767219603061676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 25/86 [D loss: 0.6695261299610138, acc.: 58.89%] [G loss: 0.7617781162261963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 26/86 [D loss: 0.6701197624206543, acc.: 58.79%] [G loss: 0.7553328275680542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 27/86 [D loss: 0.6779285371303558, acc.: 55.22%] [G loss: 0.7613691687583923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 28/86 [D loss: 0.6652251780033112, acc.: 59.72%] [G loss: 0.7740981578826904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 29/86 [D loss: 0.6697567999362946, acc.: 58.84%] [G loss: 0.7695708870887756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 30/86 [D loss: 0.6709551215171814, acc.: 58.40%] [G loss: 0.7601540684700012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 31/86 [D loss: 0.67316934466362, acc.: 57.62%] [G loss: 0.7674514055252075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 32/86 [D loss: 0.6702973246574402, acc.: 60.16%] [G loss: 0.7779710292816162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 33/86 [D loss: 0.6619197428226471, acc.: 61.72%] [G loss: 0.7623447775840759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 34/86 [D loss: 0.6723216474056244, acc.: 58.15%] [G loss: 0.7729986310005188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 35/86 [D loss: 0.6643045842647552, acc.: 60.21%] [G loss: 0.7632425427436829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 36/86 [D loss: 0.6673350036144257, acc.: 58.89%] [G loss: 0.7738732099533081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 37/86 [D loss: 0.6699016690254211, acc.: 58.74%] [G loss: 0.7642825841903687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 38/86 [D loss: 0.6734679639339447, acc.: 57.42%] [G loss: 0.768191933631897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 39/86 [D loss: 0.6713853180408478, acc.: 57.67%] [G loss: 0.7662642002105713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 40/86 [D loss: 0.6698658168315887, acc.: 59.23%] [G loss: 0.7703843712806702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 41/86 [D loss: 0.6688829362392426, acc.: 59.81%] [G loss: 0.7742798924446106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 42/86 [D loss: 0.665908694267273, acc.: 60.64%] [G loss: 0.7699304223060608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 43/86 [D loss: 0.6702598333358765, acc.: 58.59%] [G loss: 0.7631463408470154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 44/86 [D loss: 0.6641631126403809, acc.: 60.06%] [G loss: 0.7658554315567017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 45/86 [D loss: 0.6746661365032196, acc.: 57.96%] [G loss: 0.7688320875167847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 46/86 [D loss: 0.6753183007240295, acc.: 56.05%] [G loss: 0.7663373947143555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 47/86 [D loss: 0.6679741442203522, acc.: 59.33%] [G loss: 0.7651658058166504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 48/86 [D loss: 0.6676654517650604, acc.: 59.08%] [G loss: 0.7658352851867676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 49/86 [D loss: 0.674137145280838, acc.: 57.81%] [G loss: 0.7675825357437134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 50/86 [D loss: 0.667657345533371, acc.: 58.74%] [G loss: 0.769087553024292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 51/86 [D loss: 0.673192173242569, acc.: 56.98%] [G loss: 0.7548426389694214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 52/86 [D loss: 0.6781926453113556, acc.: 56.69%] [G loss: 0.7700533270835876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 53/86 [D loss: 0.6699527204036713, acc.: 59.42%] [G loss: 0.7566706538200378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 54/86 [D loss: 0.6823646426200867, acc.: 55.27%] [G loss: 0.7804933190345764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 55/86 [D loss: 0.6728144288063049, acc.: 56.74%] [G loss: 0.7619420289993286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 56/86 [D loss: 0.6680177748203278, acc.: 59.28%] [G loss: 0.7690598964691162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 57/86 [D loss: 0.6693914234638214, acc.: 59.62%] [G loss: 0.7515771389007568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 58/86 [D loss: 0.6702596545219421, acc.: 59.23%] [G loss: 0.7857042551040649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 59/86 [D loss: 0.6636589765548706, acc.: 60.40%] [G loss: 0.7633551955223083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 60/86 [D loss: 0.6685487031936646, acc.: 58.94%] [G loss: 0.765018880367279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 61/86 [D loss: 0.671787440776825, acc.: 58.30%] [G loss: 0.7635858058929443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 62/86 [D loss: 0.6795172095298767, acc.: 55.86%] [G loss: 0.7709478139877319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 63/86 [D loss: 0.664452850818634, acc.: 59.38%] [G loss: 0.7553892135620117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 64/86 [D loss: 0.6793527603149414, acc.: 54.88%] [G loss: 0.7608607411384583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 65/86 [D loss: 0.6690989136695862, acc.: 59.42%] [G loss: 0.7637190818786621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 66/86 [D loss: 0.6728863716125488, acc.: 57.28%] [G loss: 0.7669379711151123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 67/86 [D loss: 0.6705116927623749, acc.: 58.20%] [G loss: 0.7544437646865845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 68/86 [D loss: 0.6721059381961823, acc.: 58.64%] [G loss: 0.7687464356422424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 69/86 [D loss: 0.668844074010849, acc.: 58.35%] [G loss: 0.7663204669952393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 70/86 [D loss: 0.6770791113376617, acc.: 57.52%] [G loss: 0.7713907361030579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 71/86 [D loss: 0.6653603613376617, acc.: 60.60%] [G loss: 0.7688069343566895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 72/86 [D loss: 0.6721857488155365, acc.: 58.59%] [G loss: 0.7579270005226135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 73/86 [D loss: 0.6692017912864685, acc.: 57.86%] [G loss: 0.775121808052063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 74/86 [D loss: 0.6730284988880157, acc.: 56.45%] [G loss: 0.7788169980049133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 75/86 [D loss: 0.6719586551189423, acc.: 59.18%] [G loss: 0.768180787563324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 76/86 [D loss: 0.6694730520248413, acc.: 58.74%] [G loss: 0.769156813621521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 77/86 [D loss: 0.6693419218063354, acc.: 58.64%] [G loss: 0.7757858633995056]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 78/86 [D loss: 0.6733100414276123, acc.: 57.03%] [G loss: 0.7721291780471802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 79/86 [D loss: 0.6684901416301727, acc.: 59.77%] [G loss: 0.7666993141174316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 80/86 [D loss: 0.6726221740245819, acc.: 58.30%] [G loss: 0.7676099538803101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 81/86 [D loss: 0.671844482421875, acc.: 58.59%] [G loss: 0.7648096084594727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 82/86 [D loss: 0.6698148846626282, acc.: 59.81%] [G loss: 0.7642513513565063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 83/86 [D loss: 0.6689098477363586, acc.: 58.74%] [G loss: 0.7697629928588867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 84/86 [D loss: 0.6663746535778046, acc.: 60.40%] [G loss: 0.7710870504379272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 85/86 [D loss: 0.668009877204895, acc.: 59.08%] [G loss: 0.7605223655700684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 145/200, Batch 86/86 [D loss: 0.6691979467868805, acc.: 58.64%] [G loss: 0.7730568051338196]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 1/86 [D loss: 0.6769610047340393, acc.: 55.37%] [G loss: 0.7604517340660095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 2/86 [D loss: 0.6691357493400574, acc.: 59.13%] [G loss: 0.7717207074165344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 3/86 [D loss: 0.6659226417541504, acc.: 59.96%] [G loss: 0.7606582641601562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 4/86 [D loss: 0.6715806722640991, acc.: 58.79%] [G loss: 0.7784407138824463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 5/86 [D loss: 0.6705211102962494, acc.: 59.28%] [G loss: 0.7668876647949219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 6/86 [D loss: 0.6726692318916321, acc.: 58.01%] [G loss: 0.7802674770355225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 7/86 [D loss: 0.667036384344101, acc.: 59.38%] [G loss: 0.7565131187438965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 8/86 [D loss: 0.6675595939159393, acc.: 58.54%] [G loss: 0.7636366486549377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 9/86 [D loss: 0.6632813811302185, acc.: 60.89%] [G loss: 0.7744179368019104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 10/86 [D loss: 0.6715447902679443, acc.: 57.76%] [G loss: 0.7608520984649658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 11/86 [D loss: 0.6735309064388275, acc.: 57.13%] [G loss: 0.7681911587715149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 12/86 [D loss: 0.6703701317310333, acc.: 59.08%] [G loss: 0.7634533643722534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 13/86 [D loss: 0.6636844575405121, acc.: 60.40%] [G loss: 0.7770979404449463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 14/86 [D loss: 0.6697958111763, acc.: 59.13%] [G loss: 0.7716352343559265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 15/86 [D loss: 0.6662409901618958, acc.: 60.06%] [G loss: 0.7777026891708374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 16/86 [D loss: 0.6735439300537109, acc.: 57.76%] [G loss: 0.7582839131355286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 17/86 [D loss: 0.6741792857646942, acc.: 56.88%] [G loss: 0.7706894874572754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 18/86 [D loss: 0.6635095477104187, acc.: 60.35%] [G loss: 0.755630373954773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 19/86 [D loss: 0.6721539497375488, acc.: 57.86%] [G loss: 0.7738950848579407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 20/86 [D loss: 0.66958087682724, acc.: 58.15%] [G loss: 0.7744450569152832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 21/86 [D loss: 0.6735508441925049, acc.: 57.03%] [G loss: 0.7748165726661682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 22/86 [D loss: 0.6648722290992737, acc.: 59.57%] [G loss: 0.765204668045044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 23/86 [D loss: 0.6748187839984894, acc.: 57.57%] [G loss: 0.7685930728912354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 24/86 [D loss: 0.6739436089992523, acc.: 57.71%] [G loss: 0.7661094665527344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 25/86 [D loss: 0.6756144464015961, acc.: 56.59%] [G loss: 0.7574342489242554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 26/86 [D loss: 0.6638249456882477, acc.: 60.16%] [G loss: 0.7784565091133118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 27/86 [D loss: 0.6740614771842957, acc.: 58.40%] [G loss: 0.7689628601074219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 28/86 [D loss: 0.6686642169952393, acc.: 59.67%] [G loss: 0.7639826536178589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 29/86 [D loss: 0.6705113649368286, acc.: 58.25%] [G loss: 0.7725851535797119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 30/86 [D loss: 0.6738407015800476, acc.: 57.57%] [G loss: 0.7636030316352844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 31/86 [D loss: 0.6633810698986053, acc.: 60.64%] [G loss: 0.7641340494155884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 32/86 [D loss: 0.678446352481842, acc.: 55.86%] [G loss: 0.7783826589584351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 33/86 [D loss: 0.6699867844581604, acc.: 57.52%] [G loss: 0.7665299773216248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 34/86 [D loss: 0.6721823215484619, acc.: 56.98%] [G loss: 0.7786595225334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 35/86 [D loss: 0.660361647605896, acc.: 61.18%] [G loss: 0.760696530342102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 36/86 [D loss: 0.6719914674758911, acc.: 58.45%] [G loss: 0.7699425220489502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 37/86 [D loss: 0.6634543836116791, acc.: 59.57%] [G loss: 0.7732654809951782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 38/86 [D loss: 0.6772730052471161, acc.: 56.01%] [G loss: 0.7666299939155579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 39/86 [D loss: 0.6711224019527435, acc.: 58.11%] [G loss: 0.7703551054000854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 40/86 [D loss: 0.6739792823791504, acc.: 57.28%] [G loss: 0.7577620148658752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 41/86 [D loss: 0.6673877239227295, acc.: 61.18%] [G loss: 0.7698746919631958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 42/86 [D loss: 0.6736965775489807, acc.: 57.62%] [G loss: 0.7701069712638855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 43/86 [D loss: 0.6710318922996521, acc.: 58.20%] [G loss: 0.7725479602813721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 44/86 [D loss: 0.6700966358184814, acc.: 58.79%] [G loss: 0.7570130825042725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 45/86 [D loss: 0.6706930994987488, acc.: 58.74%] [G loss: 0.7803885340690613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 46/86 [D loss: 0.6705582439899445, acc.: 58.20%] [G loss: 0.7607566714286804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 47/86 [D loss: 0.6729391813278198, acc.: 56.20%] [G loss: 0.7728639245033264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 48/86 [D loss: 0.6695044040679932, acc.: 59.03%] [G loss: 0.7732886075973511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 49/86 [D loss: 0.6753701269626617, acc.: 57.08%] [G loss: 0.765546977519989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 50/86 [D loss: 0.671020120382309, acc.: 59.38%] [G loss: 0.7707697749137878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 51/86 [D loss: 0.6728132963180542, acc.: 57.13%] [G loss: 0.7633517384529114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 52/86 [D loss: 0.6728855967521667, acc.: 57.08%] [G loss: 0.7671440839767456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 53/86 [D loss: 0.6687155067920685, acc.: 59.67%] [G loss: 0.769950807094574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 54/86 [D loss: 0.6704324781894684, acc.: 57.86%] [G loss: 0.780689001083374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 55/86 [D loss: 0.667194128036499, acc.: 59.91%] [G loss: 0.7698894739151001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 56/86 [D loss: 0.6682106256484985, acc.: 58.64%] [G loss: 0.7726738452911377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 57/86 [D loss: 0.6671656966209412, acc.: 59.72%] [G loss: 0.7654203176498413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 58/86 [D loss: 0.6728611290454865, acc.: 57.96%] [G loss: 0.767656683921814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 59/86 [D loss: 0.6698495745658875, acc.: 58.79%] [G loss: 0.7586751580238342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 60/86 [D loss: 0.6700330078601837, acc.: 58.74%] [G loss: 0.7729547023773193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 61/86 [D loss: 0.6637067496776581, acc.: 59.86%] [G loss: 0.7655945420265198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 62/86 [D loss: 0.6666064262390137, acc.: 59.72%] [G loss: 0.7714856863021851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 63/86 [D loss: 0.6683309972286224, acc.: 59.08%] [G loss: 0.7551737427711487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 64/86 [D loss: 0.6750341653823853, acc.: 57.13%] [G loss: 0.7668388485908508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 65/86 [D loss: 0.6718129813671112, acc.: 58.94%] [G loss: 0.7635849714279175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 66/86 [D loss: 0.679133415222168, acc.: 55.76%] [G loss: 0.7709371447563171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 67/86 [D loss: 0.6679511964321136, acc.: 60.99%] [G loss: 0.7659838199615479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 68/86 [D loss: 0.6707432568073273, acc.: 57.76%] [G loss: 0.7702553272247314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 69/86 [D loss: 0.6707802712917328, acc.: 59.38%] [G loss: 0.7733089327812195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 70/86 [D loss: 0.6710301637649536, acc.: 58.20%] [G loss: 0.7512301206588745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 71/86 [D loss: 0.6663283109664917, acc.: 60.69%] [G loss: 0.7685161828994751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 72/86 [D loss: 0.669451892375946, acc.: 59.23%] [G loss: 0.7545948028564453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 73/86 [D loss: 0.6680595576763153, acc.: 58.94%] [G loss: 0.7658406496047974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 74/86 [D loss: 0.6676889657974243, acc.: 60.16%] [G loss: 0.7606098055839539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 75/86 [D loss: 0.6690407395362854, acc.: 58.35%] [G loss: 0.7644433379173279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 76/86 [D loss: 0.6686434745788574, acc.: 58.50%] [G loss: 0.7572265863418579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 77/86 [D loss: 0.6704153716564178, acc.: 57.57%] [G loss: 0.7688817381858826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 78/86 [D loss: 0.6670254170894623, acc.: 59.47%] [G loss: 0.7644846439361572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 79/86 [D loss: 0.6744981706142426, acc.: 56.64%] [G loss: 0.7672284841537476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 80/86 [D loss: 0.6688765585422516, acc.: 58.50%] [G loss: 0.7667813897132874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 81/86 [D loss: 0.6734450161457062, acc.: 57.71%] [G loss: 0.768664538860321]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 146/200, Batch 82/86 [D loss: 0.6695703864097595, acc.: 59.28%] [G loss: 0.766155481338501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 83/86 [D loss: 0.6696328818798065, acc.: 58.98%] [G loss: 0.7720527052879333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 84/86 [D loss: 0.6746712028980255, acc.: 57.03%] [G loss: 0.7744323015213013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 85/86 [D loss: 0.6702742874622345, acc.: 58.84%] [G loss: 0.762687623500824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 146/200, Batch 86/86 [D loss: 0.6700325310230255, acc.: 58.69%] [G loss: 0.7623727917671204]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 1/86 [D loss: 0.6647110283374786, acc.: 60.21%] [G loss: 0.757213830947876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 2/86 [D loss: 0.6730651259422302, acc.: 57.47%] [G loss: 0.7697042226791382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 3/86 [D loss: 0.6666659712791443, acc.: 59.91%] [G loss: 0.7770285606384277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 4/86 [D loss: 0.672547310590744, acc.: 58.20%] [G loss: 0.757695198059082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 5/86 [D loss: 0.6751260459423065, acc.: 58.40%] [G loss: 0.7669406533241272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 6/86 [D loss: 0.6720103025436401, acc.: 58.59%] [G loss: 0.7678611278533936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 7/86 [D loss: 0.6733914017677307, acc.: 57.71%] [G loss: 0.7720664739608765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 8/86 [D loss: 0.664459615945816, acc.: 59.91%] [G loss: 0.7645726799964905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 9/86 [D loss: 0.6718697249889374, acc.: 58.20%] [G loss: 0.7720430493354797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 10/86 [D loss: 0.664012223482132, acc.: 60.45%] [G loss: 0.7721059918403625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 11/86 [D loss: 0.6758758425712585, acc.: 56.64%] [G loss: 0.7672199010848999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 12/86 [D loss: 0.6724947094917297, acc.: 58.84%] [G loss: 0.7710131406784058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 13/86 [D loss: 0.6708150207996368, acc.: 58.54%] [G loss: 0.7656286954879761]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 14/86 [D loss: 0.6730485558509827, acc.: 57.37%] [G loss: 0.7747846841812134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 15/86 [D loss: 0.674946129322052, acc.: 56.93%] [G loss: 0.7666478157043457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 16/86 [D loss: 0.6704674661159515, acc.: 59.47%] [G loss: 0.7808641791343689]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 17/86 [D loss: 0.6666303277015686, acc.: 60.11%] [G loss: 0.7686207890510559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 18/86 [D loss: 0.6709404289722443, acc.: 58.45%] [G loss: 0.7657936215400696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 19/86 [D loss: 0.6684798300266266, acc.: 58.40%] [G loss: 0.771354079246521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 20/86 [D loss: 0.676156759262085, acc.: 56.64%] [G loss: 0.7700686454772949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 21/86 [D loss: 0.666547954082489, acc.: 58.94%] [G loss: 0.7651584148406982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 22/86 [D loss: 0.6747594177722931, acc.: 57.71%] [G loss: 0.7675575613975525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 23/86 [D loss: 0.6712541878223419, acc.: 57.47%] [G loss: 0.7720112204551697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 24/86 [D loss: 0.6742604374885559, acc.: 57.91%] [G loss: 0.763848602771759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 25/86 [D loss: 0.6736122071743011, acc.: 58.40%] [G loss: 0.773369550704956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 26/86 [D loss: 0.6726067066192627, acc.: 58.40%] [G loss: 0.7785724997520447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 27/86 [D loss: 0.6663902103900909, acc.: 58.94%] [G loss: 0.7726664543151855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 28/86 [D loss: 0.6714569628238678, acc.: 57.96%] [G loss: 0.7753284573554993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 29/86 [D loss: 0.6708320379257202, acc.: 58.50%] [G loss: 0.7725619077682495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 30/86 [D loss: 0.6764706671237946, acc.: 56.84%] [G loss: 0.7707020044326782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 31/86 [D loss: 0.6689040958881378, acc.: 58.84%] [G loss: 0.7684831619262695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 32/86 [D loss: 0.671747237443924, acc.: 57.71%] [G loss: 0.7630566358566284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 33/86 [D loss: 0.671083390712738, acc.: 59.72%] [G loss: 0.7664907574653625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 34/86 [D loss: 0.6754090189933777, acc.: 56.79%] [G loss: 0.7685975432395935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 35/86 [D loss: 0.6692862212657928, acc.: 59.28%] [G loss: 0.7610613107681274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 36/86 [D loss: 0.6714010834693909, acc.: 58.64%] [G loss: 0.7744782567024231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 37/86 [D loss: 0.6695653200149536, acc.: 58.11%] [G loss: 0.7668923735618591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 38/86 [D loss: 0.669726699590683, acc.: 59.42%] [G loss: 0.7669160962104797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 39/86 [D loss: 0.6681555211544037, acc.: 59.42%] [G loss: 0.7770451307296753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 40/86 [D loss: 0.6754027307033539, acc.: 56.15%] [G loss: 0.7769458293914795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 41/86 [D loss: 0.6696421504020691, acc.: 58.20%] [G loss: 0.7682648301124573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 42/86 [D loss: 0.6664904356002808, acc.: 60.25%] [G loss: 0.7723840475082397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 43/86 [D loss: 0.6689181923866272, acc.: 58.45%] [G loss: 0.7604845762252808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 44/86 [D loss: 0.6724216341972351, acc.: 58.11%] [G loss: 0.7643479704856873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 45/86 [D loss: 0.6713886260986328, acc.: 57.81%] [G loss: 0.769449770450592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 46/86 [D loss: 0.6642441749572754, acc.: 60.99%] [G loss: 0.7742177248001099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 47/86 [D loss: 0.6713431179523468, acc.: 59.23%] [G loss: 0.7727309465408325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 48/86 [D loss: 0.6707995533943176, acc.: 58.84%] [G loss: 0.7678361535072327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 49/86 [D loss: 0.6675497889518738, acc.: 58.74%] [G loss: 0.76427161693573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 50/86 [D loss: 0.6649145483970642, acc.: 59.81%] [G loss: 0.772435188293457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 51/86 [D loss: 0.669843465089798, acc.: 60.01%] [G loss: 0.7735936045646667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 52/86 [D loss: 0.6694064736366272, acc.: 59.62%] [G loss: 0.7688600420951843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 53/86 [D loss: 0.6700341105461121, acc.: 59.28%] [G loss: 0.7729414105415344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 54/86 [D loss: 0.6696904003620148, acc.: 58.69%] [G loss: 0.7695886492729187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 55/86 [D loss: 0.6707460582256317, acc.: 58.59%] [G loss: 0.7738528847694397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 56/86 [D loss: 0.6684173941612244, acc.: 60.50%] [G loss: 0.7675313949584961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 57/86 [D loss: 0.6687659323215485, acc.: 59.08%] [G loss: 0.7738996148109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 58/86 [D loss: 0.6675622463226318, acc.: 59.03%] [G loss: 0.7698388695716858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 59/86 [D loss: 0.6731321513652802, acc.: 57.96%] [G loss: 0.7625104188919067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 60/86 [D loss: 0.6729446053504944, acc.: 58.01%] [G loss: 0.7652416825294495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 61/86 [D loss: 0.6644471287727356, acc.: 60.01%] [G loss: 0.7667939066886902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 62/86 [D loss: 0.6726336777210236, acc.: 57.57%] [G loss: 0.7650174498558044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 63/86 [D loss: 0.6722454130649567, acc.: 58.84%] [G loss: 0.7704057097434998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 64/86 [D loss: 0.6742251813411713, acc.: 57.42%] [G loss: 0.7611244916915894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 65/86 [D loss: 0.6631145775318146, acc.: 60.69%] [G loss: 0.7805394530296326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 66/86 [D loss: 0.6667968034744263, acc.: 58.35%] [G loss: 0.7651607394218445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 67/86 [D loss: 0.6690024733543396, acc.: 58.30%] [G loss: 0.7718139290809631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 68/86 [D loss: 0.6687048673629761, acc.: 59.47%] [G loss: 0.7793707251548767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 69/86 [D loss: 0.6683382987976074, acc.: 58.35%] [G loss: 0.7805586457252502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 70/86 [D loss: 0.6662366688251495, acc.: 58.11%] [G loss: 0.7729585766792297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 71/86 [D loss: 0.6715854406356812, acc.: 58.84%] [G loss: 0.7758557796478271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 72/86 [D loss: 0.6702423989772797, acc.: 57.91%] [G loss: 0.7741411924362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 73/86 [D loss: 0.6695152521133423, acc.: 57.81%] [G loss: 0.7701547741889954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 74/86 [D loss: 0.6687110364437103, acc.: 59.13%] [G loss: 0.7631639242172241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 75/86 [D loss: 0.6659460365772247, acc.: 59.38%] [G loss: 0.768057107925415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 76/86 [D loss: 0.6721185445785522, acc.: 57.96%] [G loss: 0.7724629044532776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 77/86 [D loss: 0.6629905998706818, acc.: 61.08%] [G loss: 0.7734143137931824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 78/86 [D loss: 0.661685973405838, acc.: 60.74%] [G loss: 0.7753320932388306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 79/86 [D loss: 0.6674994230270386, acc.: 59.72%] [G loss: 0.765624463558197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 80/86 [D loss: 0.6710099279880524, acc.: 59.23%] [G loss: 0.7689316868782043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 81/86 [D loss: 0.6707479655742645, acc.: 58.11%] [G loss: 0.7613820433616638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 82/86 [D loss: 0.663889080286026, acc.: 60.84%] [G loss: 0.778827428817749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 83/86 [D loss: 0.660722404718399, acc.: 62.26%] [G loss: 0.774265468120575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 84/86 [D loss: 0.6698090732097626, acc.: 59.23%] [G loss: 0.7730064988136292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 85/86 [D loss: 0.6632518172264099, acc.: 61.62%] [G loss: 0.7755526304244995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 147/200, Batch 86/86 [D loss: 0.6718745827674866, acc.: 58.30%] [G loss: 0.766393780708313]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 1/86 [D loss: 0.6663686633110046, acc.: 57.42%] [G loss: 0.769068717956543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 2/86 [D loss: 0.6684889793395996, acc.: 59.52%] [G loss: 0.7682063579559326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 3/86 [D loss: 0.6713048219680786, acc.: 58.74%] [G loss: 0.765448808670044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 4/86 [D loss: 0.6694916188716888, acc.: 58.40%] [G loss: 0.769982099533081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 5/86 [D loss: 0.6687207520008087, acc.: 58.84%] [G loss: 0.7730584144592285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 6/86 [D loss: 0.6701897382736206, acc.: 58.64%] [G loss: 0.7611201405525208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 7/86 [D loss: 0.6702766120433807, acc.: 58.20%] [G loss: 0.7743982076644897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 8/86 [D loss: 0.6712784469127655, acc.: 58.54%] [G loss: 0.7656110525131226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 9/86 [D loss: 0.6707428991794586, acc.: 58.25%] [G loss: 0.7709817290306091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 10/86 [D loss: 0.6705487668514252, acc.: 58.79%] [G loss: 0.7724189758300781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 11/86 [D loss: 0.6722747087478638, acc.: 58.45%] [G loss: 0.7770437598228455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 12/86 [D loss: 0.6685905158519745, acc.: 59.96%] [G loss: 0.770751416683197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 13/86 [D loss: 0.6704671382904053, acc.: 58.69%] [G loss: 0.7735011577606201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 14/86 [D loss: 0.6641556918621063, acc.: 60.16%] [G loss: 0.7784450054168701]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 15/86 [D loss: 0.669003039598465, acc.: 58.69%] [G loss: 0.778972864151001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 16/86 [D loss: 0.6681632101535797, acc.: 58.64%] [G loss: 0.7683296203613281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 17/86 [D loss: 0.6706490814685822, acc.: 58.15%] [G loss: 0.7635946869850159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 18/86 [D loss: 0.6654885113239288, acc.: 59.81%] [G loss: 0.7738136053085327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 19/86 [D loss: 0.6720899641513824, acc.: 58.35%] [G loss: 0.7695267200469971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 20/86 [D loss: 0.6705026626586914, acc.: 58.01%] [G loss: 0.7586327791213989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 21/86 [D loss: 0.6787633299827576, acc.: 55.52%] [G loss: 0.7663614153862]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 22/86 [D loss: 0.6679869592189789, acc.: 59.72%] [G loss: 0.7686071395874023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 23/86 [D loss: 0.672109991312027, acc.: 56.49%] [G loss: 0.7740908861160278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 24/86 [D loss: 0.6676165759563446, acc.: 58.89%] [G loss: 0.7685990929603577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 25/86 [D loss: 0.6694470047950745, acc.: 58.01%] [G loss: 0.7729973196983337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 26/86 [D loss: 0.6645565330982208, acc.: 61.38%] [G loss: 0.7772582769393921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 27/86 [D loss: 0.669062465429306, acc.: 58.69%] [G loss: 0.7652717232704163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 28/86 [D loss: 0.665655255317688, acc.: 59.72%] [G loss: 0.7700249552726746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 29/86 [D loss: 0.669494241476059, acc.: 58.30%] [G loss: 0.7677163481712341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 30/86 [D loss: 0.670039564371109, acc.: 58.11%] [G loss: 0.7697844505310059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 31/86 [D loss: 0.6700964272022247, acc.: 59.77%] [G loss: 0.770176351070404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 32/86 [D loss: 0.6694025993347168, acc.: 58.40%] [G loss: 0.7747538089752197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 33/86 [D loss: 0.6672935783863068, acc.: 59.67%] [G loss: 0.7588157057762146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 34/86 [D loss: 0.6646153628826141, acc.: 60.01%] [G loss: 0.7638331651687622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 35/86 [D loss: 0.668254017829895, acc.: 59.72%] [G loss: 0.764178454875946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 36/86 [D loss: 0.6739306747913361, acc.: 57.08%] [G loss: 0.7758214473724365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 37/86 [D loss: 0.6635369062423706, acc.: 60.45%] [G loss: 0.7730770707130432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 38/86 [D loss: 0.6755401492118835, acc.: 56.98%] [G loss: 0.7678887844085693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 39/86 [D loss: 0.6642928719520569, acc.: 60.30%] [G loss: 0.7710273861885071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 40/86 [D loss: 0.670337051153183, acc.: 58.84%] [G loss: 0.7666551470756531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 41/86 [D loss: 0.6683363616466522, acc.: 60.40%] [G loss: 0.7681165337562561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 42/86 [D loss: 0.6708438098430634, acc.: 59.18%] [G loss: 0.7668904066085815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 43/86 [D loss: 0.6676680445671082, acc.: 59.52%] [G loss: 0.7744359970092773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 44/86 [D loss: 0.6685669422149658, acc.: 60.35%] [G loss: 0.7722828388214111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 45/86 [D loss: 0.6666930019855499, acc.: 60.21%] [G loss: 0.7634934782981873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 46/86 [D loss: 0.6674769818782806, acc.: 59.28%] [G loss: 0.7665887475013733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 47/86 [D loss: 0.6657281219959259, acc.: 61.08%] [G loss: 0.7672127485275269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 48/86 [D loss: 0.6731917858123779, acc.: 57.67%] [G loss: 0.7745589017868042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 49/86 [D loss: 0.6689899265766144, acc.: 59.13%] [G loss: 0.7743016481399536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 50/86 [D loss: 0.6685773730278015, acc.: 57.62%] [G loss: 0.7743611335754395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 51/86 [D loss: 0.6700875461101532, acc.: 60.06%] [G loss: 0.7684447765350342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 52/86 [D loss: 0.6646924316883087, acc.: 59.13%] [G loss: 0.7708384990692139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 53/86 [D loss: 0.6740178763866425, acc.: 57.62%] [G loss: 0.7742531299591064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 54/86 [D loss: 0.6698635518550873, acc.: 58.30%] [G loss: 0.7714158296585083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 55/86 [D loss: 0.6697202324867249, acc.: 58.50%] [G loss: 0.7658671736717224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 56/86 [D loss: 0.6707926094532013, acc.: 57.42%] [G loss: 0.7639086842536926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 57/86 [D loss: 0.6686654686927795, acc.: 60.11%] [G loss: 0.7739107608795166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 58/86 [D loss: 0.6622438430786133, acc.: 60.69%] [G loss: 0.7622053027153015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 59/86 [D loss: 0.6729207038879395, acc.: 58.15%] [G loss: 0.7727163434028625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 60/86 [D loss: 0.6709877848625183, acc.: 56.98%] [G loss: 0.77934330701828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 61/86 [D loss: 0.6749874949455261, acc.: 56.79%] [G loss: 0.7735841274261475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 62/86 [D loss: 0.6674800217151642, acc.: 59.57%] [G loss: 0.7731398940086365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 63/86 [D loss: 0.6717954576015472, acc.: 58.54%] [G loss: 0.7751133441925049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 64/86 [D loss: 0.6682595610618591, acc.: 58.59%] [G loss: 0.7596993446350098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 65/86 [D loss: 0.6702198088169098, acc.: 58.74%] [G loss: 0.7793510556221008]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 66/86 [D loss: 0.6650823056697845, acc.: 60.45%] [G loss: 0.7755729556083679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 67/86 [D loss: 0.6762971580028534, acc.: 57.03%] [G loss: 0.7667754888534546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 68/86 [D loss: 0.668025404214859, acc.: 60.06%] [G loss: 0.7661152482032776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 69/86 [D loss: 0.661423534154892, acc.: 59.23%] [G loss: 0.7590404152870178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 70/86 [D loss: 0.6689040660858154, acc.: 59.13%] [G loss: 0.7824659943580627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 71/86 [D loss: 0.6648307144641876, acc.: 60.45%] [G loss: 0.7737914323806763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 72/86 [D loss: 0.6695692241191864, acc.: 57.52%] [G loss: 0.7715979814529419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 73/86 [D loss: 0.6626231968402863, acc.: 60.01%] [G loss: 0.7588208317756653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 74/86 [D loss: 0.6712821125984192, acc.: 58.11%] [G loss: 0.7719690799713135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 75/86 [D loss: 0.6652438044548035, acc.: 60.84%] [G loss: 0.7686805725097656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 76/86 [D loss: 0.6707189977169037, acc.: 58.79%] [G loss: 0.7776303887367249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 77/86 [D loss: 0.6737017333507538, acc.: 58.15%] [G loss: 0.7717561721801758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 78/86 [D loss: 0.6771582961082458, acc.: 55.86%] [G loss: 0.7633329629898071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 79/86 [D loss: 0.6662423312664032, acc.: 59.86%] [G loss: 0.7739076614379883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 80/86 [D loss: 0.6745140254497528, acc.: 57.18%] [G loss: 0.7598077058792114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 81/86 [D loss: 0.6660053133964539, acc.: 59.13%] [G loss: 0.7817222476005554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 82/86 [D loss: 0.6678653061389923, acc.: 59.33%] [G loss: 0.7672157883644104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 83/86 [D loss: 0.6763801276683807, acc.: 56.30%] [G loss: 0.7772173285484314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 84/86 [D loss: 0.6699870526790619, acc.: 58.01%] [G loss: 0.7655408978462219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 85/86 [D loss: 0.6697530746459961, acc.: 59.08%] [G loss: 0.7693555355072021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 148/200, Batch 86/86 [D loss: 0.6698020994663239, acc.: 58.64%] [G loss: 0.7750493884086609]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 1/86 [D loss: 0.6744759976863861, acc.: 57.37%] [G loss: 0.781964123249054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 2/86 [D loss: 0.6698111295700073, acc.: 58.79%] [G loss: 0.7633167505264282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 3/86 [D loss: 0.6700598001480103, acc.: 59.47%] [G loss: 0.7670772075653076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 4/86 [D loss: 0.6615229547023773, acc.: 60.06%] [G loss: 0.7680327892303467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 5/86 [D loss: 0.671081155538559, acc.: 57.62%] [G loss: 0.7761029601097107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 6/86 [D loss: 0.6594688296318054, acc.: 61.47%] [G loss: 0.7712711691856384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 7/86 [D loss: 0.67930868268013, acc.: 56.35%] [G loss: 0.7694510221481323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 8/86 [D loss: 0.665784478187561, acc.: 60.69%] [G loss: 0.7732433080673218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 9/86 [D loss: 0.6734173893928528, acc.: 57.37%] [G loss: 0.761454701423645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 10/86 [D loss: 0.668721616268158, acc.: 60.40%] [G loss: 0.7746151685714722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 11/86 [D loss: 0.6709123849868774, acc.: 58.64%] [G loss: 0.7677794694900513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 12/86 [D loss: 0.6714044809341431, acc.: 57.67%] [G loss: 0.7773303389549255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 13/86 [D loss: 0.6739363670349121, acc.: 57.23%] [G loss: 0.7610836625099182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 14/86 [D loss: 0.6697614192962646, acc.: 57.96%] [G loss: 0.7882773280143738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 15/86 [D loss: 0.6690822243690491, acc.: 57.96%] [G loss: 0.7670516967773438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 16/86 [D loss: 0.6726351380348206, acc.: 59.03%] [G loss: 0.7705583572387695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 17/86 [D loss: 0.676358163356781, acc.: 57.32%] [G loss: 0.759891152381897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 18/86 [D loss: 0.6700336635112762, acc.: 58.59%] [G loss: 0.7676751017570496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 19/86 [D loss: 0.6774649620056152, acc.: 56.59%] [G loss: 0.7553712129592896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 20/86 [D loss: 0.6712242960929871, acc.: 57.76%] [G loss: 0.7692539095878601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 21/86 [D loss: 0.666338711977005, acc.: 58.84%] [G loss: 0.7655075788497925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 22/86 [D loss: 0.6787161231040955, acc.: 55.22%] [G loss: 0.7854965925216675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 23/86 [D loss: 0.6704262495040894, acc.: 57.32%] [G loss: 0.7431671619415283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 24/86 [D loss: 0.6806408762931824, acc.: 55.81%] [G loss: 0.7734795808792114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 25/86 [D loss: 0.663918673992157, acc.: 59.81%] [G loss: 0.7574427127838135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 26/86 [D loss: 0.680568516254425, acc.: 56.01%] [G loss: 0.7735844254493713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 27/86 [D loss: 0.6646393835544586, acc.: 59.96%] [G loss: 0.7648478746414185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 28/86 [D loss: 0.6713943481445312, acc.: 57.57%] [G loss: 0.7654028534889221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 29/86 [D loss: 0.6678386926651001, acc.: 59.13%] [G loss: 0.7757925391197205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 30/86 [D loss: 0.668489545583725, acc.: 59.86%] [G loss: 0.7664546966552734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 31/86 [D loss: 0.6737872362136841, acc.: 58.50%] [G loss: 0.7783761024475098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 32/86 [D loss: 0.6698472797870636, acc.: 57.32%] [G loss: 0.7709097862243652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 33/86 [D loss: 0.6779469549655914, acc.: 55.27%] [G loss: 0.7706700563430786]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 34/86 [D loss: 0.6645493507385254, acc.: 59.67%] [G loss: 0.7621204257011414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 35/86 [D loss: 0.6747869849205017, acc.: 56.93%] [G loss: 0.7757594585418701]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 36/86 [D loss: 0.6626778841018677, acc.: 61.43%] [G loss: 0.7651849389076233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 37/86 [D loss: 0.6725833714008331, acc.: 57.08%] [G loss: 0.7647976279258728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 38/86 [D loss: 0.6687686443328857, acc.: 59.13%] [G loss: 0.7750664353370667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 39/86 [D loss: 0.6737982928752899, acc.: 58.20%] [G loss: 0.7563878297805786]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 40/86 [D loss: 0.6669478416442871, acc.: 59.57%] [G loss: 0.7756965160369873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 41/86 [D loss: 0.6707511246204376, acc.: 57.91%] [G loss: 0.765819787979126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 42/86 [D loss: 0.6747507452964783, acc.: 57.42%] [G loss: 0.7730258107185364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 43/86 [D loss: 0.6675991117954254, acc.: 58.74%] [G loss: 0.7609186172485352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 44/86 [D loss: 0.6696755290031433, acc.: 59.52%] [G loss: 0.7744011878967285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 45/86 [D loss: 0.669672816991806, acc.: 57.67%] [G loss: 0.7647386193275452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 46/86 [D loss: 0.6734720766544342, acc.: 57.03%] [G loss: 0.7707418203353882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 47/86 [D loss: 0.6686680316925049, acc.: 58.89%] [G loss: 0.7711645364761353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 48/86 [D loss: 0.6698033511638641, acc.: 58.69%] [G loss: 0.7644845247268677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 49/86 [D loss: 0.6687081456184387, acc.: 57.96%] [G loss: 0.7662064433097839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 50/86 [D loss: 0.6728040874004364, acc.: 56.35%] [G loss: 0.7653548717498779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 51/86 [D loss: 0.6691955327987671, acc.: 59.72%] [G loss: 0.7689681649208069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 52/86 [D loss: 0.6661920249462128, acc.: 59.91%] [G loss: 0.7671640515327454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 53/86 [D loss: 0.6691509783267975, acc.: 60.01%] [G loss: 0.767736554145813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 54/86 [D loss: 0.6770730316638947, acc.: 57.62%] [G loss: 0.7650460004806519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 55/86 [D loss: 0.6691655218601227, acc.: 58.30%] [G loss: 0.7692590951919556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 56/86 [D loss: 0.6694735884666443, acc.: 57.81%] [G loss: 0.7648775577545166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 57/86 [D loss: 0.6676596105098724, acc.: 59.72%] [G loss: 0.7725600004196167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 58/86 [D loss: 0.6690899431705475, acc.: 59.91%] [G loss: 0.7792140245437622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 59/86 [D loss: 0.6672065854072571, acc.: 59.47%] [G loss: 0.7749733924865723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 60/86 [D loss: 0.6654242873191833, acc.: 58.06%] [G loss: 0.7772942185401917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 61/86 [D loss: 0.6649268567562103, acc.: 58.98%] [G loss: 0.7749113440513611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 62/86 [D loss: 0.6685172915458679, acc.: 58.89%] [G loss: 0.7738648056983948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 63/86 [D loss: 0.6726115345954895, acc.: 58.94%] [G loss: 0.7650493383407593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 64/86 [D loss: 0.6699409186840057, acc.: 58.64%] [G loss: 0.7617413997650146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 65/86 [D loss: 0.6706909537315369, acc.: 58.79%] [G loss: 0.7682211995124817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 66/86 [D loss: 0.6711125075817108, acc.: 60.11%] [G loss: 0.7767393589019775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 67/86 [D loss: 0.6632294654846191, acc.: 60.94%] [G loss: 0.7661342024803162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 68/86 [D loss: 0.6688607037067413, acc.: 59.08%] [G loss: 0.7688965201377869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 69/86 [D loss: 0.6714510321617126, acc.: 58.94%] [G loss: 0.7726908326148987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 70/86 [D loss: 0.6676498055458069, acc.: 58.01%] [G loss: 0.7766131162643433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 71/86 [D loss: 0.6684902608394623, acc.: 59.08%] [G loss: 0.7783223390579224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 72/86 [D loss: 0.6723206639289856, acc.: 58.59%] [G loss: 0.7600765228271484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 73/86 [D loss: 0.6702542304992676, acc.: 58.40%] [G loss: 0.7662122845649719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 74/86 [D loss: 0.6690994799137115, acc.: 56.88%] [G loss: 0.7634158134460449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 75/86 [D loss: 0.6711516082286835, acc.: 57.81%] [G loss: 0.7796808481216431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 76/86 [D loss: 0.6663981974124908, acc.: 59.96%] [G loss: 0.7807652354240417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 77/86 [D loss: 0.6696702241897583, acc.: 59.18%] [G loss: 0.779872715473175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 78/86 [D loss: 0.6689810752868652, acc.: 58.54%] [G loss: 0.771926999092102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 79/86 [D loss: 0.6707568168640137, acc.: 57.86%] [G loss: 0.7675507068634033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 80/86 [D loss: 0.6662062406539917, acc.: 58.94%] [G loss: 0.7651516795158386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 81/86 [D loss: 0.6781789660453796, acc.: 56.25%] [G loss: 0.7605911493301392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 82/86 [D loss: 0.6632229685783386, acc.: 60.55%] [G loss: 0.7622146606445312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 83/86 [D loss: 0.6673958599567413, acc.: 58.74%] [G loss: 0.7720155715942383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 84/86 [D loss: 0.6700223982334137, acc.: 58.45%] [G loss: 0.777755618095398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 85/86 [D loss: 0.6667462885379791, acc.: 59.57%] [G loss: 0.7623504996299744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 149/200, Batch 86/86 [D loss: 0.6693158745765686, acc.: 58.25%] [G loss: 0.7688981890678406]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 1/86 [D loss: 0.6671133637428284, acc.: 58.84%] [G loss: 0.7721024751663208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 2/86 [D loss: 0.6590008735656738, acc.: 59.96%] [G loss: 0.7759817838668823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 3/86 [D loss: 0.6705287098884583, acc.: 58.98%] [G loss: 0.77583909034729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 4/86 [D loss: 0.6640156209468842, acc.: 59.52%] [G loss: 0.7702274322509766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 5/86 [D loss: 0.6705141067504883, acc.: 59.57%] [G loss: 0.7706905603408813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 6/86 [D loss: 0.6631690263748169, acc.: 60.16%] [G loss: 0.7673957943916321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 7/86 [D loss: 0.6693128049373627, acc.: 59.28%] [G loss: 0.771243691444397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 8/86 [D loss: 0.6646207571029663, acc.: 59.47%] [G loss: 0.7695645093917847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 9/86 [D loss: 0.6691742837429047, acc.: 60.64%] [G loss: 0.7688513398170471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 10/86 [D loss: 0.6680153012275696, acc.: 58.89%] [G loss: 0.7704356908798218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 11/86 [D loss: 0.6667621433734894, acc.: 59.62%] [G loss: 0.7742795944213867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 12/86 [D loss: 0.6735531091690063, acc.: 57.23%] [G loss: 0.7701012492179871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 13/86 [D loss: 0.6693973541259766, acc.: 58.15%] [G loss: 0.774864673614502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 14/86 [D loss: 0.668510228395462, acc.: 59.18%] [G loss: 0.7777789235115051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 15/86 [D loss: 0.6755449175834656, acc.: 56.54%] [G loss: 0.7739459872245789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 16/86 [D loss: 0.6729343831539154, acc.: 56.98%] [G loss: 0.7617788910865784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 17/86 [D loss: 0.6681841909885406, acc.: 58.94%] [G loss: 0.7725743651390076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 18/86 [D loss: 0.6703727841377258, acc.: 58.15%] [G loss: 0.7629122734069824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 19/86 [D loss: 0.6718130111694336, acc.: 57.37%] [G loss: 0.7639405727386475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 20/86 [D loss: 0.6711395680904388, acc.: 57.52%] [G loss: 0.7651852965354919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 21/86 [D loss: 0.6659232378005981, acc.: 58.15%] [G loss: 0.7707831263542175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 22/86 [D loss: 0.6666648983955383, acc.: 59.13%] [G loss: 0.7706308364868164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 23/86 [D loss: 0.6718060672283173, acc.: 56.64%] [G loss: 0.7691782712936401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 24/86 [D loss: 0.6694009006023407, acc.: 57.76%] [G loss: 0.7675433158874512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 25/86 [D loss: 0.6701875627040863, acc.: 58.54%] [G loss: 0.7733637094497681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 26/86 [D loss: 0.6668304204940796, acc.: 59.52%] [G loss: 0.7734346985816956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 27/86 [D loss: 0.6653207838535309, acc.: 60.94%] [G loss: 0.7730424404144287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 28/86 [D loss: 0.667494148015976, acc.: 60.01%] [G loss: 0.7685061693191528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 29/86 [D loss: 0.6650510728359222, acc.: 60.60%] [G loss: 0.7745519280433655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 30/86 [D loss: 0.6713216304779053, acc.: 57.52%] [G loss: 0.7778326869010925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 31/86 [D loss: 0.6659114062786102, acc.: 59.47%] [G loss: 0.7664657831192017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 32/86 [D loss: 0.6705315113067627, acc.: 58.20%] [G loss: 0.7750974893569946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 33/86 [D loss: 0.671456515789032, acc.: 57.62%] [G loss: 0.7717631459236145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 34/86 [D loss: 0.6721086204051971, acc.: 57.91%] [G loss: 0.7705156803131104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 35/86 [D loss: 0.6681797504425049, acc.: 59.72%] [G loss: 0.7724707126617432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 36/86 [D loss: 0.6726140379905701, acc.: 57.67%] [G loss: 0.7643231153488159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 37/86 [D loss: 0.6703069508075714, acc.: 57.18%] [G loss: 0.7703858017921448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 38/86 [D loss: 0.6726369857788086, acc.: 57.91%] [G loss: 0.775995671749115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 39/86 [D loss: 0.6663244068622589, acc.: 60.84%] [G loss: 0.7680239677429199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 40/86 [D loss: 0.676021546125412, acc.: 56.20%] [G loss: 0.7717708945274353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 41/86 [D loss: 0.6686270236968994, acc.: 59.28%] [G loss: 0.7641093730926514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 42/86 [D loss: 0.67527836561203, acc.: 57.23%] [G loss: 0.7826958894729614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 43/86 [D loss: 0.6650673151016235, acc.: 60.25%] [G loss: 0.7673690319061279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 44/86 [D loss: 0.6684589385986328, acc.: 59.08%] [G loss: 0.7671743631362915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 45/86 [D loss: 0.6660303175449371, acc.: 60.50%] [G loss: 0.7716456651687622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 46/86 [D loss: 0.6714394688606262, acc.: 57.81%] [G loss: 0.7779673933982849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 47/86 [D loss: 0.6660985946655273, acc.: 59.81%] [G loss: 0.7687473297119141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 48/86 [D loss: 0.6790267527103424, acc.: 55.81%] [G loss: 0.768613338470459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 49/86 [D loss: 0.6687017679214478, acc.: 57.18%] [G loss: 0.7688065767288208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 50/86 [D loss: 0.6737118363380432, acc.: 57.18%] [G loss: 0.7795473337173462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 51/86 [D loss: 0.6715669631958008, acc.: 58.01%] [G loss: 0.7700248956680298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 52/86 [D loss: 0.6689594388008118, acc.: 59.23%] [G loss: 0.7705782651901245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 53/86 [D loss: 0.667011559009552, acc.: 59.03%] [G loss: 0.7779271006584167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 54/86 [D loss: 0.6722837686538696, acc.: 57.28%] [G loss: 0.7657492160797119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 55/86 [D loss: 0.6648094356060028, acc.: 59.96%] [G loss: 0.7762248516082764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 56/86 [D loss: 0.6658742129802704, acc.: 60.64%] [G loss: 0.775181770324707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 57/86 [D loss: 0.6673882007598877, acc.: 58.98%] [G loss: 0.7828722596168518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 58/86 [D loss: 0.6677093803882599, acc.: 59.47%] [G loss: 0.7702397108078003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 59/86 [D loss: 0.6628847122192383, acc.: 61.57%] [G loss: 0.7676913738250732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 60/86 [D loss: 0.6623960137367249, acc.: 61.18%] [G loss: 0.7578092813491821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 61/86 [D loss: 0.6707732975482941, acc.: 58.59%] [G loss: 0.7692134976387024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 62/86 [D loss: 0.6632380783557892, acc.: 60.01%] [G loss: 0.7719873189926147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 63/86 [D loss: 0.6698923408985138, acc.: 59.13%] [G loss: 0.7710217237472534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 64/86 [D loss: 0.6679158508777618, acc.: 58.94%] [G loss: 0.7724520564079285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 65/86 [D loss: 0.6665611863136292, acc.: 59.62%] [G loss: 0.7759147882461548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 66/86 [D loss: 0.6684927642345428, acc.: 58.74%] [G loss: 0.7724171876907349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 67/86 [D loss: 0.674695611000061, acc.: 57.13%] [G loss: 0.7688601016998291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 68/86 [D loss: 0.664487212896347, acc.: 61.08%] [G loss: 0.7697679996490479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 69/86 [D loss: 0.6680157780647278, acc.: 58.45%] [G loss: 0.7643774747848511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 70/86 [D loss: 0.6693212389945984, acc.: 58.30%] [G loss: 0.7732120752334595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 71/86 [D loss: 0.6701626777648926, acc.: 58.50%] [G loss: 0.7742182016372681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 72/86 [D loss: 0.6705382168292999, acc.: 58.30%] [G loss: 0.7721074819564819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 73/86 [D loss: 0.6709110736846924, acc.: 58.15%] [G loss: 0.7780432105064392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 74/86 [D loss: 0.665951281785965, acc.: 59.18%] [G loss: 0.77247154712677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 75/86 [D loss: 0.6664752960205078, acc.: 60.16%] [G loss: 0.7651217579841614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 76/86 [D loss: 0.667797327041626, acc.: 58.69%] [G loss: 0.7664864659309387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 77/86 [D loss: 0.6745080947875977, acc.: 56.84%] [G loss: 0.769381582736969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 78/86 [D loss: 0.6682093143463135, acc.: 60.74%] [G loss: 0.7822157144546509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 79/86 [D loss: 0.6694651246070862, acc.: 59.62%] [G loss: 0.7760348320007324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 80/86 [D loss: 0.666734904050827, acc.: 59.38%] [G loss: 0.7687064409255981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 81/86 [D loss: 0.6735894978046417, acc.: 57.71%] [G loss: 0.7745990753173828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 82/86 [D loss: 0.670570433139801, acc.: 57.96%] [G loss: 0.7694178223609924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 83/86 [D loss: 0.666753888130188, acc.: 59.52%] [G loss: 0.7731267213821411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 84/86 [D loss: 0.6691029369831085, acc.: 58.06%] [G loss: 0.7755317091941833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 85/86 [D loss: 0.6730672121047974, acc.: 57.81%] [G loss: 0.7746835947036743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 150/200, Batch 86/86 [D loss: 0.6721996665000916, acc.: 58.64%] [G loss: 0.7730209827423096]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 1/86 [D loss: 0.6708534061908722, acc.: 58.35%] [G loss: 0.7842532396316528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 2/86 [D loss: 0.665276437997818, acc.: 58.98%] [G loss: 0.7696855664253235]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 3/86 [D loss: 0.6658712923526764, acc.: 59.96%] [G loss: 0.7624614834785461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 4/86 [D loss: 0.6659700870513916, acc.: 59.52%] [G loss: 0.7702593207359314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 5/86 [D loss: 0.6711314916610718, acc.: 59.42%] [G loss: 0.77176833152771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 6/86 [D loss: 0.6694940626621246, acc.: 57.86%] [G loss: 0.7683860063552856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 7/86 [D loss: 0.6680768430233002, acc.: 58.84%] [G loss: 0.7751930356025696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 8/86 [D loss: 0.6674790680408478, acc.: 60.06%] [G loss: 0.7737656235694885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 9/86 [D loss: 0.6700146496295929, acc.: 59.23%] [G loss: 0.7759101390838623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 10/86 [D loss: 0.6694568693637848, acc.: 58.45%] [G loss: 0.7667118310928345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 11/86 [D loss: 0.6685504615306854, acc.: 58.84%] [G loss: 0.7749663591384888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 12/86 [D loss: 0.6695593893527985, acc.: 59.08%] [G loss: 0.7667474150657654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 13/86 [D loss: 0.6712094843387604, acc.: 57.76%] [G loss: 0.7818561792373657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 14/86 [D loss: 0.6670144498348236, acc.: 59.72%] [G loss: 0.7618603110313416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 15/86 [D loss: 0.6650082170963287, acc.: 58.69%] [G loss: 0.7661375999450684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 16/86 [D loss: 0.6639778316020966, acc.: 59.13%] [G loss: 0.76139235496521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 17/86 [D loss: 0.6701084673404694, acc.: 58.20%] [G loss: 0.7652204632759094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 18/86 [D loss: 0.6684096455574036, acc.: 60.45%] [G loss: 0.7545081973075867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 19/86 [D loss: 0.6704684793949127, acc.: 57.91%] [G loss: 0.7800105810165405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 20/86 [D loss: 0.6635066866874695, acc.: 60.01%] [G loss: 0.7600390911102295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 21/86 [D loss: 0.6744123995304108, acc.: 56.84%] [G loss: 0.7730151414871216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 22/86 [D loss: 0.6637787818908691, acc.: 60.06%] [G loss: 0.7564219236373901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 23/86 [D loss: 0.6706259548664093, acc.: 58.69%] [G loss: 0.7761747241020203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 24/86 [D loss: 0.665233701467514, acc.: 59.52%] [G loss: 0.7760763168334961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 25/86 [D loss: 0.6705150604248047, acc.: 58.25%] [G loss: 0.76688152551651]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 26/86 [D loss: 0.6622861325740814, acc.: 61.52%] [G loss: 0.7853075861930847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 27/86 [D loss: 0.6655446290969849, acc.: 59.52%] [G loss: 0.7766662836074829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 28/86 [D loss: 0.6718429923057556, acc.: 57.42%] [G loss: 0.7707527875900269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 29/86 [D loss: 0.6741373538970947, acc.: 57.96%] [G loss: 0.777028501033783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 30/86 [D loss: 0.6679982542991638, acc.: 58.30%] [G loss: 0.7812285423278809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 31/86 [D loss: 0.6630195677280426, acc.: 60.45%] [G loss: 0.7763386964797974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 32/86 [D loss: 0.6678993403911591, acc.: 59.38%] [G loss: 0.7785714268684387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 33/86 [D loss: 0.6699856817722321, acc.: 57.71%] [G loss: 0.7758963108062744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 34/86 [D loss: 0.6686700284481049, acc.: 59.03%] [G loss: 0.7685754299163818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 35/86 [D loss: 0.6737436354160309, acc.: 57.91%] [G loss: 0.7777471542358398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 36/86 [D loss: 0.6716623306274414, acc.: 57.28%] [G loss: 0.7753357887268066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 37/86 [D loss: 0.6700281500816345, acc.: 57.47%] [G loss: 0.7708086371421814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 38/86 [D loss: 0.6682242751121521, acc.: 59.23%] [G loss: 0.7686709761619568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 39/86 [D loss: 0.6678916811943054, acc.: 59.13%] [G loss: 0.77376389503479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 40/86 [D loss: 0.6696940362453461, acc.: 59.47%] [G loss: 0.75892573595047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 41/86 [D loss: 0.665754109621048, acc.: 59.13%] [G loss: 0.7699524760246277]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 151/200, Batch 42/86 [D loss: 0.6725915372371674, acc.: 56.93%] [G loss: 0.7653550505638123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 43/86 [D loss: 0.6693460941314697, acc.: 57.32%] [G loss: 0.7726537585258484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 44/86 [D loss: 0.6623076796531677, acc.: 61.23%] [G loss: 0.771705150604248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 45/86 [D loss: 0.6687719821929932, acc.: 58.84%] [G loss: 0.7730510830879211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 46/86 [D loss: 0.6699663698673248, acc.: 59.13%] [G loss: 0.7685614824295044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 47/86 [D loss: 0.6697138547897339, acc.: 56.69%] [G loss: 0.7816579341888428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 48/86 [D loss: 0.6702088415622711, acc.: 58.98%] [G loss: 0.7836626768112183]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 151/200, Batch 49/86 [D loss: 0.6722605526447296, acc.: 57.52%] [G loss: 0.7794176936149597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 50/86 [D loss: 0.6752154231071472, acc.: 56.49%] [G loss: 0.7703696489334106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 51/86 [D loss: 0.6666373312473297, acc.: 59.96%] [G loss: 0.7734009623527527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 52/86 [D loss: 0.6664963364601135, acc.: 59.38%] [G loss: 0.7721908092498779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 53/86 [D loss: 0.6685332357883453, acc.: 58.69%] [G loss: 0.7647819519042969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 54/86 [D loss: 0.6665742993354797, acc.: 58.94%] [G loss: 0.7686976194381714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 55/86 [D loss: 0.6629979610443115, acc.: 61.72%] [G loss: 0.7683485150337219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 56/86 [D loss: 0.6640183925628662, acc.: 58.89%] [G loss: 0.7740040421485901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 57/86 [D loss: 0.6693195104598999, acc.: 59.38%] [G loss: 0.7744693756103516]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 58/86 [D loss: 0.6660422384738922, acc.: 59.42%] [G loss: 0.7688334584236145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 59/86 [D loss: 0.6681825518608093, acc.: 58.94%] [G loss: 0.7761784791946411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 60/86 [D loss: 0.6685686409473419, acc.: 57.91%] [G loss: 0.7678211331367493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 61/86 [D loss: 0.6729783415794373, acc.: 57.23%] [G loss: 0.7659355998039246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 62/86 [D loss: 0.6687076091766357, acc.: 58.89%] [G loss: 0.7656075358390808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 63/86 [D loss: 0.6673188209533691, acc.: 59.13%] [G loss: 0.775154173374176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 64/86 [D loss: 0.6697295606136322, acc.: 58.35%] [G loss: 0.7707815170288086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 65/86 [D loss: 0.6714531183242798, acc.: 58.45%] [G loss: 0.7731016874313354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 66/86 [D loss: 0.6644935011863708, acc.: 60.06%] [G loss: 0.7691032886505127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 67/86 [D loss: 0.6678687334060669, acc.: 58.84%] [G loss: 0.7700350880622864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 68/86 [D loss: 0.66511669754982, acc.: 60.01%] [G loss: 0.7723914980888367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 69/86 [D loss: 0.6690516769886017, acc.: 57.67%] [G loss: 0.7721909284591675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 70/86 [D loss: 0.663914144039154, acc.: 61.08%] [G loss: 0.7753347158432007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 71/86 [D loss: 0.6642741858959198, acc.: 59.33%] [G loss: 0.7784585952758789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 72/86 [D loss: 0.6678847670555115, acc.: 60.64%] [G loss: 0.7769712805747986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 73/86 [D loss: 0.6741942465305328, acc.: 57.96%] [G loss: 0.7744372487068176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 74/86 [D loss: 0.671588271856308, acc.: 57.03%] [G loss: 0.7734713554382324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 75/86 [D loss: 0.6680999398231506, acc.: 59.13%] [G loss: 0.7742355465888977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 76/86 [D loss: 0.6700508892536163, acc.: 59.38%] [G loss: 0.7698402404785156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 77/86 [D loss: 0.6563927829265594, acc.: 61.18%] [G loss: 0.7693937420845032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 78/86 [D loss: 0.6639844477176666, acc.: 60.01%] [G loss: 0.7759870290756226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 79/86 [D loss: 0.663701981306076, acc.: 60.79%] [G loss: 0.7693400382995605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 80/86 [D loss: 0.6703479588031769, acc.: 59.23%] [G loss: 0.7691417932510376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 81/86 [D loss: 0.6642304956912994, acc.: 61.72%] [G loss: 0.7764695286750793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 82/86 [D loss: 0.6720639169216156, acc.: 58.35%] [G loss: 0.7727624177932739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 83/86 [D loss: 0.6712382435798645, acc.: 57.23%] [G loss: 0.7779272794723511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 84/86 [D loss: 0.6648896634578705, acc.: 60.21%] [G loss: 0.7713616490364075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 85/86 [D loss: 0.6721294820308685, acc.: 56.74%] [G loss: 0.7713268399238586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 151/200, Batch 86/86 [D loss: 0.6654761731624603, acc.: 58.98%] [G loss: 0.7691400647163391]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 1/86 [D loss: 0.6668619513511658, acc.: 59.03%] [G loss: 0.7741437554359436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 2/86 [D loss: 0.6683076918125153, acc.: 59.47%] [G loss: 0.7794287204742432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 3/86 [D loss: 0.6678446531295776, acc.: 60.01%] [G loss: 0.7684767246246338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 4/86 [D loss: 0.6741227507591248, acc.: 57.37%] [G loss: 0.7719626426696777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 5/86 [D loss: 0.6623641550540924, acc.: 60.50%] [G loss: 0.7729378938674927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 6/86 [D loss: 0.669683963060379, acc.: 58.35%] [G loss: 0.7752180099487305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 7/86 [D loss: 0.6744819581508636, acc.: 57.18%] [G loss: 0.7725977897644043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 8/86 [D loss: 0.6661222577095032, acc.: 59.81%] [G loss: 0.7734521627426147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 9/86 [D loss: 0.6704031825065613, acc.: 58.84%] [G loss: 0.7732692360877991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 10/86 [D loss: 0.6663924753665924, acc.: 60.35%] [G loss: 0.7778441905975342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 11/86 [D loss: 0.6654346585273743, acc.: 59.23%] [G loss: 0.7779807448387146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 12/86 [D loss: 0.6689143776893616, acc.: 58.25%] [G loss: 0.7727624177932739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 13/86 [D loss: 0.6755840480327606, acc.: 57.37%] [G loss: 0.7655540108680725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 14/86 [D loss: 0.6677582859992981, acc.: 58.50%] [G loss: 0.7763136625289917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 15/86 [D loss: 0.6612201929092407, acc.: 61.33%] [G loss: 0.7729823589324951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 16/86 [D loss: 0.6678840816020966, acc.: 58.84%] [G loss: 0.7718942165374756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 17/86 [D loss: 0.6744445860385895, acc.: 55.76%] [G loss: 0.7757041454315186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 18/86 [D loss: 0.6710569858551025, acc.: 58.25%] [G loss: 0.7779156565666199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 19/86 [D loss: 0.6692903339862823, acc.: 58.20%] [G loss: 0.77414870262146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 20/86 [D loss: 0.6700776815414429, acc.: 58.84%] [G loss: 0.7772409915924072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 21/86 [D loss: 0.6695506870746613, acc.: 57.86%] [G loss: 0.7719662189483643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 22/86 [D loss: 0.6695516407489777, acc.: 59.28%] [G loss: 0.7732868194580078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 23/86 [D loss: 0.6682533323764801, acc.: 60.30%] [G loss: 0.779473602771759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 24/86 [D loss: 0.6689250469207764, acc.: 58.84%] [G loss: 0.7738763093948364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 25/86 [D loss: 0.6730981767177582, acc.: 58.54%] [G loss: 0.7746761441230774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 26/86 [D loss: 0.6701977550983429, acc.: 58.74%] [G loss: 0.7704266309738159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 27/86 [D loss: 0.6723782122135162, acc.: 56.98%] [G loss: 0.766338050365448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 28/86 [D loss: 0.6734089255332947, acc.: 57.57%] [G loss: 0.7729945182800293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 29/86 [D loss: 0.6680313050746918, acc.: 58.50%] [G loss: 0.77838534116745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 30/86 [D loss: 0.6656093001365662, acc.: 59.77%] [G loss: 0.7744417786598206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 31/86 [D loss: 0.6697999835014343, acc.: 58.69%] [G loss: 0.7666401863098145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 32/86 [D loss: 0.6649996042251587, acc.: 58.45%] [G loss: 0.7768300771713257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 33/86 [D loss: 0.6675010621547699, acc.: 60.45%] [G loss: 0.7730023264884949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 34/86 [D loss: 0.6701763272285461, acc.: 58.89%] [G loss: 0.7750649452209473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 35/86 [D loss: 0.6655452251434326, acc.: 60.35%] [G loss: 0.7782116532325745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 36/86 [D loss: 0.665291965007782, acc.: 59.77%] [G loss: 0.770063042640686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 37/86 [D loss: 0.6698896586894989, acc.: 58.94%] [G loss: 0.7790595889091492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 38/86 [D loss: 0.6646781861782074, acc.: 59.13%] [G loss: 0.776427686214447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 39/86 [D loss: 0.6724826097488403, acc.: 57.52%] [G loss: 0.7848677039146423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 40/86 [D loss: 0.6673338115215302, acc.: 59.23%] [G loss: 0.7724924087524414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 41/86 [D loss: 0.6696610748767853, acc.: 58.35%] [G loss: 0.777178943157196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 42/86 [D loss: 0.6664785742759705, acc.: 58.74%] [G loss: 0.7630457878112793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 43/86 [D loss: 0.6707444190979004, acc.: 58.11%] [G loss: 0.7639692425727844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 44/86 [D loss: 0.6628745198249817, acc.: 60.16%] [G loss: 0.7652137875556946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 45/86 [D loss: 0.6671730279922485, acc.: 59.72%] [G loss: 0.7945766448974609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 46/86 [D loss: 0.6672283113002777, acc.: 59.81%] [G loss: 0.7777279615402222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 47/86 [D loss: 0.669896811246872, acc.: 58.59%] [G loss: 0.7741957902908325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 48/86 [D loss: 0.670345276594162, acc.: 57.86%] [G loss: 0.7801763415336609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 49/86 [D loss: 0.6636151671409607, acc.: 59.03%] [G loss: 0.7776162624359131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 50/86 [D loss: 0.6625133454799652, acc.: 61.47%] [G loss: 0.7620571851730347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 51/86 [D loss: 0.6723862290382385, acc.: 57.08%] [G loss: 0.7749857902526855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 52/86 [D loss: 0.6694648861885071, acc.: 57.71%] [G loss: 0.7740935683250427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 53/86 [D loss: 0.6649321019649506, acc.: 59.42%] [G loss: 0.7792400121688843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 54/86 [D loss: 0.6604485511779785, acc.: 62.11%] [G loss: 0.7731912136077881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 55/86 [D loss: 0.672614723443985, acc.: 57.57%] [G loss: 0.7762497067451477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 56/86 [D loss: 0.666789323091507, acc.: 58.45%] [G loss: 0.774856686592102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 57/86 [D loss: 0.6639022529125214, acc.: 60.25%] [G loss: 0.7696768045425415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 58/86 [D loss: 0.6631203889846802, acc.: 61.23%] [G loss: 0.7724043726921082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 59/86 [D loss: 0.6683399379253387, acc.: 59.23%] [G loss: 0.7742354869842529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 60/86 [D loss: 0.6671382784843445, acc.: 58.35%] [G loss: 0.7700607776641846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 61/86 [D loss: 0.6693860292434692, acc.: 58.15%] [G loss: 0.7746729254722595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 62/86 [D loss: 0.6710220277309418, acc.: 58.01%] [G loss: 0.7768838405609131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 63/86 [D loss: 0.6646047234535217, acc.: 60.45%] [G loss: 0.7746283411979675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 64/86 [D loss: 0.6701561510562897, acc.: 58.01%] [G loss: 0.776308000087738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 65/86 [D loss: 0.6670805513858795, acc.: 58.98%] [G loss: 0.7776603698730469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 66/86 [D loss: 0.6616213619709015, acc.: 61.04%] [G loss: 0.7770617008209229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 67/86 [D loss: 0.669491708278656, acc.: 57.76%] [G loss: 0.7731197476387024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 68/86 [D loss: 0.6688370108604431, acc.: 58.40%] [G loss: 0.7752693295478821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 69/86 [D loss: 0.6729847490787506, acc.: 56.35%] [G loss: 0.76899653673172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 70/86 [D loss: 0.6654526591300964, acc.: 59.33%] [G loss: 0.782459020614624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 71/86 [D loss: 0.6728892028331757, acc.: 56.30%] [G loss: 0.7777688503265381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 72/86 [D loss: 0.6703527867794037, acc.: 57.32%] [G loss: 0.7734299898147583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 73/86 [D loss: 0.6678377091884613, acc.: 59.18%] [G loss: 0.7683491706848145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 74/86 [D loss: 0.6695979535579681, acc.: 58.35%] [G loss: 0.7655941247940063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 75/86 [D loss: 0.659121036529541, acc.: 61.43%] [G loss: 0.774254560470581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 76/86 [D loss: 0.6685552299022675, acc.: 58.01%] [G loss: 0.7620870471000671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 77/86 [D loss: 0.6662994623184204, acc.: 58.84%] [G loss: 0.7771100401878357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 78/86 [D loss: 0.6695346236228943, acc.: 59.47%] [G loss: 0.7791159152984619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 79/86 [D loss: 0.6655449867248535, acc.: 60.06%] [G loss: 0.7721803784370422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 80/86 [D loss: 0.6698724627494812, acc.: 59.42%] [G loss: 0.7731382846832275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 81/86 [D loss: 0.6719569861888885, acc.: 57.47%] [G loss: 0.7770962119102478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 82/86 [D loss: 0.6666037738323212, acc.: 58.35%] [G loss: 0.7732453942298889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 83/86 [D loss: 0.669597327709198, acc.: 58.84%] [G loss: 0.7698870897293091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 84/86 [D loss: 0.6655046045780182, acc.: 59.28%] [G loss: 0.762321412563324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 85/86 [D loss: 0.6751029789447784, acc.: 57.47%] [G loss: 0.7728396058082581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 152/200, Batch 86/86 [D loss: 0.6654273867607117, acc.: 60.64%] [G loss: 0.7791895866394043]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 1/86 [D loss: 0.6692473888397217, acc.: 58.59%] [G loss: 0.7685868740081787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 2/86 [D loss: 0.6663023233413696, acc.: 58.89%] [G loss: 0.7754855751991272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 3/86 [D loss: 0.6652313768863678, acc.: 59.91%] [G loss: 0.7714115977287292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 4/86 [D loss: 0.6721727848052979, acc.: 58.06%] [G loss: 0.7736679315567017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 5/86 [D loss: 0.6693557500839233, acc.: 57.91%] [G loss: 0.778828501701355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 6/86 [D loss: 0.6643438041210175, acc.: 59.91%] [G loss: 0.7788154482841492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 7/86 [D loss: 0.6727964580059052, acc.: 58.11%] [G loss: 0.766315221786499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 8/86 [D loss: 0.665340781211853, acc.: 60.11%] [G loss: 0.7688213586807251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 9/86 [D loss: 0.6732309758663177, acc.: 56.84%] [G loss: 0.7690327167510986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 10/86 [D loss: 0.664895236492157, acc.: 60.50%] [G loss: 0.7714026570320129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 11/86 [D loss: 0.6701470613479614, acc.: 58.35%] [G loss: 0.7676064372062683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 12/86 [D loss: 0.6717402040958405, acc.: 58.11%] [G loss: 0.7728880643844604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 13/86 [D loss: 0.6728284955024719, acc.: 56.88%] [G loss: 0.7687583565711975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 14/86 [D loss: 0.6678348779678345, acc.: 57.13%] [G loss: 0.7827268242835999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 15/86 [D loss: 0.6718646585941315, acc.: 57.91%] [G loss: 0.7728496193885803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 16/86 [D loss: 0.6653282344341278, acc.: 59.81%] [G loss: 0.7745460867881775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 17/86 [D loss: 0.6676141619682312, acc.: 58.98%] [G loss: 0.7655394077301025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 18/86 [D loss: 0.6683154404163361, acc.: 59.72%] [G loss: 0.7741228938102722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 19/86 [D loss: 0.668098509311676, acc.: 58.59%] [G loss: 0.7697893381118774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 20/86 [D loss: 0.6664801239967346, acc.: 60.35%] [G loss: 0.7794174551963806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 21/86 [D loss: 0.6689806878566742, acc.: 59.47%] [G loss: 0.7729167342185974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 22/86 [D loss: 0.6611879765987396, acc.: 60.16%] [G loss: 0.7764778137207031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 23/86 [D loss: 0.6697568297386169, acc.: 58.25%] [G loss: 0.7764673233032227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 24/86 [D loss: 0.6681327819824219, acc.: 59.13%] [G loss: 0.7719237804412842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 25/86 [D loss: 0.671014279127121, acc.: 57.86%] [G loss: 0.7643373012542725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 26/86 [D loss: 0.6713813841342926, acc.: 58.11%] [G loss: 0.7672814130783081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 27/86 [D loss: 0.6684856414794922, acc.: 59.72%] [G loss: 0.7595049142837524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 28/86 [D loss: 0.6688271462917328, acc.: 59.47%] [G loss: 0.7794548273086548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 29/86 [D loss: 0.6696769297122955, acc.: 57.86%] [G loss: 0.7649340033531189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 30/86 [D loss: 0.6705097258090973, acc.: 59.28%] [G loss: 0.7672440409660339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 31/86 [D loss: 0.663634717464447, acc.: 59.08%] [G loss: 0.7707232236862183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 32/86 [D loss: 0.6657336354255676, acc.: 59.38%] [G loss: 0.7746939659118652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 33/86 [D loss: 0.6666066646575928, acc.: 59.67%] [G loss: 0.7726898193359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 34/86 [D loss: 0.6715237498283386, acc.: 57.91%] [G loss: 0.772469162940979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 35/86 [D loss: 0.662546306848526, acc.: 60.01%] [G loss: 0.7774709463119507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 36/86 [D loss: 0.661955326795578, acc.: 61.57%] [G loss: 0.7787695527076721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 37/86 [D loss: 0.6619812250137329, acc.: 61.52%] [G loss: 0.7749452590942383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 38/86 [D loss: 0.667419046163559, acc.: 58.79%] [G loss: 0.7754296660423279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 39/86 [D loss: 0.6669609248638153, acc.: 58.74%] [G loss: 0.7670671343803406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 40/86 [D loss: 0.6690614521503448, acc.: 58.06%] [G loss: 0.7751644849777222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 41/86 [D loss: 0.6673277616500854, acc.: 58.89%] [G loss: 0.7736376523971558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 42/86 [D loss: 0.665863424539566, acc.: 59.42%] [G loss: 0.7710909247398376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 43/86 [D loss: 0.6703100800514221, acc.: 58.15%] [G loss: 0.7795346975326538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 44/86 [D loss: 0.6645635068416595, acc.: 59.28%] [G loss: 0.7629871368408203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 45/86 [D loss: 0.6719205379486084, acc.: 58.20%] [G loss: 0.7693520784378052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 46/86 [D loss: 0.6712990403175354, acc.: 57.62%] [G loss: 0.77224200963974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 47/86 [D loss: 0.6644878089427948, acc.: 59.57%] [G loss: 0.7764897346496582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 48/86 [D loss: 0.6663410663604736, acc.: 59.18%] [G loss: 0.7836076617240906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 49/86 [D loss: 0.6669235229492188, acc.: 58.20%] [G loss: 0.7754778265953064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 50/86 [D loss: 0.6652836501598358, acc.: 60.74%] [G loss: 0.7729886770248413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 51/86 [D loss: 0.6686653196811676, acc.: 59.47%] [G loss: 0.7878504395484924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 52/86 [D loss: 0.6697746515274048, acc.: 58.98%] [G loss: 0.7670236229896545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 53/86 [D loss: 0.6693369150161743, acc.: 57.71%] [G loss: 0.7880083322525024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 54/86 [D loss: 0.6674748063087463, acc.: 58.98%] [G loss: 0.789463222026825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 55/86 [D loss: 0.6718830764293671, acc.: 58.84%] [G loss: 0.7855232954025269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 56/86 [D loss: 0.6610876023769379, acc.: 61.57%] [G loss: 0.7673065066337585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 57/86 [D loss: 0.6735937893390656, acc.: 57.47%] [G loss: 0.7788320183753967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 58/86 [D loss: 0.6685174405574799, acc.: 58.15%] [G loss: 0.7584450840950012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 59/86 [D loss: 0.6644530594348907, acc.: 60.01%] [G loss: 0.7753063440322876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 60/86 [D loss: 0.6769214272499084, acc.: 56.35%] [G loss: 0.7725250124931335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 61/86 [D loss: 0.6673247814178467, acc.: 59.03%] [G loss: 0.7727276086807251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 62/86 [D loss: 0.6681984066963196, acc.: 59.33%] [G loss: 0.7778546810150146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 63/86 [D loss: 0.6682557165622711, acc.: 59.28%] [G loss: 0.7798085808753967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 64/86 [D loss: 0.6671925783157349, acc.: 60.01%] [G loss: 0.7759106159210205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 65/86 [D loss: 0.6686922609806061, acc.: 58.40%] [G loss: 0.7751060128211975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 66/86 [D loss: 0.669014573097229, acc.: 59.72%] [G loss: 0.7726229429244995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 67/86 [D loss: 0.6707171499729156, acc.: 60.25%] [G loss: 0.7773066759109497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 68/86 [D loss: 0.6657752096652985, acc.: 60.01%] [G loss: 0.7785565257072449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 69/86 [D loss: 0.6672302484512329, acc.: 58.25%] [G loss: 0.7767868041992188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 70/86 [D loss: 0.6716313362121582, acc.: 57.28%] [G loss: 0.7705721259117126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 71/86 [D loss: 0.669999361038208, acc.: 59.72%] [G loss: 0.7786553502082825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 72/86 [D loss: 0.6682858169078827, acc.: 58.89%] [G loss: 0.7857136726379395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 73/86 [D loss: 0.669441282749176, acc.: 57.13%] [G loss: 0.7755602598190308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 74/86 [D loss: 0.6650276184082031, acc.: 58.20%] [G loss: 0.778920590877533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 75/86 [D loss: 0.6672394573688507, acc.: 59.03%] [G loss: 0.7750124335289001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 76/86 [D loss: 0.6683627068996429, acc.: 58.94%] [G loss: 0.7692840099334717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 77/86 [D loss: 0.6633330583572388, acc.: 60.55%] [G loss: 0.7757065892219543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 78/86 [D loss: 0.667312502861023, acc.: 59.86%] [G loss: 0.7790796160697937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 79/86 [D loss: 0.6683502197265625, acc.: 58.98%] [G loss: 0.7728817462921143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 80/86 [D loss: 0.6646226644515991, acc.: 61.28%] [G loss: 0.7725955843925476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 81/86 [D loss: 0.6675734221935272, acc.: 58.79%] [G loss: 0.7799426913261414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 82/86 [D loss: 0.667682409286499, acc.: 58.30%] [G loss: 0.7728540897369385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 83/86 [D loss: 0.6637123823165894, acc.: 60.21%] [G loss: 0.7768676280975342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 84/86 [D loss: 0.6610086560249329, acc.: 60.06%] [G loss: 0.7740277051925659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 85/86 [D loss: 0.6677026152610779, acc.: 58.84%] [G loss: 0.7712283134460449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 153/200, Batch 86/86 [D loss: 0.671128123998642, acc.: 57.62%] [G loss: 0.7746002674102783]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 1/86 [D loss: 0.6705755889415741, acc.: 58.74%] [G loss: 0.7769508361816406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 2/86 [D loss: 0.6661093831062317, acc.: 60.30%] [G loss: 0.7791107892990112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 3/86 [D loss: 0.6680905520915985, acc.: 60.06%] [G loss: 0.7763717174530029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 4/86 [D loss: 0.6679209768772125, acc.: 58.30%] [G loss: 0.7796235680580139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 5/86 [D loss: 0.6728633344173431, acc.: 57.18%] [G loss: 0.7781437039375305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 6/86 [D loss: 0.669250875711441, acc.: 59.38%] [G loss: 0.7837833762168884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 7/86 [D loss: 0.6702409088611603, acc.: 59.86%] [G loss: 0.7726379036903381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 8/86 [D loss: 0.6695983707904816, acc.: 58.45%] [G loss: 0.7724577784538269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 9/86 [D loss: 0.667908638715744, acc.: 59.13%] [G loss: 0.7815752029418945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 10/86 [D loss: 0.6695042550563812, acc.: 58.40%] [G loss: 0.7689244747161865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 11/86 [D loss: 0.663014829158783, acc.: 59.77%] [G loss: 0.7710974216461182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 12/86 [D loss: 0.6732796132564545, acc.: 56.88%] [G loss: 0.7714042067527771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 13/86 [D loss: 0.6665859222412109, acc.: 60.60%] [G loss: 0.7780124545097351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 14/86 [D loss: 0.6714346408843994, acc.: 57.42%] [G loss: 0.7880381345748901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 15/86 [D loss: 0.6703901886940002, acc.: 57.57%] [G loss: 0.7805405855178833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 16/86 [D loss: 0.6624865233898163, acc.: 61.04%] [G loss: 0.7715429663658142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 17/86 [D loss: 0.6687976717948914, acc.: 57.96%] [G loss: 0.7712737321853638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 18/86 [D loss: 0.6739921271800995, acc.: 55.76%] [G loss: 0.772616446018219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 19/86 [D loss: 0.6662539839744568, acc.: 58.94%] [G loss: 0.7713354229927063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 20/86 [D loss: 0.6714604198932648, acc.: 58.25%] [G loss: 0.7698670625686646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 21/86 [D loss: 0.6707532107830048, acc.: 58.98%] [G loss: 0.7684606909751892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 22/86 [D loss: 0.6675722002983093, acc.: 59.62%] [G loss: 0.7820156216621399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 23/86 [D loss: 0.664579451084137, acc.: 61.87%] [G loss: 0.7785319089889526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 24/86 [D loss: 0.6655875146389008, acc.: 59.28%] [G loss: 0.766346275806427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 25/86 [D loss: 0.6692647337913513, acc.: 58.50%] [G loss: 0.7727119326591492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 26/86 [D loss: 0.6670407652854919, acc.: 59.08%] [G loss: 0.7726330161094666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 27/86 [D loss: 0.6705555319786072, acc.: 57.71%] [G loss: 0.7628278136253357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 28/86 [D loss: 0.6708373427391052, acc.: 57.52%] [G loss: 0.7710835337638855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 29/86 [D loss: 0.6700639128684998, acc.: 57.32%] [G loss: 0.7707681655883789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 30/86 [D loss: 0.6677334308624268, acc.: 57.76%] [G loss: 0.7777547240257263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 31/86 [D loss: 0.6669645011425018, acc.: 59.67%] [G loss: 0.7793796062469482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 32/86 [D loss: 0.6712592244148254, acc.: 57.76%] [G loss: 0.780533492565155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 33/86 [D loss: 0.673106461763382, acc.: 57.76%] [G loss: 0.7652276158332825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 34/86 [D loss: 0.6665338575839996, acc.: 60.21%] [G loss: 0.7751059532165527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 35/86 [D loss: 0.668946236371994, acc.: 58.94%] [G loss: 0.7748032212257385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 36/86 [D loss: 0.6686153411865234, acc.: 57.86%] [G loss: 0.7710307240486145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 37/86 [D loss: 0.6684443354606628, acc.: 58.59%] [G loss: 0.785283088684082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 38/86 [D loss: 0.6721616983413696, acc.: 57.96%] [G loss: 0.7767267823219299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 39/86 [D loss: 0.6706665754318237, acc.: 57.57%] [G loss: 0.7822951674461365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 40/86 [D loss: 0.6631712913513184, acc.: 59.13%] [G loss: 0.7713682651519775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 41/86 [D loss: 0.6771174073219299, acc.: 56.59%] [G loss: 0.7800000309944153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 42/86 [D loss: 0.667076975107193, acc.: 59.23%] [G loss: 0.7770634889602661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 43/86 [D loss: 0.6743446588516235, acc.: 58.01%] [G loss: 0.7727245092391968]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 44/86 [D loss: 0.6623726487159729, acc.: 59.42%] [G loss: 0.7680184841156006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 45/86 [D loss: 0.6709772944450378, acc.: 58.11%] [G loss: 0.7734115123748779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 46/86 [D loss: 0.6572903096675873, acc.: 61.77%] [G loss: 0.7734350562095642]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 47/86 [D loss: 0.6741274297237396, acc.: 57.67%] [G loss: 0.7745571136474609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 48/86 [D loss: 0.6583149433135986, acc.: 61.72%] [G loss: 0.7750486731529236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 49/86 [D loss: 0.6668637990951538, acc.: 58.79%] [G loss: 0.7784171104431152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 50/86 [D loss: 0.6676411926746368, acc.: 58.98%] [G loss: 0.771032452583313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 51/86 [D loss: 0.6687283217906952, acc.: 58.06%] [G loss: 0.7791095972061157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 52/86 [D loss: 0.6635938584804535, acc.: 60.60%] [G loss: 0.7657382488250732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 53/86 [D loss: 0.6695091128349304, acc.: 58.45%] [G loss: 0.7797694206237793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 54/86 [D loss: 0.6741560995578766, acc.: 58.15%] [G loss: 0.7733431458473206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 55/86 [D loss: 0.6690734028816223, acc.: 59.38%] [G loss: 0.7808135747909546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 56/86 [D loss: 0.665841668844223, acc.: 58.54%] [G loss: 0.7716597318649292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 57/86 [D loss: 0.6659460067749023, acc.: 59.67%] [G loss: 0.7728922367095947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 58/86 [D loss: 0.6692997217178345, acc.: 57.96%] [G loss: 0.7708228230476379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 59/86 [D loss: 0.6760677099227905, acc.: 56.45%] [G loss: 0.7660319209098816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 60/86 [D loss: 0.6722514629364014, acc.: 58.64%] [G loss: 0.7804917693138123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 61/86 [D loss: 0.6625005602836609, acc.: 59.28%] [G loss: 0.7795232534408569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 62/86 [D loss: 0.6769155263900757, acc.: 55.47%] [G loss: 0.7767192125320435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 63/86 [D loss: 0.6663449704647064, acc.: 58.79%] [G loss: 0.7711870670318604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 64/86 [D loss: 0.6649591624736786, acc.: 59.28%] [G loss: 0.7836600542068481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 65/86 [D loss: 0.6673621237277985, acc.: 59.86%] [G loss: 0.7761861681938171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 66/86 [D loss: 0.6679647862911224, acc.: 58.74%] [G loss: 0.7770980596542358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 67/86 [D loss: 0.6672658622264862, acc.: 58.40%] [G loss: 0.7711418867111206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 68/86 [D loss: 0.6647273600101471, acc.: 58.74%] [G loss: 0.7783994674682617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 69/86 [D loss: 0.6635344922542572, acc.: 60.40%] [G loss: 0.7775139212608337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 70/86 [D loss: 0.6700201332569122, acc.: 58.45%] [G loss: 0.7812949419021606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 71/86 [D loss: 0.6612220704555511, acc.: 61.04%] [G loss: 0.7753692865371704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 72/86 [D loss: 0.6664804518222809, acc.: 58.98%] [G loss: 0.7767031192779541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 73/86 [D loss: 0.6628664135932922, acc.: 60.99%] [G loss: 0.7765361666679382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 74/86 [D loss: 0.6674683094024658, acc.: 59.28%] [G loss: 0.7657065391540527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 75/86 [D loss: 0.6644973754882812, acc.: 60.01%] [G loss: 0.7800400257110596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 76/86 [D loss: 0.6681935489177704, acc.: 59.52%] [G loss: 0.772910475730896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 77/86 [D loss: 0.6603738963603973, acc.: 61.57%] [G loss: 0.7829866409301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 78/86 [D loss: 0.6641588807106018, acc.: 59.57%] [G loss: 0.7758052945137024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 79/86 [D loss: 0.6671939194202423, acc.: 59.28%] [G loss: 0.778500497341156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 80/86 [D loss: 0.6650568246841431, acc.: 60.16%] [G loss: 0.7678735256195068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 81/86 [D loss: 0.6671406030654907, acc.: 58.79%] [G loss: 0.7692744731903076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 82/86 [D loss: 0.6612058281898499, acc.: 60.64%] [G loss: 0.783436119556427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 83/86 [D loss: 0.6616537272930145, acc.: 62.06%] [G loss: 0.783423662185669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 84/86 [D loss: 0.6693121790885925, acc.: 58.54%] [G loss: 0.7708036303520203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 85/86 [D loss: 0.6712115406990051, acc.: 59.23%] [G loss: 0.7715535759925842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 154/200, Batch 86/86 [D loss: 0.6687242388725281, acc.: 58.94%] [G loss: 0.7719858884811401]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 1/86 [D loss: 0.6631045639514923, acc.: 59.67%] [G loss: 0.779156506061554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 2/86 [D loss: 0.6679470837116241, acc.: 59.47%] [G loss: 0.7813881039619446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 3/86 [D loss: 0.6666326522827148, acc.: 58.89%] [G loss: 0.7678475975990295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 4/86 [D loss: 0.6675194203853607, acc.: 58.06%] [G loss: 0.7766773700714111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 5/86 [D loss: 0.6673944294452667, acc.: 59.86%] [G loss: 0.7710751891136169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 6/86 [D loss: 0.6688480973243713, acc.: 59.08%] [G loss: 0.7752672433853149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 7/86 [D loss: 0.6578569710254669, acc.: 61.87%] [G loss: 0.7723838090896606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 8/86 [D loss: 0.6750746965408325, acc.: 57.52%] [G loss: 0.7796225547790527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 9/86 [D loss: 0.6611530780792236, acc.: 60.99%] [G loss: 0.7681547999382019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 10/86 [D loss: 0.672703891992569, acc.: 58.54%] [G loss: 0.7740169763565063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 11/86 [D loss: 0.6666955053806305, acc.: 59.72%] [G loss: 0.7636944055557251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 12/86 [D loss: 0.6714168787002563, acc.: 57.81%] [G loss: 0.7741398811340332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 13/86 [D loss: 0.668560802936554, acc.: 58.79%] [G loss: 0.7781375646591187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 14/86 [D loss: 0.6697545647621155, acc.: 59.13%] [G loss: 0.7770304083824158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 15/86 [D loss: 0.6720570027828217, acc.: 58.94%] [G loss: 0.7707321047782898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 16/86 [D loss: 0.6641234755516052, acc.: 60.25%] [G loss: 0.7758509516716003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 17/86 [D loss: 0.669358879327774, acc.: 59.23%] [G loss: 0.782566487789154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 18/86 [D loss: 0.6635062396526337, acc.: 58.84%] [G loss: 0.7811669707298279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 19/86 [D loss: 0.6678174436092377, acc.: 58.25%] [G loss: 0.7706596255302429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 20/86 [D loss: 0.6656124889850616, acc.: 59.18%] [G loss: 0.7720553278923035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 21/86 [D loss: 0.6724741756916046, acc.: 56.45%] [G loss: 0.7732962965965271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 22/86 [D loss: 0.6686615645885468, acc.: 58.54%] [G loss: 0.7748588919639587]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 23/86 [D loss: 0.6616805195808411, acc.: 60.06%] [G loss: 0.7831485271453857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 24/86 [D loss: 0.6658433079719543, acc.: 60.50%] [G loss: 0.7682753801345825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 25/86 [D loss: 0.6679729521274567, acc.: 58.94%] [G loss: 0.7739740014076233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 26/86 [D loss: 0.6674140393733978, acc.: 59.52%] [G loss: 0.7753334045410156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 27/86 [D loss: 0.6657167971134186, acc.: 59.72%] [G loss: 0.7706345319747925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 28/86 [D loss: 0.6701610684394836, acc.: 58.50%] [G loss: 0.7769834995269775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 29/86 [D loss: 0.6620478928089142, acc.: 60.94%] [G loss: 0.7679516077041626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 30/86 [D loss: 0.6723279058933258, acc.: 58.25%] [G loss: 0.7681336998939514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 31/86 [D loss: 0.6694939136505127, acc.: 59.42%] [G loss: 0.7755640745162964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 32/86 [D loss: 0.6636005938053131, acc.: 59.96%] [G loss: 0.77428138256073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 33/86 [D loss: 0.6665025949478149, acc.: 59.38%] [G loss: 0.7804585099220276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 34/86 [D loss: 0.66988405585289, acc.: 58.59%] [G loss: 0.7767015695571899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 35/86 [D loss: 0.6690302491188049, acc.: 57.81%] [G loss: 0.7769086956977844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 36/86 [D loss: 0.6715334951877594, acc.: 57.32%] [G loss: 0.7752248048782349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 37/86 [D loss: 0.6641512513160706, acc.: 59.81%] [G loss: 0.7714195847511292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 38/86 [D loss: 0.6685777902603149, acc.: 59.57%] [G loss: 0.7749044299125671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 39/86 [D loss: 0.6617413759231567, acc.: 60.64%] [G loss: 0.7814757227897644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 40/86 [D loss: 0.6628779768943787, acc.: 59.57%] [G loss: 0.7824295163154602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 41/86 [D loss: 0.6667192280292511, acc.: 59.91%] [G loss: 0.7693874835968018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 42/86 [D loss: 0.6739536225795746, acc.: 56.93%] [G loss: 0.7674453258514404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 43/86 [D loss: 0.6678503155708313, acc.: 59.13%] [G loss: 0.769973874092102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 44/86 [D loss: 0.6693591475486755, acc.: 58.84%] [G loss: 0.771984875202179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 45/86 [D loss: 0.6653158962726593, acc.: 60.94%] [G loss: 0.7797260284423828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 46/86 [D loss: 0.6623077094554901, acc.: 59.47%] [G loss: 0.7735930681228638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 47/86 [D loss: 0.6673257350921631, acc.: 58.74%] [G loss: 0.7786005735397339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 48/86 [D loss: 0.6645870208740234, acc.: 59.72%] [G loss: 0.7782900929450989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 49/86 [D loss: 0.672240287065506, acc.: 57.47%] [G loss: 0.7751906514167786]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 50/86 [D loss: 0.6761094331741333, acc.: 57.13%] [G loss: 0.7827864289283752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 51/86 [D loss: 0.6667141318321228, acc.: 59.47%] [G loss: 0.7799042463302612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 52/86 [D loss: 0.6675415933132172, acc.: 58.74%] [G loss: 0.7799195051193237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 53/86 [D loss: 0.6629704535007477, acc.: 60.79%] [G loss: 0.7757992148399353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 54/86 [D loss: 0.671239823102951, acc.: 58.89%] [G loss: 0.7749871015548706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 55/86 [D loss: 0.6639826893806458, acc.: 60.06%] [G loss: 0.771997332572937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 56/86 [D loss: 0.6715003550052643, acc.: 58.11%] [G loss: 0.7781754732131958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 57/86 [D loss: 0.663623183965683, acc.: 60.89%] [G loss: 0.7798402905464172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 58/86 [D loss: 0.6706481575965881, acc.: 58.98%] [G loss: 0.7790221571922302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 59/86 [D loss: 0.666828066110611, acc.: 59.96%] [G loss: 0.7789417505264282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 60/86 [D loss: 0.6660884618759155, acc.: 59.86%] [G loss: 0.7734387516975403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 61/86 [D loss: 0.6646887958049774, acc.: 60.74%] [G loss: 0.7770050168037415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 62/86 [D loss: 0.6687425971031189, acc.: 56.93%] [G loss: 0.7858814001083374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 63/86 [D loss: 0.6664315164089203, acc.: 60.06%] [G loss: 0.7783740162849426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 64/86 [D loss: 0.6672237515449524, acc.: 58.06%] [G loss: 0.7843306660652161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 65/86 [D loss: 0.6631855070590973, acc.: 61.33%] [G loss: 0.7885878682136536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 66/86 [D loss: 0.6612519025802612, acc.: 59.57%] [G loss: 0.7658380270004272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 67/86 [D loss: 0.6700258851051331, acc.: 58.64%] [G loss: 0.7734643220901489]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 68/86 [D loss: 0.6654419600963593, acc.: 60.55%] [G loss: 0.7652978301048279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 69/86 [D loss: 0.6656757295131683, acc.: 60.11%] [G loss: 0.7833675742149353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 70/86 [D loss: 0.6690035164356232, acc.: 59.47%] [G loss: 0.7703483700752258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 71/86 [D loss: 0.6727157831192017, acc.: 56.20%] [G loss: 0.7694461345672607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 72/86 [D loss: 0.663411021232605, acc.: 62.06%] [G loss: 0.7804360389709473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 73/86 [D loss: 0.6676440238952637, acc.: 59.23%] [G loss: 0.7776142954826355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 74/86 [D loss: 0.6670986413955688, acc.: 59.23%] [G loss: 0.7767515778541565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 75/86 [D loss: 0.6733229160308838, acc.: 57.03%] [G loss: 0.7783541679382324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 76/86 [D loss: 0.670843094587326, acc.: 58.25%] [G loss: 0.7703801393508911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 77/86 [D loss: 0.6692446172237396, acc.: 59.28%] [G loss: 0.7772817611694336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 78/86 [D loss: 0.6590804159641266, acc.: 61.08%] [G loss: 0.7867066264152527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 79/86 [D loss: 0.6690293848514557, acc.: 58.64%] [G loss: 0.7831071615219116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 80/86 [D loss: 0.6678910255432129, acc.: 59.42%] [G loss: 0.781914234161377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 81/86 [D loss: 0.673917293548584, acc.: 57.52%] [G loss: 0.7833950519561768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 82/86 [D loss: 0.6684702336788177, acc.: 59.42%] [G loss: 0.7766414880752563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 83/86 [D loss: 0.6601568460464478, acc.: 60.50%] [G loss: 0.7788533568382263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 84/86 [D loss: 0.6681478917598724, acc.: 58.84%] [G loss: 0.7758044600486755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 85/86 [D loss: 0.6656683385372162, acc.: 59.67%] [G loss: 0.7747347950935364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 155/200, Batch 86/86 [D loss: 0.6646389067173004, acc.: 59.13%] [G loss: 0.7727062702178955]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 1/86 [D loss: 0.6599995791912079, acc.: 61.33%] [G loss: 0.7720487117767334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 2/86 [D loss: 0.6654998362064362, acc.: 59.03%] [G loss: 0.7768270373344421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 3/86 [D loss: 0.6684460043907166, acc.: 58.74%] [G loss: 0.7685331106185913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 4/86 [D loss: 0.6680092215538025, acc.: 58.54%] [G loss: 0.770428478717804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 5/86 [D loss: 0.6701435744762421, acc.: 57.47%] [G loss: 0.7742494344711304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 6/86 [D loss: 0.6642860472202301, acc.: 60.16%] [G loss: 0.7707062363624573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 7/86 [D loss: 0.6629785895347595, acc.: 60.99%] [G loss: 0.7816486954689026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 8/86 [D loss: 0.6720346510410309, acc.: 57.81%] [G loss: 0.7760856747627258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 9/86 [D loss: 0.6717017889022827, acc.: 57.91%] [G loss: 0.7766959071159363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 10/86 [D loss: 0.6704656481742859, acc.: 58.35%] [G loss: 0.7792189717292786]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 11/86 [D loss: 0.6644321382045746, acc.: 58.84%] [G loss: 0.7690439820289612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 12/86 [D loss: 0.6714919805526733, acc.: 56.59%] [G loss: 0.7727921009063721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 13/86 [D loss: 0.6684080362319946, acc.: 59.18%] [G loss: 0.773620069026947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 14/86 [D loss: 0.6679137051105499, acc.: 59.23%] [G loss: 0.7752580046653748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 15/86 [D loss: 0.6705916821956635, acc.: 58.69%] [G loss: 0.7705458402633667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 16/86 [D loss: 0.6666401326656342, acc.: 59.47%] [G loss: 0.7730494141578674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 17/86 [D loss: 0.6660455763339996, acc.: 58.25%] [G loss: 0.7742902040481567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 18/86 [D loss: 0.6677014231681824, acc.: 60.16%] [G loss: 0.7847353219985962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 19/86 [D loss: 0.6679776310920715, acc.: 58.79%] [G loss: 0.7603975534439087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 20/86 [D loss: 0.6702254116535187, acc.: 57.67%] [G loss: 0.7749894857406616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 21/86 [D loss: 0.6657742857933044, acc.: 60.40%] [G loss: 0.7630083560943604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 22/86 [D loss: 0.6737052202224731, acc.: 58.06%] [G loss: 0.7877998352050781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 23/86 [D loss: 0.6686341166496277, acc.: 58.74%] [G loss: 0.7742925882339478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 24/86 [D loss: 0.6690695583820343, acc.: 58.89%] [G loss: 0.7818440794944763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 25/86 [D loss: 0.6769155859947205, acc.: 55.81%] [G loss: 0.781426727771759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 26/86 [D loss: 0.6625530421733856, acc.: 60.89%] [G loss: 0.7861295342445374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 27/86 [D loss: 0.6727233231067657, acc.: 58.64%] [G loss: 0.7798162698745728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 28/86 [D loss: 0.6686918437480927, acc.: 59.18%] [G loss: 0.774686872959137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 29/86 [D loss: 0.6729382872581482, acc.: 57.57%] [G loss: 0.7748066782951355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 30/86 [D loss: 0.6684172451496124, acc.: 58.98%] [G loss: 0.7755445241928101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 31/86 [D loss: 0.6725662350654602, acc.: 56.64%] [G loss: 0.7793399691581726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 32/86 [D loss: 0.6582283973693848, acc.: 60.94%] [G loss: 0.7781506776809692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 33/86 [D loss: 0.6715553104877472, acc.: 57.96%] [G loss: 0.7681224346160889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 34/86 [D loss: 0.6661162376403809, acc.: 59.81%] [G loss: 0.778567910194397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 35/86 [D loss: 0.6862045526504517, acc.: 54.25%] [G loss: 0.7645224332809448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 36/86 [D loss: 0.6707210540771484, acc.: 58.40%] [G loss: 0.7890678644180298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 37/86 [D loss: 0.6730834245681763, acc.: 56.15%] [G loss: 0.7609495520591736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 38/86 [D loss: 0.6691303253173828, acc.: 60.94%] [G loss: 0.7979321479797363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 39/86 [D loss: 0.6802662014961243, acc.: 54.88%] [G loss: 0.7747527360916138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 40/86 [D loss: 0.6589019894599915, acc.: 61.18%] [G loss: 0.7797027230262756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 41/86 [D loss: 0.6886587142944336, acc.: 53.96%] [G loss: 0.7729715704917908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 42/86 [D loss: 0.6588460803031921, acc.: 62.65%] [G loss: 0.7834716439247131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 43/86 [D loss: 0.6781550645828247, acc.: 54.35%] [G loss: 0.7632880210876465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 44/86 [D loss: 0.6686636507511139, acc.: 58.06%] [G loss: 0.7782837748527527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 45/86 [D loss: 0.6719403564929962, acc.: 57.71%] [G loss: 0.7551957964897156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 46/86 [D loss: 0.6727117300033569, acc.: 59.52%] [G loss: 0.7911962270736694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 47/86 [D loss: 0.670073002576828, acc.: 58.15%] [G loss: 0.7690572142601013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 48/86 [D loss: 0.6685068607330322, acc.: 58.40%] [G loss: 0.7797967791557312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 49/86 [D loss: 0.6681637763977051, acc.: 57.23%] [G loss: 0.7609253525733948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 50/86 [D loss: 0.6790460348129272, acc.: 56.35%] [G loss: 0.8001435995101929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 51/86 [D loss: 0.6625781059265137, acc.: 61.47%] [G loss: 0.782295823097229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 52/86 [D loss: 0.6731790602207184, acc.: 57.47%] [G loss: 0.7810661196708679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 53/86 [D loss: 0.6662972271442413, acc.: 59.91%] [G loss: 0.7776349186897278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 54/86 [D loss: 0.6649190187454224, acc.: 59.62%] [G loss: 0.7798599600791931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 55/86 [D loss: 0.6720000207424164, acc.: 58.11%] [G loss: 0.785161018371582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 56/86 [D loss: 0.6682359278202057, acc.: 59.86%] [G loss: 0.7758119702339172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 57/86 [D loss: 0.6650107204914093, acc.: 59.33%] [G loss: 0.7869963049888611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 58/86 [D loss: 0.6664493978023529, acc.: 59.77%] [G loss: 0.7672727108001709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 59/86 [D loss: 0.6698017120361328, acc.: 58.01%] [G loss: 0.7834790945053101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 60/86 [D loss: 0.6702911853790283, acc.: 57.37%] [G loss: 0.7753466963768005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 61/86 [D loss: 0.6696631014347076, acc.: 58.30%] [G loss: 0.7781285047531128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 62/86 [D loss: 0.6665858626365662, acc.: 59.72%] [G loss: 0.7685499787330627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 63/86 [D loss: 0.6701617240905762, acc.: 57.67%] [G loss: 0.7741031646728516]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 64/86 [D loss: 0.6658391356468201, acc.: 59.38%] [G loss: 0.7741515636444092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 65/86 [D loss: 0.6708279252052307, acc.: 56.01%] [G loss: 0.7737264037132263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 66/86 [D loss: 0.664050817489624, acc.: 60.69%] [G loss: 0.7855794429779053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 67/86 [D loss: 0.6760902404785156, acc.: 56.10%] [G loss: 0.7726268768310547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 68/86 [D loss: 0.6650731861591339, acc.: 59.91%] [G loss: 0.7843687534332275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 69/86 [D loss: 0.6660337448120117, acc.: 58.50%] [G loss: 0.7663892507553101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 70/86 [D loss: 0.6703521013259888, acc.: 57.18%] [G loss: 0.7838832139968872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 71/86 [D loss: 0.6678971946239471, acc.: 58.74%] [G loss: 0.7709927558898926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 72/86 [D loss: 0.6673209965229034, acc.: 59.96%] [G loss: 0.7896711826324463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 73/86 [D loss: 0.6728446781635284, acc.: 57.42%] [G loss: 0.7697343826293945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 74/86 [D loss: 0.6764286756515503, acc.: 56.20%] [G loss: 0.782747745513916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 75/86 [D loss: 0.6720752418041229, acc.: 57.42%] [G loss: 0.7751031517982483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 76/86 [D loss: 0.6704730093479156, acc.: 57.71%] [G loss: 0.7798155546188354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 77/86 [D loss: 0.6592749953269958, acc.: 60.25%] [G loss: 0.7812969088554382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 78/86 [D loss: 0.6734392046928406, acc.: 57.91%] [G loss: 0.7836470603942871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 79/86 [D loss: 0.6693620383739471, acc.: 58.54%] [G loss: 0.7832565307617188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 80/86 [D loss: 0.670280784368515, acc.: 57.67%] [G loss: 0.7834650874137878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 81/86 [D loss: 0.6626289188861847, acc.: 61.08%] [G loss: 0.784568190574646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 82/86 [D loss: 0.6618832051753998, acc.: 59.33%] [G loss: 0.7830083966255188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 83/86 [D loss: 0.6658221781253815, acc.: 58.54%] [G loss: 0.7796356081962585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 84/86 [D loss: 0.6649564504623413, acc.: 59.57%] [G loss: 0.7713505625724792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 85/86 [D loss: 0.6652809083461761, acc.: 59.33%] [G loss: 0.7829157114028931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 156/200, Batch 86/86 [D loss: 0.6606459319591522, acc.: 61.18%] [G loss: 0.7673512101173401]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 1/86 [D loss: 0.6697151362895966, acc.: 58.01%] [G loss: 0.7802002429962158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 2/86 [D loss: 0.6585395932197571, acc.: 62.30%] [G loss: 0.7845725417137146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 3/86 [D loss: 0.6763019561767578, acc.: 58.11%] [G loss: 0.7826258540153503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 4/86 [D loss: 0.6684894859790802, acc.: 58.50%] [G loss: 0.7836289405822754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 5/86 [D loss: 0.6735066473484039, acc.: 57.71%] [G loss: 0.7761046886444092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 6/86 [D loss: 0.6645731329917908, acc.: 58.84%] [G loss: 0.7749339938163757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 7/86 [D loss: 0.6708336174488068, acc.: 59.47%] [G loss: 0.7673044204711914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 8/86 [D loss: 0.6643444895744324, acc.: 59.18%] [G loss: 0.7736687064170837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 9/86 [D loss: 0.6636956632137299, acc.: 59.33%] [G loss: 0.7722560167312622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 10/86 [D loss: 0.6683412194252014, acc.: 58.20%] [G loss: 0.7836889028549194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 11/86 [D loss: 0.6623018085956573, acc.: 60.79%] [G loss: 0.7702963352203369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 12/86 [D loss: 0.6695349514484406, acc.: 59.77%] [G loss: 0.777009129524231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 13/86 [D loss: 0.6678046584129333, acc.: 59.23%] [G loss: 0.775359034538269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 14/86 [D loss: 0.6713190674781799, acc.: 57.47%] [G loss: 0.773939847946167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 15/86 [D loss: 0.6652403771877289, acc.: 60.25%] [G loss: 0.7716847658157349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 16/86 [D loss: 0.669866144657135, acc.: 58.59%] [G loss: 0.7812487483024597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 17/86 [D loss: 0.6644266843795776, acc.: 59.52%] [G loss: 0.7788158655166626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 18/86 [D loss: 0.6707637906074524, acc.: 58.74%] [G loss: 0.7865827083587646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 19/86 [D loss: 0.6657804250717163, acc.: 59.38%] [G loss: 0.778339147567749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 20/86 [D loss: 0.6686200499534607, acc.: 59.38%] [G loss: 0.7787967324256897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 21/86 [D loss: 0.6703674495220184, acc.: 58.15%] [G loss: 0.7815098762512207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 22/86 [D loss: 0.6639321148395538, acc.: 58.89%] [G loss: 0.7700051069259644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 23/86 [D loss: 0.6708976924419403, acc.: 57.47%] [G loss: 0.7932487726211548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 24/86 [D loss: 0.6672217547893524, acc.: 59.52%] [G loss: 0.7738418579101562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 25/86 [D loss: 0.6723726987838745, acc.: 58.01%] [G loss: 0.7805314064025879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 26/86 [D loss: 0.6682880818843842, acc.: 60.40%] [G loss: 0.776990532875061]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 27/86 [D loss: 0.6667274832725525, acc.: 60.01%] [G loss: 0.7794424295425415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 28/86 [D loss: 0.6663782596588135, acc.: 60.01%] [G loss: 0.7737060785293579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 29/86 [D loss: 0.6739235520362854, acc.: 57.91%] [G loss: 0.7789310216903687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 30/86 [D loss: 0.6597123146057129, acc.: 61.28%] [G loss: 0.7856165170669556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 31/86 [D loss: 0.6682002246379852, acc.: 59.03%] [G loss: 0.7682923078536987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 32/86 [D loss: 0.6674010753631592, acc.: 58.94%] [G loss: 0.778472363948822]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 33/86 [D loss: 0.6610632240772247, acc.: 59.42%] [G loss: 0.7787624597549438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 34/86 [D loss: 0.6638394296169281, acc.: 60.30%] [G loss: 0.7816991209983826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 35/86 [D loss: 0.6643692851066589, acc.: 60.40%] [G loss: 0.7850554585456848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 36/86 [D loss: 0.6694780588150024, acc.: 57.52%] [G loss: 0.7857432961463928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 37/86 [D loss: 0.6644646227359772, acc.: 59.03%] [G loss: 0.7933692932128906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 38/86 [D loss: 0.6672653555870056, acc.: 59.23%] [G loss: 0.7796006202697754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 39/86 [D loss: 0.667594850063324, acc.: 58.15%] [G loss: 0.7818987965583801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 40/86 [D loss: 0.6657496988773346, acc.: 60.79%] [G loss: 0.7784093022346497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 41/86 [D loss: 0.6690062582492828, acc.: 58.74%] [G loss: 0.7780979871749878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 42/86 [D loss: 0.6656411588191986, acc.: 59.38%] [G loss: 0.7785592079162598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 43/86 [D loss: 0.6705879867076874, acc.: 57.32%] [G loss: 0.7763020396232605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 44/86 [D loss: 0.6657358407974243, acc.: 59.47%] [G loss: 0.7790803909301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 45/86 [D loss: 0.6708512008190155, acc.: 57.37%] [G loss: 0.7756714820861816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 46/86 [D loss: 0.6647888720035553, acc.: 59.96%] [G loss: 0.7769638299942017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 47/86 [D loss: 0.6728313863277435, acc.: 56.84%] [G loss: 0.7701832056045532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 48/86 [D loss: 0.6651417911052704, acc.: 59.28%] [G loss: 0.7643868923187256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 49/86 [D loss: 0.6684969663619995, acc.: 57.08%] [G loss: 0.7652601599693298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 50/86 [D loss: 0.673136293888092, acc.: 57.62%] [G loss: 0.7779654860496521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 51/86 [D loss: 0.6676033139228821, acc.: 59.86%] [G loss: 0.7882081866264343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 52/86 [D loss: 0.6694073379039764, acc.: 59.23%] [G loss: 0.7858914136886597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 53/86 [D loss: 0.6671079695224762, acc.: 56.74%] [G loss: 0.7634055614471436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 54/86 [D loss: 0.6640300750732422, acc.: 59.13%] [G loss: 0.7808797955513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 55/86 [D loss: 0.6639122664928436, acc.: 60.40%] [G loss: 0.7826333045959473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 56/86 [D loss: 0.6721271276473999, acc.: 58.35%] [G loss: 0.7803480625152588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 57/86 [D loss: 0.6636001169681549, acc.: 59.57%] [G loss: 0.7816574573516846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 58/86 [D loss: 0.6710521578788757, acc.: 57.47%] [G loss: 0.7819391489028931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 59/86 [D loss: 0.6723319888114929, acc.: 58.35%] [G loss: 0.7728842496871948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 60/86 [D loss: 0.6689492166042328, acc.: 59.03%] [G loss: 0.7760512828826904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 61/86 [D loss: 0.6685008704662323, acc.: 58.50%] [G loss: 0.7771010994911194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 62/86 [D loss: 0.6591485738754272, acc.: 60.35%] [G loss: 0.7777482271194458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 63/86 [D loss: 0.6632656455039978, acc.: 60.01%] [G loss: 0.7863293886184692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 64/86 [D loss: 0.6701626181602478, acc.: 58.50%] [G loss: 0.781511664390564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 65/86 [D loss: 0.6621370613574982, acc.: 61.43%] [G loss: 0.7783901691436768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 66/86 [D loss: 0.6651034653186798, acc.: 59.13%] [G loss: 0.7816438674926758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 67/86 [D loss: 0.6702560782432556, acc.: 57.47%] [G loss: 0.7837153673171997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 68/86 [D loss: 0.6638824641704559, acc.: 60.40%] [G loss: 0.7858509421348572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 69/86 [D loss: 0.6677925288677216, acc.: 59.08%] [G loss: 0.7755776643753052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 70/86 [D loss: 0.6641309559345245, acc.: 60.35%] [G loss: 0.7755240797996521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 71/86 [D loss: 0.6652483344078064, acc.: 58.59%] [G loss: 0.7749905586242676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 72/86 [D loss: 0.6663647890090942, acc.: 59.23%] [G loss: 0.7797377705574036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 73/86 [D loss: 0.6644933521747589, acc.: 58.94%] [G loss: 0.7756860256195068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 74/86 [D loss: 0.6637189984321594, acc.: 59.67%] [G loss: 0.7788119316101074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 75/86 [D loss: 0.6648824214935303, acc.: 59.42%] [G loss: 0.7769477367401123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 76/86 [D loss: 0.663046270608902, acc.: 60.16%] [G loss: 0.7736546993255615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 77/86 [D loss: 0.6697143614292145, acc.: 57.57%] [G loss: 0.7837072014808655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 78/86 [D loss: 0.666976660490036, acc.: 59.33%] [G loss: 0.7746998071670532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 79/86 [D loss: 0.6668547987937927, acc.: 59.52%] [G loss: 0.7773100137710571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 80/86 [D loss: 0.6633881628513336, acc.: 60.55%] [G loss: 0.7791919708251953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 81/86 [D loss: 0.6659094989299774, acc.: 59.18%] [G loss: 0.7784386277198792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 82/86 [D loss: 0.6634809374809265, acc.: 60.40%] [G loss: 0.7770360708236694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 83/86 [D loss: 0.6648639440536499, acc.: 58.69%] [G loss: 0.7796881198883057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 84/86 [D loss: 0.6685368716716766, acc.: 59.67%] [G loss: 0.7760393619537354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 85/86 [D loss: 0.6649819910526276, acc.: 60.35%] [G loss: 0.7754049301147461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 157/200, Batch 86/86 [D loss: 0.6659685373306274, acc.: 59.77%] [G loss: 0.7921217083930969]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 1/86 [D loss: 0.6706950664520264, acc.: 58.30%] [G loss: 0.7720156908035278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 2/86 [D loss: 0.6679514348506927, acc.: 58.15%] [G loss: 0.7805051803588867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 3/86 [D loss: 0.663871556520462, acc.: 60.35%] [G loss: 0.7779109477996826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 4/86 [D loss: 0.6655524969100952, acc.: 58.84%] [G loss: 0.7857300639152527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 5/86 [D loss: 0.661008208990097, acc.: 61.08%] [G loss: 0.7803246378898621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 6/86 [D loss: 0.6675181984901428, acc.: 58.64%] [G loss: 0.785843551158905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 7/86 [D loss: 0.6670800447463989, acc.: 59.13%] [G loss: 0.7666374444961548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 8/86 [D loss: 0.6631896495819092, acc.: 60.16%] [G loss: 0.7758892774581909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 9/86 [D loss: 0.664314329624176, acc.: 59.08%] [G loss: 0.7750666737556458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 10/86 [D loss: 0.6649188995361328, acc.: 60.16%] [G loss: 0.7763544917106628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 11/86 [D loss: 0.6621155738830566, acc.: 59.96%] [G loss: 0.7732406854629517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 12/86 [D loss: 0.6607884764671326, acc.: 61.33%] [G loss: 0.7796598076820374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 13/86 [D loss: 0.6629922688007355, acc.: 60.55%] [G loss: 0.772435188293457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 14/86 [D loss: 0.672103077173233, acc.: 57.62%] [G loss: 0.7773740291595459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 15/86 [D loss: 0.6716582179069519, acc.: 57.62%] [G loss: 0.781839907169342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 16/86 [D loss: 0.6625418961048126, acc.: 60.16%] [G loss: 0.7807270884513855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 17/86 [D loss: 0.6665339767932892, acc.: 58.79%] [G loss: 0.7804898619651794]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 18/86 [D loss: 0.6642406284809113, acc.: 59.81%] [G loss: 0.7819096446037292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 19/86 [D loss: 0.6639415621757507, acc.: 60.01%] [G loss: 0.7805905938148499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 20/86 [D loss: 0.6589021682739258, acc.: 60.94%] [G loss: 0.7789071798324585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 21/86 [D loss: 0.6682251989841461, acc.: 59.23%] [G loss: 0.777620792388916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 22/86 [D loss: 0.6666409969329834, acc.: 59.13%] [G loss: 0.7866834998130798]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 23/86 [D loss: 0.665533035993576, acc.: 58.89%] [G loss: 0.7755532264709473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 24/86 [D loss: 0.6609495580196381, acc.: 59.86%] [G loss: 0.7877932190895081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 25/86 [D loss: 0.6668214201927185, acc.: 58.79%] [G loss: 0.791964590549469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 26/86 [D loss: 0.6716726720333099, acc.: 56.93%] [G loss: 0.7929012179374695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 27/86 [D loss: 0.665273517370224, acc.: 59.03%] [G loss: 0.7805255651473999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 28/86 [D loss: 0.6656359434127808, acc.: 58.69%] [G loss: 0.780789315700531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 29/86 [D loss: 0.664770781993866, acc.: 58.69%] [G loss: 0.7765004634857178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 30/86 [D loss: 0.6607160866260529, acc.: 62.01%] [G loss: 0.7910301685333252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 31/86 [D loss: 0.6635101139545441, acc.: 59.62%] [G loss: 0.7819143533706665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 32/86 [D loss: 0.6614797115325928, acc.: 60.89%] [G loss: 0.7835533618927002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 33/86 [D loss: 0.6643838882446289, acc.: 60.16%] [G loss: 0.7796697020530701]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 34/86 [D loss: 0.665144294500351, acc.: 60.21%] [G loss: 0.779792308807373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 35/86 [D loss: 0.6739388704299927, acc.: 57.37%] [G loss: 0.7875798344612122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 36/86 [D loss: 0.6651054620742798, acc.: 59.91%] [G loss: 0.7768526673316956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 37/86 [D loss: 0.6630095839500427, acc.: 60.35%] [G loss: 0.7903621196746826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 38/86 [D loss: 0.6695442199707031, acc.: 58.98%] [G loss: 0.7756977677345276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 39/86 [D loss: 0.6700089573860168, acc.: 58.45%] [G loss: 0.7840080261230469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 40/86 [D loss: 0.665570855140686, acc.: 59.28%] [G loss: 0.7771836519241333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 41/86 [D loss: 0.6720064878463745, acc.: 57.76%] [G loss: 0.7804762125015259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 42/86 [D loss: 0.6714021861553192, acc.: 58.20%] [G loss: 0.7708170413970947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 43/86 [D loss: 0.6678079962730408, acc.: 58.94%] [G loss: 0.778097927570343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 44/86 [D loss: 0.6675319373607635, acc.: 58.74%] [G loss: 0.7777109146118164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 45/86 [D loss: 0.6667093932628632, acc.: 59.13%] [G loss: 0.7882338762283325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 46/86 [D loss: 0.6653572916984558, acc.: 59.81%] [G loss: 0.7762763500213623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 47/86 [D loss: 0.6717800796031952, acc.: 58.11%] [G loss: 0.7681550979614258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 48/86 [D loss: 0.666121244430542, acc.: 59.47%] [G loss: 0.7813148498535156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 49/86 [D loss: 0.6691991090774536, acc.: 58.06%] [G loss: 0.7764720916748047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 50/86 [D loss: 0.6644623577594757, acc.: 59.28%] [G loss: 0.769482433795929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 51/86 [D loss: 0.675979346036911, acc.: 57.62%] [G loss: 0.7695342302322388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 52/86 [D loss: 0.6611260771751404, acc.: 60.99%] [G loss: 0.7822998762130737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 53/86 [D loss: 0.6649664640426636, acc.: 60.21%] [G loss: 0.7803952693939209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 54/86 [D loss: 0.6727156043052673, acc.: 57.32%] [G loss: 0.7801433205604553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 55/86 [D loss: 0.6666651666164398, acc.: 59.23%] [G loss: 0.7812091112136841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 56/86 [D loss: 0.6621774733066559, acc.: 60.45%] [G loss: 0.784783661365509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 57/86 [D loss: 0.662893682718277, acc.: 60.50%] [G loss: 0.7769418954849243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 58/86 [D loss: 0.6621150076389313, acc.: 59.72%] [G loss: 0.77884840965271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 59/86 [D loss: 0.6698850095272064, acc.: 57.67%] [G loss: 0.7772042155265808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 60/86 [D loss: 0.6674345135688782, acc.: 59.67%] [G loss: 0.7905710935592651]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 61/86 [D loss: 0.6684040427207947, acc.: 57.76%] [G loss: 0.7745677828788757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 62/86 [D loss: 0.6640853881835938, acc.: 59.13%] [G loss: 0.7811405658721924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 63/86 [D loss: 0.6660796999931335, acc.: 59.96%] [G loss: 0.7743601202964783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 64/86 [D loss: 0.6647124290466309, acc.: 59.33%] [G loss: 0.7790242433547974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 65/86 [D loss: 0.66529980301857, acc.: 60.40%] [G loss: 0.7722669243812561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 66/86 [D loss: 0.6679618656635284, acc.: 58.40%] [G loss: 0.7763038277626038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 67/86 [D loss: 0.6652811467647552, acc.: 59.52%] [G loss: 0.7760940790176392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 68/86 [D loss: 0.6676372587680817, acc.: 59.47%] [G loss: 0.782593309879303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 69/86 [D loss: 0.6672058999538422, acc.: 59.72%] [G loss: 0.7742739915847778]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 70/86 [D loss: 0.6692291796207428, acc.: 57.57%] [G loss: 0.7836817502975464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 71/86 [D loss: 0.6667763292789459, acc.: 58.74%] [G loss: 0.7775454521179199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 72/86 [D loss: 0.6651467978954315, acc.: 58.98%] [G loss: 0.7909974455833435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 73/86 [D loss: 0.6652519702911377, acc.: 60.69%] [G loss: 0.7802065014839172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 74/86 [D loss: 0.6688594222068787, acc.: 58.30%] [G loss: 0.7714148759841919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 75/86 [D loss: 0.6641052961349487, acc.: 59.33%] [G loss: 0.7695049047470093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 76/86 [D loss: 0.6707830429077148, acc.: 57.28%] [G loss: 0.7838223576545715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 77/86 [D loss: 0.6619400084018707, acc.: 60.60%] [G loss: 0.7820812463760376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 78/86 [D loss: 0.6697587370872498, acc.: 57.91%] [G loss: 0.7804082632064819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 79/86 [D loss: 0.6696092486381531, acc.: 58.30%] [G loss: 0.7710259556770325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 80/86 [D loss: 0.6684983372688293, acc.: 58.74%] [G loss: 0.7813394665718079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 81/86 [D loss: 0.6643515229225159, acc.: 60.60%] [G loss: 0.7661820650100708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 82/86 [D loss: 0.6649923920631409, acc.: 58.74%] [G loss: 0.7853057384490967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 83/86 [D loss: 0.6674403846263885, acc.: 59.03%] [G loss: 0.7871713042259216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 84/86 [D loss: 0.6610485911369324, acc.: 60.16%] [G loss: 0.7862628102302551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 85/86 [D loss: 0.6622729301452637, acc.: 60.40%] [G loss: 0.7829873561859131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 158/200, Batch 86/86 [D loss: 0.6618014872074127, acc.: 60.60%] [G loss: 0.7858702540397644]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 1/86 [D loss: 0.6590058505535126, acc.: 60.89%] [G loss: 0.772244930267334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 2/86 [D loss: 0.6760107278823853, acc.: 56.74%] [G loss: 0.7725269198417664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 3/86 [D loss: 0.6639391779899597, acc.: 60.40%] [G loss: 0.7793886065483093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 4/86 [D loss: 0.6654530465602875, acc.: 59.33%] [G loss: 0.7808520197868347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 5/86 [D loss: 0.6721027195453644, acc.: 57.32%] [G loss: 0.7745644450187683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 6/86 [D loss: 0.6714944839477539, acc.: 58.01%] [G loss: 0.7857827544212341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 7/86 [D loss: 0.663087397813797, acc.: 60.69%] [G loss: 0.7851521372795105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 8/86 [D loss: 0.6608145534992218, acc.: 60.45%] [G loss: 0.7819997072219849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 9/86 [D loss: 0.6640920042991638, acc.: 59.47%] [G loss: 0.7796130180358887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 10/86 [D loss: 0.663788378238678, acc.: 60.25%] [G loss: 0.7846387624740601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 11/86 [D loss: 0.6598158180713654, acc.: 62.65%] [G loss: 0.7773570418357849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 12/86 [D loss: 0.6691117286682129, acc.: 59.42%] [G loss: 0.7851055264472961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 13/86 [D loss: 0.6629237830638885, acc.: 60.69%] [G loss: 0.7732521891593933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 14/86 [D loss: 0.6674525141716003, acc.: 58.35%] [G loss: 0.7786219716072083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 15/86 [D loss: 0.6714686155319214, acc.: 57.62%] [G loss: 0.7827305793762207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 16/86 [D loss: 0.6698793470859528, acc.: 57.37%] [G loss: 0.7745263576507568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 17/86 [D loss: 0.6667367219924927, acc.: 58.54%] [G loss: 0.7792128324508667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 18/86 [D loss: 0.6581480503082275, acc.: 61.87%] [G loss: 0.775682270526886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 19/86 [D loss: 0.6684067845344543, acc.: 58.50%] [G loss: 0.7825889587402344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 20/86 [D loss: 0.6598480343818665, acc.: 60.21%] [G loss: 0.7780444622039795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 21/86 [D loss: 0.6685749888420105, acc.: 56.98%] [G loss: 0.7883414626121521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 22/86 [D loss: 0.6641624569892883, acc.: 60.60%] [G loss: 0.7775490283966064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 23/86 [D loss: 0.6683293879032135, acc.: 57.86%] [G loss: 0.7850808501243591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 24/86 [D loss: 0.6626306176185608, acc.: 60.35%] [G loss: 0.7721796035766602]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 159/200, Batch 25/86 [D loss: 0.6731476485729218, acc.: 56.20%] [G loss: 0.7779508829116821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 26/86 [D loss: 0.6583759486675262, acc.: 61.04%] [G loss: 0.7957171201705933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 27/86 [D loss: 0.6735173761844635, acc.: 57.23%] [G loss: 0.7844341993331909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 28/86 [D loss: 0.6632202863693237, acc.: 61.47%] [G loss: 0.7777113914489746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 29/86 [D loss: 0.6636557281017303, acc.: 58.79%] [G loss: 0.7827960252761841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 30/86 [D loss: 0.6642545759677887, acc.: 62.16%] [G loss: 0.7712596654891968]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 31/86 [D loss: 0.6680123507976532, acc.: 59.33%] [G loss: 0.7833768725395203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 32/86 [D loss: 0.6655942499637604, acc.: 59.86%] [G loss: 0.7843888401985168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 33/86 [D loss: 0.6627418398857117, acc.: 61.08%] [G loss: 0.7845138311386108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 34/86 [D loss: 0.6783284544944763, acc.: 55.32%] [G loss: 0.7717584371566772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 35/86 [D loss: 0.6680582761764526, acc.: 58.64%] [G loss: 0.7829881310462952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 36/86 [D loss: 0.6633904278278351, acc.: 60.01%] [G loss: 0.7842537760734558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 37/86 [D loss: 0.6660883128643036, acc.: 58.74%] [G loss: 0.7748165726661682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 38/86 [D loss: 0.6596165001392365, acc.: 61.52%] [G loss: 0.7927141785621643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 39/86 [D loss: 0.6684605777263641, acc.: 58.01%] [G loss: 0.7854040861129761]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 40/86 [D loss: 0.664948582649231, acc.: 59.91%] [G loss: 0.7762916088104248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 41/86 [D loss: 0.6637789607048035, acc.: 58.84%] [G loss: 0.7746893167495728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 42/86 [D loss: 0.669992983341217, acc.: 57.62%] [G loss: 0.7744454741477966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 43/86 [D loss: 0.6632021367549896, acc.: 59.67%] [G loss: 0.7847017645835876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 44/86 [D loss: 0.664008617401123, acc.: 60.60%] [G loss: 0.768574059009552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 45/86 [D loss: 0.6712347865104675, acc.: 57.96%] [G loss: 0.7783448100090027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 46/86 [D loss: 0.6645110249519348, acc.: 59.23%] [G loss: 0.7836557030677795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 47/86 [D loss: 0.6548901498317719, acc.: 62.40%] [G loss: 0.78098064661026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 48/86 [D loss: 0.6701849400997162, acc.: 57.76%] [G loss: 0.7751457095146179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 49/86 [D loss: 0.6621977984905243, acc.: 61.28%] [G loss: 0.7844246029853821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 50/86 [D loss: 0.6666738390922546, acc.: 58.74%] [G loss: 0.7790648341178894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 51/86 [D loss: 0.6643130481243134, acc.: 59.38%] [G loss: 0.7816616296768188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 52/86 [D loss: 0.6652363836765289, acc.: 60.30%] [G loss: 0.775941789150238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 53/86 [D loss: 0.6584963798522949, acc.: 60.99%] [G loss: 0.7911136150360107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 54/86 [D loss: 0.6646783351898193, acc.: 60.69%] [G loss: 0.7790518999099731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 55/86 [D loss: 0.6609247028827667, acc.: 60.16%] [G loss: 0.7735666632652283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 56/86 [D loss: 0.6638080179691315, acc.: 59.81%] [G loss: 0.7767137289047241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 57/86 [D loss: 0.6610181927680969, acc.: 60.69%] [G loss: 0.7835116982460022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 58/86 [D loss: 0.6620220839977264, acc.: 60.99%] [G loss: 0.7855592370033264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 59/86 [D loss: 0.6654134094715118, acc.: 58.84%] [G loss: 0.7728301286697388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 60/86 [D loss: 0.6707045435905457, acc.: 57.47%] [G loss: 0.7704808712005615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 61/86 [D loss: 0.6650959551334381, acc.: 60.25%] [G loss: 0.7720603942871094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 62/86 [D loss: 0.6681135296821594, acc.: 58.35%] [G loss: 0.7808666229248047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 63/86 [D loss: 0.6656506657600403, acc.: 60.01%] [G loss: 0.7784838080406189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 64/86 [D loss: 0.6710141897201538, acc.: 57.76%] [G loss: 0.7837335467338562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 65/86 [D loss: 0.6674753129482269, acc.: 59.42%] [G loss: 0.7829093933105469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 66/86 [D loss: 0.6580808460712433, acc.: 61.18%] [G loss: 0.7780771851539612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 67/86 [D loss: 0.6671242117881775, acc.: 59.23%] [G loss: 0.7852520942687988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 68/86 [D loss: 0.664097785949707, acc.: 59.13%] [G loss: 0.7807632684707642]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 69/86 [D loss: 0.6655151844024658, acc.: 59.18%] [G loss: 0.784935712814331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 70/86 [D loss: 0.6664427816867828, acc.: 59.62%] [G loss: 0.7772734761238098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 71/86 [D loss: 0.6648792326450348, acc.: 60.30%] [G loss: 0.7806075215339661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 72/86 [D loss: 0.6602400839328766, acc.: 60.01%] [G loss: 0.7778258323669434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 73/86 [D loss: 0.6656002104282379, acc.: 59.96%] [G loss: 0.7797559499740601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 74/86 [D loss: 0.6634612381458282, acc.: 60.01%] [G loss: 0.7807243466377258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 75/86 [D loss: 0.6624926030635834, acc.: 59.91%] [G loss: 0.7846790552139282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 76/86 [D loss: 0.6669727563858032, acc.: 60.74%] [G loss: 0.7767037153244019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 77/86 [D loss: 0.6671828329563141, acc.: 58.45%] [G loss: 0.7798537015914917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 78/86 [D loss: 0.6625168025493622, acc.: 60.25%] [G loss: 0.7871912717819214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 79/86 [D loss: 0.667449414730072, acc.: 59.77%] [G loss: 0.7799123525619507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 80/86 [D loss: 0.669505387544632, acc.: 58.79%] [G loss: 0.777917206287384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 81/86 [D loss: 0.6601192653179169, acc.: 60.69%] [G loss: 0.7812562584877014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 82/86 [D loss: 0.6659926176071167, acc.: 59.52%] [G loss: 0.7790108323097229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 83/86 [D loss: 0.6655265092849731, acc.: 59.77%] [G loss: 0.783257007598877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 84/86 [D loss: 0.6630554497241974, acc.: 60.35%] [G loss: 0.7862585783004761]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 159/200, Batch 85/86 [D loss: 0.669715166091919, acc.: 58.25%] [G loss: 0.7806007266044617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 159/200, Batch 86/86 [D loss: 0.658107340335846, acc.: 60.50%] [G loss: 0.7700608968734741]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 1/86 [D loss: 0.6714378297328949, acc.: 56.74%] [G loss: 0.7776731848716736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 2/86 [D loss: 0.6615544259548187, acc.: 59.03%] [G loss: 0.7764115333557129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 3/86 [D loss: 0.6672603487968445, acc.: 59.67%] [G loss: 0.7850542664527893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 4/86 [D loss: 0.66902956366539, acc.: 57.86%] [G loss: 0.7802988290786743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 5/86 [D loss: 0.6616515815258026, acc.: 59.33%] [G loss: 0.7838715314865112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 6/86 [D loss: 0.6621523499488831, acc.: 60.21%] [G loss: 0.7686934471130371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 7/86 [D loss: 0.6683925986289978, acc.: 58.40%] [G loss: 0.7773197293281555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 8/86 [D loss: 0.6639582216739655, acc.: 59.77%] [G loss: 0.7827273607254028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 9/86 [D loss: 0.6697554290294647, acc.: 58.79%] [G loss: 0.7856630682945251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 10/86 [D loss: 0.6698450744152069, acc.: 58.94%] [G loss: 0.7849869728088379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 11/86 [D loss: 0.6647939383983612, acc.: 60.69%] [G loss: 0.7803285717964172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 12/86 [D loss: 0.6667540073394775, acc.: 59.13%] [G loss: 0.7826085686683655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 13/86 [D loss: 0.6650490462779999, acc.: 59.28%] [G loss: 0.7780635952949524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 14/86 [D loss: 0.666056752204895, acc.: 58.45%] [G loss: 0.7876108288764954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 15/86 [D loss: 0.6736735105514526, acc.: 56.93%] [G loss: 0.771117627620697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 16/86 [D loss: 0.6709913015365601, acc.: 58.84%] [G loss: 0.7913224697113037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 17/86 [D loss: 0.660791665315628, acc.: 61.43%] [G loss: 0.7835415601730347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 18/86 [D loss: 0.6674650013446808, acc.: 59.52%] [G loss: 0.782194972038269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 19/86 [D loss: 0.6641846001148224, acc.: 60.94%] [G loss: 0.7839834094047546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 20/86 [D loss: 0.6579203605651855, acc.: 62.94%] [G loss: 0.7790490984916687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 21/86 [D loss: 0.6715646386146545, acc.: 57.32%] [G loss: 0.7733163237571716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 22/86 [D loss: 0.6684652864933014, acc.: 57.91%] [G loss: 0.7780077457427979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 23/86 [D loss: 0.6655984818935394, acc.: 59.52%] [G loss: 0.7764782905578613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 24/86 [D loss: 0.6666750013828278, acc.: 59.77%] [G loss: 0.7752253413200378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 25/86 [D loss: 0.6705008149147034, acc.: 56.01%] [G loss: 0.7723843455314636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 26/86 [D loss: 0.6647796034812927, acc.: 59.42%] [G loss: 0.7805364727973938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 27/86 [D loss: 0.6655194163322449, acc.: 59.13%] [G loss: 0.7782637476921082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 28/86 [D loss: 0.6615375876426697, acc.: 60.40%] [G loss: 0.7838197946548462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 29/86 [D loss: 0.6671355366706848, acc.: 58.74%] [G loss: 0.7659170627593994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 30/86 [D loss: 0.6638531982898712, acc.: 58.89%] [G loss: 0.7758059501647949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 31/86 [D loss: 0.66813063621521, acc.: 57.42%] [G loss: 0.7709894776344299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 32/86 [D loss: 0.665046215057373, acc.: 60.30%] [G loss: 0.7815867066383362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 33/86 [D loss: 0.6627987027168274, acc.: 60.55%] [G loss: 0.7797664999961853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 34/86 [D loss: 0.6612116098403931, acc.: 61.38%] [G loss: 0.7745074033737183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 35/86 [D loss: 0.6674773097038269, acc.: 59.03%] [G loss: 0.7814944386482239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 36/86 [D loss: 0.6620209515094757, acc.: 60.89%] [G loss: 0.7783108353614807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 37/86 [D loss: 0.6603760123252869, acc.: 61.43%] [G loss: 0.7860068678855896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 38/86 [D loss: 0.6602611243724823, acc.: 60.35%] [G loss: 0.7697778940200806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 39/86 [D loss: 0.6706801056861877, acc.: 57.91%] [G loss: 0.7836875319480896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 40/86 [D loss: 0.6637834310531616, acc.: 59.47%] [G loss: 0.7757818102836609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 41/86 [D loss: 0.6718515753746033, acc.: 58.30%] [G loss: 0.7885531187057495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 42/86 [D loss: 0.6679372191429138, acc.: 58.79%] [G loss: 0.7788795232772827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 43/86 [D loss: 0.6699894666671753, acc.: 58.64%] [G loss: 0.7839627265930176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 44/86 [D loss: 0.6707405745983124, acc.: 57.18%] [G loss: 0.7718759179115295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 45/86 [D loss: 0.661093682050705, acc.: 61.04%] [G loss: 0.7850966453552246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 46/86 [D loss: 0.671641618013382, acc.: 57.71%] [G loss: 0.7733882665634155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 47/86 [D loss: 0.6630602478981018, acc.: 59.77%] [G loss: 0.7816706895828247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 48/86 [D loss: 0.6649785935878754, acc.: 59.81%] [G loss: 0.7766426801681519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 49/86 [D loss: 0.6697454750537872, acc.: 58.01%] [G loss: 0.7826352119445801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 50/86 [D loss: 0.6630926132202148, acc.: 60.69%] [G loss: 0.7776176929473877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 51/86 [D loss: 0.6631568968296051, acc.: 60.21%] [G loss: 0.7801237106323242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 52/86 [D loss: 0.664664626121521, acc.: 59.81%] [G loss: 0.7773772478103638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 53/86 [D loss: 0.6684542000293732, acc.: 58.11%] [G loss: 0.7706498503684998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 54/86 [D loss: 0.6642607152462006, acc.: 58.84%] [G loss: 0.7825743556022644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 55/86 [D loss: 0.6671610176563263, acc.: 59.03%] [G loss: 0.7769492268562317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 56/86 [D loss: 0.66655033826828, acc.: 58.30%] [G loss: 0.7828570008277893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 57/86 [D loss: 0.6675969958305359, acc.: 59.08%] [G loss: 0.7811570167541504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 58/86 [D loss: 0.6657655835151672, acc.: 59.91%] [G loss: 0.7722530364990234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 59/86 [D loss: 0.6698251962661743, acc.: 58.89%] [G loss: 0.7781726121902466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 60/86 [D loss: 0.663349449634552, acc.: 59.81%] [G loss: 0.7817574739456177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 61/86 [D loss: 0.66867595911026, acc.: 59.23%] [G loss: 0.7837572693824768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 62/86 [D loss: 0.6711144149303436, acc.: 58.45%] [G loss: 0.7887061834335327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 63/86 [D loss: 0.6642408668994904, acc.: 60.11%] [G loss: 0.7808767557144165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 64/86 [D loss: 0.6630279719829559, acc.: 60.64%] [G loss: 0.7782225608825684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 65/86 [D loss: 0.6595440804958344, acc.: 60.89%] [G loss: 0.778650164604187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 66/86 [D loss: 0.6667405068874359, acc.: 59.23%] [G loss: 0.7803447246551514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 67/86 [D loss: 0.6595886945724487, acc.: 61.72%] [G loss: 0.7856611609458923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 68/86 [D loss: 0.668474942445755, acc.: 58.50%] [G loss: 0.7747644186019897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 69/86 [D loss: 0.6656532883644104, acc.: 58.54%] [G loss: 0.7843466401100159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 70/86 [D loss: 0.668544590473175, acc.: 57.76%] [G loss: 0.7892158031463623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 71/86 [D loss: 0.6615229547023773, acc.: 60.84%] [G loss: 0.7882305383682251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 72/86 [D loss: 0.6713724732398987, acc.: 59.18%] [G loss: 0.789973258972168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 73/86 [D loss: 0.6663697063922882, acc.: 59.28%] [G loss: 0.780771017074585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 74/86 [D loss: 0.6681991815567017, acc.: 57.96%] [G loss: 0.7799962759017944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 75/86 [D loss: 0.6685843765735626, acc.: 60.01%] [G loss: 0.7899633646011353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 76/86 [D loss: 0.6629303693771362, acc.: 59.13%] [G loss: 0.7794895768165588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 77/86 [D loss: 0.665026992559433, acc.: 58.84%] [G loss: 0.7832795977592468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 78/86 [D loss: 0.6702047288417816, acc.: 57.52%] [G loss: 0.7744916677474976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 79/86 [D loss: 0.6669132709503174, acc.: 59.81%] [G loss: 0.7918232083320618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 80/86 [D loss: 0.6623180210590363, acc.: 60.25%] [G loss: 0.7815549969673157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 81/86 [D loss: 0.6700022220611572, acc.: 58.11%] [G loss: 0.7903079986572266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 82/86 [D loss: 0.665553867816925, acc.: 59.52%] [G loss: 0.7850834727287292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 83/86 [D loss: 0.668245404958725, acc.: 59.28%] [G loss: 0.7834982872009277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 84/86 [D loss: 0.669898122549057, acc.: 57.13%] [G loss: 0.7775445580482483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 85/86 [D loss: 0.6687529683113098, acc.: 58.40%] [G loss: 0.7792531251907349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 160/200, Batch 86/86 [D loss: 0.6652242541313171, acc.: 57.71%] [G loss: 0.7767413258552551]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 1/86 [D loss: 0.6684760749340057, acc.: 59.23%] [G loss: 0.7885263562202454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 2/86 [D loss: 0.6650723814964294, acc.: 58.40%] [G loss: 0.7883602380752563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 3/86 [D loss: 0.6658456325531006, acc.: 59.38%] [G loss: 0.7792870402336121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 4/86 [D loss: 0.6658636331558228, acc.: 58.25%] [G loss: 0.7813853025436401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 5/86 [D loss: 0.666711688041687, acc.: 58.54%] [G loss: 0.779705822467804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 6/86 [D loss: 0.6604825556278229, acc.: 60.45%] [G loss: 0.7860433459281921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 7/86 [D loss: 0.6643375754356384, acc.: 59.62%] [G loss: 0.7723807096481323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 8/86 [D loss: 0.6632559597492218, acc.: 61.28%] [G loss: 0.7845273017883301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 9/86 [D loss: 0.6724774241447449, acc.: 57.96%] [G loss: 0.7835208773612976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 10/86 [D loss: 0.6634826362133026, acc.: 60.40%] [G loss: 0.7760117053985596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 11/86 [D loss: 0.6720983386039734, acc.: 57.52%] [G loss: 0.7905001044273376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 12/86 [D loss: 0.655982255935669, acc.: 60.69%] [G loss: 0.7854061722755432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 13/86 [D loss: 0.6645680665969849, acc.: 61.08%] [G loss: 0.7761224508285522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 14/86 [D loss: 0.6684763729572296, acc.: 59.18%] [G loss: 0.7921497225761414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 15/86 [D loss: 0.6680742800235748, acc.: 57.67%] [G loss: 0.7802659869194031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 16/86 [D loss: 0.6636674106121063, acc.: 59.18%] [G loss: 0.7844240665435791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 17/86 [D loss: 0.6705793142318726, acc.: 57.52%] [G loss: 0.774727463722229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 18/86 [D loss: 0.6620033085346222, acc.: 59.96%] [G loss: 0.792097806930542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 19/86 [D loss: 0.6668488085269928, acc.: 59.96%] [G loss: 0.7840638160705566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 20/86 [D loss: 0.6633422374725342, acc.: 58.79%] [G loss: 0.77974933385849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 21/86 [D loss: 0.6629794836044312, acc.: 59.77%] [G loss: 0.7794627547264099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 22/86 [D loss: 0.6677587926387787, acc.: 59.81%] [G loss: 0.7765624523162842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 23/86 [D loss: 0.6676340401172638, acc.: 60.01%] [G loss: 0.7767969965934753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 24/86 [D loss: 0.6664464771747589, acc.: 59.38%] [G loss: 0.7802937030792236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 25/86 [D loss: 0.6634863018989563, acc.: 59.86%] [G loss: 0.7838082313537598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 26/86 [D loss: 0.6657827198505402, acc.: 59.47%] [G loss: 0.7723448276519775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 27/86 [D loss: 0.6649409532546997, acc.: 60.69%] [G loss: 0.7895547747612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 28/86 [D loss: 0.6663863062858582, acc.: 59.81%] [G loss: 0.7718344330787659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 29/86 [D loss: 0.6676985919475555, acc.: 58.06%] [G loss: 0.78618985414505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 30/86 [D loss: 0.6646210551261902, acc.: 60.21%] [G loss: 0.7764739990234375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 31/86 [D loss: 0.6597828269004822, acc.: 60.06%] [G loss: 0.7820683121681213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 32/86 [D loss: 0.6665484011173248, acc.: 58.30%] [G loss: 0.7796227931976318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 33/86 [D loss: 0.6622259616851807, acc.: 61.43%] [G loss: 0.7863784432411194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 34/86 [D loss: 0.6642245650291443, acc.: 61.23%] [G loss: 0.7874197363853455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 35/86 [D loss: 0.6684907078742981, acc.: 58.25%] [G loss: 0.7762629985809326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 36/86 [D loss: 0.6639190018177032, acc.: 58.94%] [G loss: 0.7835520505905151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 37/86 [D loss: 0.6675898134708405, acc.: 58.25%] [G loss: 0.7813199758529663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 38/86 [D loss: 0.6674751937389374, acc.: 59.57%] [G loss: 0.7931158542633057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 39/86 [D loss: 0.667498767375946, acc.: 59.23%] [G loss: 0.7811453938484192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 40/86 [D loss: 0.6628913283348083, acc.: 58.79%] [G loss: 0.7878318428993225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 41/86 [D loss: 0.658218115568161, acc.: 61.28%] [G loss: 0.78924560546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 42/86 [D loss: 0.6699183583259583, acc.: 58.01%] [G loss: 0.7850550413131714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 43/86 [D loss: 0.6658598482608795, acc.: 59.03%] [G loss: 0.7761735916137695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 44/86 [D loss: 0.6702634692192078, acc.: 57.81%] [G loss: 0.7831342220306396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 45/86 [D loss: 0.6620470285415649, acc.: 60.35%] [G loss: 0.7652490735054016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 46/86 [D loss: 0.6843777000904083, acc.: 53.66%] [G loss: 0.8018680214881897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 47/86 [D loss: 0.6733382344245911, acc.: 55.37%] [G loss: 0.7642730474472046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 48/86 [D loss: 0.6737256050109863, acc.: 57.81%] [G loss: 0.791885495185852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 49/86 [D loss: 0.6688688397407532, acc.: 58.50%] [G loss: 0.7512850761413574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 50/86 [D loss: 0.6752371191978455, acc.: 56.10%] [G loss: 0.8062359094619751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 51/86 [D loss: 0.6782095432281494, acc.: 55.37%] [G loss: 0.7605983018875122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 52/86 [D loss: 0.6694158911705017, acc.: 58.69%] [G loss: 0.7888904213905334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 53/86 [D loss: 0.6668400764465332, acc.: 60.89%] [G loss: 0.764784574508667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 54/86 [D loss: 0.6649873554706573, acc.: 59.18%] [G loss: 0.7884012460708618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 55/86 [D loss: 0.6767698526382446, acc.: 57.23%] [G loss: 0.7743979692459106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 56/86 [D loss: 0.6622985005378723, acc.: 59.28%] [G loss: 0.7981361746788025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 57/86 [D loss: 0.6768649220466614, acc.: 56.93%] [G loss: 0.7805187106132507]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 161/200, Batch 58/86 [D loss: 0.667725533246994, acc.: 58.50%] [G loss: 0.7898250222206116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 59/86 [D loss: 0.6672957837581635, acc.: 59.33%] [G loss: 0.7720452547073364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 60/86 [D loss: 0.6723956167697906, acc.: 56.79%] [G loss: 0.7814049124717712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 61/86 [D loss: 0.6686868965625763, acc.: 59.03%] [G loss: 0.7728832960128784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 62/86 [D loss: 0.669765293598175, acc.: 59.23%] [G loss: 0.7928775548934937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 63/86 [D loss: 0.6644153594970703, acc.: 59.08%] [G loss: 0.7772970199584961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 64/86 [D loss: 0.6735762655735016, acc.: 56.84%] [G loss: 0.7854732275009155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 65/86 [D loss: 0.6623198390007019, acc.: 60.79%] [G loss: 0.7779834270477295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 66/86 [D loss: 0.6723061203956604, acc.: 58.11%] [G loss: 0.7727800011634827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 67/86 [D loss: 0.6614977419376373, acc.: 60.89%] [G loss: 0.785477340221405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 68/86 [D loss: 0.6647210717201233, acc.: 59.67%] [G loss: 0.7753143310546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 69/86 [D loss: 0.6644797027111053, acc.: 59.86%] [G loss: 0.7901817560195923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 70/86 [D loss: 0.6599839329719543, acc.: 61.38%] [G loss: 0.7892089486122131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 71/86 [D loss: 0.6700851619243622, acc.: 59.77%] [G loss: 0.7929254174232483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 72/86 [D loss: 0.6605948209762573, acc.: 59.81%] [G loss: 0.7852655649185181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 73/86 [D loss: 0.6650334894657135, acc.: 57.76%] [G loss: 0.7823273539543152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 74/86 [D loss: 0.6657682061195374, acc.: 60.06%] [G loss: 0.7778671979904175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 75/86 [D loss: 0.6657596230506897, acc.: 59.47%] [G loss: 0.7743299007415771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 76/86 [D loss: 0.6686370670795441, acc.: 59.13%] [G loss: 0.7800977230072021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 77/86 [D loss: 0.6630366742610931, acc.: 60.01%] [G loss: 0.776482105255127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 78/86 [D loss: 0.6610868573188782, acc.: 59.86%] [G loss: 0.7856310606002808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 79/86 [D loss: 0.6672987043857574, acc.: 58.59%] [G loss: 0.7847540378570557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 80/86 [D loss: 0.666745126247406, acc.: 59.33%] [G loss: 0.7875416278839111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 81/86 [D loss: 0.6665388345718384, acc.: 58.50%] [G loss: 0.7845925688743591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 82/86 [D loss: 0.6609347462654114, acc.: 59.86%] [G loss: 0.7868962287902832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 83/86 [D loss: 0.6646016240119934, acc.: 60.16%] [G loss: 0.7911761999130249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 84/86 [D loss: 0.6635856628417969, acc.: 60.30%] [G loss: 0.7845751047134399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 85/86 [D loss: 0.6657994389533997, acc.: 59.23%] [G loss: 0.7879488468170166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 161/200, Batch 86/86 [D loss: 0.6675041913986206, acc.: 58.79%] [G loss: 0.7911943793296814]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 162/200, Batch 1/86 [D loss: 0.6658617854118347, acc.: 58.25%] [G loss: 0.7813593745231628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 2/86 [D loss: 0.6650997698307037, acc.: 59.81%] [G loss: 0.7755601406097412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 3/86 [D loss: 0.6705224215984344, acc.: 57.28%] [G loss: 0.7847380042076111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 4/86 [D loss: 0.6584495604038239, acc.: 61.62%] [G loss: 0.7795144319534302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 5/86 [D loss: 0.66448774933815, acc.: 59.62%] [G loss: 0.7866078019142151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 6/86 [D loss: 0.6677231788635254, acc.: 57.76%] [G loss: 0.7839083671569824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 7/86 [D loss: 0.6619321703910828, acc.: 60.01%] [G loss: 0.7723656296730042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 8/86 [D loss: 0.6560295224189758, acc.: 62.21%] [G loss: 0.7917281985282898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 9/86 [D loss: 0.6637409627437592, acc.: 59.08%] [G loss: 0.7813442945480347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 10/86 [D loss: 0.6696751415729523, acc.: 58.79%] [G loss: 0.7787661552429199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 11/86 [D loss: 0.6661992073059082, acc.: 57.67%] [G loss: 0.7799438238143921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 12/86 [D loss: 0.6755474507808685, acc.: 57.08%] [G loss: 0.7857897877693176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 13/86 [D loss: 0.6718831956386566, acc.: 57.71%] [G loss: 0.7825759649276733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 14/86 [D loss: 0.6655177474021912, acc.: 60.11%] [G loss: 0.783236026763916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 15/86 [D loss: 0.6678530871868134, acc.: 57.67%] [G loss: 0.7874524593353271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 16/86 [D loss: 0.6655184626579285, acc.: 60.11%] [G loss: 0.7891221642494202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 17/86 [D loss: 0.6716514825820923, acc.: 56.05%] [G loss: 0.7805994749069214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 18/86 [D loss: 0.6637486517429352, acc.: 58.94%] [G loss: 0.7793151140213013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 19/86 [D loss: 0.6658807694911957, acc.: 59.62%] [G loss: 0.777648389339447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 20/86 [D loss: 0.6614043414592743, acc.: 60.74%] [G loss: 0.7905095219612122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 21/86 [D loss: 0.6625409126281738, acc.: 59.91%] [G loss: 0.7876695394515991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 22/86 [D loss: 0.6674637794494629, acc.: 58.40%] [G loss: 0.7770028114318848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 23/86 [D loss: 0.6672825813293457, acc.: 59.03%] [G loss: 0.7842139005661011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 24/86 [D loss: 0.6604841351509094, acc.: 59.91%] [G loss: 0.7935280799865723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 25/86 [D loss: 0.6652717590332031, acc.: 59.72%] [G loss: 0.7777678966522217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 26/86 [D loss: 0.6648202240467072, acc.: 58.15%] [G loss: 0.787805438041687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 27/86 [D loss: 0.6675398051738739, acc.: 58.06%] [G loss: 0.7830023765563965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 28/86 [D loss: 0.6631660461425781, acc.: 59.72%] [G loss: 0.7926263809204102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 29/86 [D loss: 0.6597221493721008, acc.: 60.55%] [G loss: 0.7775015234947205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 30/86 [D loss: 0.6641602218151093, acc.: 60.21%] [G loss: 0.7851946949958801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 31/86 [D loss: 0.665225088596344, acc.: 59.38%] [G loss: 0.7772895097732544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 32/86 [D loss: 0.6748355329036713, acc.: 57.86%] [G loss: 0.7936922907829285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 33/86 [D loss: 0.6640242040157318, acc.: 61.62%] [G loss: 0.785934567451477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 34/86 [D loss: 0.6662069857120514, acc.: 59.67%] [G loss: 0.7872336506843567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 35/86 [D loss: 0.6680145561695099, acc.: 57.52%] [G loss: 0.7797406911849976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 36/86 [D loss: 0.6672441065311432, acc.: 58.54%] [G loss: 0.7847491502761841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 37/86 [D loss: 0.6692532300949097, acc.: 59.67%] [G loss: 0.7703750133514404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 38/86 [D loss: 0.6659587621688843, acc.: 59.77%] [G loss: 0.7795292735099792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 39/86 [D loss: 0.6652613282203674, acc.: 60.11%] [G loss: 0.7761164903640747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 40/86 [D loss: 0.6643942594528198, acc.: 59.77%] [G loss: 0.7837578058242798]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 41/86 [D loss: 0.6608076691627502, acc.: 60.30%] [G loss: 0.7798860669136047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 42/86 [D loss: 0.662790834903717, acc.: 59.42%] [G loss: 0.7828246355056763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 43/86 [D loss: 0.663994163274765, acc.: 61.33%] [G loss: 0.787284791469574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 44/86 [D loss: 0.668834924697876, acc.: 59.47%] [G loss: 0.7815330028533936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 45/86 [D loss: 0.6625073850154877, acc.: 60.99%] [G loss: 0.7933425307273865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 46/86 [D loss: 0.6683736443519592, acc.: 58.01%] [G loss: 0.7811771631240845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 47/86 [D loss: 0.6688738465309143, acc.: 58.20%] [G loss: 0.7758217453956604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 48/86 [D loss: 0.6712118983268738, acc.: 56.98%] [G loss: 0.7800502777099609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 49/86 [D loss: 0.6686662137508392, acc.: 58.45%] [G loss: 0.7846888303756714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 50/86 [D loss: 0.6672171354293823, acc.: 58.11%] [G loss: 0.7846218943595886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 51/86 [D loss: 0.666676938533783, acc.: 58.84%] [G loss: 0.7906917333602905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 52/86 [D loss: 0.6715212762355804, acc.: 56.59%] [G loss: 0.7905149459838867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 53/86 [D loss: 0.6657201945781708, acc.: 59.33%] [G loss: 0.7799738049507141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 54/86 [D loss: 0.6645628809928894, acc.: 58.74%] [G loss: 0.7827098369598389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 55/86 [D loss: 0.6632258892059326, acc.: 59.72%] [G loss: 0.7830653190612793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 56/86 [D loss: 0.6688753068447113, acc.: 58.11%] [G loss: 0.7873740792274475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 57/86 [D loss: 0.6654681265354156, acc.: 59.72%] [G loss: 0.7768183350563049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 58/86 [D loss: 0.6674320995807648, acc.: 58.79%] [G loss: 0.7770891785621643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 59/86 [D loss: 0.6689525544643402, acc.: 58.79%] [G loss: 0.7808147072792053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 60/86 [D loss: 0.6689910888671875, acc.: 59.38%] [G loss: 0.7759143710136414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 61/86 [D loss: 0.6646292805671692, acc.: 59.33%] [G loss: 0.790901243686676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 62/86 [D loss: 0.6719362437725067, acc.: 57.62%] [G loss: 0.7841983437538147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 63/86 [D loss: 0.6633994579315186, acc.: 59.77%] [G loss: 0.7899250388145447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 64/86 [D loss: 0.6623415052890778, acc.: 61.62%] [G loss: 0.7874935269355774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 65/86 [D loss: 0.6661438345909119, acc.: 58.98%] [G loss: 0.7739958763122559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 66/86 [D loss: 0.6720706820487976, acc.: 57.42%] [G loss: 0.7787238955497742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 67/86 [D loss: 0.6664343476295471, acc.: 58.35%] [G loss: 0.7792293429374695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 68/86 [D loss: 0.6657992303371429, acc.: 59.72%] [G loss: 0.7744028568267822]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 69/86 [D loss: 0.6683987081050873, acc.: 58.89%] [G loss: 0.7887091636657715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 70/86 [D loss: 0.6736319363117218, acc.: 57.03%] [G loss: 0.7731336355209351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 71/86 [D loss: 0.6662878096103668, acc.: 59.72%] [G loss: 0.7785851955413818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 72/86 [D loss: 0.6631691455841064, acc.: 58.59%] [G loss: 0.7703245878219604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 73/86 [D loss: 0.6659479439258575, acc.: 60.50%] [G loss: 0.786460280418396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 74/86 [D loss: 0.666002482175827, acc.: 59.38%] [G loss: 0.7822161316871643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 75/86 [D loss: 0.6665951609611511, acc.: 58.50%] [G loss: 0.7874815464019775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 76/86 [D loss: 0.6694961488246918, acc.: 58.79%] [G loss: 0.7750068306922913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 77/86 [D loss: 0.6742605865001678, acc.: 56.84%] [G loss: 0.7842279672622681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 78/86 [D loss: 0.6597315669059753, acc.: 61.43%] [G loss: 0.7874282002449036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 79/86 [D loss: 0.6647834181785583, acc.: 58.74%] [G loss: 0.7755879163742065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 80/86 [D loss: 0.6659647524356842, acc.: 58.74%] [G loss: 0.7833614945411682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 81/86 [D loss: 0.6694457828998566, acc.: 58.30%] [G loss: 0.7874606847763062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 82/86 [D loss: 0.664994478225708, acc.: 60.60%] [G loss: 0.7917770147323608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 83/86 [D loss: 0.6707454919815063, acc.: 58.74%] [G loss: 0.7785013318061829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 84/86 [D loss: 0.6644147038459778, acc.: 60.45%] [G loss: 0.7928544878959656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 85/86 [D loss: 0.664084792137146, acc.: 59.13%] [G loss: 0.7743512988090515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 162/200, Batch 86/86 [D loss: 0.6673002243041992, acc.: 60.60%] [G loss: 0.7843007445335388]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 1/86 [D loss: 0.6597206294536591, acc.: 60.01%] [G loss: 0.7743237018585205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 2/86 [D loss: 0.6654593348503113, acc.: 59.96%] [G loss: 0.785075306892395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 3/86 [D loss: 0.6635966897010803, acc.: 59.67%] [G loss: 0.7765783667564392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 4/86 [D loss: 0.6685611307621002, acc.: 58.11%] [G loss: 0.7883639335632324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 5/86 [D loss: 0.6638597548007965, acc.: 59.38%] [G loss: 0.7741619944572449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 6/86 [D loss: 0.6650752425193787, acc.: 59.23%] [G loss: 0.7860239744186401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 7/86 [D loss: 0.6627191603183746, acc.: 59.86%] [G loss: 0.7892530560493469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 8/86 [D loss: 0.6706028878688812, acc.: 58.59%] [G loss: 0.788722574710846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 9/86 [D loss: 0.663232147693634, acc.: 59.23%] [G loss: 0.7845209836959839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 10/86 [D loss: 0.6639802157878876, acc.: 60.60%] [G loss: 0.7848326563835144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 11/86 [D loss: 0.6672824323177338, acc.: 59.67%] [G loss: 0.7778933644294739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 12/86 [D loss: 0.6662347316741943, acc.: 60.30%] [G loss: 0.7834417819976807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 13/86 [D loss: 0.6628806591033936, acc.: 59.57%] [G loss: 0.7885335683822632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 14/86 [D loss: 0.6678103804588318, acc.: 59.52%] [G loss: 0.79179447889328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 15/86 [D loss: 0.6620659530162811, acc.: 60.45%] [G loss: 0.7855378985404968]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 16/86 [D loss: 0.6623390913009644, acc.: 60.01%] [G loss: 0.7902881503105164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 17/86 [D loss: 0.6665357351303101, acc.: 59.67%] [G loss: 0.7913695573806763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 18/86 [D loss: 0.6719090044498444, acc.: 56.84%] [G loss: 0.7783282995223999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 19/86 [D loss: 0.6649617254734039, acc.: 59.08%] [G loss: 0.7856707572937012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 20/86 [D loss: 0.666142463684082, acc.: 58.89%] [G loss: 0.7827848196029663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 21/86 [D loss: 0.6705713868141174, acc.: 57.96%] [G loss: 0.7885123491287231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 22/86 [D loss: 0.670060396194458, acc.: 56.93%] [G loss: 0.7737704515457153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 23/86 [D loss: 0.6610777378082275, acc.: 59.47%] [G loss: 0.7834874391555786]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 24/86 [D loss: 0.6615067720413208, acc.: 60.30%] [G loss: 0.7826621532440186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 25/86 [D loss: 0.6634105443954468, acc.: 59.96%] [G loss: 0.7841358780860901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 26/86 [D loss: 0.6593954265117645, acc.: 59.91%] [G loss: 0.7834171056747437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 27/86 [D loss: 0.6632561385631561, acc.: 59.33%] [G loss: 0.7814804911613464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 28/86 [D loss: 0.6668588817119598, acc.: 58.84%] [G loss: 0.7816177010536194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 29/86 [D loss: 0.6665052175521851, acc.: 58.45%] [G loss: 0.7840046286582947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 30/86 [D loss: 0.6538582146167755, acc.: 62.55%] [G loss: 0.7901355624198914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 31/86 [D loss: 0.6620902717113495, acc.: 60.64%] [G loss: 0.7880057096481323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 32/86 [D loss: 0.6675040423870087, acc.: 58.54%] [G loss: 0.7885149717330933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 33/86 [D loss: 0.6663521528244019, acc.: 58.74%] [G loss: 0.7853280305862427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 34/86 [D loss: 0.6652762293815613, acc.: 61.13%] [G loss: 0.7894731760025024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 35/86 [D loss: 0.669899970293045, acc.: 58.79%] [G loss: 0.7891163229942322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 36/86 [D loss: 0.6636146903038025, acc.: 59.38%] [G loss: 0.7892218828201294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 37/86 [D loss: 0.6627325415611267, acc.: 58.84%] [G loss: 0.7869184017181396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 38/86 [D loss: 0.6662156879901886, acc.: 60.35%] [G loss: 0.7842307686805725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 39/86 [D loss: 0.6661678850650787, acc.: 59.28%] [G loss: 0.7829543352127075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 40/86 [D loss: 0.6702830791473389, acc.: 58.30%] [G loss: 0.7814307808876038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 41/86 [D loss: 0.6644618213176727, acc.: 60.21%] [G loss: 0.7890152335166931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 42/86 [D loss: 0.6628783047199249, acc.: 60.89%] [G loss: 0.7817620635032654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 43/86 [D loss: 0.6634535491466522, acc.: 59.67%] [G loss: 0.7912682890892029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 44/86 [D loss: 0.6656067371368408, acc.: 58.94%] [G loss: 0.7696670293807983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 45/86 [D loss: 0.669731467962265, acc.: 57.67%] [G loss: 0.7874612808227539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 46/86 [D loss: 0.6696731150150299, acc.: 56.35%] [G loss: 0.7790920734405518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 47/86 [D loss: 0.6580637395381927, acc.: 61.96%] [G loss: 0.7739816904067993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 48/86 [D loss: 0.6663517653942108, acc.: 60.01%] [G loss: 0.785576343536377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 49/86 [D loss: 0.6625887453556061, acc.: 59.72%] [G loss: 0.785977303981781]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 163/200, Batch 50/86 [D loss: 0.6625300347805023, acc.: 59.81%] [G loss: 0.7887175679206848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 51/86 [D loss: 0.6672495305538177, acc.: 58.84%] [G loss: 0.7811800837516785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 52/86 [D loss: 0.668172299861908, acc.: 58.20%] [G loss: 0.7802516222000122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 53/86 [D loss: 0.6679434776306152, acc.: 58.79%] [G loss: 0.7781186103820801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 54/86 [D loss: 0.6598590612411499, acc.: 60.45%] [G loss: 0.7737587094306946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 55/86 [D loss: 0.6633337140083313, acc.: 59.13%] [G loss: 0.7898604869842529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 56/86 [D loss: 0.6680630445480347, acc.: 58.69%] [G loss: 0.7814397811889648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 57/86 [D loss: 0.6585983633995056, acc.: 61.04%] [G loss: 0.7867650389671326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 58/86 [D loss: 0.667932540178299, acc.: 58.45%] [G loss: 0.7695314884185791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 59/86 [D loss: 0.6674793362617493, acc.: 60.21%] [G loss: 0.7927809357643127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 60/86 [D loss: 0.6653998792171478, acc.: 59.23%] [G loss: 0.7697531580924988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 61/86 [D loss: 0.6676613092422485, acc.: 59.57%] [G loss: 0.7901244759559631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 62/86 [D loss: 0.6669092774391174, acc.: 59.42%] [G loss: 0.7857542037963867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 63/86 [D loss: 0.6599879860877991, acc.: 61.23%] [G loss: 0.7970268726348877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 64/86 [D loss: 0.6694160103797913, acc.: 58.25%] [G loss: 0.782785177230835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 65/86 [D loss: 0.6669850945472717, acc.: 59.28%] [G loss: 0.7799516916275024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 66/86 [D loss: 0.6566453874111176, acc.: 62.55%] [G loss: 0.7729236483573914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 67/86 [D loss: 0.668403297662735, acc.: 58.64%] [G loss: 0.7945983409881592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 68/86 [D loss: 0.6611472368240356, acc.: 60.25%] [G loss: 0.7672842144966125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 69/86 [D loss: 0.6664279997348785, acc.: 58.25%] [G loss: 0.7858314514160156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 70/86 [D loss: 0.66694375872612, acc.: 59.42%] [G loss: 0.7739773392677307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 71/86 [D loss: 0.6713048219680786, acc.: 57.42%] [G loss: 0.7780868411064148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 72/86 [D loss: 0.6662721931934357, acc.: 59.03%] [G loss: 0.7791310548782349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 73/86 [D loss: 0.6665518283843994, acc.: 58.11%] [G loss: 0.7776240706443787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 74/86 [D loss: 0.6671902239322662, acc.: 59.52%] [G loss: 0.7817294001579285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 75/86 [D loss: 0.6723375916481018, acc.: 58.40%] [G loss: 0.7920763492584229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 76/86 [D loss: 0.6603264212608337, acc.: 61.52%] [G loss: 0.7847341895103455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 77/86 [D loss: 0.6694245338439941, acc.: 57.08%] [G loss: 0.7806441783905029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 78/86 [D loss: 0.667412281036377, acc.: 58.69%] [G loss: 0.7797302007675171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 79/86 [D loss: 0.6641683876514435, acc.: 60.21%] [G loss: 0.790507435798645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 80/86 [D loss: 0.6629733741283417, acc.: 60.06%] [G loss: 0.7905278205871582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 81/86 [D loss: 0.6691339015960693, acc.: 58.89%] [G loss: 0.7838159799575806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 82/86 [D loss: 0.6697525680065155, acc.: 58.50%] [G loss: 0.7849748134613037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 83/86 [D loss: 0.6614856123924255, acc.: 61.28%] [G loss: 0.7820073366165161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 84/86 [D loss: 0.6594168543815613, acc.: 60.99%] [G loss: 0.7915577292442322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 85/86 [D loss: 0.6677323281764984, acc.: 58.54%] [G loss: 0.7837004661560059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 163/200, Batch 86/86 [D loss: 0.6672750413417816, acc.: 57.32%] [G loss: 0.7792901992797852]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 1/86 [D loss: 0.6629664599895477, acc.: 60.01%] [G loss: 0.7891331911087036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 2/86 [D loss: 0.6690714359283447, acc.: 59.28%] [G loss: 0.7785001397132874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 3/86 [D loss: 0.6626394093036652, acc.: 59.91%] [G loss: 0.7860374450683594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 4/86 [D loss: 0.6642943620681763, acc.: 58.84%] [G loss: 0.7798860669136047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 5/86 [D loss: 0.6647103130817413, acc.: 61.23%] [G loss: 0.7843247652053833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 6/86 [D loss: 0.6639336049556732, acc.: 59.72%] [G loss: 0.7880649566650391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 7/86 [D loss: 0.6644728779792786, acc.: 59.33%] [G loss: 0.7921305894851685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 8/86 [D loss: 0.6632522344589233, acc.: 58.94%] [G loss: 0.783190906047821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 9/86 [D loss: 0.6682640910148621, acc.: 58.84%] [G loss: 0.7789050936698914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 10/86 [D loss: 0.6637993454933167, acc.: 60.64%] [G loss: 0.776453971862793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 11/86 [D loss: 0.6585108935832977, acc.: 61.13%] [G loss: 0.7804094552993774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 12/86 [D loss: 0.6656647622585297, acc.: 58.94%] [G loss: 0.7819731831550598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 13/86 [D loss: 0.6651545166969299, acc.: 58.84%] [G loss: 0.7934066653251648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 14/86 [D loss: 0.6660183966159821, acc.: 58.59%] [G loss: 0.7742034196853638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 15/86 [D loss: 0.670194000005722, acc.: 57.57%] [G loss: 0.7887934446334839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 16/86 [D loss: 0.6608763635158539, acc.: 61.18%] [G loss: 0.7835642099380493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 17/86 [D loss: 0.6709260046482086, acc.: 58.98%] [G loss: 0.7841466069221497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 18/86 [D loss: 0.6582098603248596, acc.: 60.40%] [G loss: 0.7825664281845093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 19/86 [D loss: 0.6669893264770508, acc.: 58.98%] [G loss: 0.7921197414398193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 20/86 [D loss: 0.6635909676551819, acc.: 60.11%] [G loss: 0.7890827655792236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 21/86 [D loss: 0.6664985120296478, acc.: 58.30%] [G loss: 0.7857780456542969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 22/86 [D loss: 0.6603924632072449, acc.: 61.04%] [G loss: 0.79600989818573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 23/86 [D loss: 0.6657619178295135, acc.: 59.03%] [G loss: 0.7892580032348633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 24/86 [D loss: 0.6685536801815033, acc.: 59.52%] [G loss: 0.7885229587554932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 25/86 [D loss: 0.6645411550998688, acc.: 58.94%] [G loss: 0.7773340940475464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 26/86 [D loss: 0.66401207447052, acc.: 59.67%] [G loss: 0.7883057594299316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 27/86 [D loss: 0.6653892695903778, acc.: 59.81%] [G loss: 0.7711808085441589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 28/86 [D loss: 0.6685444116592407, acc.: 59.38%] [G loss: 0.7962099313735962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 29/86 [D loss: 0.6652604341506958, acc.: 58.59%] [G loss: 0.7813894748687744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 30/86 [D loss: 0.6644379496574402, acc.: 59.42%] [G loss: 0.7808628082275391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 31/86 [D loss: 0.6705702841281891, acc.: 58.79%] [G loss: 0.7823372483253479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 32/86 [D loss: 0.6657986640930176, acc.: 59.62%] [G loss: 0.7839046716690063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 33/86 [D loss: 0.6642555594444275, acc.: 59.72%] [G loss: 0.7857110500335693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 34/86 [D loss: 0.6628376245498657, acc.: 58.54%] [G loss: 0.7883484363555908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 35/86 [D loss: 0.663543701171875, acc.: 59.33%] [G loss: 0.7798101305961609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 36/86 [D loss: 0.6672413349151611, acc.: 59.57%] [G loss: 0.7846783995628357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 37/86 [D loss: 0.6621111929416656, acc.: 59.72%] [G loss: 0.78880375623703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 38/86 [D loss: 0.6604584455490112, acc.: 60.25%] [G loss: 0.7899062633514404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 39/86 [D loss: 0.6677059531211853, acc.: 59.23%] [G loss: 0.7811532020568848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 40/86 [D loss: 0.6688845455646515, acc.: 58.35%] [G loss: 0.7920270562171936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 41/86 [D loss: 0.6619347333908081, acc.: 60.16%] [G loss: 0.7812488675117493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 42/86 [D loss: 0.6648540794849396, acc.: 59.77%] [G loss: 0.7825832962989807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 43/86 [D loss: 0.6687571108341217, acc.: 58.59%] [G loss: 0.782913088798523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 44/86 [D loss: 0.6644897162914276, acc.: 60.69%] [G loss: 0.7794533967971802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 45/86 [D loss: 0.6704472005367279, acc.: 58.50%] [G loss: 0.7890832424163818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 46/86 [D loss: 0.6582453846931458, acc.: 61.67%] [G loss: 0.7758039236068726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 47/86 [D loss: 0.6677806377410889, acc.: 58.45%] [G loss: 0.7767526507377625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 48/86 [D loss: 0.6528249680995941, acc.: 62.40%] [G loss: 0.7862709760665894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 49/86 [D loss: 0.672008752822876, acc.: 57.76%] [G loss: 0.7863391637802124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 50/86 [D loss: 0.6683589518070221, acc.: 58.59%] [G loss: 0.769253134727478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 51/86 [D loss: 0.6762890219688416, acc.: 56.35%] [G loss: 0.7934249043464661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 52/86 [D loss: 0.6575523912906647, acc.: 61.33%] [G loss: 0.7840453386306763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 53/86 [D loss: 0.6660051941871643, acc.: 58.74%] [G loss: 0.7754018902778625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 54/86 [D loss: 0.6656401455402374, acc.: 60.11%] [G loss: 0.7890416383743286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 55/86 [D loss: 0.6674559712409973, acc.: 58.69%] [G loss: 0.7795064449310303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 56/86 [D loss: 0.6633814871311188, acc.: 59.52%] [G loss: 0.7858554124832153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 57/86 [D loss: 0.6638211607933044, acc.: 60.35%] [G loss: 0.7745890021324158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 58/86 [D loss: 0.6611724197864532, acc.: 61.38%] [G loss: 0.8020226359367371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 59/86 [D loss: 0.6550998985767365, acc.: 63.38%] [G loss: 0.7861072421073914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 60/86 [D loss: 0.6713812053203583, acc.: 58.30%] [G loss: 0.787687361240387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 61/86 [D loss: 0.6618735194206238, acc.: 59.86%] [G loss: 0.774902880191803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 62/86 [D loss: 0.6659015715122223, acc.: 59.18%] [G loss: 0.7928149700164795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 63/86 [D loss: 0.662945419549942, acc.: 59.08%] [G loss: 0.7795853614807129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 64/86 [D loss: 0.6715221107006073, acc.: 58.69%] [G loss: 0.7738109827041626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 65/86 [D loss: 0.6670706868171692, acc.: 57.96%] [G loss: 0.7850059270858765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 66/86 [D loss: 0.6638646721839905, acc.: 60.40%] [G loss: 0.7809879779815674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 67/86 [D loss: 0.6634513735771179, acc.: 61.33%] [G loss: 0.7890551090240479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 68/86 [D loss: 0.6679631471633911, acc.: 57.76%] [G loss: 0.7820747494697571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 69/86 [D loss: 0.6653803288936615, acc.: 58.89%] [G loss: 0.786118745803833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 70/86 [D loss: 0.6645458042621613, acc.: 59.91%] [G loss: 0.7775574326515198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 71/86 [D loss: 0.6713017225265503, acc.: 58.84%] [G loss: 0.7917720675468445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 72/86 [D loss: 0.666863739490509, acc.: 59.08%] [G loss: 0.7770304679870605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 73/86 [D loss: 0.6652980744838715, acc.: 60.06%] [G loss: 0.7919260859489441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 74/86 [D loss: 0.6636893153190613, acc.: 60.01%] [G loss: 0.7889226675033569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 75/86 [D loss: 0.6604738831520081, acc.: 62.01%] [G loss: 0.7928380370140076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 76/86 [D loss: 0.6619870662689209, acc.: 60.40%] [G loss: 0.7861058712005615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 77/86 [D loss: 0.670965701341629, acc.: 58.74%] [G loss: 0.7904790639877319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 78/86 [D loss: 0.6634389460086823, acc.: 60.55%] [G loss: 0.7847416400909424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 79/86 [D loss: 0.6678059697151184, acc.: 58.15%] [G loss: 0.7808173298835754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 80/86 [D loss: 0.6646037697792053, acc.: 58.98%] [G loss: 0.7772432565689087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 81/86 [D loss: 0.6664988100528717, acc.: 58.50%] [G loss: 0.781172513961792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 82/86 [D loss: 0.6607589721679688, acc.: 60.45%] [G loss: 0.7858110070228577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 83/86 [D loss: 0.6699042618274689, acc.: 57.91%] [G loss: 0.7910399436950684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 84/86 [D loss: 0.6620237827301025, acc.: 59.62%] [G loss: 0.7828996777534485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 85/86 [D loss: 0.6740414202213287, acc.: 57.71%] [G loss: 0.7796768546104431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 164/200, Batch 86/86 [D loss: 0.6605362296104431, acc.: 60.99%] [G loss: 0.7792896032333374]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 1/86 [D loss: 0.6643412709236145, acc.: 58.94%] [G loss: 0.7753956317901611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 2/86 [D loss: 0.6672323644161224, acc.: 58.35%] [G loss: 0.7837963700294495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 3/86 [D loss: 0.6605902910232544, acc.: 62.01%] [G loss: 0.7888741493225098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 4/86 [D loss: 0.6645128130912781, acc.: 59.57%] [G loss: 0.7916027307510376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 5/86 [D loss: 0.662681370973587, acc.: 59.91%] [G loss: 0.7910966873168945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 6/86 [D loss: 0.6613460183143616, acc.: 61.23%] [G loss: 0.7890375256538391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 7/86 [D loss: 0.6656897664070129, acc.: 59.52%] [G loss: 0.7830712795257568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 8/86 [D loss: 0.6624977588653564, acc.: 60.89%] [G loss: 0.7771613597869873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 9/86 [D loss: 0.6613040268421173, acc.: 60.55%] [G loss: 0.7853077054023743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 10/86 [D loss: 0.6715669333934784, acc.: 57.13%] [G loss: 0.7816904187202454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 11/86 [D loss: 0.6628708839416504, acc.: 59.03%] [G loss: 0.7747645378112793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 12/86 [D loss: 0.669125884771347, acc.: 57.91%] [G loss: 0.7807126045227051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 13/86 [D loss: 0.6633416712284088, acc.: 59.96%] [G loss: 0.7868838906288147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 14/86 [D loss: 0.6731803119182587, acc.: 57.81%] [G loss: 0.7865173816680908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 15/86 [D loss: 0.6653002500534058, acc.: 59.96%] [G loss: 0.7840561866760254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 16/86 [D loss: 0.6619285345077515, acc.: 59.77%] [G loss: 0.7819470763206482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 17/86 [D loss: 0.6604012250900269, acc.: 60.35%] [G loss: 0.7857747673988342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 18/86 [D loss: 0.6621832549571991, acc.: 60.25%] [G loss: 0.7807838916778564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 19/86 [D loss: 0.6689886450767517, acc.: 58.89%] [G loss: 0.7962983846664429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 20/86 [D loss: 0.6603428721427917, acc.: 61.72%] [G loss: 0.7751863598823547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 21/86 [D loss: 0.6656584441661835, acc.: 58.89%] [G loss: 0.7801753282546997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 22/86 [D loss: 0.6607441902160645, acc.: 59.81%] [G loss: 0.7722479104995728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 23/86 [D loss: 0.6733987927436829, acc.: 56.93%] [G loss: 0.7853296995162964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 24/86 [D loss: 0.659928172826767, acc.: 61.18%] [G loss: 0.7839507460594177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 25/86 [D loss: 0.6703819930553436, acc.: 58.98%] [G loss: 0.7845407724380493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 26/86 [D loss: 0.6609772741794586, acc.: 57.67%] [G loss: 0.7908965945243835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 27/86 [D loss: 0.6705341637134552, acc.: 58.01%] [G loss: 0.7781442403793335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 28/86 [D loss: 0.6635220944881439, acc.: 59.03%] [G loss: 0.7893521785736084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 29/86 [D loss: 0.669086217880249, acc.: 58.20%] [G loss: 0.7805696725845337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 30/86 [D loss: 0.6656399667263031, acc.: 60.06%] [G loss: 0.7780640721321106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 31/86 [D loss: 0.6729373037815094, acc.: 57.08%] [G loss: 0.7800453305244446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 32/86 [D loss: 0.663064032793045, acc.: 59.33%] [G loss: 0.7946785688400269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 33/86 [D loss: 0.6684040129184723, acc.: 59.28%] [G loss: 0.7801536321640015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 34/86 [D loss: 0.6668261885643005, acc.: 59.03%] [G loss: 0.7908684611320496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 35/86 [D loss: 0.6693739891052246, acc.: 58.59%] [G loss: 0.7828534245491028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 36/86 [D loss: 0.659284383058548, acc.: 61.82%] [G loss: 0.7917006015777588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 37/86 [D loss: 0.6634473204612732, acc.: 58.50%] [G loss: 0.776016116142273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 38/86 [D loss: 0.663823664188385, acc.: 58.84%] [G loss: 0.781525731086731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 39/86 [D loss: 0.662055104970932, acc.: 59.62%] [G loss: 0.7717947363853455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 40/86 [D loss: 0.6696305871009827, acc.: 57.08%] [G loss: 0.7982788681983948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 41/86 [D loss: 0.6631573438644409, acc.: 59.08%] [G loss: 0.7695032954216003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 42/86 [D loss: 0.6668407022953033, acc.: 58.79%] [G loss: 0.7897397875785828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 43/86 [D loss: 0.6602576971054077, acc.: 59.91%] [G loss: 0.7921169400215149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 44/86 [D loss: 0.6683407127857208, acc.: 58.84%] [G loss: 0.7911710143089294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 45/86 [D loss: 0.66126549243927, acc.: 61.72%] [G loss: 0.7802018523216248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 46/86 [D loss: 0.6688222885131836, acc.: 57.71%] [G loss: 0.7873587608337402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 47/86 [D loss: 0.6566753685474396, acc.: 62.11%] [G loss: 0.7857633829116821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 48/86 [D loss: 0.6604726612567902, acc.: 60.01%] [G loss: 0.7834916114807129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 49/86 [D loss: 0.6671141088008881, acc.: 58.89%] [G loss: 0.7892167568206787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 50/86 [D loss: 0.661512017250061, acc.: 59.23%] [G loss: 0.7782478332519531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 51/86 [D loss: 0.6672664880752563, acc.: 58.15%] [G loss: 0.7845193147659302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 52/86 [D loss: 0.6670607030391693, acc.: 59.42%] [G loss: 0.7805107235908508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 53/86 [D loss: 0.6633144617080688, acc.: 58.69%] [G loss: 0.786902666091919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 54/86 [D loss: 0.6688513457775116, acc.: 58.94%] [G loss: 0.7858684062957764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 55/86 [D loss: 0.661374568939209, acc.: 61.04%] [G loss: 0.7807314395904541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 56/86 [D loss: 0.6627797782421112, acc.: 59.77%] [G loss: 0.79018235206604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 57/86 [D loss: 0.6673887073993683, acc.: 58.59%] [G loss: 0.780915379524231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 58/86 [D loss: 0.6630069613456726, acc.: 58.59%] [G loss: 0.7880526185035706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 59/86 [D loss: 0.6684982478618622, acc.: 57.86%] [G loss: 0.7855381369590759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 60/86 [D loss: 0.6686490774154663, acc.: 56.98%] [G loss: 0.7868390679359436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 61/86 [D loss: 0.6716871559619904, acc.: 56.84%] [G loss: 0.7883081436157227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 62/86 [D loss: 0.6626046895980835, acc.: 60.21%] [G loss: 0.79193514585495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 63/86 [D loss: 0.6642471253871918, acc.: 60.30%] [G loss: 0.7755239605903625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 64/86 [D loss: 0.6648880243301392, acc.: 57.71%] [G loss: 0.7982268333435059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 65/86 [D loss: 0.6648586690425873, acc.: 60.40%] [G loss: 0.7877343893051147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 66/86 [D loss: 0.6662639081478119, acc.: 59.86%] [G loss: 0.7920262217521667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 67/86 [D loss: 0.6631442904472351, acc.: 58.79%] [G loss: 0.7777808308601379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 68/86 [D loss: 0.6644397675991058, acc.: 59.18%] [G loss: 0.7820225358009338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 69/86 [D loss: 0.6667696833610535, acc.: 59.18%] [G loss: 0.7788475155830383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 70/86 [D loss: 0.6620286107063293, acc.: 61.47%] [G loss: 0.7963703870773315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 71/86 [D loss: 0.6628990471363068, acc.: 61.28%] [G loss: 0.7918974757194519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 72/86 [D loss: 0.6615985333919525, acc.: 60.50%] [G loss: 0.7901262044906616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 73/86 [D loss: 0.6592124700546265, acc.: 60.35%] [G loss: 0.7827904224395752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 74/86 [D loss: 0.6649344563484192, acc.: 59.91%] [G loss: 0.7738023400306702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 75/86 [D loss: 0.6660133302211761, acc.: 59.18%] [G loss: 0.7804151773452759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 76/86 [D loss: 0.6621033549308777, acc.: 60.16%] [G loss: 0.7837926149368286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 77/86 [D loss: 0.6585714221000671, acc.: 60.40%] [G loss: 0.7943043112754822]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 78/86 [D loss: 0.6644309759140015, acc.: 60.30%] [G loss: 0.7822983860969543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 79/86 [D loss: 0.6650252938270569, acc.: 60.35%] [G loss: 0.7878227829933167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 80/86 [D loss: 0.6616285443305969, acc.: 60.01%] [G loss: 0.7761756777763367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 81/86 [D loss: 0.6676615178585052, acc.: 57.86%] [G loss: 0.7888259291648865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 82/86 [D loss: 0.6670762598514557, acc.: 57.13%] [G loss: 0.7775895595550537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 83/86 [D loss: 0.6669765114784241, acc.: 59.13%] [G loss: 0.790761411190033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 84/86 [D loss: 0.6632663607597351, acc.: 59.67%] [G loss: 0.7812549471855164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 85/86 [D loss: 0.6599966585636139, acc.: 60.69%] [G loss: 0.7956864833831787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 165/200, Batch 86/86 [D loss: 0.6618330180644989, acc.: 59.62%] [G loss: 0.789005696773529]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 1/86 [D loss: 0.6653975546360016, acc.: 60.35%] [G loss: 0.7955560684204102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 2/86 [D loss: 0.6596519649028778, acc.: 60.45%] [G loss: 0.7896444201469421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 3/86 [D loss: 0.6669255197048187, acc.: 58.54%] [G loss: 0.7818509340286255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 4/86 [D loss: 0.669173002243042, acc.: 57.23%] [G loss: 0.7820981740951538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 5/86 [D loss: 0.6606529355049133, acc.: 60.01%] [G loss: 0.7786914110183716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 6/86 [D loss: 0.6603570282459259, acc.: 60.55%] [G loss: 0.7768193483352661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 7/86 [D loss: 0.6573060154914856, acc.: 60.99%] [G loss: 0.7895139455795288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 8/86 [D loss: 0.6632359623908997, acc.: 59.86%] [G loss: 0.7835552096366882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 9/86 [D loss: 0.6585121750831604, acc.: 61.28%] [G loss: 0.7896348834037781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 10/86 [D loss: 0.6623533964157104, acc.: 60.40%] [G loss: 0.795627772808075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 11/86 [D loss: 0.6678033173084259, acc.: 59.08%] [G loss: 0.799572229385376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 12/86 [D loss: 0.666880875825882, acc.: 57.91%] [G loss: 0.7859790325164795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 13/86 [D loss: 0.6631344258785248, acc.: 59.81%] [G loss: 0.7926802635192871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 14/86 [D loss: 0.667404294013977, acc.: 59.28%] [G loss: 0.7885335087776184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 15/86 [D loss: 0.6663759350776672, acc.: 60.01%] [G loss: 0.7855563759803772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 16/86 [D loss: 0.6629899442195892, acc.: 59.42%] [G loss: 0.7737480998039246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 17/86 [D loss: 0.6650271415710449, acc.: 59.86%] [G loss: 0.789130449295044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 18/86 [D loss: 0.6625435054302216, acc.: 58.98%] [G loss: 0.7783312797546387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 19/86 [D loss: 0.670635849237442, acc.: 58.35%] [G loss: 0.7803801894187927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 20/86 [D loss: 0.6605077385902405, acc.: 60.11%] [G loss: 0.790163516998291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 21/86 [D loss: 0.6641415357589722, acc.: 59.28%] [G loss: 0.7755231261253357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 22/86 [D loss: 0.6602992713451385, acc.: 59.28%] [G loss: 0.7849133610725403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 23/86 [D loss: 0.6716185510158539, acc.: 58.84%] [G loss: 0.775782585144043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 24/86 [D loss: 0.6591461002826691, acc.: 59.57%] [G loss: 0.799023449420929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 25/86 [D loss: 0.6626040935516357, acc.: 60.11%] [G loss: 0.7872470021247864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 26/86 [D loss: 0.6572267115116119, acc.: 62.01%] [G loss: 0.7792326211929321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 27/86 [D loss: 0.6648281812667847, acc.: 59.23%] [G loss: 0.7782132625579834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 28/86 [D loss: 0.6628668904304504, acc.: 59.13%] [G loss: 0.7857688069343567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 29/86 [D loss: 0.6667317748069763, acc.: 58.15%] [G loss: 0.7869284749031067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 30/86 [D loss: 0.6648494601249695, acc.: 59.42%] [G loss: 0.7850683331489563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 31/86 [D loss: 0.6676631271839142, acc.: 58.20%] [G loss: 0.7862470149993896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 32/86 [D loss: 0.6669744253158569, acc.: 59.08%] [G loss: 0.784102201461792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 33/86 [D loss: 0.6623992025852203, acc.: 58.74%] [G loss: 0.7897030711174011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 34/86 [D loss: 0.6675322949886322, acc.: 58.69%] [G loss: 0.7885079979896545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 35/86 [D loss: 0.6671663820743561, acc.: 58.35%] [G loss: 0.7957701086997986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 36/86 [D loss: 0.665417343378067, acc.: 60.21%] [G loss: 0.7852210998535156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 37/86 [D loss: 0.6702322363853455, acc.: 58.35%] [G loss: 0.782282829284668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 38/86 [D loss: 0.6644687056541443, acc.: 59.28%] [G loss: 0.781248927116394]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 39/86 [D loss: 0.6638492941856384, acc.: 58.98%] [G loss: 0.7797930240631104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 40/86 [D loss: 0.6648480296134949, acc.: 59.62%] [G loss: 0.7842966318130493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 41/86 [D loss: 0.6653918027877808, acc.: 59.72%] [G loss: 0.784699022769928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 42/86 [D loss: 0.6659516096115112, acc.: 58.94%] [G loss: 0.7850841879844666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 43/86 [D loss: 0.6637791097164154, acc.: 59.33%] [G loss: 0.7830673456192017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 44/86 [D loss: 0.659982293844223, acc.: 61.38%] [G loss: 0.7914266586303711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 45/86 [D loss: 0.6616497933864594, acc.: 60.25%] [G loss: 0.7861534953117371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 46/86 [D loss: 0.662192314863205, acc.: 59.91%] [G loss: 0.7849815487861633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 47/86 [D loss: 0.663916140794754, acc.: 60.21%] [G loss: 0.7888781428337097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 48/86 [D loss: 0.6739237308502197, acc.: 57.57%] [G loss: 0.7862191200256348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 49/86 [D loss: 0.6611088216304779, acc.: 60.40%] [G loss: 0.7946364879608154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 50/86 [D loss: 0.6652167439460754, acc.: 61.23%] [G loss: 0.7877458333969116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 51/86 [D loss: 0.6683091521263123, acc.: 59.08%] [G loss: 0.7869219779968262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 52/86 [D loss: 0.6636662781238556, acc.: 59.52%] [G loss: 0.7769126892089844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 53/86 [D loss: 0.665868729352951, acc.: 59.28%] [G loss: 0.7865772843360901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 54/86 [D loss: 0.6651010513305664, acc.: 59.81%] [G loss: 0.7856022119522095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 55/86 [D loss: 0.6598368287086487, acc.: 60.60%] [G loss: 0.7795329093933105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 56/86 [D loss: 0.6665900349617004, acc.: 60.06%] [G loss: 0.7769284844398499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 57/86 [D loss: 0.6679503321647644, acc.: 58.50%] [G loss: 0.7843605279922485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 58/86 [D loss: 0.6564342677593231, acc.: 60.55%] [G loss: 0.7767293453216553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 59/86 [D loss: 0.66845703125, acc.: 57.32%] [G loss: 0.7852762341499329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 60/86 [D loss: 0.6670695543289185, acc.: 59.62%] [G loss: 0.7820953726768494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 61/86 [D loss: 0.6645621359348297, acc.: 59.91%] [G loss: 0.7928950190544128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 62/86 [D loss: 0.6620049178600311, acc.: 60.60%] [G loss: 0.7824228405952454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 63/86 [D loss: 0.6647137403488159, acc.: 58.15%] [G loss: 0.7866439819335938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 64/86 [D loss: 0.6615297198295593, acc.: 60.60%] [G loss: 0.793759286403656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 65/86 [D loss: 0.6633622050285339, acc.: 59.67%] [G loss: 0.7871816754341125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 66/86 [D loss: 0.6606435179710388, acc.: 59.91%] [G loss: 0.7760453224182129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 67/86 [D loss: 0.6647895276546478, acc.: 58.94%] [G loss: 0.7947602868080139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 68/86 [D loss: 0.6638011932373047, acc.: 60.01%] [G loss: 0.7867128849029541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 69/86 [D loss: 0.663956880569458, acc.: 59.38%] [G loss: 0.7911565899848938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 70/86 [D loss: 0.6646166443824768, acc.: 58.06%] [G loss: 0.7988911867141724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 71/86 [D loss: 0.6630483269691467, acc.: 60.79%] [G loss: 0.7906550168991089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 72/86 [D loss: 0.6611140370368958, acc.: 60.60%] [G loss: 0.79100102186203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 73/86 [D loss: 0.6584697961807251, acc.: 60.45%] [G loss: 0.793272078037262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 74/86 [D loss: 0.6688707768917084, acc.: 57.86%] [G loss: 0.7830383777618408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 75/86 [D loss: 0.6676207780838013, acc.: 58.89%] [G loss: 0.7847616672515869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 76/86 [D loss: 0.6639581024646759, acc.: 59.86%] [G loss: 0.7807275056838989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 77/86 [D loss: 0.6722893118858337, acc.: 58.54%] [G loss: 0.7811340093612671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 78/86 [D loss: 0.6601775586605072, acc.: 60.55%] [G loss: 0.7875054478645325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 79/86 [D loss: 0.6572017669677734, acc.: 60.89%] [G loss: 0.7884035706520081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 80/86 [D loss: 0.6673194169998169, acc.: 58.89%] [G loss: 0.7875885963439941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 81/86 [D loss: 0.6623255908489227, acc.: 60.84%] [G loss: 0.77854323387146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 82/86 [D loss: 0.6599842011928558, acc.: 60.69%] [G loss: 0.7932144999504089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 83/86 [D loss: 0.6660825312137604, acc.: 58.94%] [G loss: 0.7897812724113464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 84/86 [D loss: 0.6623080968856812, acc.: 59.18%] [G loss: 0.7891276478767395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 85/86 [D loss: 0.6611503064632416, acc.: 60.79%] [G loss: 0.7843438982963562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 166/200, Batch 86/86 [D loss: 0.6594251990318298, acc.: 62.06%] [G loss: 0.7884798049926758]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 1/86 [D loss: 0.6627008318901062, acc.: 59.62%] [G loss: 0.7896894812583923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 2/86 [D loss: 0.6580130457878113, acc.: 61.04%] [G loss: 0.7836574912071228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 3/86 [D loss: 0.6692286133766174, acc.: 58.20%] [G loss: 0.7857295870780945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 4/86 [D loss: 0.6699956059455872, acc.: 57.81%] [G loss: 0.780918300151825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 5/86 [D loss: 0.6587821841239929, acc.: 60.69%] [G loss: 0.791757345199585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 6/86 [D loss: 0.6673343181610107, acc.: 58.11%] [G loss: 0.7877410650253296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 7/86 [D loss: 0.6667141020298004, acc.: 59.72%] [G loss: 0.7891547083854675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 8/86 [D loss: 0.6609626710414886, acc.: 60.89%] [G loss: 0.7834271788597107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 9/86 [D loss: 0.6687963008880615, acc.: 58.35%] [G loss: 0.7760170698165894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 10/86 [D loss: 0.6583587527275085, acc.: 60.21%] [G loss: 0.7778458595275879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 11/86 [D loss: 0.6611228883266449, acc.: 59.38%] [G loss: 0.7946009635925293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 12/86 [D loss: 0.6594676077365875, acc.: 60.64%] [G loss: 0.7865828275680542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 13/86 [D loss: 0.6660170257091522, acc.: 59.23%] [G loss: 0.7860701680183411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 14/86 [D loss: 0.6616987884044647, acc.: 60.55%] [G loss: 0.7868727445602417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 15/86 [D loss: 0.6677123308181763, acc.: 59.18%] [G loss: 0.7857056260108948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 16/86 [D loss: 0.6605804264545441, acc.: 60.79%] [G loss: 0.7899176478385925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 17/86 [D loss: 0.6624003052711487, acc.: 60.30%] [G loss: 0.7840873599052429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 18/86 [D loss: 0.6588268280029297, acc.: 60.11%] [G loss: 0.7842206954956055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 19/86 [D loss: 0.6656527817249298, acc.: 57.96%] [G loss: 0.7915835380554199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 20/86 [D loss: 0.6609286665916443, acc.: 60.11%] [G loss: 0.7879887223243713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 21/86 [D loss: 0.6638672649860382, acc.: 60.99%] [G loss: 0.780476987361908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 22/86 [D loss: 0.6691744029521942, acc.: 58.20%] [G loss: 0.7850066423416138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 23/86 [D loss: 0.6592956781387329, acc.: 60.50%] [G loss: 0.7806221842765808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 24/86 [D loss: 0.657955527305603, acc.: 60.69%] [G loss: 0.7891818284988403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 25/86 [D loss: 0.6630199551582336, acc.: 60.30%] [G loss: 0.786256730556488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 26/86 [D loss: 0.664712131023407, acc.: 57.52%] [G loss: 0.7929793000221252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 27/86 [D loss: 0.6688984334468842, acc.: 59.08%] [G loss: 0.7867839336395264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 28/86 [D loss: 0.664106160402298, acc.: 60.01%] [G loss: 0.7866725921630859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 29/86 [D loss: 0.6616882383823395, acc.: 60.84%] [G loss: 0.7940952777862549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 30/86 [D loss: 0.6664930284023285, acc.: 59.23%] [G loss: 0.7836257219314575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 31/86 [D loss: 0.6705536544322968, acc.: 57.91%] [G loss: 0.785933256149292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 32/86 [D loss: 0.6687135696411133, acc.: 57.96%] [G loss: 0.7985996603965759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 33/86 [D loss: 0.6488863825798035, acc.: 63.33%] [G loss: 0.7940073609352112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 34/86 [D loss: 0.671270102262497, acc.: 58.79%] [G loss: 0.7741290926933289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 35/86 [D loss: 0.6644440591335297, acc.: 59.13%] [G loss: 0.7891981601715088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 36/86 [D loss: 0.6635594964027405, acc.: 60.21%] [G loss: 0.7760447859764099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 37/86 [D loss: 0.6653659045696259, acc.: 59.67%] [G loss: 0.7906280755996704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 38/86 [D loss: 0.6697535216808319, acc.: 57.42%] [G loss: 0.7920886278152466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 39/86 [D loss: 0.6705543994903564, acc.: 56.98%] [G loss: 0.7991000413894653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 40/86 [D loss: 0.668455570936203, acc.: 58.20%] [G loss: 0.7936444282531738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 41/86 [D loss: 0.664873868227005, acc.: 59.23%] [G loss: 0.8041657209396362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 42/86 [D loss: 0.6649556756019592, acc.: 59.28%] [G loss: 0.7803553938865662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 43/86 [D loss: 0.6625726521015167, acc.: 59.67%] [G loss: 0.7824685573577881]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 44/86 [D loss: 0.6665034294128418, acc.: 59.72%] [G loss: 0.7776402235031128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 45/86 [D loss: 0.6672138869762421, acc.: 58.98%] [G loss: 0.7909432649612427]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 46/86 [D loss: 0.6586378812789917, acc.: 60.84%] [G loss: 0.7878322601318359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 47/86 [D loss: 0.668067455291748, acc.: 58.40%] [G loss: 0.7807712554931641]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 48/86 [D loss: 0.6695845425128937, acc.: 57.81%] [G loss: 0.7835977077484131]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 49/86 [D loss: 0.6578916609287262, acc.: 62.11%] [G loss: 0.7823917865753174]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 50/86 [D loss: 0.6577869653701782, acc.: 61.57%] [G loss: 0.7886401414871216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 167/200, Batch 51/86 [D loss: 0.65495365858078, acc.: 62.40%] [G loss: 0.7808512449264526]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 52/86 [D loss: 0.6657807230949402, acc.: 58.40%] [G loss: 0.7795394659042358]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 53/86 [D loss: 0.6641024947166443, acc.: 60.21%] [G loss: 0.7870802879333496]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 54/86 [D loss: 0.6647980511188507, acc.: 60.79%] [G loss: 0.7842522263526917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 55/86 [D loss: 0.6605243682861328, acc.: 60.06%] [G loss: 0.7890172600746155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 56/86 [D loss: 0.6604389548301697, acc.: 60.55%] [G loss: 0.78826904296875]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 57/86 [D loss: 0.6625239849090576, acc.: 58.35%] [G loss: 0.7924232482910156]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 58/86 [D loss: 0.6597328186035156, acc.: 61.04%] [G loss: 0.7848589420318604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 59/86 [D loss: 0.6685374081134796, acc.: 58.54%] [G loss: 0.7814021110534668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 60/86 [D loss: 0.6647512018680573, acc.: 60.06%] [G loss: 0.7923478484153748]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 61/86 [D loss: 0.6631617248058319, acc.: 59.47%] [G loss: 0.7865557670593262]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 62/86 [D loss: 0.6652330160140991, acc.: 59.33%] [G loss: 0.7881435751914978]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 63/86 [D loss: 0.6667732000350952, acc.: 58.54%] [G loss: 0.7882480621337891]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 64/86 [D loss: 0.6635619401931763, acc.: 61.28%] [G loss: 0.7789085507392883]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 167/200, Batch 65/86 [D loss: 0.6636224985122681, acc.: 59.33%] [G loss: 0.7884520292282104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 66/86 [D loss: 0.6641651690006256, acc.: 59.23%] [G loss: 0.7915847301483154]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 67/86 [D loss: 0.6586723923683167, acc.: 60.06%] [G loss: 0.7848474979400635]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 68/86 [D loss: 0.6650539636611938, acc.: 59.33%] [G loss: 0.7871336936950684]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 167/200, Batch 69/86 [D loss: 0.6637071967124939, acc.: 59.23%] [G loss: 0.7914703488349915]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 70/86 [D loss: 0.6599394381046295, acc.: 60.79%] [G loss: 0.7954166531562805]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 167/200, Batch 71/86 [D loss: 0.6624275743961334, acc.: 58.98%] [G loss: 0.790950357913971]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 72/86 [D loss: 0.6575630605220795, acc.: 61.08%] [G loss: 0.7929820418357849]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 73/86 [D loss: 0.6636451482772827, acc.: 61.13%] [G loss: 0.7830030918121338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 74/86 [D loss: 0.6640285849571228, acc.: 59.62%] [G loss: 0.7881449460983276]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 75/86 [D loss: 0.659334659576416, acc.: 61.72%] [G loss: 0.7838714122772217]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 76/86 [D loss: 0.6666240096092224, acc.: 59.52%] [G loss: 0.7935966849327087]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 167/200, Batch 77/86 [D loss: 0.6615994870662689, acc.: 59.77%] [G loss: 0.7870957255363464]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 78/86 [D loss: 0.6716434061527252, acc.: 57.62%] [G loss: 0.789289116859436]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 167/200, Batch 79/86 [D loss: 0.6650553643703461, acc.: 59.13%] [G loss: 0.7862769365310669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 80/86 [D loss: 0.6677208244800568, acc.: 58.89%] [G loss: 0.7825750112533569]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 81/86 [D loss: 0.6606713235378265, acc.: 59.42%] [G loss: 0.7832624912261963]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 167/200, Batch 82/86 [D loss: 0.671255499124527, acc.: 57.47%] [G loss: 0.7890833616256714]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 83/86 [D loss: 0.6667459011077881, acc.: 58.94%] [G loss: 0.7982062101364136]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 167/200, Batch 84/86 [D loss: 0.6609417796134949, acc.: 59.13%] [G loss: 0.7857869863510132]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 85/86 [D loss: 0.6636019647121429, acc.: 60.45%] [G loss: 0.7849616408348083]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 167/200, Batch 86/86 [D loss: 0.6675179302692413, acc.: 57.52%] [G loss: 0.7808704972267151]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 168/200, Batch 1/86 [D loss: 0.6625427305698395, acc.: 60.50%] [G loss: 0.7719960808753967]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 2/86 [D loss: 0.6689664423465729, acc.: 56.05%] [G loss: 0.7833507061004639]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 3/86 [D loss: 0.6571430265903473, acc.: 60.89%] [G loss: 0.7865297198295593]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 4/86 [D loss: 0.6866074204444885, acc.: 53.56%] [G loss: 0.8020877838134766]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 168/200, Batch 5/86 [D loss: 0.6561305820941925, acc.: 61.82%] [G loss: 0.7681823372840881]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 6/86 [D loss: 0.6779178380966187, acc.: 56.20%] [G loss: 0.7796655893325806]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 7/86 [D loss: 0.6678994596004486, acc.: 58.35%] [G loss: 0.7842991352081299]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 8/86 [D loss: 0.6794561147689819, acc.: 56.20%] [G loss: 0.7950639724731445]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 9/86 [D loss: 0.6577423810958862, acc.: 60.84%] [G loss: 0.7821767330169678]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 10/86 [D loss: 0.6805922091007233, acc.: 54.83%] [G loss: 0.7897630929946899]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 168/200, Batch 11/86 [D loss: 0.657186359167099, acc.: 60.45%] [G loss: 0.7897511124610901]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 12/86 [D loss: 0.6713408827781677, acc.: 57.62%] [G loss: 0.787996768951416]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 13/86 [D loss: 0.6566509008407593, acc.: 60.79%] [G loss: 0.804732084274292]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 14/86 [D loss: 0.6735413372516632, acc.: 56.93%] [G loss: 0.7893297672271729]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 15/86 [D loss: 0.6621260643005371, acc.: 59.96%] [G loss: 0.7890497446060181]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 16/86 [D loss: 0.6730181574821472, acc.: 57.37%] [G loss: 0.7978289723396301]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 17/86 [D loss: 0.6619397401809692, acc.: 61.04%] [G loss: 0.803798258304596]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 18/86 [D loss: 0.667594850063324, acc.: 59.13%] [G loss: 0.7797290086746216]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 19/86 [D loss: 0.6595593094825745, acc.: 61.52%] [G loss: 0.7999249696731567]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 20/86 [D loss: 0.66665318608284, acc.: 58.45%] [G loss: 0.7932427525520325]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 21/86 [D loss: 0.6642619967460632, acc.: 60.11%] [G loss: 0.7862187027931213]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 22/86 [D loss: 0.66745325922966, acc.: 59.86%] [G loss: 0.7783510088920593]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 23/86 [D loss: 0.6694362461566925, acc.: 58.20%] [G loss: 0.7998971939086914]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 24/86 [D loss: 0.6679394841194153, acc.: 59.13%] [G loss: 0.7834792733192444]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 25/86 [D loss: 0.6737341582775116, acc.: 56.84%] [G loss: 0.8045487403869629]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 26/86 [D loss: 0.6594868898391724, acc.: 60.64%] [G loss: 0.7908685207366943]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 27/86 [D loss: 0.6569581031799316, acc.: 60.40%] [G loss: 0.7945023775100708]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 28/86 [D loss: 0.6610580384731293, acc.: 61.13%] [G loss: 0.783462643623352]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 29/86 [D loss: 0.6710071265697479, acc.: 57.52%] [G loss: 0.7953107953071594]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 30/86 [D loss: 0.6626269519329071, acc.: 60.25%] [G loss: 0.7792221903800964]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 31/86 [D loss: 0.6646288931369781, acc.: 59.81%] [G loss: 0.7919390201568604]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 32/86 [D loss: 0.6607073843479156, acc.: 60.25%] [G loss: 0.7793280482292175]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 33/86 [D loss: 0.6721413135528564, acc.: 57.91%] [G loss: 0.7852960824966431]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 34/86 [D loss: 0.6613355278968811, acc.: 60.94%] [G loss: 0.7812170386314392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 35/86 [D loss: 0.6685071587562561, acc.: 57.81%] [G loss: 0.7740810513496399]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 36/86 [D loss: 0.6633524596691132, acc.: 58.54%] [G loss: 0.7879334092140198]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 37/86 [D loss: 0.6649512052536011, acc.: 58.59%] [G loss: 0.7856686115264893]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 38/86 [D loss: 0.6700483560562134, acc.: 58.01%] [G loss: 0.7939024567604065]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 39/86 [D loss: 0.6586980819702148, acc.: 61.28%] [G loss: 0.7809346914291382]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 40/86 [D loss: 0.6558071672916412, acc.: 61.33%] [G loss: 0.7814890146255493]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 41/86 [D loss: 0.6559830605983734, acc.: 60.50%] [G loss: 0.7959024906158447]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 42/86 [D loss: 0.6672565639019012, acc.: 58.59%] [G loss: 0.7938811779022217]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 43/86 [D loss: 0.6599907279014587, acc.: 61.72%] [G loss: 0.7979612946510315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 44/86 [D loss: 0.6639541983604431, acc.: 58.98%] [G loss: 0.7882059812545776]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 45/86 [D loss: 0.661969780921936, acc.: 59.77%] [G loss: 0.7811849117279053]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 46/86 [D loss: 0.6647436320781708, acc.: 59.52%] [G loss: 0.7735164165496826]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 47/86 [D loss: 0.6522603631019592, acc.: 62.11%] [G loss: 0.7902872562408447]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 48/86 [D loss: 0.6627136170864105, acc.: 60.89%] [G loss: 0.7932280898094177]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 49/86 [D loss: 0.665457546710968, acc.: 59.13%] [G loss: 0.7921215891838074]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 50/86 [D loss: 0.6657429337501526, acc.: 58.84%] [G loss: 0.7924187779426575]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 51/86 [D loss: 0.6665254831314087, acc.: 59.77%] [G loss: 0.7895523309707642]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 52/86 [D loss: 0.6649063527584076, acc.: 58.59%] [G loss: 0.7966106534004211]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 53/86 [D loss: 0.665383517742157, acc.: 57.71%] [G loss: 0.7856354713439941]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 54/86 [D loss: 0.6611181795597076, acc.: 60.45%] [G loss: 0.7884189486503601]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 55/86 [D loss: 0.6655665040016174, acc.: 59.57%] [G loss: 0.796444296836853]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 56/86 [D loss: 0.6626301407814026, acc.: 60.55%] [G loss: 0.7851685881614685]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 57/86 [D loss: 0.6665643155574799, acc.: 58.50%] [G loss: 0.7892452478408813]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 168/200, Batch 58/86 [D loss: 0.6646918058395386, acc.: 58.94%] [G loss: 0.787222683429718]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 59/86 [D loss: 0.657665342092514, acc.: 60.21%] [G loss: 0.7860013842582703]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 168/200, Batch 60/86 [D loss: 0.6631104946136475, acc.: 60.79%] [G loss: 0.7967116236686707]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 61/86 [D loss: 0.6681391894817352, acc.: 58.25%] [G loss: 0.7854443788528442]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 62/86 [D loss: 0.6640612781047821, acc.: 59.86%] [G loss: 0.7868116497993469]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 63/86 [D loss: 0.66136234998703, acc.: 60.50%] [G loss: 0.7879106998443604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 64/86 [D loss: 0.6626943945884705, acc.: 59.77%] [G loss: 0.7861382961273193]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 65/86 [D loss: 0.6636226773262024, acc.: 59.91%] [G loss: 0.7883800268173218]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 66/86 [D loss: 0.6589529514312744, acc.: 59.72%] [G loss: 0.7933857440948486]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 67/86 [D loss: 0.6623237729072571, acc.: 60.01%] [G loss: 0.7833081483840942]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 68/86 [D loss: 0.6580587327480316, acc.: 61.77%] [G loss: 0.7923663854598999]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 168/200, Batch 69/86 [D loss: 0.6657494902610779, acc.: 58.59%] [G loss: 0.789587140083313]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 70/86 [D loss: 0.6647778451442719, acc.: 60.21%] [G loss: 0.7891350984573364]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 71/86 [D loss: 0.6592195332050323, acc.: 61.57%] [G loss: 0.7828680872917175]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 168/200, Batch 72/86 [D loss: 0.6651382446289062, acc.: 59.96%] [G loss: 0.7850350141525269]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 73/86 [D loss: 0.6571885049343109, acc.: 60.99%] [G loss: 0.7791416645050049]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 74/86 [D loss: 0.6640065610408783, acc.: 60.40%] [G loss: 0.7949531674385071]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 75/86 [D loss: 0.6643002927303314, acc.: 58.69%] [G loss: 0.7879465818405151]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 76/86 [D loss: 0.6638798415660858, acc.: 59.47%] [G loss: 0.7957189083099365]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 77/86 [D loss: 0.663160890340805, acc.: 59.13%] [G loss: 0.7828170657157898]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 78/86 [D loss: 0.6713820993900299, acc.: 57.42%] [G loss: 0.7952644228935242]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 79/86 [D loss: 0.6630899906158447, acc.: 60.06%] [G loss: 0.7725756168365479]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 80/86 [D loss: 0.6590679287910461, acc.: 61.04%] [G loss: 0.7785279750823975]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 81/86 [D loss: 0.6575180888175964, acc.: 61.23%] [G loss: 0.7832323908805847]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 82/86 [D loss: 0.6636229455471039, acc.: 59.28%] [G loss: 0.7991693615913391]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 83/86 [D loss: 0.6565831899642944, acc.: 60.55%] [G loss: 0.7928805351257324]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 168/200, Batch 84/86 [D loss: 0.6669030785560608, acc.: 58.50%] [G loss: 0.7916369438171387]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 85/86 [D loss: 0.6623268127441406, acc.: 59.96%] [G loss: 0.7944572567939758]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 168/200, Batch 86/86 [D loss: 0.6693839728832245, acc.: 57.71%] [G loss: 0.7951694130897522]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 1/86 [D loss: 0.6605876982212067, acc.: 60.89%] [G loss: 0.7869745492935181]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 2/86 [D loss: 0.672057032585144, acc.: 56.84%] [G loss: 0.794829785823822]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 3/86 [D loss: 0.6641969084739685, acc.: 58.54%] [G loss: 0.7838121652603149]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 169/200, Batch 4/86 [D loss: 0.6637046337127686, acc.: 58.94%] [G loss: 0.8013847470283508]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 5/86 [D loss: 0.6624591946601868, acc.: 59.47%] [G loss: 0.7812719345092773]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 6/86 [D loss: 0.6684249639511108, acc.: 57.52%] [G loss: 0.7930924296379089]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 7/86 [D loss: 0.6658787131309509, acc.: 58.06%] [G loss: 0.7919538617134094]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 8/86 [D loss: 0.664692610502243, acc.: 59.57%] [G loss: 0.7963438630104065]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 9/86 [D loss: 0.6660184562206268, acc.: 59.23%] [G loss: 0.7858717441558838]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 169/200, Batch 10/86 [D loss: 0.6631104350090027, acc.: 59.52%] [G loss: 0.7888997793197632]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 169/200, Batch 11/86 [D loss: 0.656243622303009, acc.: 62.01%] [G loss: 0.7822942137718201]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 12/86 [D loss: 0.6704688966274261, acc.: 57.71%] [G loss: 0.7909610867500305]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 13/86 [D loss: 0.6598553359508514, acc.: 58.40%] [G loss: 0.7812341451644897]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 14/86 [D loss: 0.6642511188983917, acc.: 59.57%] [G loss: 0.7870743274688721]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 169/200, Batch 15/86 [D loss: 0.6627254784107208, acc.: 59.77%] [G loss: 0.7820199728012085]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 16/86 [D loss: 0.6657856404781342, acc.: 58.84%] [G loss: 0.7966040968894958]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 17/86 [D loss: 0.6560459733009338, acc.: 61.38%] [G loss: 0.7839993238449097]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 18/86 [D loss: 0.6632270812988281, acc.: 59.67%] [G loss: 0.7891600131988525]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 19/86 [D loss: 0.6583142876625061, acc.: 61.52%] [G loss: 0.776461124420166]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 20/86 [D loss: 0.6674884557723999, acc.: 59.67%] [G loss: 0.8065379858016968]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 21/86 [D loss: 0.6567278802394867, acc.: 60.40%] [G loss: 0.7938175201416016]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 22/86 [D loss: 0.6624054610729218, acc.: 59.13%] [G loss: 0.7849847078323364]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 169/200, Batch 23/86 [D loss: 0.664831668138504, acc.: 59.47%] [G loss: 0.7989646196365356]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 24/86 [D loss: 0.6635992527008057, acc.: 58.50%] [G loss: 0.7738906145095825]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 25/86 [D loss: 0.6691363155841827, acc.: 57.96%] [G loss: 0.8019573092460632]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 26/86 [D loss: 0.6641348600387573, acc.: 60.16%] [G loss: 0.7822697162628174]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 27/86 [D loss: 0.6736897826194763, acc.: 56.84%] [G loss: 0.7917364835739136]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 28/86 [D loss: 0.6648236811161041, acc.: 59.72%] [G loss: 0.7823907136917114]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 29/86 [D loss: 0.6651855409145355, acc.: 58.30%] [G loss: 0.8131058812141418]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 30/86 [D loss: 0.6638232171535492, acc.: 60.16%] [G loss: 0.7861478328704834]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 31/86 [D loss: 0.6663259565830231, acc.: 58.30%] [G loss: 0.7937255501747131]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 32/86 [D loss: 0.666376531124115, acc.: 58.84%] [G loss: 0.7835268974304199]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 33/86 [D loss: 0.6675137281417847, acc.: 57.96%] [G loss: 0.794103741645813]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 34/86 [D loss: 0.6610137522220612, acc.: 58.94%] [G loss: 0.7933464050292969]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 35/86 [D loss: 0.6626235842704773, acc.: 60.35%] [G loss: 0.7958961725234985]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 36/86 [D loss: 0.6628066599369049, acc.: 60.74%] [G loss: 0.7842721939086914]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 37/86 [D loss: 0.662886768579483, acc.: 59.72%] [G loss: 0.7986327409744263]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 38/86 [D loss: 0.6587444245815277, acc.: 61.91%] [G loss: 0.7884601354598999]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 39/86 [D loss: 0.6630909442901611, acc.: 59.57%] [G loss: 0.7938845753669739]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 40/86 [D loss: 0.6638011932373047, acc.: 58.64%] [G loss: 0.7840225696563721]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 41/86 [D loss: 0.6697463393211365, acc.: 57.67%] [G loss: 0.796563982963562]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 42/86 [D loss: 0.6620914340019226, acc.: 60.45%] [G loss: 0.7883443832397461]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 43/86 [D loss: 0.6645877659320831, acc.: 59.77%] [G loss: 0.7859768271446228]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 44/86 [D loss: 0.6685289740562439, acc.: 58.01%] [G loss: 0.7877312302589417]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 45/86 [D loss: 0.6674215495586395, acc.: 59.18%] [G loss: 0.7967923283576965]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 46/86 [D loss: 0.6628718078136444, acc.: 59.03%] [G loss: 0.7809657454490662]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 47/86 [D loss: 0.6688488125801086, acc.: 57.47%] [G loss: 0.7888389825820923]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 48/86 [D loss: 0.6660910248756409, acc.: 58.40%] [G loss: 0.7874428629875183]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 49/86 [D loss: 0.6668456494808197, acc.: 57.57%] [G loss: 0.7943105697631836]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 50/86 [D loss: 0.6620776355266571, acc.: 61.72%] [G loss: 0.7820725440979004]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 51/86 [D loss: 0.6698330044746399, acc.: 57.86%] [G loss: 0.7789273262023926]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 52/86 [D loss: 0.6645078063011169, acc.: 58.25%] [G loss: 0.7925204634666443]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 53/86 [D loss: 0.6629209220409393, acc.: 60.89%] [G loss: 0.7870497107505798]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 54/86 [D loss: 0.6690367460250854, acc.: 59.23%] [G loss: 0.8148515224456787]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 55/86 [D loss: 0.6600746214389801, acc.: 60.40%] [G loss: 0.7758721113204956]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 56/86 [D loss: 0.6608183085918427, acc.: 60.11%] [G loss: 0.7866902947425842]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 57/86 [D loss: 0.6636093854904175, acc.: 58.74%] [G loss: 0.7932019233703613]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 58/86 [D loss: 0.6620416045188904, acc.: 60.16%] [G loss: 0.7809708118438721]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 59/86 [D loss: 0.6630945205688477, acc.: 59.18%] [G loss: 0.7870876789093018]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 60/86 [D loss: 0.6663477122783661, acc.: 59.62%] [G loss: 0.7910709381103516]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 61/86 [D loss: 0.664837509393692, acc.: 58.40%] [G loss: 0.7797624468803406]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 62/86 [D loss: 0.6652442216873169, acc.: 58.98%] [G loss: 0.7860605716705322]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 63/86 [D loss: 0.660080075263977, acc.: 60.64%] [G loss: 0.7838611602783203]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 64/86 [D loss: 0.6679397225379944, acc.: 57.71%] [G loss: 0.7930135130882263]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 65/86 [D loss: 0.6613304615020752, acc.: 58.89%] [G loss: 0.7926703095436096]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 66/86 [D loss: 0.6627853810787201, acc.: 60.40%] [G loss: 0.7856994271278381]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 67/86 [D loss: 0.6623396277427673, acc.: 60.45%] [G loss: 0.7959917783737183]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 68/86 [D loss: 0.6661839783191681, acc.: 58.11%] [G loss: 0.7974724173545837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 169/200, Batch 69/86 [D loss: 0.6669692397117615, acc.: 58.50%] [G loss: 0.7951017022132874]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 70/86 [D loss: 0.6670730113983154, acc.: 57.76%] [G loss: 0.7901189923286438]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 71/86 [D loss: 0.6626115143299103, acc.: 59.42%] [G loss: 0.7812455296516418]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 72/86 [D loss: 0.6621800661087036, acc.: 60.64%] [G loss: 0.8015771508216858]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 73/86 [D loss: 0.6652987599372864, acc.: 60.25%] [G loss: 0.7883104085922241]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 74/86 [D loss: 0.6652355790138245, acc.: 59.62%] [G loss: 0.7789848446846008]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 75/86 [D loss: 0.6623703837394714, acc.: 59.96%] [G loss: 0.7908223271369934]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 76/86 [D loss: 0.6614249646663666, acc.: 60.99%] [G loss: 0.7884398102760315]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 77/86 [D loss: 0.6607378721237183, acc.: 59.72%] [G loss: 0.7869980931282043]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 78/86 [D loss: 0.6624095439910889, acc.: 60.11%] [G loss: 0.7845702171325684]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 79/86 [D loss: 0.6651447117328644, acc.: 60.50%] [G loss: 0.7884469628334045]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 80/86 [D loss: 0.6630805134773254, acc.: 60.30%] [G loss: 0.7924971580505371]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 81/86 [D loss: 0.6686091125011444, acc.: 58.20%] [G loss: 0.7889522314071655]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 82/86 [D loss: 0.6633000373840332, acc.: 60.06%] [G loss: 0.7992804646492004]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 83/86 [D loss: 0.669636994600296, acc.: 59.08%] [G loss: 0.7916016578674316]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 169/200, Batch 84/86 [D loss: 0.6676775515079498, acc.: 59.03%] [G loss: 0.790893018245697]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 85/86 [D loss: 0.6660365462303162, acc.: 59.03%] [G loss: 0.789554238319397]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 169/200, Batch 86/86 [D loss: 0.6649097204208374, acc.: 60.01%] [G loss: 0.7929217219352722]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 170/200, Batch 1/86 [D loss: 0.6620219349861145, acc.: 59.96%] [G loss: 0.7820650339126587]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 2/86 [D loss: 0.6630469262599945, acc.: 60.25%] [G loss: 0.7869486808776855]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 3/86 [D loss: 0.6609213054180145, acc.: 60.79%] [G loss: 0.7829614281654358]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 4/86 [D loss: 0.6654499769210815, acc.: 59.42%] [G loss: 0.7988739013671875]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 5/86 [D loss: 0.661804735660553, acc.: 60.64%] [G loss: 0.7840859293937683]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 6/86 [D loss: 0.6641198992729187, acc.: 59.52%] [G loss: 0.7933716773986816]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 7/86 [D loss: 0.6611218452453613, acc.: 60.21%] [G loss: 0.797470211982727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 170/200, Batch 8/86 [D loss: 0.6702574193477631, acc.: 58.11%] [G loss: 0.794120728969574]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 9/86 [D loss: 0.6692657768726349, acc.: 57.18%] [G loss: 0.7972314357757568]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 10/86 [D loss: 0.6613283455371857, acc.: 58.54%] [G loss: 0.7826778888702393]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 11/86 [D loss: 0.6630498170852661, acc.: 59.91%] [G loss: 0.796327531337738]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 170/200, Batch 12/86 [D loss: 0.6647962629795074, acc.: 59.81%] [G loss: 0.7911416888237]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 13/86 [D loss: 0.6648198068141937, acc.: 58.40%] [G loss: 0.7946449518203735]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 14/86 [D loss: 0.6702256798744202, acc.: 56.88%] [G loss: 0.78450608253479]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 15/86 [D loss: 0.667329877614975, acc.: 57.71%] [G loss: 0.7893795371055603]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 170/200, Batch 16/86 [D loss: 0.6640039086341858, acc.: 59.81%] [G loss: 0.7938889265060425]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 17/86 [D loss: 0.6609161496162415, acc.: 60.60%] [G loss: 0.7822449207305908]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 18/86 [D loss: 0.6637684404850006, acc.: 59.86%] [G loss: 0.8012673854827881]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 19/86 [D loss: 0.6643819212913513, acc.: 58.79%] [G loss: 0.79372239112854]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 20/86 [D loss: 0.6642804145812988, acc.: 59.42%] [G loss: 0.804335355758667]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 21/86 [D loss: 0.6581481993198395, acc.: 59.38%] [G loss: 0.7862855792045593]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 170/200, Batch 22/86 [D loss: 0.6616963446140289, acc.: 61.33%] [G loss: 0.7814707159996033]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 23/86 [D loss: 0.6626394689083099, acc.: 60.89%] [G loss: 0.7881746888160706]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 24/86 [D loss: 0.662017434835434, acc.: 59.52%] [G loss: 0.8003809452056885]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 25/86 [D loss: 0.6593993008136749, acc.: 60.60%] [G loss: 0.7823717594146729]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 26/86 [D loss: 0.6648019254207611, acc.: 60.06%] [G loss: 0.777985155582428]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 27/86 [D loss: 0.6674689650535583, acc.: 58.84%] [G loss: 0.7931097149848938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 170/200, Batch 28/86 [D loss: 0.6627256870269775, acc.: 60.74%] [G loss: 0.7885156273841858]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 29/86 [D loss: 0.6621411144733429, acc.: 59.28%] [G loss: 0.7847859859466553]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 30/86 [D loss: 0.6626676619052887, acc.: 58.69%] [G loss: 0.7989107370376587]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 170/200, Batch 31/86 [D loss: 0.665742963552475, acc.: 58.25%] [G loss: 0.7858777642250061]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 32/86 [D loss: 0.658479243516922, acc.: 61.08%] [G loss: 0.802325963973999]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 33/86 [D loss: 0.6588228642940521, acc.: 60.60%] [G loss: 0.7939130067825317]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 34/86 [D loss: 0.6620288789272308, acc.: 60.30%] [G loss: 0.7969931364059448]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 35/86 [D loss: 0.6551257967948914, acc.: 62.26%] [G loss: 0.7927508354187012]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 36/86 [D loss: 0.663258969783783, acc.: 60.79%] [G loss: 0.7884700298309326]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 37/86 [D loss: 0.6626707315444946, acc.: 60.21%] [G loss: 0.7911158204078674]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 38/86 [D loss: 0.6647835075855255, acc.: 59.57%] [G loss: 0.7838327884674072]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 39/86 [D loss: 0.6567236483097076, acc.: 61.52%] [G loss: 0.790219783782959]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 40/86 [D loss: 0.6588698327541351, acc.: 61.43%] [G loss: 0.7887664437294006]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 41/86 [D loss: 0.6638056933879852, acc.: 60.01%] [G loss: 0.782574474811554]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 42/86 [D loss: 0.6673748195171356, acc.: 59.13%] [G loss: 0.7875399589538574]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 43/86 [D loss: 0.6645428836345673, acc.: 59.18%] [G loss: 0.7935376763343811]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 170/200, Batch 44/86 [D loss: 0.6596476137638092, acc.: 60.69%] [G loss: 0.786585807800293]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 45/86 [D loss: 0.663166880607605, acc.: 59.77%] [G loss: 0.7879396080970764]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 46/86 [D loss: 0.6627431809902191, acc.: 59.67%] [G loss: 0.8000110983848572]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 47/86 [D loss: 0.6597588360309601, acc.: 61.47%] [G loss: 0.7792388200759888]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 48/86 [D loss: 0.6620187163352966, acc.: 60.21%] [G loss: 0.783259391784668]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 49/86 [D loss: 0.6664755940437317, acc.: 59.77%] [G loss: 0.7802808284759521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 170/200, Batch 50/86 [D loss: 0.6626301109790802, acc.: 59.42%] [G loss: 0.7900644540786743]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 51/86 [D loss: 0.6674965620040894, acc.: 58.59%] [G loss: 0.7913660407066345]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 52/86 [D loss: 0.6611512303352356, acc.: 60.35%] [G loss: 0.7912647724151611]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 53/86 [D loss: 0.6621635556221008, acc.: 59.62%] [G loss: 0.7860879302024841]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 54/86 [D loss: 0.6625406742095947, acc.: 59.96%] [G loss: 0.7923370599746704]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 170/200, Batch 55/86 [D loss: 0.6685020923614502, acc.: 59.72%] [G loss: 0.790129542350769]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 56/86 [D loss: 0.6631679534912109, acc.: 59.47%] [G loss: 0.8028212785720825]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 57/86 [D loss: 0.6602080166339874, acc.: 60.89%] [G loss: 0.7963027358055115]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 58/86 [D loss: 0.6605575680732727, acc.: 61.28%] [G loss: 0.7959102988243103]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 59/86 [D loss: 0.6652284860610962, acc.: 59.08%] [G loss: 0.7910182476043701]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 170/200, Batch 60/86 [D loss: 0.6679968237876892, acc.: 58.35%] [G loss: 0.8000264763832092]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 170/200, Batch 61/86 [D loss: 0.6635440587997437, acc.: 58.40%] [G loss: 0.7985016107559204]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 62/86 [D loss: 0.6650834083557129, acc.: 58.59%] [G loss: 0.8093647956848145]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 63/86 [D loss: 0.6563629508018494, acc.: 61.77%] [G loss: 0.7772807478904724]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 64/86 [D loss: 0.6671591997146606, acc.: 59.13%] [G loss: 0.7931641936302185]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 65/86 [D loss: 0.6552431285381317, acc.: 62.74%] [G loss: 0.7893866896629333]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 66/86 [D loss: 0.658619612455368, acc.: 61.13%] [G loss: 0.7936794757843018]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 67/86 [D loss: 0.6601825952529907, acc.: 60.16%] [G loss: 0.7865822315216064]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 68/86 [D loss: 0.6616787314414978, acc.: 60.35%] [G loss: 0.7836211919784546]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 69/86 [D loss: 0.6619754731655121, acc.: 60.94%] [G loss: 0.7991091012954712]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 70/86 [D loss: 0.6645045876502991, acc.: 59.42%] [G loss: 0.7994951009750366]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 71/86 [D loss: 0.6620515882968903, acc.: 59.81%] [G loss: 0.7933739423751831]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 72/86 [D loss: 0.6605558693408966, acc.: 59.96%] [G loss: 0.7896720767021179]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 73/86 [D loss: 0.6637186408042908, acc.: 59.86%] [G loss: 0.7965428829193115]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 170/200, Batch 74/86 [D loss: 0.6628186404705048, acc.: 60.25%] [G loss: 0.7866963744163513]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 75/86 [D loss: 0.6602155864238739, acc.: 60.60%] [G loss: 0.8000016212463379]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 76/86 [D loss: 0.663945198059082, acc.: 59.96%] [G loss: 0.7799628376960754]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 77/86 [D loss: 0.6593579351902008, acc.: 59.77%] [G loss: 0.788468062877655]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 78/86 [D loss: 0.6541385650634766, acc.: 61.08%] [G loss: 0.7965238690376282]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 79/86 [D loss: 0.6663675904273987, acc.: 60.01%] [G loss: 0.7925631999969482]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 80/86 [D loss: 0.6633223295211792, acc.: 60.06%] [G loss: 0.7878061532974243]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 170/200, Batch 81/86 [D loss: 0.6619256734848022, acc.: 59.38%] [G loss: 0.7902382612228394]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 82/86 [D loss: 0.6598773002624512, acc.: 59.72%] [G loss: 0.7923529744148254]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 170/200, Batch 83/86 [D loss: 0.6649674475193024, acc.: 58.74%] [G loss: 0.7882586717605591]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 84/86 [D loss: 0.6584886908531189, acc.: 60.79%] [G loss: 0.7868303060531616]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 85/86 [D loss: 0.6616758704185486, acc.: 60.35%] [G loss: 0.7885944843292236]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 170/200, Batch 86/86 [D loss: 0.6542620658874512, acc.: 62.01%] [G loss: 0.7842552661895752]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 171/200, Batch 1/86 [D loss: 0.6651610136032104, acc.: 58.89%] [G loss: 0.7886493802070618]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 2/86 [D loss: 0.6594570875167847, acc.: 60.74%] [G loss: 0.7926989793777466]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 3/86 [D loss: 0.6661335825920105, acc.: 58.79%] [G loss: 0.789304256439209]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 4/86 [D loss: 0.664660781621933, acc.: 59.28%] [G loss: 0.789722204208374]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 5/86 [D loss: 0.6517673134803772, acc.: 63.13%] [G loss: 0.794253945350647]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 6/86 [D loss: 0.6622920334339142, acc.: 59.38%] [G loss: 0.7900738716125488]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 7/86 [D loss: 0.6583307683467865, acc.: 61.13%] [G loss: 0.8003450036048889]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 8/86 [D loss: 0.6580475568771362, acc.: 61.57%] [G loss: 0.7916333675384521]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 9/86 [D loss: 0.6565388143062592, acc.: 62.26%] [G loss: 0.7961424589157104]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 171/200, Batch 10/86 [D loss: 0.66487255692482, acc.: 59.08%] [G loss: 0.7983948588371277]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 11/86 [D loss: 0.6660727560520172, acc.: 58.79%] [G loss: 0.7974698543548584]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 12/86 [D loss: 0.6637179553508759, acc.: 58.94%] [G loss: 0.7963635325431824]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 13/86 [D loss: 0.6683619916439056, acc.: 59.03%] [G loss: 0.7933827042579651]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 14/86 [D loss: 0.663372129201889, acc.: 59.81%] [G loss: 0.7894147634506226]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 15/86 [D loss: 0.6644238829612732, acc.: 59.81%] [G loss: 0.7938862442970276]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 16/86 [D loss: 0.6646872162818909, acc.: 58.64%] [G loss: 0.7918921709060669]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 17/86 [D loss: 0.6611882150173187, acc.: 60.99%] [G loss: 0.8030651211738586]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 18/86 [D loss: 0.6689704060554504, acc.: 59.42%] [G loss: 0.7986119985580444]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 19/86 [D loss: 0.6624303758144379, acc.: 57.86%] [G loss: 0.789242684841156]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 20/86 [D loss: 0.6638063788414001, acc.: 59.67%] [G loss: 0.78896164894104]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 21/86 [D loss: 0.6609163284301758, acc.: 59.52%] [G loss: 0.787238359451294]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 22/86 [D loss: 0.6576638519763947, acc.: 60.64%] [G loss: 0.7877012491226196]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 23/86 [D loss: 0.6574607789516449, acc.: 60.84%] [G loss: 0.7932474613189697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 24/86 [D loss: 0.6598098278045654, acc.: 59.91%] [G loss: 0.7985619306564331]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 25/86 [D loss: 0.66683229804039, acc.: 58.15%] [G loss: 0.7888703346252441]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 26/86 [D loss: 0.6691645681858063, acc.: 59.23%] [G loss: 0.8024432063102722]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 27/86 [D loss: 0.664269745349884, acc.: 58.50%] [G loss: 0.7910362482070923]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 28/86 [D loss: 0.6654712855815887, acc.: 58.11%] [G loss: 0.8022323846817017]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 29/86 [D loss: 0.6598495543003082, acc.: 61.38%] [G loss: 0.7974138259887695]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 30/86 [D loss: 0.6592642068862915, acc.: 61.47%] [G loss: 0.7901925444602966]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 31/86 [D loss: 0.6643322706222534, acc.: 59.08%] [G loss: 0.7929536700248718]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 32/86 [D loss: 0.664168119430542, acc.: 60.11%] [G loss: 0.7917059063911438]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 33/86 [D loss: 0.6570673882961273, acc.: 60.40%] [G loss: 0.790143609046936]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 34/86 [D loss: 0.6600253283977509, acc.: 60.89%] [G loss: 0.7902892827987671]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 35/86 [D loss: 0.6657435297966003, acc.: 58.74%] [G loss: 0.7934472560882568]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 36/86 [D loss: 0.6646093130111694, acc.: 60.16%] [G loss: 0.7914801239967346]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 37/86 [D loss: 0.660381019115448, acc.: 59.62%] [G loss: 0.7967360019683838]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 38/86 [D loss: 0.6630832254886627, acc.: 58.01%] [G loss: 0.7905629873275757]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 39/86 [D loss: 0.6652944386005402, acc.: 58.11%] [G loss: 0.7936183214187622]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 40/86 [D loss: 0.6674087643623352, acc.: 57.91%] [G loss: 0.80056232213974]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 41/86 [D loss: 0.657011479139328, acc.: 61.91%] [G loss: 0.793502151966095]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 42/86 [D loss: 0.6606255769729614, acc.: 59.96%] [G loss: 0.8041090965270996]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 43/86 [D loss: 0.6733245849609375, acc.: 55.91%] [G loss: 0.7958621978759766]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 44/86 [D loss: 0.665332019329071, acc.: 59.62%] [G loss: 0.7867780923843384]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 171/200, Batch 45/86 [D loss: 0.669085681438446, acc.: 58.11%] [G loss: 0.784476637840271]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 46/86 [D loss: 0.6596745252609253, acc.: 61.57%] [G loss: 0.7844927310943604]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 47/86 [D loss: 0.6658863127231598, acc.: 59.67%] [G loss: 0.796117901802063]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 48/86 [D loss: 0.6532299220561981, acc.: 61.08%] [G loss: 0.7824190855026245]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 49/86 [D loss: 0.6645402014255524, acc.: 58.11%] [G loss: 0.787675678730011]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 50/86 [D loss: 0.6618687212467194, acc.: 59.28%] [G loss: 0.7787454724311829]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 51/86 [D loss: 0.6611595749855042, acc.: 59.81%] [G loss: 0.7974531054496765]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 52/86 [D loss: 0.6588024497032166, acc.: 60.11%] [G loss: 0.7865021824836731]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 53/86 [D loss: 0.6595742106437683, acc.: 61.52%] [G loss: 0.792744517326355]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 54/86 [D loss: 0.6636088490486145, acc.: 59.08%] [G loss: 0.802130401134491]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 55/86 [D loss: 0.6589727401733398, acc.: 60.89%] [G loss: 0.7844555377960205]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 56/86 [D loss: 0.6509459018707275, acc.: 63.18%] [G loss: 0.7951774597167969]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 57/86 [D loss: 0.6634581089019775, acc.: 60.30%] [G loss: 0.7786400318145752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 58/86 [D loss: 0.6631971001625061, acc.: 58.54%] [G loss: 0.7912614941596985]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 59/86 [D loss: 0.6580194234848022, acc.: 60.45%] [G loss: 0.784235417842865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 60/86 [D loss: 0.6658845543861389, acc.: 59.72%] [G loss: 0.7937510013580322]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 61/86 [D loss: 0.6626555025577545, acc.: 59.62%] [G loss: 0.7936075925827026]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 62/86 [D loss: 0.6595485508441925, acc.: 60.69%] [G loss: 0.7890071868896484]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 63/86 [D loss: 0.6575907766819, acc.: 60.55%] [G loss: 0.7853742241859436]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 171/200, Batch 64/86 [D loss: 0.663259357213974, acc.: 58.54%] [G loss: 0.7892992496490479]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 65/86 [D loss: 0.667693018913269, acc.: 58.35%] [G loss: 0.7878417372703552]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 66/86 [D loss: 0.66165491938591, acc.: 61.08%] [G loss: 0.7811204195022583]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 67/86 [D loss: 0.6635761559009552, acc.: 60.40%] [G loss: 0.7865430116653442]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 68/86 [D loss: 0.6649479568004608, acc.: 58.35%] [G loss: 0.7912589907646179]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 69/86 [D loss: 0.6715865731239319, acc.: 57.76%] [G loss: 0.7819042801856995]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 70/86 [D loss: 0.669013649225235, acc.: 58.84%] [G loss: 0.7903031706809998]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 71/86 [D loss: 0.6592388451099396, acc.: 60.35%] [G loss: 0.7913245558738708]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 72/86 [D loss: 0.6660739481449127, acc.: 58.45%] [G loss: 0.7836524248123169]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 73/86 [D loss: 0.6562260389328003, acc.: 61.96%] [G loss: 0.7938037514686584]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 74/86 [D loss: 0.6638163328170776, acc.: 59.33%] [G loss: 0.7872284054756165]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 171/200, Batch 75/86 [D loss: 0.6584362089633942, acc.: 61.87%] [G loss: 0.7923902869224548]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 76/86 [D loss: 0.6667971312999725, acc.: 58.89%] [G loss: 0.7984605431556702]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 171/200, Batch 77/86 [D loss: 0.6621204316616058, acc.: 60.50%] [G loss: 0.7891567945480347]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 78/86 [D loss: 0.6555895805358887, acc.: 61.91%] [G loss: 0.7814960479736328]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 79/86 [D loss: 0.6680751442909241, acc.: 57.62%] [G loss: 0.8006019592285156]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 80/86 [D loss: 0.6612266600131989, acc.: 60.69%] [G loss: 0.7755188941955566]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 81/86 [D loss: 0.6636611223220825, acc.: 59.38%] [G loss: 0.794830322265625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 171/200, Batch 82/86 [D loss: 0.6644640862941742, acc.: 60.21%] [G loss: 0.7884841561317444]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 83/86 [D loss: 0.6745053827762604, acc.: 56.20%] [G loss: 0.800937294960022]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 84/86 [D loss: 0.6605232656002045, acc.: 60.60%] [G loss: 0.7840996980667114]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 85/86 [D loss: 0.6697669625282288, acc.: 57.76%] [G loss: 0.7876396179199219]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 171/200, Batch 86/86 [D loss: 0.6573792695999146, acc.: 60.16%] [G loss: 0.7930476665496826]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 1/86 [D loss: 0.6675115525722504, acc.: 59.03%] [G loss: 0.8004847764968872]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 2/86 [D loss: 0.66355761885643, acc.: 58.64%] [G loss: 0.7941699028015137]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 3/86 [D loss: 0.6651275455951691, acc.: 58.59%] [G loss: 0.7952038049697876]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 4/86 [D loss: 0.6571390926837921, acc.: 61.91%] [G loss: 0.7966942191123962]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 5/86 [D loss: 0.6654869914054871, acc.: 59.67%] [G loss: 0.8098582029342651]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 6/86 [D loss: 0.6651106476783752, acc.: 58.50%] [G loss: 0.7896072268486023]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 7/86 [D loss: 0.6643793284893036, acc.: 58.94%] [G loss: 0.7928122282028198]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 8/86 [D loss: 0.6573514044284821, acc.: 60.30%] [G loss: 0.7855348587036133]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 9/86 [D loss: 0.6566172242164612, acc.: 60.69%] [G loss: 0.8088385462760925]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 10/86 [D loss: 0.6625559628009796, acc.: 58.84%] [G loss: 0.7949696779251099]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 11/86 [D loss: 0.6600087583065033, acc.: 59.81%] [G loss: 0.7952458262443542]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 12/86 [D loss: 0.6647312045097351, acc.: 59.28%] [G loss: 0.7899202704429626]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 13/86 [D loss: 0.6645162105560303, acc.: 60.60%] [G loss: 0.8029435873031616]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 14/86 [D loss: 0.661468118429184, acc.: 59.72%] [G loss: 0.7962174415588379]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 15/86 [D loss: 0.6607379615306854, acc.: 59.42%] [G loss: 0.7823212146759033]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 16/86 [D loss: 0.6588995456695557, acc.: 59.57%] [G loss: 0.7928276062011719]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 172/200, Batch 17/86 [D loss: 0.6577780544757843, acc.: 60.74%] [G loss: 0.7968846559524536]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 18/86 [D loss: 0.6623726785182953, acc.: 60.74%] [G loss: 0.7954107522964478]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 19/86 [D loss: 0.664186418056488, acc.: 58.98%] [G loss: 0.7992851138114929]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 20/86 [D loss: 0.663102000951767, acc.: 60.55%] [G loss: 0.7884931564331055]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 21/86 [D loss: 0.6681271195411682, acc.: 57.91%] [G loss: 0.7952407002449036]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 22/86 [D loss: 0.6604326367378235, acc.: 59.77%] [G loss: 0.7890468239784241]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 23/86 [D loss: 0.6704799234867096, acc.: 58.11%] [G loss: 0.7898563146591187]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 172/200, Batch 24/86 [D loss: 0.6651285290718079, acc.: 59.67%] [G loss: 0.7931281924247742]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 25/86 [D loss: 0.6683456301689148, acc.: 59.57%] [G loss: 0.7882195711135864]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 172/200, Batch 26/86 [D loss: 0.6542156040668488, acc.: 61.82%] [G loss: 0.7865440845489502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 27/86 [D loss: 0.6653342247009277, acc.: 56.84%] [G loss: 0.7855141758918762]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 28/86 [D loss: 0.6587449610233307, acc.: 60.11%] [G loss: 0.7886976003646851]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 29/86 [D loss: 0.6688108444213867, acc.: 59.28%] [G loss: 0.8008809685707092]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 30/86 [D loss: 0.659995824098587, acc.: 60.89%] [G loss: 0.78975510597229]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 172/200, Batch 31/86 [D loss: 0.6581478118896484, acc.: 61.38%] [G loss: 0.792915403842926]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 32/86 [D loss: 0.6604674458503723, acc.: 59.86%] [G loss: 0.7904375195503235]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 33/86 [D loss: 0.6655817329883575, acc.: 59.18%] [G loss: 0.7876780033111572]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 172/200, Batch 34/86 [D loss: 0.6655464768409729, acc.: 58.64%] [G loss: 0.7899230122566223]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 35/86 [D loss: 0.65345898270607, acc.: 63.09%] [G loss: 0.7956321835517883]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 36/86 [D loss: 0.6612761914730072, acc.: 60.55%] [G loss: 0.7937201261520386]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 37/86 [D loss: 0.6620288491249084, acc.: 59.47%] [G loss: 0.7859864830970764]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 38/86 [D loss: 0.6626984477043152, acc.: 59.47%] [G loss: 0.7880135774612427]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 39/86 [D loss: 0.6598986685276031, acc.: 60.69%] [G loss: 0.8015304803848267]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 40/86 [D loss: 0.66393181681633, acc.: 59.67%] [G loss: 0.7904369831085205]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 41/86 [D loss: 0.6567648351192474, acc.: 62.65%] [G loss: 0.798507034778595]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 42/86 [D loss: 0.6675963401794434, acc.: 58.35%] [G loss: 0.7905636429786682]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 43/86 [D loss: 0.6610716283321381, acc.: 61.18%] [G loss: 0.7887455821037292]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 44/86 [D loss: 0.6658773422241211, acc.: 59.18%] [G loss: 0.7956170439720154]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 172/200, Batch 45/86 [D loss: 0.6646672189235687, acc.: 58.98%] [G loss: 0.7943871021270752]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 46/86 [D loss: 0.6598628461360931, acc.: 59.91%] [G loss: 0.7915012836456299]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 47/86 [D loss: 0.6683161556720734, acc.: 58.74%] [G loss: 0.7981909513473511]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 48/86 [D loss: 0.6679202914237976, acc.: 57.76%] [G loss: 0.7840776443481445]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 49/86 [D loss: 0.6684505939483643, acc.: 57.71%] [G loss: 0.7916175127029419]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 50/86 [D loss: 0.6620106101036072, acc.: 61.23%] [G loss: 0.7927746772766113]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 51/86 [D loss: 0.6636323034763336, acc.: 60.50%] [G loss: 0.8001679182052612]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 52/86 [D loss: 0.6579409241676331, acc.: 59.52%] [G loss: 0.7932052612304688]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 53/86 [D loss: 0.674823135137558, acc.: 55.66%] [G loss: 0.7979819774627686]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 54/86 [D loss: 0.6641151309013367, acc.: 59.96%] [G loss: 0.7906153202056885]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 55/86 [D loss: 0.6615622043609619, acc.: 59.86%] [G loss: 0.803037166595459]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 56/86 [D loss: 0.6584090292453766, acc.: 60.79%] [G loss: 0.7957004904747009]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 57/86 [D loss: 0.6681547462940216, acc.: 59.08%] [G loss: 0.7965482473373413]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 58/86 [D loss: 0.6581605672836304, acc.: 60.79%] [G loss: 0.7810603976249695]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 59/86 [D loss: 0.6680003106594086, acc.: 58.50%] [G loss: 0.8003547191619873]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 60/86 [D loss: 0.6548440456390381, acc.: 61.96%] [G loss: 0.7869337797164917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 61/86 [D loss: 0.6616839468479156, acc.: 60.74%] [G loss: 0.7901172637939453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 62/86 [D loss: 0.6605124175548553, acc.: 60.06%] [G loss: 0.7910593152046204]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 63/86 [D loss: 0.6654991209506989, acc.: 58.79%] [G loss: 0.8045544624328613]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 64/86 [D loss: 0.6599071025848389, acc.: 59.72%] [G loss: 0.7759631276130676]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 65/86 [D loss: 0.6735576093196869, acc.: 57.32%] [G loss: 0.7922666668891907]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 66/86 [D loss: 0.6541866064071655, acc.: 61.52%] [G loss: 0.7812632322311401]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 67/86 [D loss: 0.666802704334259, acc.: 59.42%] [G loss: 0.7959016561508179]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 68/86 [D loss: 0.6647849977016449, acc.: 59.42%] [G loss: 0.8022925853729248]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 69/86 [D loss: 0.6669553816318512, acc.: 58.74%] [G loss: 0.7932495474815369]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 70/86 [D loss: 0.6666464805603027, acc.: 58.30%] [G loss: 0.8046185374259949]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 71/86 [D loss: 0.662553995847702, acc.: 60.01%] [G loss: 0.792671799659729]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 72/86 [D loss: 0.6619791388511658, acc.: 60.40%] [G loss: 0.7987355589866638]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 73/86 [D loss: 0.6725907027721405, acc.: 57.76%] [G loss: 0.7997387051582336]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 172/200, Batch 74/86 [D loss: 0.6586422324180603, acc.: 61.87%] [G loss: 0.7878898978233337]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 75/86 [D loss: 0.6645504534244537, acc.: 59.33%] [G loss: 0.7866687774658203]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 76/86 [D loss: 0.6634471416473389, acc.: 60.25%] [G loss: 0.7813615798950195]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 77/86 [D loss: 0.665962815284729, acc.: 59.67%] [G loss: 0.7841557860374451]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 78/86 [D loss: 0.6594372391700745, acc.: 60.60%] [G loss: 0.7832614183425903]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 79/86 [D loss: 0.6605893969535828, acc.: 59.33%] [G loss: 0.7938141226768494]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 80/86 [D loss: 0.6563260853290558, acc.: 61.04%] [G loss: 0.7905377745628357]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 81/86 [D loss: 0.6681588590145111, acc.: 57.81%] [G loss: 0.7922808527946472]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 82/86 [D loss: 0.6589161157608032, acc.: 60.84%] [G loss: 0.7996890544891357]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 83/86 [D loss: 0.6590680181980133, acc.: 60.84%] [G loss: 0.7903423309326172]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 172/200, Batch 84/86 [D loss: 0.6702622473239899, acc.: 58.64%] [G loss: 0.8031301498413086]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 85/86 [D loss: 0.6670356690883636, acc.: 58.30%] [G loss: 0.7889270186424255]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 172/200, Batch 86/86 [D loss: 0.6684622466564178, acc.: 58.25%] [G loss: 0.7856963872909546]\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 173/200, Batch 1/86 [D loss: 0.6621285974979401, acc.: 60.74%] [G loss: 0.791938066482544]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 2/86 [D loss: 0.664045512676239, acc.: 60.21%] [G loss: 0.7887439131736755]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 3/86 [D loss: 0.6645643711090088, acc.: 60.25%] [G loss: 0.795345664024353]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 4/86 [D loss: 0.6715816259384155, acc.: 56.98%] [G loss: 0.7839800119400024]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 5/86 [D loss: 0.6697512269020081, acc.: 57.37%] [G loss: 0.7926970720291138]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 6/86 [D loss: 0.6621808111667633, acc.: 59.62%] [G loss: 0.7920610904693604]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 7/86 [D loss: 0.6620233356952667, acc.: 59.72%] [G loss: 0.7907662987709045]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 8/86 [D loss: 0.6679551303386688, acc.: 58.79%] [G loss: 0.7893663644790649]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 9/86 [D loss: 0.660874217748642, acc.: 59.91%] [G loss: 0.7928968667984009]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 10/86 [D loss: 0.6667948663234711, acc.: 58.79%] [G loss: 0.8014813661575317]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 11/86 [D loss: 0.6602334976196289, acc.: 60.35%] [G loss: 0.784936249256134]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 12/86 [D loss: 0.6555930376052856, acc.: 62.06%] [G loss: 0.7876594066619873]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 173/200, Batch 13/86 [D loss: 0.6607131063938141, acc.: 60.21%] [G loss: 0.7897888422012329]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 14/86 [D loss: 0.6653636395931244, acc.: 59.57%] [G loss: 0.7932590246200562]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 15/86 [D loss: 0.6591273546218872, acc.: 60.99%] [G loss: 0.7998732328414917]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 16/86 [D loss: 0.6634111404418945, acc.: 59.77%] [G loss: 0.7891120314598083]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 17/86 [D loss: 0.6613588035106659, acc.: 59.86%] [G loss: 0.7970615029335022]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 18/86 [D loss: 0.6598679721355438, acc.: 60.89%] [G loss: 0.7855350375175476]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 19/86 [D loss: 0.6641975939273834, acc.: 59.47%] [G loss: 0.7852694988250732]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 20/86 [D loss: 0.6630136072635651, acc.: 60.99%] [G loss: 0.7899496555328369]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 173/200, Batch 21/86 [D loss: 0.6575448215007782, acc.: 60.16%] [G loss: 0.8004138469696045]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 22/86 [D loss: 0.6607370376586914, acc.: 59.47%] [G loss: 0.7907885909080505]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 23/86 [D loss: 0.6613677144050598, acc.: 59.42%] [G loss: 0.7941896915435791]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 24/86 [D loss: 0.6660093665122986, acc.: 58.59%] [G loss: 0.7935281991958618]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 25/86 [D loss: 0.669052392244339, acc.: 56.45%] [G loss: 0.7978619337081909]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 26/86 [D loss: 0.6624132990837097, acc.: 59.28%] [G loss: 0.7930259704589844]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 173/200, Batch 27/86 [D loss: 0.6657575368881226, acc.: 60.55%] [G loss: 0.796745240688324]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 173/200, Batch 28/86 [D loss: 0.6588531732559204, acc.: 60.40%] [G loss: 0.7930300235748291]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 29/86 [D loss: 0.6642352938652039, acc.: 59.28%] [G loss: 0.7961376309394836]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 30/86 [D loss: 0.6571263074874878, acc.: 59.67%] [G loss: 0.7954798340797424]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 31/86 [D loss: 0.6626251637935638, acc.: 60.50%] [G loss: 0.7925161719322205]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 32/86 [D loss: 0.6588640511035919, acc.: 61.43%] [G loss: 0.8067940473556519]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 173/200, Batch 33/86 [D loss: 0.6593470573425293, acc.: 60.21%] [G loss: 0.7943194508552551]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 173/200, Batch 34/86 [D loss: 0.6625842154026031, acc.: 59.72%] [G loss: 0.7998286485671997]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 35/86 [D loss: 0.6599286794662476, acc.: 61.08%] [G loss: 0.7845375537872314]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 173/200, Batch 36/86 [D loss: 0.661793053150177, acc.: 60.06%] [G loss: 0.7933872938156128]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 37/86 [D loss: 0.6645883321762085, acc.: 61.08%] [G loss: 0.7956082820892334]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 38/86 [D loss: 0.6626535654067993, acc.: 58.54%] [G loss: 0.7831711769104004]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 39/86 [D loss: 0.6633374989032745, acc.: 58.98%] [G loss: 0.7989602088928223]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 40/86 [D loss: 0.6595163941383362, acc.: 59.96%] [G loss: 0.7933276891708374]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 173/200, Batch 41/86 [D loss: 0.6624860167503357, acc.: 59.08%] [G loss: 0.7860155701637268]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 42/86 [D loss: 0.6612476706504822, acc.: 60.79%] [G loss: 0.8007737994194031]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 43/86 [D loss: 0.6602727472782135, acc.: 60.69%] [G loss: 0.8010601997375488]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 44/86 [D loss: 0.6687081158161163, acc.: 59.86%] [G loss: 0.7925958633422852]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 45/86 [D loss: 0.6577345430850983, acc.: 61.91%] [G loss: 0.8093007206916809]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 46/86 [D loss: 0.6619726121425629, acc.: 59.72%] [G loss: 0.7903249263763428]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 47/86 [D loss: 0.6551629304885864, acc.: 61.33%] [G loss: 0.7928476929664612]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 48/86 [D loss: 0.6584430634975433, acc.: 60.60%] [G loss: 0.7980533838272095]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 49/86 [D loss: 0.6597334742546082, acc.: 60.30%] [G loss: 0.80284184217453]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 50/86 [D loss: 0.6564550995826721, acc.: 63.23%] [G loss: 0.7904113531112671]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 51/86 [D loss: 0.6686145067214966, acc.: 57.42%] [G loss: 0.8080140948295593]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 52/86 [D loss: 0.6669792234897614, acc.: 57.96%] [G loss: 0.7862271666526794]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 53/86 [D loss: 0.6632903516292572, acc.: 60.11%] [G loss: 0.7924507260322571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 173/200, Batch 54/86 [D loss: 0.6624370217323303, acc.: 59.96%] [G loss: 0.811790943145752]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 55/86 [D loss: 0.6625208556652069, acc.: 59.86%] [G loss: 0.8018366694450378]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 56/86 [D loss: 0.6649599075317383, acc.: 58.40%] [G loss: 0.7903575897216797]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 57/86 [D loss: 0.6656252145767212, acc.: 60.01%] [G loss: 0.8013036251068115]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 58/86 [D loss: 0.6582618355751038, acc.: 62.50%] [G loss: 0.7958329319953918]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 59/86 [D loss: 0.664139449596405, acc.: 60.11%] [G loss: 0.7950611114501953]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 60/86 [D loss: 0.6549231112003326, acc.: 62.74%] [G loss: 0.7989195585250854]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 173/200, Batch 61/86 [D loss: 0.664083331823349, acc.: 60.40%] [G loss: 0.7896866202354431]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 62/86 [D loss: 0.6562406420707703, acc.: 61.38%] [G loss: 0.7956186532974243]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 63/86 [D loss: 0.6692063808441162, acc.: 58.35%] [G loss: 0.7959005832672119]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 64/86 [D loss: 0.6665282249450684, acc.: 58.79%] [G loss: 0.7887699604034424]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 173/200, Batch 65/86 [D loss: 0.6618594527244568, acc.: 59.47%] [G loss: 0.7907448410987854]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 66/86 [D loss: 0.6675876975059509, acc.: 58.50%] [G loss: 0.7777701616287231]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 67/86 [D loss: 0.66361004114151, acc.: 60.21%] [G loss: 0.803022027015686]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 68/86 [D loss: 0.6649764180183411, acc.: 59.86%] [G loss: 0.7925240993499756]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 173/200, Batch 69/86 [D loss: 0.6652630865573883, acc.: 58.06%] [G loss: 0.7930150032043457]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 70/86 [D loss: 0.6588375270366669, acc.: 60.30%] [G loss: 0.7955849170684814]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 71/86 [D loss: 0.6667181253433228, acc.: 59.08%] [G loss: 0.8076635003089905]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 72/86 [D loss: 0.6547314822673798, acc.: 61.33%] [G loss: 0.7951409816741943]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 73/86 [D loss: 0.6687497496604919, acc.: 57.32%] [G loss: 0.7886581420898438]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 74/86 [D loss: 0.6621635854244232, acc.: 60.55%] [G loss: 0.7822055816650391]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 75/86 [D loss: 0.6617863476276398, acc.: 60.35%] [G loss: 0.8024096488952637]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 76/86 [D loss: 0.6501960754394531, acc.: 62.79%] [G loss: 0.7920781373977661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 173/200, Batch 77/86 [D loss: 0.6709512174129486, acc.: 57.28%] [G loss: 0.7931111454963684]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 78/86 [D loss: 0.6572908163070679, acc.: 60.99%] [G loss: 0.7865546941757202]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 79/86 [D loss: 0.6703501641750336, acc.: 55.66%] [G loss: 0.8109767436981201]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 80/86 [D loss: 0.6629309058189392, acc.: 58.50%] [G loss: 0.7823131084442139]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 173/200, Batch 81/86 [D loss: 0.6596563756465912, acc.: 59.08%] [G loss: 0.7947667241096497]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 82/86 [D loss: 0.6688155829906464, acc.: 58.69%] [G loss: 0.7834025025367737]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 83/86 [D loss: 0.6698704063892365, acc.: 58.74%] [G loss: 0.8156269788742065]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 84/86 [D loss: 0.6593315005302429, acc.: 61.72%] [G loss: 0.7994908690452576]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 173/200, Batch 85/86 [D loss: 0.6683120727539062, acc.: 59.23%] [G loss: 0.8043472766876221]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 173/200, Batch 86/86 [D loss: 0.6586645841598511, acc.: 61.23%] [G loss: 0.7939481735229492]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 174/200, Batch 1/86 [D loss: 0.6655665040016174, acc.: 60.45%] [G loss: 0.7993079423904419]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 174/200, Batch 2/86 [D loss: 0.6572019755840302, acc.: 61.67%] [G loss: 0.7768763899803162]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 3/86 [D loss: 0.6654504239559174, acc.: 57.62%] [G loss: 0.8039466142654419]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 4/86 [D loss: 0.6609202921390533, acc.: 59.86%] [G loss: 0.7882747650146484]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 5/86 [D loss: 0.6808928549289703, acc.: 54.39%] [G loss: 0.7902052402496338]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 6/86 [D loss: 0.6705491840839386, acc.: 57.86%] [G loss: 0.7917118668556213]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 7/86 [D loss: 0.6689433753490448, acc.: 57.23%] [G loss: 0.79815274477005]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 8/86 [D loss: 0.6624219417572021, acc.: 60.60%] [G loss: 0.7651203274726868]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 9/86 [D loss: 0.6770165264606476, acc.: 55.62%] [G loss: 0.8034513592720032]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 10/86 [D loss: 0.6578846871852875, acc.: 60.06%] [G loss: 0.7878901958465576]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 11/86 [D loss: 0.668483167886734, acc.: 57.81%] [G loss: 0.783538818359375]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 12/86 [D loss: 0.6695825159549713, acc.: 57.52%] [G loss: 0.7933504581451416]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 174/200, Batch 13/86 [D loss: 0.6701360046863556, acc.: 58.98%] [G loss: 0.825930655002594]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 14/86 [D loss: 0.6676795482635498, acc.: 58.69%] [G loss: 0.7671381831169128]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 174/200, Batch 15/86 [D loss: 0.686936765909195, acc.: 53.61%] [G loss: 0.8331707715988159]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 16/86 [D loss: 0.6503385603427887, acc.: 62.60%] [G loss: 0.7696222066879272]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 17/86 [D loss: 0.678325355052948, acc.: 54.88%] [G loss: 0.8008887767791748]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 174/200, Batch 18/86 [D loss: 0.6940941214561462, acc.: 53.27%] [G loss: 0.8015375733375549]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 19/86 [D loss: 0.6419845521450043, acc.: 64.84%] [G loss: 0.8494242429733276]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 20/86 [D loss: 0.6744852662086487, acc.: 57.23%] [G loss: 0.7418056726455688]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 21/86 [D loss: 0.701054722070694, acc.: 50.29%] [G loss: 0.7725428938865662]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 22/86 [D loss: 0.6786841452121735, acc.: 56.84%] [G loss: 0.8909891843795776]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 23/86 [D loss: 0.6188531517982483, acc.: 70.80%] [G loss: 0.7551616430282593]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 174/200, Batch 24/86 [D loss: 0.7521410882472992, acc.: 42.29%] [G loss: 0.5702615976333618]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 25/86 [D loss: 1.1529441475868225, acc.: 32.62%] [G loss: 1.6748204231262207]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 26/86 [D loss: 0.5259991437196732, acc.: 73.05%] [G loss: 0.2771674692630768]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 27/86 [D loss: 1.6761064231395721, acc.: 25.59%] [G loss: 0.20140855014324188]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 28/86 [D loss: 1.9117898643016815, acc.: 16.02%] [G loss: 0.30455273389816284]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 174/200, Batch 29/86 [D loss: 1.7274046838283539, acc.: 19.48%] [G loss: 3.2282326221466064]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 30/86 [D loss: 0.3727917924989015, acc.: 75.10%] [G loss: 5.547911167144775]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 31/86 [D loss: 0.32129964604973793, acc.: 85.30%] [G loss: 0.3909256160259247]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 174/200, Batch 32/86 [D loss: 2.5895433723926544, acc.: 43.16%] [G loss: 0.12450775504112244]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 33/86 [D loss: 0.9561313390731812, acc.: 36.96%] [G loss: 1.1176807880401611]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 34/86 [D loss: 0.5603918731212616, acc.: 69.34%] [G loss: 1.8679423332214355]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 35/86 [D loss: 0.5150729939341545, acc.: 68.31%] [G loss: 2.0764358043670654]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 36/86 [D loss: 0.46808701008558273, acc.: 75.20%] [G loss: 1.9630142450332642]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 37/86 [D loss: 0.4095957279205322, acc.: 83.74%] [G loss: 1.6055350303649902]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 174/200, Batch 38/86 [D loss: 0.5995466709136963, acc.: 69.58%] [G loss: 0.3128323256969452]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 39/86 [D loss: 1.5259668827056885, acc.: 37.01%] [G loss: 0.3339369595050812]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 40/86 [D loss: 0.8736342787742615, acc.: 43.36%] [G loss: 2.2088418006896973]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 41/86 [D loss: 0.3944835513830185, acc.: 77.00%] [G loss: 2.6059727668762207]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 42/86 [D loss: 0.36506134644150734, acc.: 83.25%] [G loss: 2.0583038330078125]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 43/86 [D loss: 0.3559562563896179, acc.: 91.85%] [G loss: 1.5861895084381104]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 44/86 [D loss: 0.39395759999752045, acc.: 92.33%] [G loss: 1.0295895338058472]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 45/86 [D loss: 0.5110987424850464, acc.: 79.64%] [G loss: 0.8698740601539612]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 46/86 [D loss: 0.5121919214725494, acc.: 80.42%] [G loss: 1.0054295063018799]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 47/86 [D loss: 0.4643741101026535, acc.: 89.06%] [G loss: 1.2170610427856445]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 48/86 [D loss: 0.444069042801857, acc.: 89.50%] [G loss: 1.2936031818389893]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 49/86 [D loss: 0.4380346089601517, acc.: 88.33%] [G loss: 1.2890669107437134]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 50/86 [D loss: 0.45411089062690735, acc.: 88.28%] [G loss: 1.2881032228469849]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 51/86 [D loss: 0.4374432861804962, acc.: 88.96%] [G loss: 1.3131675720214844]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 52/86 [D loss: 0.413971871137619, acc.: 91.02%] [G loss: 1.377049446105957]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 53/86 [D loss: 0.3962540477514267, acc.: 92.43%] [G loss: 1.41636323928833]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 54/86 [D loss: 0.3906231224536896, acc.: 92.43%] [G loss: 1.3972183465957642]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 55/86 [D loss: 0.3791874051094055, acc.: 93.85%] [G loss: 1.3675241470336914]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 56/86 [D loss: 0.38550835847854614, acc.: 93.90%] [G loss: 1.3908520936965942]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 57/86 [D loss: 0.38283926248550415, acc.: 93.95%] [G loss: 1.3722355365753174]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 58/86 [D loss: 0.3820426017045975, acc.: 93.41%] [G loss: 1.3378489017486572]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 59/86 [D loss: 0.3833848237991333, acc.: 92.77%] [G loss: 1.2997429370880127]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 60/86 [D loss: 0.38988159596920013, acc.: 92.19%] [G loss: 1.2696913480758667]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 61/86 [D loss: 0.4022580087184906, acc.: 91.65%] [G loss: 1.3247997760772705]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 62/86 [D loss: 0.381013423204422, acc.: 92.97%] [G loss: 1.4460314512252808]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 63/86 [D loss: 0.3521122485399246, acc.: 94.29%] [G loss: 1.476414680480957]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 64/86 [D loss: 0.3555421829223633, acc.: 94.14%] [G loss: 1.2572648525238037]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 65/86 [D loss: 0.4110366851091385, acc.: 90.53%] [G loss: 1.0033740997314453]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 66/86 [D loss: 0.5077627003192902, acc.: 77.83%] [G loss: 0.7536290287971497]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 67/86 [D loss: 0.6408976465463638, acc.: 60.11%] [G loss: 0.8540135025978088]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 68/86 [D loss: 0.5383071154356003, acc.: 76.51%] [G loss: 1.4629909992218018]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 69/86 [D loss: 0.4465668648481369, acc.: 84.96%] [G loss: 1.1153781414031982]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 70/86 [D loss: 0.5895512700080872, acc.: 72.27%] [G loss: 0.7393448948860168]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 71/86 [D loss: 0.8075397610664368, acc.: 52.39%] [G loss: 0.46117326617240906]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 72/86 [D loss: 1.0821764171123505, acc.: 38.38%] [G loss: 0.5924311876296997]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 73/86 [D loss: 0.6635752320289612, acc.: 63.67%] [G loss: 2.396731376647949]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 74/86 [D loss: 0.47295109927654266, acc.: 76.66%] [G loss: 1.041056752204895]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 75/86 [D loss: 0.7308182716369629, acc.: 51.86%] [G loss: 0.6139352917671204]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 76/86 [D loss: 0.852088451385498, acc.: 41.06%] [G loss: 0.5626572370529175]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 174/200, Batch 77/86 [D loss: 0.862738847732544, acc.: 38.18%] [G loss: 0.9159780144691467]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 78/86 [D loss: 0.6508877277374268, acc.: 65.53%] [G loss: 1.1770639419555664]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 79/86 [D loss: 0.6155845522880554, acc.: 69.14%] [G loss: 1.5587141513824463]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 80/86 [D loss: 0.6887136995792389, acc.: 57.28%] [G loss: 0.8219134211540222]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 174/200, Batch 81/86 [D loss: 0.7108772397041321, acc.: 55.37%] [G loss: 0.7687229514122009]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 82/86 [D loss: 0.7281789481639862, acc.: 50.54%] [G loss: 0.7260299921035767]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 83/86 [D loss: 0.7608119547367096, acc.: 46.39%] [G loss: 0.7441273927688599]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 174/200, Batch 84/86 [D loss: 0.7261117100715637, acc.: 51.03%] [G loss: 0.8286020159721375]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 85/86 [D loss: 0.7065698504447937, acc.: 53.42%] [G loss: 0.8450469970703125]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 174/200, Batch 86/86 [D loss: 0.703779935836792, acc.: 54.74%] [G loss: 0.8074505925178528]\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 1/86 [D loss: 0.7160948812961578, acc.: 52.10%] [G loss: 0.7723703384399414]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 175/200, Batch 2/86 [D loss: 0.7326539158821106, acc.: 47.80%] [G loss: 0.7408161759376526]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 3/86 [D loss: 0.7357359230518341, acc.: 46.58%] [G loss: 0.7213751077651978]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 4/86 [D loss: 0.7517593204975128, acc.: 44.68%] [G loss: 0.69893878698349]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 5/86 [D loss: 0.7617848515510559, acc.: 42.68%] [G loss: 0.6998555064201355]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 6/86 [D loss: 0.7491953074932098, acc.: 42.72%] [G loss: 0.7132712602615356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 175/200, Batch 7/86 [D loss: 0.7534266412258148, acc.: 44.04%] [G loss: 0.7264398336410522]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 8/86 [D loss: 0.7527368366718292, acc.: 43.36%] [G loss: 0.7285585403442383]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 175/200, Batch 9/86 [D loss: 0.7427454888820648, acc.: 44.34%] [G loss: 0.7335780262947083]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 10/86 [D loss: 0.7454766929149628, acc.: 42.92%] [G loss: 0.7507234215736389]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 11/86 [D loss: 0.7270091474056244, acc.: 48.83%] [G loss: 0.7397127151489258]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 12/86 [D loss: 0.7293838858604431, acc.: 46.92%] [G loss: 0.7426929473876953]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 13/86 [D loss: 0.7342053949832916, acc.: 45.41%] [G loss: 0.7421798706054688]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 14/86 [D loss: 0.7400994300842285, acc.: 43.65%] [G loss: 0.7378219962120056]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 15/86 [D loss: 0.7374337613582611, acc.: 45.46%] [G loss: 0.7411378622055054]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 16/86 [D loss: 0.7213877141475677, acc.: 47.02%] [G loss: 0.7283747792243958]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 175/200, Batch 17/86 [D loss: 0.725903183221817, acc.: 46.29%] [G loss: 0.727785050868988]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 18/86 [D loss: 0.7325986623764038, acc.: 45.41%] [G loss: 0.7399643063545227]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 19/86 [D loss: 0.7258852422237396, acc.: 46.29%] [G loss: 0.7437461018562317]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 20/86 [D loss: 0.7211602628231049, acc.: 47.07%] [G loss: 0.7328988313674927]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 21/86 [D loss: 0.7175273001194, acc.: 46.83%] [G loss: 0.731104850769043]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 22/86 [D loss: 0.7239943742752075, acc.: 47.27%] [G loss: 0.7402336001396179]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 23/86 [D loss: 0.7129564583301544, acc.: 48.73%] [G loss: 0.748867928981781]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 24/86 [D loss: 0.7148326337337494, acc.: 47.85%] [G loss: 0.741254448890686]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 175/200, Batch 25/86 [D loss: 0.7046490907669067, acc.: 51.17%] [G loss: 0.754776120185852]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 26/86 [D loss: 0.7055366039276123, acc.: 49.22%] [G loss: 0.7510098814964294]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 27/86 [D loss: 0.7129867970943451, acc.: 50.05%] [G loss: 0.7564984560012817]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 28/86 [D loss: 0.7142087817192078, acc.: 49.27%] [G loss: 0.7430983781814575]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 29/86 [D loss: 0.7052921652793884, acc.: 50.34%] [G loss: 0.7561814785003662]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 30/86 [D loss: 0.7105766534805298, acc.: 49.41%] [G loss: 0.7567852735519409]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 31/86 [D loss: 0.7040018737316132, acc.: 50.05%] [G loss: 0.7583759427070618]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 32/86 [D loss: 0.694512128829956, acc.: 53.47%] [G loss: 0.7583510875701904]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 175/200, Batch 33/86 [D loss: 0.7073425650596619, acc.: 50.05%] [G loss: 0.755352795124054]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 34/86 [D loss: 0.707773894071579, acc.: 49.66%] [G loss: 0.7471047639846802]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 35/86 [D loss: 0.696168452501297, acc.: 51.42%] [G loss: 0.7583779692649841]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 36/86 [D loss: 0.7005766034126282, acc.: 51.03%] [G loss: 0.7561725974082947]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 37/86 [D loss: 0.7016685009002686, acc.: 51.12%] [G loss: 0.7604038715362549]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 38/86 [D loss: 0.697466641664505, acc.: 51.32%] [G loss: 0.7589662671089172]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 39/86 [D loss: 0.6889188587665558, acc.: 52.59%] [G loss: 0.7544057369232178]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 40/86 [D loss: 0.6982325315475464, acc.: 52.69%] [G loss: 0.7636346220970154]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 41/86 [D loss: 0.6986922025680542, acc.: 50.83%] [G loss: 0.750475287437439]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 42/86 [D loss: 0.6907166540622711, acc.: 54.05%] [G loss: 0.7709534168243408]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 43/86 [D loss: 0.6945907473564148, acc.: 52.10%] [G loss: 0.7474068403244019]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 44/86 [D loss: 0.6932103037834167, acc.: 53.17%] [G loss: 0.7688021659851074]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 45/86 [D loss: 0.6946932673454285, acc.: 53.08%] [G loss: 0.7676259875297546]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 175/200, Batch 46/86 [D loss: 0.6884325444698334, acc.: 52.98%] [G loss: 0.7592587471008301]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 47/86 [D loss: 0.6996826529502869, acc.: 50.93%] [G loss: 0.765592098236084]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 48/86 [D loss: 0.6939793229103088, acc.: 52.10%] [G loss: 0.7735083103179932]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 49/86 [D loss: 0.6877908110618591, acc.: 54.10%] [G loss: 0.7985609173774719]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 50/86 [D loss: 0.6830124258995056, acc.: 54.54%] [G loss: 0.7881731986999512]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 51/86 [D loss: 0.6839245855808258, acc.: 54.10%] [G loss: 0.7843243479728699]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 52/86 [D loss: 0.6791799366474152, acc.: 55.57%] [G loss: 0.7834542989730835]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 53/86 [D loss: 0.684318333864212, acc.: 54.64%] [G loss: 0.781468391418457]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 54/86 [D loss: 0.69113689661026, acc.: 52.83%] [G loss: 0.7736413478851318]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 175/200, Batch 55/86 [D loss: 0.6921074688434601, acc.: 52.69%] [G loss: 0.7703909873962402]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 56/86 [D loss: 0.686995655298233, acc.: 53.52%] [G loss: 0.7806833982467651]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 57/86 [D loss: 0.6911717057228088, acc.: 51.81%] [G loss: 0.7782493829727173]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 58/86 [D loss: 0.687802642583847, acc.: 52.78%] [G loss: 0.7716888189315796]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 59/86 [D loss: 0.6847701668739319, acc.: 54.25%] [G loss: 0.7705326676368713]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 60/86 [D loss: 0.684921145439148, acc.: 54.69%] [G loss: 0.7812358736991882]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 61/86 [D loss: 0.6811303794384003, acc.: 56.05%] [G loss: 0.7919940948486328]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 62/86 [D loss: 0.6839426159858704, acc.: 55.22%] [G loss: 0.7746050357818604]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 175/200, Batch 63/86 [D loss: 0.6872762441635132, acc.: 54.54%] [G loss: 0.780397891998291]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 64/86 [D loss: 0.6858470439910889, acc.: 55.37%] [G loss: 0.7732447385787964]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 65/86 [D loss: 0.6873635053634644, acc.: 54.44%] [G loss: 0.782151460647583]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 66/86 [D loss: 0.6845594048500061, acc.: 54.49%] [G loss: 0.7815226912498474]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 67/86 [D loss: 0.6854653358459473, acc.: 53.71%] [G loss: 0.7821106314659119]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 68/86 [D loss: 0.6837821304798126, acc.: 55.42%] [G loss: 0.7843541502952576]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 69/86 [D loss: 0.6798674464225769, acc.: 56.01%] [G loss: 0.7696349024772644]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 175/200, Batch 70/86 [D loss: 0.6834759414196014, acc.: 54.64%] [G loss: 0.7869062423706055]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 175/200, Batch 71/86 [D loss: 0.6812500953674316, acc.: 55.76%] [G loss: 0.7852669954299927]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 72/86 [D loss: 0.681164413690567, acc.: 54.93%] [G loss: 0.7893614768981934]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 175/200, Batch 73/86 [D loss: 0.6835589408874512, acc.: 54.88%] [G loss: 0.7741801738739014]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 74/86 [D loss: 0.6830607354640961, acc.: 55.66%] [G loss: 0.7705435752868652]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 75/86 [D loss: 0.6784728765487671, acc.: 55.08%] [G loss: 0.7786487340927124]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 76/86 [D loss: 0.6834107041358948, acc.: 54.35%] [G loss: 0.7863026857376099]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 77/86 [D loss: 0.6761924028396606, acc.: 55.91%] [G loss: 0.7583540081977844]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 78/86 [D loss: 0.6834996342658997, acc.: 53.42%] [G loss: 0.7829409837722778]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 175/200, Batch 79/86 [D loss: 0.6741304695606232, acc.: 56.74%] [G loss: 0.7741848826408386]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 80/86 [D loss: 0.6779212057590485, acc.: 55.47%] [G loss: 0.7857934832572937]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 81/86 [D loss: 0.6813881397247314, acc.: 55.18%] [G loss: 0.7850164175033569]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 82/86 [D loss: 0.6801873445510864, acc.: 54.93%] [G loss: 0.7861050963401794]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 175/200, Batch 83/86 [D loss: 0.6760475039482117, acc.: 55.57%] [G loss: 0.7844178676605225]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 84/86 [D loss: 0.6812069118022919, acc.: 54.49%] [G loss: 0.7808684706687927]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 175/200, Batch 85/86 [D loss: 0.6789190173149109, acc.: 54.88%] [G loss: 0.776668131351471]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 175/200, Batch 86/86 [D loss: 0.6744030714035034, acc.: 55.62%] [G loss: 0.7813882827758789]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 176/200, Batch 1/86 [D loss: 0.6761335730552673, acc.: 56.59%] [G loss: 0.7804751992225647]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 2/86 [D loss: 0.6809442043304443, acc.: 55.57%] [G loss: 0.7885915637016296]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 3/86 [D loss: 0.6795860826969147, acc.: 54.54%] [G loss: 0.7863188982009888]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 4/86 [D loss: 0.680602639913559, acc.: 53.81%] [G loss: 0.7795387506484985]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 5/86 [D loss: 0.6795293688774109, acc.: 55.42%] [G loss: 0.7748777270317078]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 6/86 [D loss: 0.6823784708976746, acc.: 55.62%] [G loss: 0.7840718030929565]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 7/86 [D loss: 0.6805475354194641, acc.: 55.47%] [G loss: 0.7792912125587463]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 8/86 [D loss: 0.6789052784442902, acc.: 56.30%] [G loss: 0.7797452807426453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 176/200, Batch 9/86 [D loss: 0.6772216558456421, acc.: 57.18%] [G loss: 0.7776686549186707]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 10/86 [D loss: 0.6742116808891296, acc.: 55.91%] [G loss: 0.7838011384010315]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 11/86 [D loss: 0.6796858310699463, acc.: 54.93%] [G loss: 0.7749748229980469]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 12/86 [D loss: 0.6759207546710968, acc.: 54.98%] [G loss: 0.7742878794670105]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 13/86 [D loss: 0.6885953545570374, acc.: 53.52%] [G loss: 0.780707597732544]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 14/86 [D loss: 0.6779013276100159, acc.: 55.62%] [G loss: 0.7777930498123169]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 15/86 [D loss: 0.674854576587677, acc.: 55.62%] [G loss: 0.7824605107307434]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 16/86 [D loss: 0.6742497682571411, acc.: 56.69%] [G loss: 0.789779543876648]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 17/86 [D loss: 0.6809708476066589, acc.: 54.83%] [G loss: 0.7759048342704773]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 18/86 [D loss: 0.6713320314884186, acc.: 56.59%] [G loss: 0.780363917350769]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 19/86 [D loss: 0.6825230121612549, acc.: 56.45%] [G loss: 0.7864207029342651]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 20/86 [D loss: 0.6764379441738129, acc.: 56.64%] [G loss: 0.7834576964378357]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 21/86 [D loss: 0.6797378957271576, acc.: 56.69%] [G loss: 0.7821056842803955]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 22/86 [D loss: 0.6738574206829071, acc.: 56.79%] [G loss: 0.7806721925735474]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 23/86 [D loss: 0.6795794367790222, acc.: 54.69%] [G loss: 0.7783056497573853]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 24/86 [D loss: 0.677653580904007, acc.: 56.05%] [G loss: 0.7867048978805542]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 25/86 [D loss: 0.6805890202522278, acc.: 54.74%] [G loss: 0.7691494226455688]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 26/86 [D loss: 0.6752251088619232, acc.: 56.25%] [G loss: 0.7803029417991638]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 27/86 [D loss: 0.6824682950973511, acc.: 55.71%] [G loss: 0.7811859250068665]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 28/86 [D loss: 0.6786331236362457, acc.: 55.18%] [G loss: 0.7801784873008728]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 29/86 [D loss: 0.6753562688827515, acc.: 55.71%] [G loss: 0.7824845314025879]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 176/200, Batch 30/86 [D loss: 0.668790727853775, acc.: 58.35%] [G loss: 0.7746853232383728]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 31/86 [D loss: 0.6713645756244659, acc.: 56.54%] [G loss: 0.7804728746414185]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 32/86 [D loss: 0.6732678413391113, acc.: 56.40%] [G loss: 0.7754743099212646]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 33/86 [D loss: 0.679681271314621, acc.: 55.42%] [G loss: 0.7856476306915283]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 34/86 [D loss: 0.6723074018955231, acc.: 57.47%] [G loss: 0.7840480208396912]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 35/86 [D loss: 0.6688692569732666, acc.: 57.42%] [G loss: 0.7882375121116638]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 36/86 [D loss: 0.6747134625911713, acc.: 56.69%] [G loss: 0.7872538566589355]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 37/86 [D loss: 0.6720274984836578, acc.: 57.13%] [G loss: 0.7808592319488525]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 38/86 [D loss: 0.6749342978000641, acc.: 56.69%] [G loss: 0.7895459532737732]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 39/86 [D loss: 0.6813241839408875, acc.: 56.40%] [G loss: 0.7780208587646484]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 40/86 [D loss: 0.6762461364269257, acc.: 56.20%] [G loss: 0.7783006429672241]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 176/200, Batch 41/86 [D loss: 0.6704630553722382, acc.: 57.08%] [G loss: 0.7777335047721863]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 42/86 [D loss: 0.6718679070472717, acc.: 56.93%] [G loss: 0.7835222482681274]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 176/200, Batch 43/86 [D loss: 0.6715491414070129, acc.: 57.71%] [G loss: 0.7893639802932739]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 176/200, Batch 44/86 [D loss: 0.6737507879734039, acc.: 57.13%] [G loss: 0.7789046764373779]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 45/86 [D loss: 0.679326742887497, acc.: 56.20%] [G loss: 0.7811422944068909]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 176/200, Batch 46/86 [D loss: 0.6706686615943909, acc.: 58.06%] [G loss: 0.786029577255249]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 47/86 [D loss: 0.6700280010700226, acc.: 57.71%] [G loss: 0.7832455635070801]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 48/86 [D loss: 0.6775346100330353, acc.: 56.05%] [G loss: 0.7820031642913818]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 49/86 [D loss: 0.6730522811412811, acc.: 57.13%] [G loss: 0.7650519013404846]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 50/86 [D loss: 0.6800136268138885, acc.: 54.30%] [G loss: 0.7474571466445923]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 51/86 [D loss: 0.681870311498642, acc.: 52.98%] [G loss: 0.7536084055900574]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 52/86 [D loss: 0.6977567076683044, acc.: 51.42%] [G loss: 0.7536420226097107]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 53/86 [D loss: 0.693213552236557, acc.: 51.17%] [G loss: 0.7694270610809326]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 54/86 [D loss: 0.68377885222435, acc.: 54.88%] [G loss: 0.7930862307548523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 176/200, Batch 55/86 [D loss: 0.6689324378967285, acc.: 58.15%] [G loss: 0.8011407256126404]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 56/86 [D loss: 0.6699079871177673, acc.: 57.37%] [G loss: 0.7950879335403442]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 57/86 [D loss: 0.6650327444076538, acc.: 58.20%] [G loss: 0.7963938117027283]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 58/86 [D loss: 0.6646595597267151, acc.: 57.37%] [G loss: 0.7995362877845764]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 59/86 [D loss: 0.6703108847141266, acc.: 58.20%] [G loss: 0.7973686456680298]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 60/86 [D loss: 0.6774565875530243, acc.: 56.79%] [G loss: 0.7889903783798218]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 61/86 [D loss: 0.6677028834819794, acc.: 58.11%] [G loss: 0.7905257344245911]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 62/86 [D loss: 0.6654992699623108, acc.: 57.81%] [G loss: 0.7896420955657959]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 63/86 [D loss: 0.6680005788803101, acc.: 58.84%] [G loss: 0.7841708660125732]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 64/86 [D loss: 0.6667410433292389, acc.: 59.23%] [G loss: 0.780381441116333]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 65/86 [D loss: 0.6666956543922424, acc.: 58.25%] [G loss: 0.794126033782959]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 66/86 [D loss: 0.663888543844223, acc.: 59.23%] [G loss: 0.7876596450805664]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 67/86 [D loss: 0.6710509359836578, acc.: 57.08%] [G loss: 0.7783195376396179]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 68/86 [D loss: 0.6697048544883728, acc.: 57.57%] [G loss: 0.7801662683486938]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 69/86 [D loss: 0.6668815612792969, acc.: 58.89%] [G loss: 0.7718662619590759]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 70/86 [D loss: 0.6674062013626099, acc.: 58.50%] [G loss: 0.7962066531181335]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 71/86 [D loss: 0.6706713140010834, acc.: 57.23%] [G loss: 0.770628809928894]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 176/200, Batch 72/86 [D loss: 0.6676115691661835, acc.: 58.69%] [G loss: 0.7828419208526611]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 73/86 [D loss: 0.6704035103321075, acc.: 57.42%] [G loss: 0.7692350149154663]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 74/86 [D loss: 0.6688474416732788, acc.: 56.54%] [G loss: 0.7906509637832642]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 75/86 [D loss: 0.6669077277183533, acc.: 57.67%] [G loss: 0.785313606262207]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 76/86 [D loss: 0.6733866333961487, acc.: 56.79%] [G loss: 0.7684043049812317]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 77/86 [D loss: 0.666441798210144, acc.: 58.35%] [G loss: 0.790852963924408]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 78/86 [D loss: 0.6684571504592896, acc.: 58.94%] [G loss: 0.7793199419975281]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 79/86 [D loss: 0.6716161072254181, acc.: 55.86%] [G loss: 0.7842803597450256]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 80/86 [D loss: 0.6701163649559021, acc.: 57.47%] [G loss: 0.7814538478851318]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 81/86 [D loss: 0.6679170727729797, acc.: 57.91%] [G loss: 0.7754157781600952]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 176/200, Batch 82/86 [D loss: 0.6782918572425842, acc.: 56.59%] [G loss: 0.7890748381614685]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 83/86 [D loss: 0.6653372049331665, acc.: 59.13%] [G loss: 0.7862859964370728]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 84/86 [D loss: 0.6702094376087189, acc.: 57.52%] [G loss: 0.7778326272964478]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 176/200, Batch 85/86 [D loss: 0.6619389653205872, acc.: 59.52%] [G loss: 0.7748690843582153]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 176/200, Batch 86/86 [D loss: 0.6754708886146545, acc.: 57.13%] [G loss: 0.7815244793891907]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 177/200, Batch 1/86 [D loss: 0.6710427701473236, acc.: 56.05%] [G loss: 0.7916375398635864]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 2/86 [D loss: 0.6684026718139648, acc.: 57.08%] [G loss: 0.7837216258049011]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 3/86 [D loss: 0.6700771152973175, acc.: 57.08%] [G loss: 0.7830921411514282]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 177/200, Batch 4/86 [D loss: 0.6701141893863678, acc.: 57.23%] [G loss: 0.7857314944267273]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 5/86 [D loss: 0.6660134196281433, acc.: 57.96%] [G loss: 0.7874266505241394]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 6/86 [D loss: 0.6752962172031403, acc.: 56.20%] [G loss: 0.7844244241714478]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 7/86 [D loss: 0.6701211035251617, acc.: 57.37%] [G loss: 0.7873241305351257]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 8/86 [D loss: 0.6756491959095001, acc.: 55.47%] [G loss: 0.7884047627449036]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 9/86 [D loss: 0.668262243270874, acc.: 58.98%] [G loss: 0.7879126667976379]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 10/86 [D loss: 0.6666207909584045, acc.: 57.86%] [G loss: 0.7867755889892578]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 11/86 [D loss: 0.6681345105171204, acc.: 58.35%] [G loss: 0.7841206789016724]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 177/200, Batch 12/86 [D loss: 0.6686101257801056, acc.: 57.96%] [G loss: 0.7755361795425415]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 13/86 [D loss: 0.6642357409000397, acc.: 59.33%] [G loss: 0.7791084051132202]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 14/86 [D loss: 0.663764476776123, acc.: 59.72%] [G loss: 0.7747217416763306]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 15/86 [D loss: 0.6661254465579987, acc.: 57.76%] [G loss: 0.7790393829345703]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 16/86 [D loss: 0.6630704700946808, acc.: 59.96%] [G loss: 0.7754353880882263]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 17/86 [D loss: 0.6676137149333954, acc.: 57.76%] [G loss: 0.7873141169548035]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 18/86 [D loss: 0.6672075092792511, acc.: 57.52%] [G loss: 0.7841322422027588]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 19/86 [D loss: 0.6675828993320465, acc.: 57.52%] [G loss: 0.7774871587753296]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 20/86 [D loss: 0.6739329397678375, acc.: 56.49%] [G loss: 0.7763755321502686]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 21/86 [D loss: 0.6730556190013885, acc.: 57.13%] [G loss: 0.7827146649360657]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 177/200, Batch 22/86 [D loss: 0.664420485496521, acc.: 58.98%] [G loss: 0.7787203192710876]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 23/86 [D loss: 0.6728876829147339, acc.: 56.05%] [G loss: 0.7857069373130798]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 24/86 [D loss: 0.6678230464458466, acc.: 58.84%] [G loss: 0.779973030090332]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 25/86 [D loss: 0.6672496199607849, acc.: 58.06%] [G loss: 0.7796133756637573]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 26/86 [D loss: 0.6732083559036255, acc.: 57.23%] [G loss: 0.7735515236854553]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 27/86 [D loss: 0.664311408996582, acc.: 57.91%] [G loss: 0.7736726999282837]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 28/86 [D loss: 0.6638011336326599, acc.: 58.98%] [G loss: 0.7877524495124817]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 29/86 [D loss: 0.6743806600570679, acc.: 58.64%] [G loss: 0.7788636684417725]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 30/86 [D loss: 0.6682031452655792, acc.: 58.45%] [G loss: 0.7895592451095581]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 31/86 [D loss: 0.6700824797153473, acc.: 56.35%] [G loss: 0.7751656770706177]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 32/86 [D loss: 0.6732036471366882, acc.: 57.28%] [G loss: 0.7812585234642029]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 33/86 [D loss: 0.6703003346920013, acc.: 56.84%] [G loss: 0.782062292098999]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 34/86 [D loss: 0.6693026423454285, acc.: 56.93%] [G loss: 0.786115288734436]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 177/200, Batch 35/86 [D loss: 0.6677744388580322, acc.: 58.06%] [G loss: 0.785055935382843]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 36/86 [D loss: 0.6679543256759644, acc.: 58.64%] [G loss: 0.7856000661849976]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 37/86 [D loss: 0.6695626676082611, acc.: 56.64%] [G loss: 0.7793318629264832]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 38/86 [D loss: 0.6696099638938904, acc.: 58.20%] [G loss: 0.784626841545105]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 39/86 [D loss: 0.6684746742248535, acc.: 58.06%] [G loss: 0.7857457995414734]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 40/86 [D loss: 0.6599441766738892, acc.: 60.11%] [G loss: 0.7878311276435852]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 41/86 [D loss: 0.6702146232128143, acc.: 56.98%] [G loss: 0.7851396203041077]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 177/200, Batch 42/86 [D loss: 0.6668222844600677, acc.: 59.23%] [G loss: 0.7843512296676636]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 43/86 [D loss: 0.6679522693157196, acc.: 58.45%] [G loss: 0.7849878668785095]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 44/86 [D loss: 0.6626980006694794, acc.: 59.77%] [G loss: 0.7898069024085999]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 45/86 [D loss: 0.6652348339557648, acc.: 58.15%] [G loss: 0.7828289270401001]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 46/86 [D loss: 0.668000727891922, acc.: 58.15%] [G loss: 0.77877277135849]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 47/86 [D loss: 0.6667066216468811, acc.: 59.03%] [G loss: 0.7836514711380005]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 48/86 [D loss: 0.6651352047920227, acc.: 58.94%] [G loss: 0.7815811634063721]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 49/86 [D loss: 0.6642325520515442, acc.: 58.89%] [G loss: 0.7914478778839111]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 177/200, Batch 50/86 [D loss: 0.669423371553421, acc.: 58.69%] [G loss: 0.7860873937606812]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 51/86 [D loss: 0.6639443337917328, acc.: 59.96%] [G loss: 0.7907226085662842]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 52/86 [D loss: 0.6655019819736481, acc.: 59.18%] [G loss: 0.7924691438674927]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 53/86 [D loss: 0.6645756363868713, acc.: 59.03%] [G loss: 0.7840968370437622]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 54/86 [D loss: 0.6638319194316864, acc.: 58.64%] [G loss: 0.7830226421356201]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 55/86 [D loss: 0.666800856590271, acc.: 58.64%] [G loss: 0.7846026420593262]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 56/86 [D loss: 0.6636885106563568, acc.: 59.52%] [G loss: 0.7907803058624268]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 57/86 [D loss: 0.6691645383834839, acc.: 58.74%] [G loss: 0.782983124256134]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 58/86 [D loss: 0.6737850904464722, acc.: 56.59%] [G loss: 0.7895926833152771]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 59/86 [D loss: 0.6723556518554688, acc.: 57.42%] [G loss: 0.7967637777328491]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 60/86 [D loss: 0.6648015677928925, acc.: 59.62%] [G loss: 0.7874730229377747]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 61/86 [D loss: 0.6726577281951904, acc.: 57.03%] [G loss: 0.7856488227844238]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 177/200, Batch 62/86 [D loss: 0.6663404703140259, acc.: 59.28%] [G loss: 0.7879002690315247]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 63/86 [D loss: 0.6653485298156738, acc.: 58.40%] [G loss: 0.7842910289764404]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 177/200, Batch 64/86 [D loss: 0.6619901657104492, acc.: 59.77%] [G loss: 0.7774283289909363]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 65/86 [D loss: 0.6640940606594086, acc.: 58.15%] [G loss: 0.78805011510849]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 177/200, Batch 66/86 [D loss: 0.670661598443985, acc.: 57.76%] [G loss: 0.7883784770965576]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 67/86 [D loss: 0.6636053323745728, acc.: 59.62%] [G loss: 0.7835840582847595]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 68/86 [D loss: 0.6635288298130035, acc.: 58.64%] [G loss: 0.7845005989074707]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 69/86 [D loss: 0.6671485006809235, acc.: 56.79%] [G loss: 0.7872226238250732]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 70/86 [D loss: 0.6659470200538635, acc.: 58.64%] [G loss: 0.7867701053619385]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 71/86 [D loss: 0.6693499088287354, acc.: 57.23%] [G loss: 0.7855865955352783]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 72/86 [D loss: 0.6665927171707153, acc.: 58.69%] [G loss: 0.7907639741897583]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 73/86 [D loss: 0.6646105051040649, acc.: 59.81%] [G loss: 0.7827104330062866]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 74/86 [D loss: 0.671749085187912, acc.: 57.47%] [G loss: 0.7842438817024231]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 177/200, Batch 75/86 [D loss: 0.6631518602371216, acc.: 59.08%] [G loss: 0.7840629816055298]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 76/86 [D loss: 0.6728652715682983, acc.: 57.52%] [G loss: 0.7873353362083435]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 77/86 [D loss: 0.6677426695823669, acc.: 57.76%] [G loss: 0.7826399207115173]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 177/200, Batch 78/86 [D loss: 0.6659899353981018, acc.: 58.15%] [G loss: 0.7859039902687073]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 79/86 [D loss: 0.6622901558876038, acc.: 59.38%] [G loss: 0.7910126447677612]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 80/86 [D loss: 0.6627346873283386, acc.: 60.16%] [G loss: 0.7852617502212524]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 81/86 [D loss: 0.6665534377098083, acc.: 58.45%] [G loss: 0.7849592566490173]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 82/86 [D loss: 0.6687540411949158, acc.: 57.42%] [G loss: 0.7887343168258667]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 83/86 [D loss: 0.6662916839122772, acc.: 58.20%] [G loss: 0.7831763029098511]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 84/86 [D loss: 0.670741856098175, acc.: 58.50%] [G loss: 0.7850253582000732]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 85/86 [D loss: 0.6556624174118042, acc.: 61.28%] [G loss: 0.7938446402549744]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 177/200, Batch 86/86 [D loss: 0.6716287434101105, acc.: 57.52%] [G loss: 0.7864485383033752]\n",
      "4/4 [==============================] - 0s 22ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 1/86 [D loss: 0.663838267326355, acc.: 59.67%] [G loss: 0.7912718653678894]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 2/86 [D loss: 0.6591282784938812, acc.: 60.55%] [G loss: 0.7767258882522583]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 3/86 [D loss: 0.6654072701931, acc.: 59.33%] [G loss: 0.789408266544342]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 4/86 [D loss: 0.6591987609863281, acc.: 59.52%] [G loss: 0.7837014198303223]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 5/86 [D loss: 0.6597694158554077, acc.: 60.55%] [G loss: 0.7770107984542847]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 6/86 [D loss: 0.6662582159042358, acc.: 58.35%] [G loss: 0.7912278771400452]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 7/86 [D loss: 0.6645352244377136, acc.: 59.13%] [G loss: 0.7827383279800415]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 8/86 [D loss: 0.6664212942123413, acc.: 58.89%] [G loss: 0.7887258529663086]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 9/86 [D loss: 0.6736147999763489, acc.: 56.49%] [G loss: 0.7843764424324036]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 10/86 [D loss: 0.6589944362640381, acc.: 60.16%] [G loss: 0.7846667170524597]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 11/86 [D loss: 0.6624882519245148, acc.: 58.25%] [G loss: 0.777292788028717]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 12/86 [D loss: 0.669880747795105, acc.: 58.25%] [G loss: 0.7900886535644531]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 13/86 [D loss: 0.6644127666950226, acc.: 58.35%] [G loss: 0.7824047207832336]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 14/86 [D loss: 0.6641689538955688, acc.: 59.08%] [G loss: 0.7841262221336365]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 15/86 [D loss: 0.6608415246009827, acc.: 59.57%] [G loss: 0.7834149599075317]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 16/86 [D loss: 0.6717427372932434, acc.: 58.01%] [G loss: 0.7880925536155701]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 17/86 [D loss: 0.6688742637634277, acc.: 57.71%] [G loss: 0.7921105623245239]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 18/86 [D loss: 0.6643843054771423, acc.: 59.13%] [G loss: 0.7927923798561096]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 19/86 [D loss: 0.6625214219093323, acc.: 60.25%] [G loss: 0.799759030342102]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 20/86 [D loss: 0.6615261435508728, acc.: 60.55%] [G loss: 0.7894831299781799]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 21/86 [D loss: 0.6634582281112671, acc.: 59.33%] [G loss: 0.7874789834022522]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 22/86 [D loss: 0.6692389249801636, acc.: 57.91%] [G loss: 0.7821801900863647]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 178/200, Batch 23/86 [D loss: 0.662868320941925, acc.: 58.84%] [G loss: 0.7857150435447693]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 24/86 [D loss: 0.6726571917533875, acc.: 57.67%] [G loss: 0.7904645204544067]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 25/86 [D loss: 0.6639423966407776, acc.: 59.18%] [G loss: 0.7897566556930542]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 26/86 [D loss: 0.6683460772037506, acc.: 58.64%] [G loss: 0.7830560803413391]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 27/86 [D loss: 0.6673011779785156, acc.: 59.33%] [G loss: 0.7929500937461853]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 28/86 [D loss: 0.6614095866680145, acc.: 60.35%] [G loss: 0.7751445770263672]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 29/86 [D loss: 0.6653823852539062, acc.: 58.69%] [G loss: 0.7869695425033569]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 30/86 [D loss: 0.665113240480423, acc.: 58.40%] [G loss: 0.7781408429145813]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 31/86 [D loss: 0.6651938855648041, acc.: 58.64%] [G loss: 0.7892327904701233]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 32/86 [D loss: 0.6635589599609375, acc.: 59.67%] [G loss: 0.7882363796234131]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 33/86 [D loss: 0.6644805669784546, acc.: 59.57%] [G loss: 0.7738033533096313]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 34/86 [D loss: 0.6639828383922577, acc.: 59.91%] [G loss: 0.78952956199646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 35/86 [D loss: 0.669327974319458, acc.: 57.52%] [G loss: 0.7782975435256958]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 36/86 [D loss: 0.6619603633880615, acc.: 59.96%] [G loss: 0.7805954217910767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 37/86 [D loss: 0.6670504808425903, acc.: 57.86%] [G loss: 0.7843365669250488]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 38/86 [D loss: 0.667259156703949, acc.: 57.52%] [G loss: 0.7807093858718872]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 39/86 [D loss: 0.6661453545093536, acc.: 58.54%] [G loss: 0.7690122127532959]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 40/86 [D loss: 0.6677075624465942, acc.: 58.40%] [G loss: 0.7858031988143921]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 41/86 [D loss: 0.6600072681903839, acc.: 60.79%] [G loss: 0.7844845652580261]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 42/86 [D loss: 0.6667457222938538, acc.: 58.74%] [G loss: 0.7770645618438721]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 43/86 [D loss: 0.6676257848739624, acc.: 58.35%] [G loss: 0.7859029769897461]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 44/86 [D loss: 0.6730558574199677, acc.: 56.69%] [G loss: 0.7841325998306274]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 45/86 [D loss: 0.6646171808242798, acc.: 59.52%] [G loss: 0.7929856181144714]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 46/86 [D loss: 0.6590937077999115, acc.: 61.62%] [G loss: 0.7840961813926697]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 47/86 [D loss: 0.6674886643886566, acc.: 58.69%] [G loss: 0.7799306511878967]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 48/86 [D loss: 0.6565234363079071, acc.: 61.23%] [G loss: 0.7801660895347595]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 49/86 [D loss: 0.6695877015590668, acc.: 58.35%] [G loss: 0.7848771214485168]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 50/86 [D loss: 0.6587141752243042, acc.: 59.03%] [G loss: 0.7956586480140686]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 51/86 [D loss: 0.661138504743576, acc.: 58.69%] [G loss: 0.785057008266449]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 52/86 [D loss: 0.6633066833019257, acc.: 58.79%] [G loss: 0.7868518233299255]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 53/86 [D loss: 0.6659026443958282, acc.: 58.30%] [G loss: 0.7886814475059509]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 54/86 [D loss: 0.6697583198547363, acc.: 57.86%] [G loss: 0.7795286774635315]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 55/86 [D loss: 0.6672028601169586, acc.: 58.54%] [G loss: 0.7843345403671265]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 56/86 [D loss: 0.6641174256801605, acc.: 59.57%] [G loss: 0.785981297492981]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 57/86 [D loss: 0.6626310646533966, acc.: 60.21%] [G loss: 0.7906098365783691]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 58/86 [D loss: 0.6665844619274139, acc.: 57.91%] [G loss: 0.7872297763824463]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 59/86 [D loss: 0.6663977205753326, acc.: 59.72%] [G loss: 0.7874642610549927]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 60/86 [D loss: 0.6657537817955017, acc.: 59.47%] [G loss: 0.7863587737083435]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 61/86 [D loss: 0.6648260951042175, acc.: 58.89%] [G loss: 0.7912072539329529]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 62/86 [D loss: 0.6609450876712799, acc.: 60.40%] [G loss: 0.7834257483482361]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 63/86 [D loss: 0.6617953777313232, acc.: 59.72%] [G loss: 0.7913116216659546]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 64/86 [D loss: 0.6610470414161682, acc.: 59.67%] [G loss: 0.7973037958145142]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 65/86 [D loss: 0.6674416065216064, acc.: 58.06%] [G loss: 0.7869388461112976]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 66/86 [D loss: 0.6616941094398499, acc.: 59.52%] [G loss: 0.7796220779418945]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 67/86 [D loss: 0.6648279428482056, acc.: 58.50%] [G loss: 0.7799601554870605]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 178/200, Batch 68/86 [D loss: 0.6585300266742706, acc.: 60.74%] [G loss: 0.7940356135368347]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 69/86 [D loss: 0.668231725692749, acc.: 56.98%] [G loss: 0.7843795418739319]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 70/86 [D loss: 0.6658883094787598, acc.: 59.67%] [G loss: 0.7962048649787903]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 71/86 [D loss: 0.665437251329422, acc.: 59.86%] [G loss: 0.7968403100967407]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 72/86 [D loss: 0.6661711037158966, acc.: 57.71%] [G loss: 0.7902612686157227]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 73/86 [D loss: 0.6647317409515381, acc.: 59.33%] [G loss: 0.7796385884284973]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 74/86 [D loss: 0.6651368141174316, acc.: 58.69%] [G loss: 0.7865428924560547]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 75/86 [D loss: 0.6610879600048065, acc.: 59.18%] [G loss: 0.7931711673736572]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 76/86 [D loss: 0.6643700003623962, acc.: 58.98%] [G loss: 0.7839799523353577]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 77/86 [D loss: 0.6603545546531677, acc.: 59.47%] [G loss: 0.7847853302955627]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 78/86 [D loss: 0.6652115285396576, acc.: 58.50%] [G loss: 0.7876697778701782]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 79/86 [D loss: 0.6664198637008667, acc.: 58.89%] [G loss: 0.7886250615119934]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 80/86 [D loss: 0.6693914234638214, acc.: 57.76%] [G loss: 0.7880954742431641]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 81/86 [D loss: 0.6611812710762024, acc.: 60.35%] [G loss: 0.7884262800216675]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 82/86 [D loss: 0.6605351567268372, acc.: 59.67%] [G loss: 0.791020393371582]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 83/86 [D loss: 0.6590310335159302, acc.: 60.16%] [G loss: 0.7981444597244263]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 84/86 [D loss: 0.6684356331825256, acc.: 58.15%] [G loss: 0.7836526036262512]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 178/200, Batch 85/86 [D loss: 0.6622260808944702, acc.: 60.01%] [G loss: 0.7830498218536377]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 178/200, Batch 86/86 [D loss: 0.6699183583259583, acc.: 58.79%] [G loss: 0.7896500825881958]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 179/200, Batch 1/86 [D loss: 0.6701550483703613, acc.: 57.47%] [G loss: 0.7936556935310364]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 2/86 [D loss: 0.6672315001487732, acc.: 58.25%] [G loss: 0.7986249923706055]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 3/86 [D loss: 0.6659075915813446, acc.: 59.18%] [G loss: 0.7962218523025513]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 4/86 [D loss: 0.6606533229351044, acc.: 59.81%] [G loss: 0.7986548542976379]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 5/86 [D loss: 0.6609019339084625, acc.: 59.57%] [G loss: 0.7891498804092407]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 6/86 [D loss: 0.661190003156662, acc.: 59.38%] [G loss: 0.7875750064849854]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 7/86 [D loss: 0.6549502015113831, acc.: 61.13%] [G loss: 0.7862472534179688]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 8/86 [D loss: 0.6520921587944031, acc.: 63.77%] [G loss: 0.7897934913635254]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 179/200, Batch 9/86 [D loss: 0.6626597940921783, acc.: 59.96%] [G loss: 0.7870141267776489]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 179/200, Batch 10/86 [D loss: 0.664959043264389, acc.: 59.91%] [G loss: 0.7926120758056641]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 179/200, Batch 11/86 [D loss: 0.6642439961433411, acc.: 59.52%] [G loss: 0.792299211025238]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 12/86 [D loss: 0.662310928106308, acc.: 58.64%] [G loss: 0.7863203287124634]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 13/86 [D loss: 0.6639216840267181, acc.: 59.81%] [G loss: 0.7839831113815308]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 14/86 [D loss: 0.662632018327713, acc.: 59.77%] [G loss: 0.7929163575172424]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 179/200, Batch 15/86 [D loss: 0.6677805185317993, acc.: 58.54%] [G loss: 0.797925591468811]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 16/86 [D loss: 0.6632077395915985, acc.: 59.33%] [G loss: 0.7931026220321655]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 17/86 [D loss: 0.6643026769161224, acc.: 58.40%] [G loss: 0.7799261808395386]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 18/86 [D loss: 0.6593421399593353, acc.: 59.72%] [G loss: 0.7908901572227478]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 179/200, Batch 19/86 [D loss: 0.6598847806453705, acc.: 62.06%] [G loss: 0.7840033769607544]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 20/86 [D loss: 0.6649506986141205, acc.: 59.42%] [G loss: 0.7847256064414978]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 179/200, Batch 21/86 [D loss: 0.6616669595241547, acc.: 59.77%] [G loss: 0.7825602889060974]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 22/86 [D loss: 0.6595868170261383, acc.: 59.72%] [G loss: 0.7856072187423706]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 23/86 [D loss: 0.6672486364841461, acc.: 58.25%] [G loss: 0.8045881390571594]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 24/86 [D loss: 0.6681138873100281, acc.: 59.18%] [G loss: 0.7909024953842163]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 25/86 [D loss: 0.6629236340522766, acc.: 60.01%] [G loss: 0.7832961082458496]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 26/86 [D loss: 0.6614318788051605, acc.: 59.96%] [G loss: 0.7893360257148743]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 27/86 [D loss: 0.6648442149162292, acc.: 58.64%] [G loss: 0.778875470161438]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 28/86 [D loss: 0.6623147130012512, acc.: 59.13%] [G loss: 0.7897584438323975]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 29/86 [D loss: 0.660537451505661, acc.: 59.72%] [G loss: 0.795007050037384]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 30/86 [D loss: 0.6627226769924164, acc.: 60.21%] [G loss: 0.7988799214363098]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 31/86 [D loss: 0.6675828397274017, acc.: 57.71%] [G loss: 0.7948046326637268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 179/200, Batch 32/86 [D loss: 0.661888062953949, acc.: 59.33%] [G loss: 0.7990719676017761]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 33/86 [D loss: 0.6628193855285645, acc.: 59.86%] [G loss: 0.7947354316711426]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 34/86 [D loss: 0.664456695318222, acc.: 59.23%] [G loss: 0.7850637435913086]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 35/86 [D loss: 0.6668645143508911, acc.: 59.18%] [G loss: 0.7857881784439087]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 36/86 [D loss: 0.66872239112854, acc.: 58.01%] [G loss: 0.7876541614532471]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 37/86 [D loss: 0.6650450825691223, acc.: 59.47%] [G loss: 0.7849931716918945]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 179/200, Batch 38/86 [D loss: 0.6612849235534668, acc.: 60.11%] [G loss: 0.7797051072120667]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 179/200, Batch 39/86 [D loss: 0.6645006835460663, acc.: 58.84%] [G loss: 0.7876628041267395]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 40/86 [D loss: 0.6639722883701324, acc.: 59.33%] [G loss: 0.7938363552093506]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 41/86 [D loss: 0.6679423749446869, acc.: 58.30%] [G loss: 0.7940853834152222]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 42/86 [D loss: 0.6677525639533997, acc.: 58.54%] [G loss: 0.7742509841918945]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 43/86 [D loss: 0.6559585332870483, acc.: 61.67%] [G loss: 0.7967439889907837]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 44/86 [D loss: 0.6621188521385193, acc.: 57.76%] [G loss: 0.7902102470397949]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 45/86 [D loss: 0.658124566078186, acc.: 59.72%] [G loss: 0.7882143259048462]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 46/86 [D loss: 0.6616375744342804, acc.: 61.23%] [G loss: 0.7978209853172302]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 47/86 [D loss: 0.6580088436603546, acc.: 58.69%] [G loss: 0.7858884334564209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 179/200, Batch 48/86 [D loss: 0.6684001088142395, acc.: 57.52%] [G loss: 0.7815893292427063]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 179/200, Batch 49/86 [D loss: 0.6687364876270294, acc.: 57.37%] [G loss: 0.787227988243103]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 50/86 [D loss: 0.6591309607028961, acc.: 61.18%] [G loss: 0.7867392301559448]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 51/86 [D loss: 0.662909597158432, acc.: 59.96%] [G loss: 0.7808703184127808]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 52/86 [D loss: 0.6657410562038422, acc.: 57.81%] [G loss: 0.7909538745880127]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 53/86 [D loss: 0.6559971868991852, acc.: 60.89%] [G loss: 0.7789273262023926]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 54/86 [D loss: 0.665409505367279, acc.: 57.86%] [G loss: 0.7862318158149719]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 55/86 [D loss: 0.6596508026123047, acc.: 58.84%] [G loss: 0.7864983677864075]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 56/86 [D loss: 0.6601190865039825, acc.: 59.67%] [G loss: 0.7944371700286865]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 57/86 [D loss: 0.6636249423027039, acc.: 59.77%] [G loss: 0.7928555607795715]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 58/86 [D loss: 0.6600066125392914, acc.: 60.89%] [G loss: 0.7953327894210815]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 59/86 [D loss: 0.6557551324367523, acc.: 61.38%] [G loss: 0.7929220199584961]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 60/86 [D loss: 0.6586295366287231, acc.: 61.04%] [G loss: 0.7961927056312561]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 179/200, Batch 61/86 [D loss: 0.6623929738998413, acc.: 59.57%] [G loss: 0.7917647361755371]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 62/86 [D loss: 0.6652823686599731, acc.: 58.11%] [G loss: 0.800371527671814]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 179/200, Batch 63/86 [D loss: 0.6652086675167084, acc.: 58.50%] [G loss: 0.7854493856430054]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 64/86 [D loss: 0.663568913936615, acc.: 59.62%] [G loss: 0.790133535861969]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 65/86 [D loss: 0.6614512801170349, acc.: 59.62%] [G loss: 0.7961610555648804]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 66/86 [D loss: 0.6590792238712311, acc.: 60.01%] [G loss: 0.7858229875564575]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 67/86 [D loss: 0.6615320146083832, acc.: 59.72%] [G loss: 0.7994537949562073]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 68/86 [D loss: 0.655833512544632, acc.: 60.89%] [G loss: 0.7839594483375549]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 69/86 [D loss: 0.6544798314571381, acc.: 61.82%] [G loss: 0.7851058840751648]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 70/86 [D loss: 0.6559552252292633, acc.: 62.01%] [G loss: 0.7933843731880188]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 71/86 [D loss: 0.6651639044284821, acc.: 58.84%] [G loss: 0.791035532951355]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 179/200, Batch 72/86 [D loss: 0.6561608612537384, acc.: 60.55%] [G loss: 0.7868692874908447]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 73/86 [D loss: 0.6634402871131897, acc.: 58.25%] [G loss: 0.7996633052825928]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 74/86 [D loss: 0.6588785350322723, acc.: 60.25%] [G loss: 0.7934519052505493]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 75/86 [D loss: 0.6677810251712799, acc.: 58.69%] [G loss: 0.7889848947525024]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 76/86 [D loss: 0.6648248136043549, acc.: 58.45%] [G loss: 0.7797660827636719]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 77/86 [D loss: 0.6651820540428162, acc.: 58.25%] [G loss: 0.7984296679496765]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 179/200, Batch 78/86 [D loss: 0.6656793653964996, acc.: 57.86%] [G loss: 0.7828755974769592]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 79/86 [D loss: 0.6668590903282166, acc.: 59.33%] [G loss: 0.7919586300849915]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 80/86 [D loss: 0.6631218194961548, acc.: 59.62%] [G loss: 0.7950292229652405]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 179/200, Batch 81/86 [D loss: 0.6642181873321533, acc.: 58.98%] [G loss: 0.7912455201148987]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 82/86 [D loss: 0.6622656583786011, acc.: 59.77%] [G loss: 0.7949426770210266]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 83/86 [D loss: 0.6624679565429688, acc.: 59.18%] [G loss: 0.7973729968070984]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 84/86 [D loss: 0.6622129082679749, acc.: 60.55%] [G loss: 0.7893005609512329]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 85/86 [D loss: 0.667345404624939, acc.: 57.76%] [G loss: 0.7907973527908325]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 179/200, Batch 86/86 [D loss: 0.661048412322998, acc.: 60.21%] [G loss: 0.7903367280960083]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 180/200, Batch 1/86 [D loss: 0.662535697221756, acc.: 59.33%] [G loss: 0.7834543585777283]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 2/86 [D loss: 0.6590902507305145, acc.: 59.77%] [G loss: 0.7868449687957764]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 3/86 [D loss: 0.6640365719795227, acc.: 58.74%] [G loss: 0.7982324957847595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 4/86 [D loss: 0.6599167585372925, acc.: 59.57%] [G loss: 0.7916074395179749]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 5/86 [D loss: 0.6608210802078247, acc.: 60.84%] [G loss: 0.7695795297622681]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 6/86 [D loss: 0.6630218923091888, acc.: 59.08%] [G loss: 0.7898582816123962]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 7/86 [D loss: 0.6537639200687408, acc.: 62.45%] [G loss: 0.7866898775100708]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 8/86 [D loss: 0.6578819751739502, acc.: 61.08%] [G loss: 0.7743330001831055]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 9/86 [D loss: 0.6612294316291809, acc.: 60.89%] [G loss: 0.790827751159668]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 10/86 [D loss: 0.6633095145225525, acc.: 58.69%] [G loss: 0.7941952347755432]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 11/86 [D loss: 0.664277970790863, acc.: 59.67%] [G loss: 0.7886122465133667]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 12/86 [D loss: 0.6627149283885956, acc.: 60.25%] [G loss: 0.783347487449646]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 13/86 [D loss: 0.6673097908496857, acc.: 59.13%] [G loss: 0.78369140625]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 14/86 [D loss: 0.6628820896148682, acc.: 59.77%] [G loss: 0.7906994223594666]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 15/86 [D loss: 0.6637953221797943, acc.: 58.40%] [G loss: 0.7991127967834473]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 16/86 [D loss: 0.6688172221183777, acc.: 58.06%] [G loss: 0.7918375134468079]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 17/86 [D loss: 0.6620306670665741, acc.: 58.59%] [G loss: 0.7821841239929199]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 18/86 [D loss: 0.6650681495666504, acc.: 58.94%] [G loss: 0.7810021638870239]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 19/86 [D loss: 0.6606194972991943, acc.: 60.11%] [G loss: 0.7904762029647827]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 20/86 [D loss: 0.666905015707016, acc.: 58.40%] [G loss: 0.7940707206726074]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 21/86 [D loss: 0.6556736826896667, acc.: 62.74%] [G loss: 0.789703369140625]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 22/86 [D loss: 0.6622596681118011, acc.: 60.21%] [G loss: 0.8003360033035278]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 23/86 [D loss: 0.6606689393520355, acc.: 60.30%] [G loss: 0.7860634326934814]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 24/86 [D loss: 0.6574403941631317, acc.: 60.89%] [G loss: 0.7931060194969177]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 25/86 [D loss: 0.659707099199295, acc.: 59.57%] [G loss: 0.7904989719390869]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 26/86 [D loss: 0.6591605842113495, acc.: 60.89%] [G loss: 0.8019877076148987]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 27/86 [D loss: 0.663962334394455, acc.: 58.15%] [G loss: 0.795072078704834]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 28/86 [D loss: 0.6665935516357422, acc.: 59.47%] [G loss: 0.7815324068069458]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 29/86 [D loss: 0.6602904200553894, acc.: 58.59%] [G loss: 0.79090416431427]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 30/86 [D loss: 0.6593291163444519, acc.: 59.13%] [G loss: 0.7828302383422852]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 31/86 [D loss: 0.6577324569225311, acc.: 60.94%] [G loss: 0.7828413844108582]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 32/86 [D loss: 0.6638079583644867, acc.: 60.30%] [G loss: 0.7959804534912109]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 33/86 [D loss: 0.6619731783866882, acc.: 60.40%] [G loss: 0.7930831909179688]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 34/86 [D loss: 0.6623291969299316, acc.: 59.96%] [G loss: 0.7846284508705139]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 35/86 [D loss: 0.6584392786026001, acc.: 59.33%] [G loss: 0.7856848239898682]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 36/86 [D loss: 0.6605005264282227, acc.: 60.16%] [G loss: 0.7922267317771912]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 37/86 [D loss: 0.6595802307128906, acc.: 60.60%] [G loss: 0.7866246700286865]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 38/86 [D loss: 0.6571681201457977, acc.: 60.84%] [G loss: 0.7910656332969666]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 39/86 [D loss: 0.6654911637306213, acc.: 59.08%] [G loss: 0.7873932719230652]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 40/86 [D loss: 0.6599535942077637, acc.: 59.42%] [G loss: 0.7794734239578247]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 41/86 [D loss: 0.6631745398044586, acc.: 58.94%] [G loss: 0.7985284328460693]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 42/86 [D loss: 0.6694530844688416, acc.: 58.25%] [G loss: 0.7873028516769409]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 180/200, Batch 43/86 [D loss: 0.661575198173523, acc.: 59.91%] [G loss: 0.7964582443237305]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 44/86 [D loss: 0.663407176733017, acc.: 59.81%] [G loss: 0.7782112956047058]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 45/86 [D loss: 0.6650055050849915, acc.: 58.35%] [G loss: 0.7840943336486816]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 46/86 [D loss: 0.6545415818691254, acc.: 60.60%] [G loss: 0.7849026322364807]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 47/86 [D loss: 0.6625365316867828, acc.: 59.28%] [G loss: 0.7903276681900024]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 48/86 [D loss: 0.6637780368328094, acc.: 59.86%] [G loss: 0.7910621166229248]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 49/86 [D loss: 0.6596408188343048, acc.: 60.50%] [G loss: 0.7876614332199097]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 50/86 [D loss: 0.6725541949272156, acc.: 58.40%] [G loss: 0.781558632850647]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 51/86 [D loss: 0.6620195508003235, acc.: 59.33%] [G loss: 0.7860070466995239]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 52/86 [D loss: 0.6569925546646118, acc.: 61.23%] [G loss: 0.7959091067314148]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 180/200, Batch 53/86 [D loss: 0.6625179946422577, acc.: 59.96%] [G loss: 0.798900306224823]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 54/86 [D loss: 0.6564218997955322, acc.: 61.08%] [G loss: 0.7952957153320312]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 55/86 [D loss: 0.6625422537326813, acc.: 59.47%] [G loss: 0.777852475643158]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 56/86 [D loss: 0.6539779305458069, acc.: 61.91%] [G loss: 0.7866143584251404]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 57/86 [D loss: 0.6630387604236603, acc.: 59.03%] [G loss: 0.7882652878761292]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 58/86 [D loss: 0.6609569787979126, acc.: 60.74%] [G loss: 0.7981193661689758]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 59/86 [D loss: 0.6681218147277832, acc.: 58.25%] [G loss: 0.7921384572982788]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 60/86 [D loss: 0.6599337160587311, acc.: 59.47%] [G loss: 0.7862963080406189]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 61/86 [D loss: 0.6620426177978516, acc.: 60.16%] [G loss: 0.7792832851409912]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 62/86 [D loss: 0.6622757911682129, acc.: 59.67%] [G loss: 0.7971382737159729]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 63/86 [D loss: 0.6688408553600311, acc.: 58.11%] [G loss: 0.7777426242828369]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 64/86 [D loss: 0.6638325154781342, acc.: 58.94%] [G loss: 0.791873037815094]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 65/86 [D loss: 0.6599895358085632, acc.: 59.18%] [G loss: 0.7858622074127197]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 66/86 [D loss: 0.6600965857505798, acc.: 59.08%] [G loss: 0.7845271229743958]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 180/200, Batch 67/86 [D loss: 0.660108357667923, acc.: 60.60%] [G loss: 0.7821018099784851]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 68/86 [D loss: 0.6673692762851715, acc.: 58.54%] [G loss: 0.7830383777618408]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 69/86 [D loss: 0.6655340194702148, acc.: 58.11%] [G loss: 0.7904679179191589]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 70/86 [D loss: 0.6559596061706543, acc.: 61.04%] [G loss: 0.7843651175498962]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 71/86 [D loss: 0.658850908279419, acc.: 59.42%] [G loss: 0.785097062587738]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 72/86 [D loss: 0.6631976068019867, acc.: 59.77%] [G loss: 0.7874309420585632]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 73/86 [D loss: 0.6636471152305603, acc.: 59.18%] [G loss: 0.7912259101867676]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 74/86 [D loss: 0.6565656960010529, acc.: 59.47%] [G loss: 0.7804656028747559]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 75/86 [D loss: 0.6652943789958954, acc.: 57.62%] [G loss: 0.7835431098937988]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 76/86 [D loss: 0.6574034094810486, acc.: 61.38%] [G loss: 0.7848418354988098]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 77/86 [D loss: 0.6642255783081055, acc.: 58.74%] [G loss: 0.795298159122467]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 78/86 [D loss: 0.6586484909057617, acc.: 61.52%] [G loss: 0.7894150018692017]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 79/86 [D loss: 0.6661487519741058, acc.: 58.50%] [G loss: 0.7884624600410461]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 80/86 [D loss: 0.6590458452701569, acc.: 59.47%] [G loss: 0.7900491952896118]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 81/86 [D loss: 0.666276603937149, acc.: 58.84%] [G loss: 0.7981101870536804]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 82/86 [D loss: 0.6580861508846283, acc.: 60.94%] [G loss: 0.7876646518707275]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 180/200, Batch 83/86 [D loss: 0.6632419228553772, acc.: 59.57%] [G loss: 0.796966016292572]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 180/200, Batch 84/86 [D loss: 0.6638713479042053, acc.: 59.57%] [G loss: 0.798268735408783]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 85/86 [D loss: 0.6655579805374146, acc.: 58.74%] [G loss: 0.7818044424057007]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 180/200, Batch 86/86 [D loss: 0.6651669144630432, acc.: 60.60%] [G loss: 0.7953709363937378]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 1/86 [D loss: 0.6641755700111389, acc.: 58.15%] [G loss: 0.8106054067611694]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 2/86 [D loss: 0.6588671207427979, acc.: 60.69%] [G loss: 0.8008613586425781]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 3/86 [D loss: 0.6617015898227692, acc.: 59.47%] [G loss: 0.7739678025245667]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 4/86 [D loss: 0.666795551776886, acc.: 57.91%] [G loss: 0.7854684591293335]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 5/86 [D loss: 0.6639041900634766, acc.: 59.96%] [G loss: 0.799598217010498]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 6/86 [D loss: 0.6595673561096191, acc.: 60.45%] [G loss: 0.7876065373420715]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 7/86 [D loss: 0.6597041189670563, acc.: 60.45%] [G loss: 0.7886450290679932]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 8/86 [D loss: 0.6653878390789032, acc.: 58.35%] [G loss: 0.7932277917861938]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 9/86 [D loss: 0.6641684174537659, acc.: 59.96%] [G loss: 0.7963953614234924]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 10/86 [D loss: 0.6592617630958557, acc.: 61.18%] [G loss: 0.7921338677406311]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 181/200, Batch 11/86 [D loss: 0.6632632613182068, acc.: 59.33%] [G loss: 0.7949073910713196]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 12/86 [D loss: 0.6656632423400879, acc.: 58.40%] [G loss: 0.799440324306488]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 13/86 [D loss: 0.6584509909152985, acc.: 60.60%] [G loss: 0.8031967878341675]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 14/86 [D loss: 0.661972314119339, acc.: 58.84%] [G loss: 0.800295889377594]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 15/86 [D loss: 0.6554363071918488, acc.: 61.52%] [G loss: 0.7895157933235168]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 16/86 [D loss: 0.6687649190425873, acc.: 57.96%] [G loss: 0.8028478622436523]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 17/86 [D loss: 0.6570969820022583, acc.: 60.25%] [G loss: 0.785689115524292]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 18/86 [D loss: 0.6629995703697205, acc.: 59.91%] [G loss: 0.795279324054718]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 19/86 [D loss: 0.6579576432704926, acc.: 60.64%] [G loss: 0.7909911870956421]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 20/86 [D loss: 0.6611118316650391, acc.: 59.72%] [G loss: 0.7913481593132019]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 21/86 [D loss: 0.6640319526195526, acc.: 58.50%] [G loss: 0.784845769405365]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 22/86 [D loss: 0.6635543704032898, acc.: 59.57%] [G loss: 0.7870927453041077]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 23/86 [D loss: 0.6574099659919739, acc.: 59.96%] [G loss: 0.7826060056686401]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 24/86 [D loss: 0.664070188999176, acc.: 58.94%] [G loss: 0.790191650390625]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 25/86 [D loss: 0.6642784178256989, acc.: 59.72%] [G loss: 0.7871672511100769]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 26/86 [D loss: 0.6537224352359772, acc.: 61.57%] [G loss: 0.7854307889938354]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 27/86 [D loss: 0.6576060652732849, acc.: 60.84%] [G loss: 0.7914458513259888]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 28/86 [D loss: 0.6692735254764557, acc.: 57.71%] [G loss: 0.7854493260383606]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 29/86 [D loss: 0.661218672990799, acc.: 59.13%] [G loss: 0.7891579270362854]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 30/86 [D loss: 0.6581704020500183, acc.: 60.30%] [G loss: 0.7850280404090881]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 31/86 [D loss: 0.6632784307003021, acc.: 60.21%] [G loss: 0.7936069369316101]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 32/86 [D loss: 0.6632740199565887, acc.: 59.38%] [G loss: 0.7917382717132568]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 33/86 [D loss: 0.6570537090301514, acc.: 60.21%] [G loss: 0.8000258207321167]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 181/200, Batch 34/86 [D loss: 0.6574609875679016, acc.: 59.91%] [G loss: 0.8002579212188721]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 35/86 [D loss: 0.6625587940216064, acc.: 59.81%] [G loss: 0.7938065528869629]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 36/86 [D loss: 0.6616254150867462, acc.: 59.72%] [G loss: 0.7910956144332886]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 37/86 [D loss: 0.6631313860416412, acc.: 59.72%] [G loss: 0.7841964960098267]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 38/86 [D loss: 0.6570591330528259, acc.: 60.30%] [G loss: 0.7887105345726013]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 39/86 [D loss: 0.6583266854286194, acc.: 61.57%] [G loss: 0.7980268597602844]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 40/86 [D loss: 0.6554171442985535, acc.: 61.57%] [G loss: 0.7971144914627075]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 41/86 [D loss: 0.6615646779537201, acc.: 60.01%] [G loss: 0.8030303120613098]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 42/86 [D loss: 0.6584310829639435, acc.: 60.64%] [G loss: 0.7870616912841797]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 43/86 [D loss: 0.6567484140396118, acc.: 60.64%] [G loss: 0.7892748117446899]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 44/86 [D loss: 0.6582203209400177, acc.: 60.45%] [G loss: 0.7930358648300171]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 45/86 [D loss: 0.660175621509552, acc.: 59.13%] [G loss: 0.7902791500091553]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 46/86 [D loss: 0.6592950820922852, acc.: 59.28%] [G loss: 0.7898573875427246]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 47/86 [D loss: 0.6586255431175232, acc.: 61.28%] [G loss: 0.7835847735404968]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 48/86 [D loss: 0.6553566157817841, acc.: 60.94%] [G loss: 0.7865080237388611]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 49/86 [D loss: 0.6605135202407837, acc.: 61.18%] [G loss: 0.7832844257354736]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 50/86 [D loss: 0.6642149090766907, acc.: 59.47%] [G loss: 0.785254716873169]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 51/86 [D loss: 0.6608271598815918, acc.: 59.91%] [G loss: 0.7889789342880249]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 52/86 [D loss: 0.6598160266876221, acc.: 60.21%] [G loss: 0.7899397611618042]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 53/86 [D loss: 0.6586881875991821, acc.: 59.28%] [G loss: 0.7977358102798462]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 54/86 [D loss: 0.6580784320831299, acc.: 60.40%] [G loss: 0.7916191816329956]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 55/86 [D loss: 0.6586893200874329, acc.: 58.25%] [G loss: 0.782507598400116]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 56/86 [D loss: 0.6630291640758514, acc.: 61.04%] [G loss: 0.7815684080123901]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 181/200, Batch 57/86 [D loss: 0.6592109799385071, acc.: 59.47%] [G loss: 0.788875937461853]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 181/200, Batch 58/86 [D loss: 0.6627650260925293, acc.: 58.89%] [G loss: 0.7807607054710388]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 59/86 [D loss: 0.6591463088989258, acc.: 60.16%] [G loss: 0.7906794548034668]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 60/86 [D loss: 0.6623929440975189, acc.: 59.67%] [G loss: 0.7913022637367249]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 61/86 [D loss: 0.662516325712204, acc.: 60.35%] [G loss: 0.7991955876350403]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 62/86 [D loss: 0.6534935235977173, acc.: 62.16%] [G loss: 0.7947080135345459]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 181/200, Batch 63/86 [D loss: 0.6564293205738068, acc.: 60.69%] [G loss: 0.7815191149711609]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 64/86 [D loss: 0.6652912795543671, acc.: 58.35%] [G loss: 0.7879111766815186]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 65/86 [D loss: 0.6630370914936066, acc.: 59.28%] [G loss: 0.7976177334785461]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 66/86 [D loss: 0.6589157283306122, acc.: 61.38%] [G loss: 0.7907587885856628]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 67/86 [D loss: 0.6596370935440063, acc.: 61.08%] [G loss: 0.7881307005882263]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 68/86 [D loss: 0.6697373390197754, acc.: 57.42%] [G loss: 0.7954452037811279]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 69/86 [D loss: 0.6618398129940033, acc.: 58.40%] [G loss: 0.7893821001052856]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 181/200, Batch 70/86 [D loss: 0.6597480177879333, acc.: 61.18%] [G loss: 0.7974045276641846]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 71/86 [D loss: 0.6616389155387878, acc.: 59.18%] [G loss: 0.798469603061676]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 72/86 [D loss: 0.6630382835865021, acc.: 58.25%] [G loss: 0.7894236445426941]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 73/86 [D loss: 0.661584734916687, acc.: 59.62%] [G loss: 0.7842310070991516]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 74/86 [D loss: 0.660736620426178, acc.: 60.35%] [G loss: 0.7878917455673218]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 75/86 [D loss: 0.663741946220398, acc.: 59.18%] [G loss: 0.7978300452232361]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 76/86 [D loss: 0.6559590399265289, acc.: 61.23%] [G loss: 0.7935070991516113]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 77/86 [D loss: 0.6620565950870514, acc.: 60.16%] [G loss: 0.787135660648346]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 78/86 [D loss: 0.6642414331436157, acc.: 59.52%] [G loss: 0.7930833697319031]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 79/86 [D loss: 0.6646625697612762, acc.: 59.42%] [G loss: 0.7958986759185791]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 80/86 [D loss: 0.6559396088123322, acc.: 60.35%] [G loss: 0.7922494411468506]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 81/86 [D loss: 0.6665626764297485, acc.: 59.03%] [G loss: 0.804551362991333]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 181/200, Batch 82/86 [D loss: 0.6636182367801666, acc.: 60.64%] [G loss: 0.7872839570045471]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 83/86 [D loss: 0.6678407490253448, acc.: 58.01%] [G loss: 0.796735942363739]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 84/86 [D loss: 0.6579564809799194, acc.: 61.52%] [G loss: 0.7847222089767456]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 181/200, Batch 85/86 [D loss: 0.656778872013092, acc.: 61.33%] [G loss: 0.779741108417511]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 181/200, Batch 86/86 [D loss: 0.6536622941493988, acc.: 62.01%] [G loss: 0.7903478741645813]\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 182/200, Batch 1/86 [D loss: 0.6587408483028412, acc.: 60.60%] [G loss: 0.8009606003761292]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 182/200, Batch 2/86 [D loss: 0.6655746400356293, acc.: 59.57%] [G loss: 0.7928667664527893]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 3/86 [D loss: 0.6585933268070221, acc.: 60.25%] [G loss: 0.7952568531036377]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 182/200, Batch 4/86 [D loss: 0.6595326066017151, acc.: 60.25%] [G loss: 0.7815640568733215]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 182/200, Batch 5/86 [D loss: 0.6639730334281921, acc.: 58.59%] [G loss: 0.7989981174468994]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 6/86 [D loss: 0.6610848009586334, acc.: 59.03%] [G loss: 0.7849900126457214]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 7/86 [D loss: 0.6573917269706726, acc.: 60.40%] [G loss: 0.7969123721122742]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 8/86 [D loss: 0.6566725969314575, acc.: 61.47%] [G loss: 0.7872606515884399]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 9/86 [D loss: 0.6631960272789001, acc.: 60.35%] [G loss: 0.7830358147621155]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 10/86 [D loss: 0.6592774987220764, acc.: 61.13%] [G loss: 0.7903790473937988]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 11/86 [D loss: 0.6570588946342468, acc.: 60.11%] [G loss: 0.7875155806541443]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 12/86 [D loss: 0.6560664474964142, acc.: 61.47%] [G loss: 0.7979671359062195]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 13/86 [D loss: 0.6589415073394775, acc.: 60.35%] [G loss: 0.7912914752960205]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 14/86 [D loss: 0.6654125452041626, acc.: 59.42%] [G loss: 0.7912372350692749]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 15/86 [D loss: 0.6639167368412018, acc.: 59.42%] [G loss: 0.796547532081604]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 16/86 [D loss: 0.6597793400287628, acc.: 59.42%] [G loss: 0.7913166284561157]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 17/86 [D loss: 0.662384420633316, acc.: 58.94%] [G loss: 0.7919711470603943]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 18/86 [D loss: 0.6595085263252258, acc.: 60.11%] [G loss: 0.7941524386405945]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 19/86 [D loss: 0.6586725115776062, acc.: 60.35%] [G loss: 0.7892657518386841]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 20/86 [D loss: 0.6604886054992676, acc.: 60.50%] [G loss: 0.7811980247497559]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 21/86 [D loss: 0.6695415675640106, acc.: 58.94%] [G loss: 0.7749427556991577]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 22/86 [D loss: 0.6599657237529755, acc.: 60.25%] [G loss: 0.7900912165641785]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 23/86 [D loss: 0.6653662025928497, acc.: 59.77%] [G loss: 0.7782791256904602]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 24/86 [D loss: 0.6627210080623627, acc.: 58.06%] [G loss: 0.7858785390853882]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 182/200, Batch 25/86 [D loss: 0.6615433990955353, acc.: 58.94%] [G loss: 0.7810776233673096]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 182/200, Batch 26/86 [D loss: 0.6567201614379883, acc.: 60.64%] [G loss: 0.8065500259399414]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 182/200, Batch 27/86 [D loss: 0.6623501181602478, acc.: 59.57%] [G loss: 0.7823888063430786]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 28/86 [D loss: 0.6545902192592621, acc.: 62.06%] [G loss: 0.7884430885314941]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 182/200, Batch 29/86 [D loss: 0.6654098331928253, acc.: 58.45%] [G loss: 0.7911292910575867]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 30/86 [D loss: 0.6637222766876221, acc.: 59.81%] [G loss: 0.7788261771202087]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 31/86 [D loss: 0.6656379699707031, acc.: 58.98%] [G loss: 0.8055449724197388]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 182/200, Batch 32/86 [D loss: 0.6561481654644012, acc.: 61.38%] [G loss: 0.7819236516952515]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 33/86 [D loss: 0.6693057715892792, acc.: 57.67%] [G loss: 0.7821860909461975]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 34/86 [D loss: 0.6626758277416229, acc.: 59.42%] [G loss: 0.784884512424469]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 35/86 [D loss: 0.6571747362613678, acc.: 61.91%] [G loss: 0.7862565517425537]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 36/86 [D loss: 0.6509475708007812, acc.: 61.62%] [G loss: 0.7930033206939697]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 37/86 [D loss: 0.6625243425369263, acc.: 59.28%] [G loss: 0.7953181862831116]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 182/200, Batch 38/86 [D loss: 0.65785151720047, acc.: 62.11%] [G loss: 0.797362208366394]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 39/86 [D loss: 0.6603860855102539, acc.: 60.35%] [G loss: 0.7920563817024231]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 40/86 [D loss: 0.6646786034107208, acc.: 60.11%] [G loss: 0.7829586267471313]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 41/86 [D loss: 0.6594883501529694, acc.: 59.47%] [G loss: 0.7997683882713318]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 42/86 [D loss: 0.6596797704696655, acc.: 60.30%] [G loss: 0.7968507409095764]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 43/86 [D loss: 0.6526903212070465, acc.: 62.50%] [G loss: 0.7935458421707153]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 182/200, Batch 44/86 [D loss: 0.6569858193397522, acc.: 60.21%] [G loss: 0.7879075407981873]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 45/86 [D loss: 0.6684129536151886, acc.: 58.94%] [G loss: 0.7880245447158813]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 46/86 [D loss: 0.6590049564838409, acc.: 59.91%] [G loss: 0.7878470420837402]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 182/200, Batch 47/86 [D loss: 0.6570773720741272, acc.: 60.94%] [G loss: 0.7851565480232239]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 48/86 [D loss: 0.6599291563034058, acc.: 60.60%] [G loss: 0.7941713333129883]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 49/86 [D loss: 0.6616052389144897, acc.: 58.45%] [G loss: 0.7817153334617615]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 50/86 [D loss: 0.662342756986618, acc.: 59.67%] [G loss: 0.7856535315513611]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 51/86 [D loss: 0.657985657453537, acc.: 60.69%] [G loss: 0.7889468669891357]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 52/86 [D loss: 0.6571380198001862, acc.: 61.62%] [G loss: 0.7886345386505127]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 53/86 [D loss: 0.6586306691169739, acc.: 59.91%] [G loss: 0.792894184589386]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 54/86 [D loss: 0.6599243879318237, acc.: 59.38%] [G loss: 0.78193199634552]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 55/86 [D loss: 0.6624939143657684, acc.: 59.62%] [G loss: 0.8012040257453918]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 182/200, Batch 56/86 [D loss: 0.655528724193573, acc.: 61.23%] [G loss: 0.7910634875297546]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 57/86 [D loss: 0.6639327108860016, acc.: 58.50%] [G loss: 0.7985919713973999]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 58/86 [D loss: 0.6588543355464935, acc.: 60.79%] [G loss: 0.781800389289856]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 59/86 [D loss: 0.6624478399753571, acc.: 58.98%] [G loss: 0.7919626235961914]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 182/200, Batch 60/86 [D loss: 0.659146636724472, acc.: 60.64%] [G loss: 0.7958897352218628]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 61/86 [D loss: 0.6625857651233673, acc.: 61.08%] [G loss: 0.7844720482826233]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 62/86 [D loss: 0.6588623225688934, acc.: 61.08%] [G loss: 0.7953574061393738]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 63/86 [D loss: 0.6639224588871002, acc.: 60.16%] [G loss: 0.7820941805839539]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 64/86 [D loss: 0.6669757068157196, acc.: 58.40%] [G loss: 0.7856705784797668]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 65/86 [D loss: 0.6571601331233978, acc.: 60.11%] [G loss: 0.7746495008468628]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 66/86 [D loss: 0.6625325679779053, acc.: 59.18%] [G loss: 0.7900344133377075]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 67/86 [D loss: 0.6574212908744812, acc.: 60.45%] [G loss: 0.7919292449951172]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 68/86 [D loss: 0.6685973107814789, acc.: 59.42%] [G loss: 0.7941098809242249]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 69/86 [D loss: 0.6600064039230347, acc.: 59.86%] [G loss: 0.7938507199287415]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 70/86 [D loss: 0.6677176058292389, acc.: 59.23%] [G loss: 0.7927084565162659]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 71/86 [D loss: 0.6624250113964081, acc.: 59.77%] [G loss: 0.8019171953201294]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 182/200, Batch 72/86 [D loss: 0.6612386405467987, acc.: 58.69%] [G loss: 0.7980104684829712]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 73/86 [D loss: 0.6646323502063751, acc.: 59.13%] [G loss: 0.802180826663971]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 74/86 [D loss: 0.6645768284797668, acc.: 58.74%] [G loss: 0.7967894673347473]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 75/86 [D loss: 0.6586901545524597, acc.: 60.55%] [G loss: 0.7964708805084229]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 182/200, Batch 76/86 [D loss: 0.6610561907291412, acc.: 60.21%] [G loss: 0.7937368750572205]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 77/86 [D loss: 0.6615115106105804, acc.: 60.01%] [G loss: 0.7912008762359619]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 182/200, Batch 78/86 [D loss: 0.6671041548252106, acc.: 59.33%] [G loss: 0.8002109527587891]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 79/86 [D loss: 0.6674607396125793, acc.: 58.30%] [G loss: 0.7911559343338013]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 80/86 [D loss: 0.6576371788978577, acc.: 61.13%] [G loss: 0.7944920063018799]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 81/86 [D loss: 0.6617206931114197, acc.: 60.11%] [G loss: 0.7819687724113464]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 82/86 [D loss: 0.6590092182159424, acc.: 61.08%] [G loss: 0.7874647378921509]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 182/200, Batch 83/86 [D loss: 0.6604624688625336, acc.: 60.30%] [G loss: 0.7896260023117065]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 84/86 [D loss: 0.6605517566204071, acc.: 59.81%] [G loss: 0.788004994392395]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 182/200, Batch 85/86 [D loss: 0.6615647375583649, acc.: 59.67%] [G loss: 0.7936123609542847]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 182/200, Batch 86/86 [D loss: 0.6568093001842499, acc.: 61.38%] [G loss: 0.7965399026870728]\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 183/200, Batch 1/86 [D loss: 0.6536128222942352, acc.: 60.99%] [G loss: 0.7951726913452148]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 2/86 [D loss: 0.6616034209728241, acc.: 58.94%] [G loss: 0.7928433418273926]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 3/86 [D loss: 0.6544718742370605, acc.: 62.16%] [G loss: 0.7976053357124329]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 4/86 [D loss: 0.6558104455471039, acc.: 60.50%] [G loss: 0.7923367023468018]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 5/86 [D loss: 0.6604495942592621, acc.: 58.79%] [G loss: 0.7936096787452698]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 6/86 [D loss: 0.6634924411773682, acc.: 58.25%] [G loss: 0.7946648597717285]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 183/200, Batch 7/86 [D loss: 0.6570059061050415, acc.: 60.01%] [G loss: 0.7895776033401489]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 8/86 [D loss: 0.6594512760639191, acc.: 59.81%] [G loss: 0.7919487953186035]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 9/86 [D loss: 0.6594487130641937, acc.: 61.72%] [G loss: 0.7885392308235168]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 10/86 [D loss: 0.665049821138382, acc.: 59.18%] [G loss: 0.7947247624397278]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 11/86 [D loss: 0.6583802103996277, acc.: 60.45%] [G loss: 0.8058048486709595]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 12/86 [D loss: 0.6614270210266113, acc.: 60.35%] [G loss: 0.7979771494865417]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 13/86 [D loss: 0.6612108647823334, acc.: 59.18%] [G loss: 0.7980213165283203]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 14/86 [D loss: 0.6581762135028839, acc.: 60.40%] [G loss: 0.7897973656654358]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 183/200, Batch 15/86 [D loss: 0.6636945009231567, acc.: 59.81%] [G loss: 0.7926779985427856]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 16/86 [D loss: 0.6624563336372375, acc.: 59.81%] [G loss: 0.8009942770004272]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 17/86 [D loss: 0.6587321162223816, acc.: 61.82%] [G loss: 0.7982755899429321]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 18/86 [D loss: 0.6597255170345306, acc.: 61.72%] [G loss: 0.7851526737213135]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 19/86 [D loss: 0.660451203584671, acc.: 61.47%] [G loss: 0.7951772809028625]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 20/86 [D loss: 0.6579020321369171, acc.: 60.25%] [G loss: 0.8027240037918091]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 21/86 [D loss: 0.6610603630542755, acc.: 59.38%] [G loss: 0.7918416857719421]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 22/86 [D loss: 0.6671808362007141, acc.: 59.23%] [G loss: 0.7924584150314331]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 23/86 [D loss: 0.6632602214813232, acc.: 58.69%] [G loss: 0.7845219373703003]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 24/86 [D loss: 0.660728394985199, acc.: 61.04%] [G loss: 0.7846291661262512]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 25/86 [D loss: 0.6556406617164612, acc.: 61.47%] [G loss: 0.7911050319671631]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 183/200, Batch 26/86 [D loss: 0.6580370664596558, acc.: 61.28%] [G loss: 0.7917686104774475]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 27/86 [D loss: 0.6624650359153748, acc.: 60.11%] [G loss: 0.7899229526519775]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 28/86 [D loss: 0.6636317074298859, acc.: 58.69%] [G loss: 0.7982649803161621]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 29/86 [D loss: 0.6622103750705719, acc.: 59.81%] [G loss: 0.788533627986908]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 30/86 [D loss: 0.6662076115608215, acc.: 58.79%] [G loss: 0.7917972207069397]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 31/86 [D loss: 0.6602330803871155, acc.: 59.77%] [G loss: 0.7905861735343933]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 32/86 [D loss: 0.664503812789917, acc.: 59.62%] [G loss: 0.7959004640579224]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 33/86 [D loss: 0.6622231900691986, acc.: 60.06%] [G loss: 0.7984001636505127]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 34/86 [D loss: 0.658711314201355, acc.: 61.08%] [G loss: 0.7860247492790222]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 35/86 [D loss: 0.6629557609558105, acc.: 59.03%] [G loss: 0.7848432064056396]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 36/86 [D loss: 0.6584182977676392, acc.: 61.52%] [G loss: 0.7915092706680298]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 37/86 [D loss: 0.6688187122344971, acc.: 58.30%] [G loss: 0.7948931455612183]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 38/86 [D loss: 0.6631341278553009, acc.: 59.33%] [G loss: 0.7935763597488403]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 39/86 [D loss: 0.6582030653953552, acc.: 60.40%] [G loss: 0.788184642791748]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 40/86 [D loss: 0.6558602750301361, acc.: 61.18%] [G loss: 0.7950273752212524]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 41/86 [D loss: 0.6625654697418213, acc.: 59.42%] [G loss: 0.7941738367080688]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 42/86 [D loss: 0.6610994935035706, acc.: 60.35%] [G loss: 0.794340968132019]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 43/86 [D loss: 0.654559999704361, acc.: 60.79%] [G loss: 0.7962115406990051]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 44/86 [D loss: 0.6588384509086609, acc.: 60.25%] [G loss: 0.7988989949226379]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 183/200, Batch 45/86 [D loss: 0.6570612192153931, acc.: 61.18%] [G loss: 0.8029481172561646]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 183/200, Batch 46/86 [D loss: 0.6585407853126526, acc.: 60.69%] [G loss: 0.7916097044944763]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 47/86 [D loss: 0.6602059900760651, acc.: 60.06%] [G loss: 0.7978230118751526]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 48/86 [D loss: 0.6562926769256592, acc.: 60.01%] [G loss: 0.8002683520317078]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 49/86 [D loss: 0.6564394533634186, acc.: 59.86%] [G loss: 0.7929354310035706]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 183/200, Batch 50/86 [D loss: 0.6694885790348053, acc.: 58.11%] [G loss: 0.7948980331420898]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 51/86 [D loss: 0.655677318572998, acc.: 62.01%] [G loss: 0.8016834855079651]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 183/200, Batch 52/86 [D loss: 0.6592978835105896, acc.: 59.52%] [G loss: 0.8067734241485596]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 53/86 [D loss: 0.6615744829177856, acc.: 59.47%] [G loss: 0.8044270873069763]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 54/86 [D loss: 0.6604416370391846, acc.: 59.72%] [G loss: 0.7917641997337341]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 55/86 [D loss: 0.6631911396980286, acc.: 58.64%] [G loss: 0.7822737097740173]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 183/200, Batch 56/86 [D loss: 0.6550791561603546, acc.: 61.91%] [G loss: 0.7966192960739136]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 57/86 [D loss: 0.654120534658432, acc.: 60.94%] [G loss: 0.791084885597229]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 58/86 [D loss: 0.6597066819667816, acc.: 61.23%] [G loss: 0.793204665184021]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 59/86 [D loss: 0.6619330048561096, acc.: 60.11%] [G loss: 0.7880650758743286]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 183/200, Batch 60/86 [D loss: 0.6588306725025177, acc.: 59.38%] [G loss: 0.799091100692749]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 61/86 [D loss: 0.6599754691123962, acc.: 60.40%] [G loss: 0.7793626189231873]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 62/86 [D loss: 0.6602912247180939, acc.: 60.40%] [G loss: 0.8015780448913574]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 63/86 [D loss: 0.6664797067642212, acc.: 58.15%] [G loss: 0.7908308506011963]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 183/200, Batch 64/86 [D loss: 0.662121057510376, acc.: 60.01%] [G loss: 0.7948275804519653]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 65/86 [D loss: 0.669453352689743, acc.: 57.86%] [G loss: 0.8018053770065308]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 66/86 [D loss: 0.6557133197784424, acc.: 61.57%] [G loss: 0.787438690662384]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 67/86 [D loss: 0.6599137485027313, acc.: 59.42%] [G loss: 0.7914680242538452]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 68/86 [D loss: 0.6604738235473633, acc.: 59.52%] [G loss: 0.7880921363830566]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 69/86 [D loss: 0.6605840623378754, acc.: 60.69%] [G loss: 0.7903545498847961]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 70/86 [D loss: 0.6553038060665131, acc.: 60.94%] [G loss: 0.794503390789032]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 71/86 [D loss: 0.6564218103885651, acc.: 60.21%] [G loss: 0.7947006225585938]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 72/86 [D loss: 0.6641346514225006, acc.: 60.89%] [G loss: 0.797805905342102]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 73/86 [D loss: 0.6604432761669159, acc.: 60.40%] [G loss: 0.7831298112869263]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 74/86 [D loss: 0.6572944521903992, acc.: 61.38%] [G loss: 0.7905269265174866]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 75/86 [D loss: 0.657663106918335, acc.: 60.06%] [G loss: 0.7834124565124512]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 183/200, Batch 76/86 [D loss: 0.6518326103687286, acc.: 61.87%] [G loss: 0.7917218804359436]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 77/86 [D loss: 0.6529905200004578, acc.: 60.55%] [G loss: 0.8043543696403503]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 183/200, Batch 78/86 [D loss: 0.6586738228797913, acc.: 60.30%] [G loss: 0.8055722713470459]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 79/86 [D loss: 0.6580067276954651, acc.: 61.13%] [G loss: 0.7904558777809143]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 80/86 [D loss: 0.6598680019378662, acc.: 60.99%] [G loss: 0.7904416918754578]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 81/86 [D loss: 0.6623395681381226, acc.: 58.45%] [G loss: 0.7891330718994141]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 183/200, Batch 82/86 [D loss: 0.6631238460540771, acc.: 59.62%] [G loss: 0.7860788106918335]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 183/200, Batch 83/86 [D loss: 0.6592232584953308, acc.: 60.50%] [G loss: 0.7955361604690552]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 84/86 [D loss: 0.6673423051834106, acc.: 58.69%] [G loss: 0.7885973453521729]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 85/86 [D loss: 0.6611450016498566, acc.: 59.77%] [G loss: 0.7820525169372559]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 183/200, Batch 86/86 [D loss: 0.6572409570217133, acc.: 59.96%] [G loss: 0.7824522852897644]\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 184/200, Batch 1/86 [D loss: 0.6581887900829315, acc.: 61.57%] [G loss: 0.7879397869110107]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 184/200, Batch 2/86 [D loss: 0.6612229943275452, acc.: 60.55%] [G loss: 0.7985349893569946]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 3/86 [D loss: 0.6588112115859985, acc.: 60.45%] [G loss: 0.7981526255607605]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 4/86 [D loss: 0.6597648561000824, acc.: 60.30%] [G loss: 0.7972922325134277]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 5/86 [D loss: 0.6549709141254425, acc.: 60.94%] [G loss: 0.7947816848754883]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 6/86 [D loss: 0.6594482362270355, acc.: 60.25%] [G loss: 0.8013080358505249]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 7/86 [D loss: 0.6584140956401825, acc.: 61.82%] [G loss: 0.7848906517028809]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 8/86 [D loss: 0.6593586802482605, acc.: 60.99%] [G loss: 0.788196861743927]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 9/86 [D loss: 0.659095823764801, acc.: 61.23%] [G loss: 0.7936286926269531]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 10/86 [D loss: 0.6600520610809326, acc.: 59.28%] [G loss: 0.7812579870223999]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 11/86 [D loss: 0.6595970988273621, acc.: 60.94%] [G loss: 0.7908105254173279]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 12/86 [D loss: 0.6604045629501343, acc.: 60.69%] [G loss: 0.7840035557746887]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 13/86 [D loss: 0.6593100726604462, acc.: 60.11%] [G loss: 0.7918849587440491]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 14/86 [D loss: 0.6618881821632385, acc.: 58.54%] [G loss: 0.7871630191802979]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 15/86 [D loss: 0.655958503484726, acc.: 60.35%] [G loss: 0.7904069423675537]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 184/200, Batch 16/86 [D loss: 0.6626542210578918, acc.: 58.64%] [G loss: 0.788169264793396]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 17/86 [D loss: 0.6619372367858887, acc.: 59.81%] [G loss: 0.7901943922042847]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 18/86 [D loss: 0.6631001234054565, acc.: 58.84%] [G loss: 0.7936015129089355]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 19/86 [D loss: 0.6571779549121857, acc.: 60.84%] [G loss: 0.8012698888778687]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 20/86 [D loss: 0.6518118679523468, acc.: 63.33%] [G loss: 0.7898080348968506]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 21/86 [D loss: 0.6581869721412659, acc.: 60.89%] [G loss: 0.7897293567657471]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 22/86 [D loss: 0.6602177917957306, acc.: 59.77%] [G loss: 0.7867029905319214]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 23/86 [D loss: 0.6590168476104736, acc.: 60.55%] [G loss: 0.7837510108947754]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 24/86 [D loss: 0.6625096201896667, acc.: 58.50%] [G loss: 0.7956474423408508]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 25/86 [D loss: 0.6633325517177582, acc.: 60.55%] [G loss: 0.8014076948165894]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 184/200, Batch 26/86 [D loss: 0.6637464463710785, acc.: 59.28%] [G loss: 0.8049717545509338]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 27/86 [D loss: 0.6564923524856567, acc.: 61.87%] [G loss: 0.7912390232086182]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 28/86 [D loss: 0.6588334441184998, acc.: 59.72%] [G loss: 0.8013437986373901]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 184/200, Batch 29/86 [D loss: 0.6604278385639191, acc.: 60.50%] [G loss: 0.7854152321815491]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 30/86 [D loss: 0.6671166121959686, acc.: 58.79%] [G loss: 0.7866186499595642]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 184/200, Batch 31/86 [D loss: 0.6668481528759003, acc.: 58.40%] [G loss: 0.7943447828292847]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 32/86 [D loss: 0.6616927981376648, acc.: 58.79%] [G loss: 0.7854991555213928]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 33/86 [D loss: 0.6516659557819366, acc.: 62.16%] [G loss: 0.7987595796585083]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 184/200, Batch 34/86 [D loss: 0.6638089418411255, acc.: 59.77%] [G loss: 0.7924034595489502]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 35/86 [D loss: 0.6571236848831177, acc.: 60.35%] [G loss: 0.7993320822715759]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 36/86 [D loss: 0.662040114402771, acc.: 58.69%] [G loss: 0.798416256904602]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 37/86 [D loss: 0.6603806614875793, acc.: 60.01%] [G loss: 0.7954246997833252]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 38/86 [D loss: 0.6702002286911011, acc.: 57.81%] [G loss: 0.800618052482605]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 39/86 [D loss: 0.6600934267044067, acc.: 59.67%] [G loss: 0.7911716103553772]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 40/86 [D loss: 0.6629282534122467, acc.: 59.13%] [G loss: 0.7800394296646118]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 41/86 [D loss: 0.6596704125404358, acc.: 59.91%] [G loss: 0.790170431137085]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 42/86 [D loss: 0.6607455015182495, acc.: 60.40%] [G loss: 0.7839743494987488]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 43/86 [D loss: 0.6571455597877502, acc.: 60.74%] [G loss: 0.7906172871589661]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 44/86 [D loss: 0.6584985256195068, acc.: 59.81%] [G loss: 0.8007123470306396]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 45/86 [D loss: 0.659818559885025, acc.: 60.84%] [G loss: 0.7929202318191528]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 46/86 [D loss: 0.6639156341552734, acc.: 59.77%] [G loss: 0.7979003190994263]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 184/200, Batch 47/86 [D loss: 0.652206152677536, acc.: 62.84%] [G loss: 0.79929518699646]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 184/200, Batch 48/86 [D loss: 0.6574165523052216, acc.: 60.69%] [G loss: 0.8041613101959229]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 49/86 [D loss: 0.6572327613830566, acc.: 61.08%] [G loss: 0.8061521053314209]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 50/86 [D loss: 0.6589683890342712, acc.: 59.91%] [G loss: 0.7948241233825684]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 184/200, Batch 51/86 [D loss: 0.6605566143989563, acc.: 59.52%] [G loss: 0.790259838104248]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 52/86 [D loss: 0.6542047560214996, acc.: 61.43%] [G loss: 0.7899929285049438]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 53/86 [D loss: 0.6637001037597656, acc.: 58.79%] [G loss: 0.801068902015686]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 54/86 [D loss: 0.6612963974475861, acc.: 59.67%] [G loss: 0.7930049896240234]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 55/86 [D loss: 0.6638281345367432, acc.: 58.50%] [G loss: 0.7891466617584229]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 56/86 [D loss: 0.6589109301567078, acc.: 59.33%] [G loss: 0.7861106395721436]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 57/86 [D loss: 0.6589242517948151, acc.: 61.23%] [G loss: 0.7912351489067078]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 58/86 [D loss: 0.6623090207576752, acc.: 60.16%] [G loss: 0.7865965366363525]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 59/86 [D loss: 0.6583948135375977, acc.: 60.60%] [G loss: 0.7863399386405945]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 60/86 [D loss: 0.6611573696136475, acc.: 60.35%] [G loss: 0.8000875115394592]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 184/200, Batch 61/86 [D loss: 0.6567764580249786, acc.: 62.35%] [G loss: 0.796944260597229]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 62/86 [D loss: 0.6618086099624634, acc.: 60.40%] [G loss: 0.7943477034568787]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 63/86 [D loss: 0.6585259139537811, acc.: 60.69%] [G loss: 0.7861334085464478]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 64/86 [D loss: 0.6589312255382538, acc.: 61.62%] [G loss: 0.7970066070556641]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 65/86 [D loss: 0.6585986614227295, acc.: 61.43%] [G loss: 0.8041293621063232]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 66/86 [D loss: 0.6604473292827606, acc.: 61.08%] [G loss: 0.7909197211265564]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 67/86 [D loss: 0.6576009392738342, acc.: 60.99%] [G loss: 0.7951065897941589]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 68/86 [D loss: 0.6608249843120575, acc.: 59.96%] [G loss: 0.7998219728469849]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 69/86 [D loss: 0.6655584275722504, acc.: 58.45%] [G loss: 0.8023419380187988]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 70/86 [D loss: 0.6595265567302704, acc.: 61.43%] [G loss: 0.806117594242096]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 71/86 [D loss: 0.6559320092201233, acc.: 61.87%] [G loss: 0.7927188277244568]\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "Epoch 184/200, Batch 72/86 [D loss: 0.6578982174396515, acc.: 60.06%] [G loss: 0.7910272479057312]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 73/86 [D loss: 0.6650987565517426, acc.: 60.45%] [G loss: 0.793120801448822]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 74/86 [D loss: 0.6642704904079437, acc.: 60.30%] [G loss: 0.7884786128997803]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 75/86 [D loss: 0.6614444255828857, acc.: 59.47%] [G loss: 0.8016781806945801]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 76/86 [D loss: 0.6617089211940765, acc.: 60.25%] [G loss: 0.785780131816864]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 77/86 [D loss: 0.6579720675945282, acc.: 60.94%] [G loss: 0.7961166501045227]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 78/86 [D loss: 0.6572134792804718, acc.: 60.01%] [G loss: 0.7945543527603149]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 79/86 [D loss: 0.6530870497226715, acc.: 60.79%] [G loss: 0.8038927316665649]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 80/86 [D loss: 0.6621589958667755, acc.: 60.16%] [G loss: 0.7943052053451538]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 184/200, Batch 81/86 [D loss: 0.6614565849304199, acc.: 60.25%] [G loss: 0.7926721572875977]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 184/200, Batch 82/86 [D loss: 0.6659608781337738, acc.: 58.79%] [G loss: 0.7894883751869202]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 83/86 [D loss: 0.6614926159381866, acc.: 59.57%] [G loss: 0.7967031002044678]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 84/86 [D loss: 0.6610691249370575, acc.: 59.86%] [G loss: 0.7947884798049927]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 184/200, Batch 85/86 [D loss: 0.6566397547721863, acc.: 59.67%] [G loss: 0.8065813779830933]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 184/200, Batch 86/86 [D loss: 0.6563345491886139, acc.: 60.16%] [G loss: 0.7982285022735596]\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 185/200, Batch 1/86 [D loss: 0.6576212644577026, acc.: 61.18%] [G loss: 0.7957205176353455]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 2/86 [D loss: 0.6529180109500885, acc.: 62.60%] [G loss: 0.7805455923080444]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 3/86 [D loss: 0.66026571393013, acc.: 60.21%] [G loss: 0.8008142709732056]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 185/200, Batch 4/86 [D loss: 0.6615322232246399, acc.: 60.40%] [G loss: 0.7953704595565796]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 5/86 [D loss: 0.6536858677864075, acc.: 61.18%] [G loss: 0.8007309436798096]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 185/200, Batch 6/86 [D loss: 0.6599043905735016, acc.: 60.11%] [G loss: 0.7865902781486511]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 7/86 [D loss: 0.6614775359630585, acc.: 60.16%] [G loss: 0.7971535325050354]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 8/86 [D loss: 0.6538875699043274, acc.: 62.70%] [G loss: 0.7985497713088989]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 9/86 [D loss: 0.6596019268035889, acc.: 60.16%] [G loss: 0.7881549596786499]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 10/86 [D loss: 0.6570495069026947, acc.: 61.33%] [G loss: 0.7877774238586426]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 185/200, Batch 11/86 [D loss: 0.6600494682788849, acc.: 60.50%] [G loss: 0.7936117053031921]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 12/86 [D loss: 0.656504213809967, acc.: 60.84%] [G loss: 0.7864824533462524]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 13/86 [D loss: 0.6616753041744232, acc.: 59.86%] [G loss: 0.8013601899147034]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 14/86 [D loss: 0.6648755669593811, acc.: 59.47%] [G loss: 0.7917814254760742]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 15/86 [D loss: 0.6568166017532349, acc.: 61.28%] [G loss: 0.7898582816123962]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 16/86 [D loss: 0.6548559963703156, acc.: 62.50%] [G loss: 0.793880045413971]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 185/200, Batch 17/86 [D loss: 0.6622731685638428, acc.: 58.69%] [G loss: 0.7879472970962524]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 18/86 [D loss: 0.6664352416992188, acc.: 58.69%] [G loss: 0.7968683242797852]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 19/86 [D loss: 0.6567975878715515, acc.: 60.74%] [G loss: 0.7935078740119934]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 185/200, Batch 20/86 [D loss: 0.6581146121025085, acc.: 60.84%] [G loss: 0.7930174469947815]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 21/86 [D loss: 0.6676312983036041, acc.: 59.77%] [G loss: 0.7919414639472961]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 22/86 [D loss: 0.6664644479751587, acc.: 59.28%] [G loss: 0.7865201830863953]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 23/86 [D loss: 0.6653278470039368, acc.: 58.64%] [G loss: 0.7977883815765381]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 24/86 [D loss: 0.6529560089111328, acc.: 61.52%] [G loss: 0.796206533908844]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 25/86 [D loss: 0.6621240377426147, acc.: 60.60%] [G loss: 0.7994827628135681]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 26/86 [D loss: 0.6558532118797302, acc.: 61.62%] [G loss: 0.8071820139884949]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 27/86 [D loss: 0.6687681972980499, acc.: 59.03%] [G loss: 0.7869449257850647]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 28/86 [D loss: 0.6616653800010681, acc.: 59.08%] [G loss: 0.7942875623703003]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 29/86 [D loss: 0.6623286008834839, acc.: 58.50%] [G loss: 0.7937346696853638]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 30/86 [D loss: 0.6688153147697449, acc.: 58.01%] [G loss: 0.7891567945480347]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 31/86 [D loss: 0.6583118736743927, acc.: 61.08%] [G loss: 0.7918065786361694]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 32/86 [D loss: 0.6588067412376404, acc.: 60.64%] [G loss: 0.7881661653518677]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 33/86 [D loss: 0.6620135307312012, acc.: 59.86%] [G loss: 0.793961763381958]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 34/86 [D loss: 0.6618209183216095, acc.: 59.18%] [G loss: 0.7951676249504089]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 35/86 [D loss: 0.6610704660415649, acc.: 60.64%] [G loss: 0.796184778213501]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 36/86 [D loss: 0.659005731344223, acc.: 61.04%] [G loss: 0.7935242056846619]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 37/86 [D loss: 0.6565350890159607, acc.: 60.94%] [G loss: 0.7938591837882996]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 38/86 [D loss: 0.6683304905891418, acc.: 58.15%] [G loss: 0.7911061644554138]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 39/86 [D loss: 0.6595533788204193, acc.: 61.08%] [G loss: 0.8048271536827087]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 40/86 [D loss: 0.6508857905864716, acc.: 62.26%] [G loss: 0.7963193655014038]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 41/86 [D loss: 0.6583095490932465, acc.: 60.25%] [G loss: 0.7961684465408325]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 42/86 [D loss: 0.6547673940658569, acc.: 62.50%] [G loss: 0.8059219121932983]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 43/86 [D loss: 0.6631112396717072, acc.: 59.57%] [G loss: 0.7956419587135315]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 44/86 [D loss: 0.6598660349845886, acc.: 61.08%] [G loss: 0.7885153889656067]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 45/86 [D loss: 0.662029892206192, acc.: 60.06%] [G loss: 0.7813541889190674]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 46/86 [D loss: 0.6611039638519287, acc.: 58.64%] [G loss: 0.7954105734825134]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 47/86 [D loss: 0.653410792350769, acc.: 61.04%] [G loss: 0.805011510848999]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 48/86 [D loss: 0.6608055830001831, acc.: 59.81%] [G loss: 0.8024416565895081]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 49/86 [D loss: 0.654739648103714, acc.: 61.33%] [G loss: 0.7928388714790344]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 50/86 [D loss: 0.6622081696987152, acc.: 58.98%] [G loss: 0.8023401498794556]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 51/86 [D loss: 0.6658982336521149, acc.: 58.69%] [G loss: 0.789015531539917]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 52/86 [D loss: 0.6632850766181946, acc.: 60.74%] [G loss: 0.7914726138114929]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 53/86 [D loss: 0.6599164009094238, acc.: 59.47%] [G loss: 0.7896568179130554]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 185/200, Batch 54/86 [D loss: 0.6620270907878876, acc.: 60.50%] [G loss: 0.7967379689216614]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 55/86 [D loss: 0.6553258001804352, acc.: 60.45%] [G loss: 0.8046139478683472]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 56/86 [D loss: 0.6650963425636292, acc.: 58.40%] [G loss: 0.8011501431465149]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 185/200, Batch 57/86 [D loss: 0.6577993035316467, acc.: 60.84%] [G loss: 0.796931266784668]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 185/200, Batch 58/86 [D loss: 0.6643884479999542, acc.: 58.35%] [G loss: 0.7917138934135437]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 59/86 [D loss: 0.6590811610221863, acc.: 60.25%] [G loss: 0.7957859635353088]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 60/86 [D loss: 0.6560951769351959, acc.: 62.16%] [G loss: 0.7966835498809814]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 61/86 [D loss: 0.6575839221477509, acc.: 59.96%] [G loss: 0.8031418919563293]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 185/200, Batch 62/86 [D loss: 0.6602440476417542, acc.: 60.50%] [G loss: 0.7902835011482239]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 63/86 [D loss: 0.65299391746521, acc.: 61.52%] [G loss: 0.7980092167854309]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 64/86 [D loss: 0.6557677090167999, acc.: 61.91%] [G loss: 0.8015606999397278]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 185/200, Batch 65/86 [D loss: 0.6571125388145447, acc.: 60.94%] [G loss: 0.7976893186569214]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 66/86 [D loss: 0.6639702916145325, acc.: 58.94%] [G loss: 0.8026418685913086]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 67/86 [D loss: 0.6630536615848541, acc.: 58.89%] [G loss: 0.7923679351806641]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 68/86 [D loss: 0.6606974005699158, acc.: 60.45%] [G loss: 0.7916508913040161]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 185/200, Batch 69/86 [D loss: 0.6598874032497406, acc.: 60.69%] [G loss: 0.8051899671554565]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 185/200, Batch 70/86 [D loss: 0.6549872756004333, acc.: 61.23%] [G loss: 0.7926823496818542]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 71/86 [D loss: 0.6568731665611267, acc.: 60.11%] [G loss: 0.7937870025634766]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 72/86 [D loss: 0.6579164862632751, acc.: 60.60%] [G loss: 0.7935482263565063]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 73/86 [D loss: 0.6673983633518219, acc.: 57.91%] [G loss: 0.7935201525688171]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 74/86 [D loss: 0.6594648957252502, acc.: 60.01%] [G loss: 0.7950703501701355]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 75/86 [D loss: 0.6560986340045929, acc.: 62.89%] [G loss: 0.798638105392456]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 76/86 [D loss: 0.6596735715866089, acc.: 60.55%] [G loss: 0.8030834197998047]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 77/86 [D loss: 0.661378026008606, acc.: 59.57%] [G loss: 0.800489068031311]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 78/86 [D loss: 0.6597432792186737, acc.: 59.77%] [G loss: 0.79599928855896]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 79/86 [D loss: 0.6555742621421814, acc.: 61.87%] [G loss: 0.7987918853759766]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 80/86 [D loss: 0.6634337604045868, acc.: 58.84%] [G loss: 0.793073296546936]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 81/86 [D loss: 0.6609847545623779, acc.: 59.62%] [G loss: 0.7976697683334351]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 185/200, Batch 82/86 [D loss: 0.6602843403816223, acc.: 59.13%] [G loss: 0.794432520866394]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 83/86 [D loss: 0.6618883609771729, acc.: 61.52%] [G loss: 0.7934156656265259]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 84/86 [D loss: 0.6657964587211609, acc.: 57.81%] [G loss: 0.7943986654281616]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 185/200, Batch 85/86 [D loss: 0.6585046947002411, acc.: 59.91%] [G loss: 0.7950863838195801]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 185/200, Batch 86/86 [D loss: 0.6632876694202423, acc.: 59.57%] [G loss: 0.7982625961303711]\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 186/200, Batch 1/86 [D loss: 0.6633742153644562, acc.: 58.98%] [G loss: 0.7971989512443542]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 2/86 [D loss: 0.6659582555294037, acc.: 58.35%] [G loss: 0.7938003540039062]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 3/86 [D loss: 0.6621421277523041, acc.: 58.79%] [G loss: 0.8006231784820557]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 4/86 [D loss: 0.6592684090137482, acc.: 61.82%] [G loss: 0.789608895778656]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 5/86 [D loss: 0.6629541516304016, acc.: 59.23%] [G loss: 0.794342041015625]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 6/86 [D loss: 0.6613906025886536, acc.: 60.11%] [G loss: 0.7903749942779541]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 7/86 [D loss: 0.6574139297008514, acc.: 60.21%] [G loss: 0.8029629588127136]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 186/200, Batch 8/86 [D loss: 0.6652409136295319, acc.: 59.28%] [G loss: 0.8022812604904175]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 9/86 [D loss: 0.657625675201416, acc.: 61.23%] [G loss: 0.8072368502616882]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 10/86 [D loss: 0.6605522334575653, acc.: 59.96%] [G loss: 0.7948142290115356]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 11/86 [D loss: 0.6597960293292999, acc.: 60.60%] [G loss: 0.7912307381629944]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 12/86 [D loss: 0.6603478789329529, acc.: 60.01%] [G loss: 0.7952141761779785]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 13/86 [D loss: 0.6588347554206848, acc.: 61.43%] [G loss: 0.7980940341949463]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 14/86 [D loss: 0.650932639837265, acc.: 62.89%] [G loss: 0.7929849624633789]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 15/86 [D loss: 0.6584701538085938, acc.: 60.30%] [G loss: 0.8138267993927002]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 186/200, Batch 16/86 [D loss: 0.6556619107723236, acc.: 61.13%] [G loss: 0.796211302280426]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 17/86 [D loss: 0.650164008140564, acc.: 63.33%] [G loss: 0.8044556975364685]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 18/86 [D loss: 0.6588259339332581, acc.: 60.45%] [G loss: 0.8015021085739136]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 19/86 [D loss: 0.661124974489212, acc.: 59.08%] [G loss: 0.8056195974349976]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 20/86 [D loss: 0.6596689224243164, acc.: 60.35%] [G loss: 0.8046615123748779]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 21/86 [D loss: 0.6643296778202057, acc.: 59.81%] [G loss: 0.7888867259025574]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 22/86 [D loss: 0.6571151912212372, acc.: 60.25%] [G loss: 0.7927691340446472]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 23/86 [D loss: 0.6673549711704254, acc.: 58.79%] [G loss: 0.7939954996109009]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 186/200, Batch 24/86 [D loss: 0.6649892330169678, acc.: 59.42%] [G loss: 0.7985355854034424]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 25/86 [D loss: 0.6563051044940948, acc.: 61.08%] [G loss: 0.8039354085922241]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 26/86 [D loss: 0.6582165658473969, acc.: 59.72%] [G loss: 0.797634482383728]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 27/86 [D loss: 0.6630941331386566, acc.: 60.55%] [G loss: 0.7964386940002441]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 28/86 [D loss: 0.6590840220451355, acc.: 60.45%] [G loss: 0.7840821146965027]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 29/86 [D loss: 0.6606206893920898, acc.: 60.11%] [G loss: 0.7888754606246948]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 30/86 [D loss: 0.6646369993686676, acc.: 59.18%] [G loss: 0.7999939918518066]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 31/86 [D loss: 0.6634197235107422, acc.: 59.81%] [G loss: 0.7942028641700745]\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "Epoch 186/200, Batch 32/86 [D loss: 0.6547324657440186, acc.: 61.62%] [G loss: 0.8001440167427063]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 186/200, Batch 33/86 [D loss: 0.6581210792064667, acc.: 61.43%] [G loss: 0.7940645217895508]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 34/86 [D loss: 0.660021036863327, acc.: 60.99%] [G loss: 0.7889800667762756]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 35/86 [D loss: 0.6592416763305664, acc.: 60.30%] [G loss: 0.7932460308074951]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 36/86 [D loss: 0.6588151156902313, acc.: 60.30%] [G loss: 0.7901054620742798]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 37/86 [D loss: 0.661277711391449, acc.: 59.57%] [G loss: 0.8014492988586426]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 186/200, Batch 38/86 [D loss: 0.6548675298690796, acc.: 61.87%] [G loss: 0.7987317442893982]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 39/86 [D loss: 0.6562634110450745, acc.: 60.16%] [G loss: 0.7892566919326782]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 40/86 [D loss: 0.665363222360611, acc.: 58.89%] [G loss: 0.7883212566375732]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 41/86 [D loss: 0.6590699553489685, acc.: 61.38%] [G loss: 0.79189532995224]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 42/86 [D loss: 0.6609137058258057, acc.: 59.96%] [G loss: 0.7938240170478821]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 43/86 [D loss: 0.6596348285675049, acc.: 60.69%] [G loss: 0.7886307239532471]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 44/86 [D loss: 0.6606611609458923, acc.: 59.47%] [G loss: 0.7944494485855103]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 45/86 [D loss: 0.6517556309700012, acc.: 60.89%] [G loss: 0.8131334781646729]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 186/200, Batch 46/86 [D loss: 0.6599811911582947, acc.: 59.08%] [G loss: 0.794549822807312]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 186/200, Batch 47/86 [D loss: 0.6666874587535858, acc.: 57.47%] [G loss: 0.809873104095459]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 186/200, Batch 48/86 [D loss: 0.6624939143657684, acc.: 58.84%] [G loss: 0.7879307270050049]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 49/86 [D loss: 0.6599864661693573, acc.: 60.79%] [G loss: 0.80544513463974]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 50/86 [D loss: 0.6579286158084869, acc.: 62.21%] [G loss: 0.8016232252120972]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 51/86 [D loss: 0.6593906283378601, acc.: 60.11%] [G loss: 0.791435718536377]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 52/86 [D loss: 0.6564695239067078, acc.: 60.64%] [G loss: 0.7968642711639404]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 53/86 [D loss: 0.6661131083965302, acc.: 59.18%] [G loss: 0.7861603498458862]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 54/86 [D loss: 0.6673551201820374, acc.: 58.59%] [G loss: 0.7992671132087708]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 186/200, Batch 55/86 [D loss: 0.6610029637813568, acc.: 61.28%] [G loss: 0.7893913388252258]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 56/86 [D loss: 0.6570463478565216, acc.: 61.57%] [G loss: 0.803995668888092]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 57/86 [D loss: 0.6590631902217865, acc.: 60.64%] [G loss: 0.7929016947746277]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 58/86 [D loss: 0.6624622344970703, acc.: 59.91%] [G loss: 0.8054363131523132]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 59/86 [D loss: 0.6606438159942627, acc.: 60.40%] [G loss: 0.8017948269844055]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 60/86 [D loss: 0.6618724167346954, acc.: 60.21%] [G loss: 0.807659387588501]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 61/86 [D loss: 0.6559112966060638, acc.: 60.74%] [G loss: 0.7936165928840637]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 62/86 [D loss: 0.6538594961166382, acc.: 63.28%] [G loss: 0.796916127204895]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 63/86 [D loss: 0.6605015397071838, acc.: 58.98%] [G loss: 0.8068023920059204]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 64/86 [D loss: 0.6611103415489197, acc.: 59.86%] [G loss: 0.8009918332099915]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 186/200, Batch 65/86 [D loss: 0.6571679413318634, acc.: 61.96%] [G loss: 0.8015518188476562]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 186/200, Batch 66/86 [D loss: 0.658695787191391, acc.: 60.74%] [G loss: 0.7962220907211304]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 67/86 [D loss: 0.6549266576766968, acc.: 60.79%] [G loss: 0.7956815361976624]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 68/86 [D loss: 0.6626808047294617, acc.: 60.50%] [G loss: 0.8053948879241943]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 69/86 [D loss: 0.6601887941360474, acc.: 60.69%] [G loss: 0.7931191921234131]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 186/200, Batch 70/86 [D loss: 0.6612890660762787, acc.: 58.20%] [G loss: 0.7959245443344116]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 71/86 [D loss: 0.6553892493247986, acc.: 60.74%] [G loss: 0.7914098501205444]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 72/86 [D loss: 0.6652159690856934, acc.: 59.91%] [G loss: 0.7920849323272705]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 73/86 [D loss: 0.6540093123912811, acc.: 61.87%] [G loss: 0.7895628213882446]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 74/86 [D loss: 0.6592025756835938, acc.: 60.79%] [G loss: 0.8026883602142334]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 75/86 [D loss: 0.6596289873123169, acc.: 59.23%] [G loss: 0.78355473279953]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 186/200, Batch 76/86 [D loss: 0.6670409739017487, acc.: 58.50%] [G loss: 0.7912707328796387]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 77/86 [D loss: 0.6562003791332245, acc.: 59.72%] [G loss: 0.7986859083175659]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 78/86 [D loss: 0.6521062552928925, acc.: 63.28%] [G loss: 0.7961181402206421]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 79/86 [D loss: 0.6587253212928772, acc.: 61.08%] [G loss: 0.7871928811073303]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 186/200, Batch 80/86 [D loss: 0.6649931371212006, acc.: 59.03%] [G loss: 0.7998040318489075]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 81/86 [D loss: 0.6631627678871155, acc.: 57.96%] [G loss: 0.7974964380264282]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 82/86 [D loss: 0.6584338843822479, acc.: 60.40%] [G loss: 0.796046257019043]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 186/200, Batch 83/86 [D loss: 0.6586215496063232, acc.: 61.67%] [G loss: 0.7931826114654541]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 186/200, Batch 84/86 [D loss: 0.6589841246604919, acc.: 60.79%] [G loss: 0.7744954228401184]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 186/200, Batch 85/86 [D loss: 0.6579623222351074, acc.: 61.18%] [G loss: 0.8013362884521484]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 186/200, Batch 86/86 [D loss: 0.6638411283493042, acc.: 60.79%] [G loss: 0.797882616519928]\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 187/200, Batch 1/86 [D loss: 0.6562874615192413, acc.: 62.30%] [G loss: 0.7955491542816162]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 2/86 [D loss: 0.6567373275756836, acc.: 60.40%] [G loss: 0.7961210012435913]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 3/86 [D loss: 0.6668701469898224, acc.: 57.91%] [G loss: 0.7971819043159485]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 187/200, Batch 4/86 [D loss: 0.6552879810333252, acc.: 62.01%] [G loss: 0.8027021884918213]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 5/86 [D loss: 0.6609236598014832, acc.: 60.84%] [G loss: 0.7900610566139221]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 6/86 [D loss: 0.6547643542289734, acc.: 60.69%] [G loss: 0.8011847734451294]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 7/86 [D loss: 0.6570735275745392, acc.: 61.43%] [G loss: 0.793186604976654]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 8/86 [D loss: 0.6556136906147003, acc.: 61.77%] [G loss: 0.7970578670501709]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 9/86 [D loss: 0.6580453515052795, acc.: 60.40%] [G loss: 0.7957918047904968]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 10/86 [D loss: 0.6589643657207489, acc.: 61.57%] [G loss: 0.8035681247711182]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 11/86 [D loss: 0.6633050739765167, acc.: 59.91%] [G loss: 0.8044551610946655]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 12/86 [D loss: 0.657884269952774, acc.: 61.91%] [G loss: 0.7967089414596558]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 13/86 [D loss: 0.6586182415485382, acc.: 60.21%] [G loss: 0.7959153652191162]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 14/86 [D loss: 0.6579966843128204, acc.: 61.62%] [G loss: 0.7939481139183044]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 187/200, Batch 15/86 [D loss: 0.6638028025627136, acc.: 57.91%] [G loss: 0.8065253496170044]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 16/86 [D loss: 0.6657266020774841, acc.: 59.47%] [G loss: 0.8062950372695923]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 17/86 [D loss: 0.6578201949596405, acc.: 60.21%] [G loss: 0.806725025177002]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 18/86 [D loss: 0.6575065553188324, acc.: 61.52%] [G loss: 0.8043172359466553]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 19/86 [D loss: 0.6543234586715698, acc.: 62.30%] [G loss: 0.7970839738845825]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 20/86 [D loss: 0.6633731424808502, acc.: 59.18%] [G loss: 0.7961615920066833]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 21/86 [D loss: 0.6591167449951172, acc.: 60.25%] [G loss: 0.7979804277420044]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 22/86 [D loss: 0.660176694393158, acc.: 60.99%] [G loss: 0.7915646433830261]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 23/86 [D loss: 0.659772515296936, acc.: 60.69%] [G loss: 0.7871530055999756]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 24/86 [D loss: 0.661853164434433, acc.: 60.25%] [G loss: 0.8018373250961304]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 25/86 [D loss: 0.6635047793388367, acc.: 59.72%] [G loss: 0.7948111295700073]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 26/86 [D loss: 0.6589736938476562, acc.: 60.21%] [G loss: 0.7924817204475403]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 27/86 [D loss: 0.6656582057476044, acc.: 58.40%] [G loss: 0.8006035685539246]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 28/86 [D loss: 0.6576467752456665, acc.: 60.55%] [G loss: 0.8088018894195557]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 29/86 [D loss: 0.6631757915019989, acc.: 60.35%] [G loss: 0.8078438639640808]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 30/86 [D loss: 0.6634067296981812, acc.: 60.06%] [G loss: 0.7937244176864624]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 187/200, Batch 31/86 [D loss: 0.6624841690063477, acc.: 59.81%] [G loss: 0.7916672825813293]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 32/86 [D loss: 0.6549582183361053, acc.: 62.06%] [G loss: 0.8046388030052185]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 33/86 [D loss: 0.661008894443512, acc.: 59.33%] [G loss: 0.7984917163848877]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 34/86 [D loss: 0.6549636721611023, acc.: 61.04%] [G loss: 0.7950985431671143]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 35/86 [D loss: 0.6536661684513092, acc.: 61.72%] [G loss: 0.8033639788627625]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 36/86 [D loss: 0.65473273396492, acc.: 62.60%] [G loss: 0.8103255033493042]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 37/86 [D loss: 0.6609689891338348, acc.: 60.40%] [G loss: 0.8045282959938049]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 187/200, Batch 38/86 [D loss: 0.653239369392395, acc.: 59.62%] [G loss: 0.7933281064033508]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 39/86 [D loss: 0.6642783880233765, acc.: 58.79%] [G loss: 0.7887552380561829]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 40/86 [D loss: 0.6622510552406311, acc.: 59.18%] [G loss: 0.7983307242393494]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 187/200, Batch 41/86 [D loss: 0.661627322435379, acc.: 59.57%] [G loss: 0.7914816737174988]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 42/86 [D loss: 0.653327614068985, acc.: 62.99%] [G loss: 0.7943453788757324]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 43/86 [D loss: 0.6630738377571106, acc.: 58.54%] [G loss: 0.8037968873977661]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 44/86 [D loss: 0.6591753959655762, acc.: 61.13%] [G loss: 0.8065623044967651]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 45/86 [D loss: 0.6656470596790314, acc.: 59.03%] [G loss: 0.804224967956543]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 46/86 [D loss: 0.6651362180709839, acc.: 59.47%] [G loss: 0.8001909852027893]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 47/86 [D loss: 0.6561361253261566, acc.: 61.96%] [G loss: 0.8030840754508972]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 48/86 [D loss: 0.6668551564216614, acc.: 58.25%] [G loss: 0.8061668872833252]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 49/86 [D loss: 0.6540753245353699, acc.: 62.16%] [G loss: 0.7991642951965332]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 50/86 [D loss: 0.6599552035331726, acc.: 61.04%] [G loss: 0.791434109210968]\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "Epoch 187/200, Batch 51/86 [D loss: 0.6590358018875122, acc.: 59.33%] [G loss: 0.7841755151748657]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 52/86 [D loss: 0.6536595821380615, acc.: 62.11%] [G loss: 0.8078547716140747]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 187/200, Batch 53/86 [D loss: 0.66693115234375, acc.: 58.59%] [G loss: 0.8009359240531921]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 54/86 [D loss: 0.6629151999950409, acc.: 59.57%] [G loss: 0.8025866746902466]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 55/86 [D loss: 0.6629728078842163, acc.: 60.06%] [G loss: 0.7911272644996643]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 56/86 [D loss: 0.6659497618675232, acc.: 60.06%] [G loss: 0.7942447662353516]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 57/86 [D loss: 0.66152423620224, acc.: 59.52%] [G loss: 0.7959975004196167]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 187/200, Batch 58/86 [D loss: 0.6598197817802429, acc.: 61.47%] [G loss: 0.7932043671607971]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 59/86 [D loss: 0.6630971431732178, acc.: 60.45%] [G loss: 0.7914406061172485]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 60/86 [D loss: 0.6617608666419983, acc.: 60.55%] [G loss: 0.7908784747123718]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 187/200, Batch 61/86 [D loss: 0.6540086269378662, acc.: 60.94%] [G loss: 0.7947144508361816]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 62/86 [D loss: 0.6605570614337921, acc.: 58.74%] [G loss: 0.7962117791175842]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 63/86 [D loss: 0.6557126045227051, acc.: 61.57%] [G loss: 0.798537015914917]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 64/86 [D loss: 0.6636536121368408, acc.: 59.33%] [G loss: 0.7932855486869812]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 187/200, Batch 65/86 [D loss: 0.6516695916652679, acc.: 62.74%] [G loss: 0.8070135712623596]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 66/86 [D loss: 0.6571940183639526, acc.: 60.11%] [G loss: 0.7904706001281738]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 67/86 [D loss: 0.6599160730838776, acc.: 60.64%] [G loss: 0.7950242757797241]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 68/86 [D loss: 0.6606993079185486, acc.: 60.21%] [G loss: 0.7999436855316162]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 69/86 [D loss: 0.6623231470584869, acc.: 59.38%] [G loss: 0.7892472147941589]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 70/86 [D loss: 0.6571502685546875, acc.: 59.81%] [G loss: 0.7994505167007446]\n",
      "32/32 [==============================] - 1s 21ms/step\n",
      "Epoch 187/200, Batch 71/86 [D loss: 0.6588902175426483, acc.: 59.96%] [G loss: 0.803137481212616]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 72/86 [D loss: 0.6553744971752167, acc.: 60.16%] [G loss: 0.8033261299133301]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 187/200, Batch 73/86 [D loss: 0.6547451913356781, acc.: 60.06%] [G loss: 0.8036829233169556]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 187/200, Batch 74/86 [D loss: 0.665669322013855, acc.: 57.32%] [G loss: 0.8011337518692017]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 75/86 [D loss: 0.6591387987136841, acc.: 60.69%] [G loss: 0.7883615493774414]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 76/86 [D loss: 0.6594359576702118, acc.: 60.30%] [G loss: 0.799414336681366]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 77/86 [D loss: 0.6591233313083649, acc.: 59.42%] [G loss: 0.7960941195487976]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 78/86 [D loss: 0.6593692600727081, acc.: 59.47%] [G loss: 0.7947452664375305]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 79/86 [D loss: 0.6659471988677979, acc.: 58.94%] [G loss: 0.8022662401199341]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 80/86 [D loss: 0.6569690406322479, acc.: 61.33%] [G loss: 0.789452850818634]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 81/86 [D loss: 0.6628192663192749, acc.: 59.81%] [G loss: 0.798732578754425]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 187/200, Batch 82/86 [D loss: 0.6591179966926575, acc.: 59.96%] [G loss: 0.8070489168167114]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 83/86 [D loss: 0.660736620426178, acc.: 60.25%] [G loss: 0.7894809246063232]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 187/200, Batch 84/86 [D loss: 0.6619177162647247, acc.: 60.06%] [G loss: 0.8032021522521973]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 187/200, Batch 85/86 [D loss: 0.6587617695331573, acc.: 59.42%] [G loss: 0.7940807938575745]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 187/200, Batch 86/86 [D loss: 0.6557428538799286, acc.: 61.13%] [G loss: 0.797210156917572]\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 188/200, Batch 1/86 [D loss: 0.6514991223812103, acc.: 61.91%] [G loss: 0.7990933656692505]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 2/86 [D loss: 0.6524426341056824, acc.: 62.21%] [G loss: 0.8117889761924744]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 3/86 [D loss: 0.6540088951587677, acc.: 62.45%] [G loss: 0.8053796887397766]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 4/86 [D loss: 0.6536075174808502, acc.: 62.40%] [G loss: 0.7920554280281067]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 5/86 [D loss: 0.6555446684360504, acc.: 62.16%] [G loss: 0.7943321466445923]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 6/86 [D loss: 0.664214700460434, acc.: 59.18%] [G loss: 0.7865106463432312]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 7/86 [D loss: 0.6585830748081207, acc.: 60.45%] [G loss: 0.7885938882827759]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 8/86 [D loss: 0.6572145819664001, acc.: 60.94%] [G loss: 0.7969095706939697]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 188/200, Batch 9/86 [D loss: 0.6608952283859253, acc.: 58.89%] [G loss: 0.7994862794876099]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 10/86 [D loss: 0.6600809395313263, acc.: 60.06%] [G loss: 0.7979361414909363]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 11/86 [D loss: 0.6591141819953918, acc.: 59.72%] [G loss: 0.7972680926322937]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 12/86 [D loss: 0.6637678444385529, acc.: 60.25%] [G loss: 0.7903017401695251]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 13/86 [D loss: 0.6635267734527588, acc.: 58.79%] [G loss: 0.7873899936676025]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 14/86 [D loss: 0.6585048735141754, acc.: 59.38%] [G loss: 0.796183705329895]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 15/86 [D loss: 0.6610448956489563, acc.: 59.86%] [G loss: 0.8019903898239136]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 16/86 [D loss: 0.6574310064315796, acc.: 60.79%] [G loss: 0.8027281761169434]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 188/200, Batch 17/86 [D loss: 0.6619955599308014, acc.: 59.42%] [G loss: 0.8102697134017944]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 18/86 [D loss: 0.6527803838253021, acc.: 62.21%] [G loss: 0.7996162176132202]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 19/86 [D loss: 0.6558615267276764, acc.: 61.72%] [G loss: 0.8067938089370728]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 20/86 [D loss: 0.6557861566543579, acc.: 60.79%] [G loss: 0.8081223964691162]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 188/200, Batch 21/86 [D loss: 0.6563518941402435, acc.: 60.06%] [G loss: 0.7961231470108032]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 22/86 [D loss: 0.6645665168762207, acc.: 58.94%] [G loss: 0.7976288199424744]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 23/86 [D loss: 0.6560431718826294, acc.: 60.94%] [G loss: 0.7992253303527832]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 24/86 [D loss: 0.6618508100509644, acc.: 60.50%] [G loss: 0.8091573715209961]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 25/86 [D loss: 0.6588751971721649, acc.: 60.40%] [G loss: 0.7980591058731079]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 26/86 [D loss: 0.666348934173584, acc.: 58.25%] [G loss: 0.8023986220359802]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 27/86 [D loss: 0.6513813436031342, acc.: 61.96%] [G loss: 0.8041483163833618]\n",
      "32/32 [==============================] - 1s 20ms/step\n",
      "Epoch 188/200, Batch 28/86 [D loss: 0.6604132950305939, acc.: 59.18%] [G loss: 0.8074035048484802]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 29/86 [D loss: 0.6585068106651306, acc.: 59.86%] [G loss: 0.8131449222564697]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 30/86 [D loss: 0.6601075828075409, acc.: 60.30%] [G loss: 0.7964959740638733]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 31/86 [D loss: 0.6621761620044708, acc.: 58.98%] [G loss: 0.8030800223350525]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 188/200, Batch 32/86 [D loss: 0.6621047556400299, acc.: 60.74%] [G loss: 0.7930304408073425]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 188/200, Batch 33/86 [D loss: 0.6660565435886383, acc.: 59.91%] [G loss: 0.7905793190002441]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 34/86 [D loss: 0.6570813059806824, acc.: 60.79%] [G loss: 0.7966338992118835]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 35/86 [D loss: 0.66145458817482, acc.: 59.18%] [G loss: 0.7918779850006104]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 36/86 [D loss: 0.6578060388565063, acc.: 61.33%] [G loss: 0.7960723042488098]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 37/86 [D loss: 0.658416748046875, acc.: 60.55%] [G loss: 0.79443359375]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 38/86 [D loss: 0.6570003032684326, acc.: 60.60%] [G loss: 0.8036068677902222]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 39/86 [D loss: 0.660598874092102, acc.: 59.62%] [G loss: 0.8024642467498779]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 188/200, Batch 40/86 [D loss: 0.6634187698364258, acc.: 59.38%] [G loss: 0.8041402697563171]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 188/200, Batch 41/86 [D loss: 0.6593091487884521, acc.: 60.64%] [G loss: 0.8007687926292419]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 42/86 [D loss: 0.6554516553878784, acc.: 61.47%] [G loss: 0.8055779933929443]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 43/86 [D loss: 0.6632490456104279, acc.: 59.57%] [G loss: 0.8029148578643799]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 188/200, Batch 44/86 [D loss: 0.6632982492446899, acc.: 61.18%] [G loss: 0.7981532216072083]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 45/86 [D loss: 0.665046215057373, acc.: 58.54%] [G loss: 0.7908891439437866]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 46/86 [D loss: 0.6616411805152893, acc.: 59.86%] [G loss: 0.8091058731079102]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 47/86 [D loss: 0.6686910688877106, acc.: 57.67%] [G loss: 0.7961217164993286]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 48/86 [D loss: 0.65733402967453, acc.: 61.23%] [G loss: 0.8005926609039307]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 188/200, Batch 49/86 [D loss: 0.6633155643939972, acc.: 60.55%] [G loss: 0.8046008348464966]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 188/200, Batch 50/86 [D loss: 0.6603468060493469, acc.: 59.86%] [G loss: 0.8062964677810669]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 188/200, Batch 51/86 [D loss: 0.6606694757938385, acc.: 60.45%] [G loss: 0.7930361032485962]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 52/86 [D loss: 0.6613013744354248, acc.: 60.64%] [G loss: 0.7940284609794617]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 53/86 [D loss: 0.6628587245941162, acc.: 59.23%] [G loss: 0.8009316921234131]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 54/86 [D loss: 0.6564289331436157, acc.: 60.30%] [G loss: 0.8002299666404724]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 55/86 [D loss: 0.6630035042762756, acc.: 59.81%] [G loss: 0.7994101047515869]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 56/86 [D loss: 0.666721761226654, acc.: 58.64%] [G loss: 0.7962058186531067]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 57/86 [D loss: 0.65960893034935, acc.: 61.23%] [G loss: 0.8048722743988037]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 188/200, Batch 58/86 [D loss: 0.6634301245212555, acc.: 58.59%] [G loss: 0.8072919845581055]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 188/200, Batch 59/86 [D loss: 0.6556951999664307, acc.: 61.91%] [G loss: 0.7886471748352051]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 60/86 [D loss: 0.6614665985107422, acc.: 59.38%] [G loss: 0.8016780614852905]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 61/86 [D loss: 0.6585991978645325, acc.: 60.40%] [G loss: 0.7962468862533569]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 62/86 [D loss: 0.6591356098651886, acc.: 59.23%] [G loss: 0.7949929237365723]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 63/86 [D loss: 0.6627586781978607, acc.: 59.38%] [G loss: 0.7924596071243286]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 188/200, Batch 64/86 [D loss: 0.662738174200058, acc.: 60.11%] [G loss: 0.788318932056427]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 65/86 [D loss: 0.6639690101146698, acc.: 59.72%] [G loss: 0.7981709837913513]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 66/86 [D loss: 0.6528155207633972, acc.: 62.79%] [G loss: 0.7997405529022217]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 188/200, Batch 67/86 [D loss: 0.6636348068714142, acc.: 60.30%] [G loss: 0.7943382859230042]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 68/86 [D loss: 0.6615112721920013, acc.: 59.86%] [G loss: 0.7924149632453918]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 69/86 [D loss: 0.661955863237381, acc.: 59.72%] [G loss: 0.797612726688385]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 70/86 [D loss: 0.6571263372898102, acc.: 61.18%] [G loss: 0.7935535311698914]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 188/200, Batch 71/86 [D loss: 0.6598699688911438, acc.: 58.98%] [G loss: 0.7886990308761597]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 72/86 [D loss: 0.658585399389267, acc.: 60.94%] [G loss: 0.7943977117538452]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 73/86 [D loss: 0.65897998213768, acc.: 60.30%] [G loss: 0.8037015795707703]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 188/200, Batch 74/86 [D loss: 0.6531420946121216, acc.: 62.89%] [G loss: 0.7906423807144165]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 188/200, Batch 75/86 [D loss: 0.6619636714458466, acc.: 59.72%] [G loss: 0.8008348345756531]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 76/86 [D loss: 0.6580382287502289, acc.: 61.08%] [G loss: 0.8037046194076538]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 77/86 [D loss: 0.6581637263298035, acc.: 60.55%] [G loss: 0.7925517559051514]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 78/86 [D loss: 0.6518007516860962, acc.: 61.18%] [G loss: 0.8055707216262817]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 79/86 [D loss: 0.670892745256424, acc.: 56.93%] [G loss: 0.7949046492576599]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 80/86 [D loss: 0.655801922082901, acc.: 61.77%] [G loss: 0.7954455018043518]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 81/86 [D loss: 0.6571562588214874, acc.: 61.38%] [G loss: 0.8033826351165771]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 188/200, Batch 82/86 [D loss: 0.6644655168056488, acc.: 57.91%] [G loss: 0.787755012512207]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 83/86 [D loss: 0.6562687456607819, acc.: 61.43%] [G loss: 0.8048860430717468]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 188/200, Batch 84/86 [D loss: 0.6608476042747498, acc.: 60.99%] [G loss: 0.8147850632667542]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 85/86 [D loss: 0.6609039008617401, acc.: 61.04%] [G loss: 0.7996112704277039]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 188/200, Batch 86/86 [D loss: 0.6642056107521057, acc.: 58.59%] [G loss: 0.8022363185882568]\n",
      "4/4 [==============================] - 0s 22ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 189/200, Batch 1/86 [D loss: 0.6587215960025787, acc.: 60.11%] [G loss: 0.792078971862793]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 2/86 [D loss: 0.6567714810371399, acc.: 60.79%] [G loss: 0.7976014018058777]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 189/200, Batch 3/86 [D loss: 0.6553393006324768, acc.: 62.60%] [G loss: 0.8108126521110535]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 4/86 [D loss: 0.6570325195789337, acc.: 61.18%] [G loss: 0.8058303594589233]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 5/86 [D loss: 0.6607290208339691, acc.: 59.42%] [G loss: 0.7882304191589355]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 6/86 [D loss: 0.6633219420909882, acc.: 58.94%] [G loss: 0.8044544458389282]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 7/86 [D loss: 0.6587614417076111, acc.: 60.99%] [G loss: 0.7949891090393066]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 8/86 [D loss: 0.65719074010849, acc.: 61.23%] [G loss: 0.8118789196014404]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 9/86 [D loss: 0.6501106023788452, acc.: 62.35%] [G loss: 0.8038676381111145]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 10/86 [D loss: 0.6609334945678711, acc.: 60.64%] [G loss: 0.7933291792869568]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 11/86 [D loss: 0.6647500991821289, acc.: 57.91%] [G loss: 0.7929359674453735]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 12/86 [D loss: 0.6632827818393707, acc.: 58.30%] [G loss: 0.7976932525634766]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 13/86 [D loss: 0.6583391427993774, acc.: 60.50%] [G loss: 0.7964857220649719]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 14/86 [D loss: 0.6623093783855438, acc.: 59.72%] [G loss: 0.8067607879638672]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 15/86 [D loss: 0.6601734757423401, acc.: 58.98%] [G loss: 0.789011538028717]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 16/86 [D loss: 0.6608380973339081, acc.: 59.08%] [G loss: 0.7901244759559631]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 189/200, Batch 17/86 [D loss: 0.6587710678577423, acc.: 61.57%] [G loss: 0.7911084294319153]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 18/86 [D loss: 0.6620750427246094, acc.: 59.18%] [G loss: 0.7902064323425293]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 19/86 [D loss: 0.6600769460201263, acc.: 59.67%] [G loss: 0.8006405830383301]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 20/86 [D loss: 0.6629964709281921, acc.: 58.59%] [G loss: 0.7951505184173584]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 189/200, Batch 21/86 [D loss: 0.6619158089160919, acc.: 59.72%] [G loss: 0.7890554666519165]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 22/86 [D loss: 0.655275285243988, acc.: 60.99%] [G loss: 0.8073704838752747]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 23/86 [D loss: 0.6532965898513794, acc.: 62.30%] [G loss: 0.8002196550369263]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 189/200, Batch 24/86 [D loss: 0.6542811989784241, acc.: 62.01%] [G loss: 0.7964161038398743]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 25/86 [D loss: 0.6551656424999237, acc.: 62.11%] [G loss: 0.8015000224113464]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 189/200, Batch 26/86 [D loss: 0.659760057926178, acc.: 60.21%] [G loss: 0.7913858890533447]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 189/200, Batch 27/86 [D loss: 0.6584032475948334, acc.: 58.84%] [G loss: 0.7966287136077881]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 28/86 [D loss: 0.664696455001831, acc.: 59.03%] [G loss: 0.7952001690864563]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 29/86 [D loss: 0.657887727022171, acc.: 61.33%] [G loss: 0.7957369089126587]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 30/86 [D loss: 0.6600932776927948, acc.: 59.57%] [G loss: 0.7866781949996948]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 189/200, Batch 31/86 [D loss: 0.6598125994205475, acc.: 59.91%] [G loss: 0.8033369183540344]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 32/86 [D loss: 0.6629794836044312, acc.: 58.54%] [G loss: 0.7924352884292603]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 33/86 [D loss: 0.657002866268158, acc.: 60.79%] [G loss: 0.7894269227981567]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 34/86 [D loss: 0.6572915017604828, acc.: 60.84%] [G loss: 0.8044599294662476]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 35/86 [D loss: 0.6565978229045868, acc.: 61.62%] [G loss: 0.8026623725891113]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 36/86 [D loss: 0.668249785900116, acc.: 57.28%] [G loss: 0.7924903035163879]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 37/86 [D loss: 0.6634226143360138, acc.: 58.94%] [G loss: 0.7986945509910583]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 38/86 [D loss: 0.6513174176216125, acc.: 61.62%] [G loss: 0.7971141934394836]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 39/86 [D loss: 0.6610883176326752, acc.: 61.28%] [G loss: 0.7949992418289185]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 40/86 [D loss: 0.659202367067337, acc.: 59.42%] [G loss: 0.8060891032218933]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 41/86 [D loss: 0.6543432176113129, acc.: 62.79%] [G loss: 0.8046709895133972]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 42/86 [D loss: 0.6590960621833801, acc.: 61.23%] [G loss: 0.7963674664497375]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 43/86 [D loss: 0.6576730012893677, acc.: 59.86%] [G loss: 0.8130261898040771]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 44/86 [D loss: 0.6574482619762421, acc.: 61.52%] [G loss: 0.8167290091514587]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 45/86 [D loss: 0.6557335257530212, acc.: 61.43%] [G loss: 0.791449785232544]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 46/86 [D loss: 0.6556445062160492, acc.: 61.67%] [G loss: 0.7956717014312744]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 47/86 [D loss: 0.6556921899318695, acc.: 60.16%] [G loss: 0.8090358972549438]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 48/86 [D loss: 0.6623470783233643, acc.: 59.42%] [G loss: 0.7969386577606201]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 49/86 [D loss: 0.6597316563129425, acc.: 60.25%] [G loss: 0.8015403747558594]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 189/200, Batch 50/86 [D loss: 0.6535342931747437, acc.: 61.08%] [G loss: 0.7894055843353271]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 189/200, Batch 51/86 [D loss: 0.6592780947685242, acc.: 60.60%] [G loss: 0.7924411296844482]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 52/86 [D loss: 0.6570204794406891, acc.: 60.99%] [G loss: 0.7917477488517761]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 53/86 [D loss: 0.6570535004138947, acc.: 60.74%] [G loss: 0.7900435328483582]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 54/86 [D loss: 0.6585183441638947, acc.: 60.45%] [G loss: 0.8120103478431702]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 55/86 [D loss: 0.656871110200882, acc.: 61.33%] [G loss: 0.7957745790481567]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 189/200, Batch 56/86 [D loss: 0.6617454290390015, acc.: 59.77%] [G loss: 0.7958521246910095]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 57/86 [D loss: 0.6551114916801453, acc.: 60.40%] [G loss: 0.803362250328064]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 58/86 [D loss: 0.6550959348678589, acc.: 60.84%] [G loss: 0.7961327433586121]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 59/86 [D loss: 0.6577594876289368, acc.: 60.94%] [G loss: 0.7975261211395264]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 60/86 [D loss: 0.6651615798473358, acc.: 58.59%] [G loss: 0.7972716689109802]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 61/86 [D loss: 0.6609252393245697, acc.: 60.45%] [G loss: 0.7968742847442627]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 62/86 [D loss: 0.6618805527687073, acc.: 60.30%] [G loss: 0.8054327368736267]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 63/86 [D loss: 0.6555781066417694, acc.: 61.67%] [G loss: 0.800977885723114]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 64/86 [D loss: 0.6656111180782318, acc.: 59.91%] [G loss: 0.791926383972168]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 65/86 [D loss: 0.6531079709529877, acc.: 60.94%] [G loss: 0.7954725027084351]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 66/86 [D loss: 0.6588554382324219, acc.: 60.11%] [G loss: 0.79127037525177]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 67/86 [D loss: 0.6623531877994537, acc.: 60.89%] [G loss: 0.8086896538734436]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 68/86 [D loss: 0.6616540849208832, acc.: 60.01%] [G loss: 0.8083826899528503]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 69/86 [D loss: 0.6557844579219818, acc.: 61.23%] [G loss: 0.8165862560272217]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 70/86 [D loss: 0.6542114913463593, acc.: 61.18%] [G loss: 0.8055720925331116]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 71/86 [D loss: 0.6522971987724304, acc.: 61.67%] [G loss: 0.8048881888389587]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 72/86 [D loss: 0.6612535119056702, acc.: 59.86%] [G loss: 0.7985919117927551]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 73/86 [D loss: 0.658293753862381, acc.: 61.43%] [G loss: 0.8046770095825195]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 189/200, Batch 74/86 [D loss: 0.6615859568119049, acc.: 60.01%] [G loss: 0.8051742315292358]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 75/86 [D loss: 0.656771183013916, acc.: 61.52%] [G loss: 0.7947060465812683]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 76/86 [D loss: 0.6613528728485107, acc.: 60.60%] [G loss: 0.7948849201202393]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 77/86 [D loss: 0.6597585082054138, acc.: 60.89%] [G loss: 0.8028379082679749]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 189/200, Batch 78/86 [D loss: 0.6637440919876099, acc.: 59.38%] [G loss: 0.7954205274581909]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 79/86 [D loss: 0.6591041088104248, acc.: 60.21%] [G loss: 0.8003427982330322]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 80/86 [D loss: 0.6607436835765839, acc.: 60.45%] [G loss: 0.7932513356208801]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 189/200, Batch 81/86 [D loss: 0.6616563498973846, acc.: 61.33%] [G loss: 0.8092529773712158]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 189/200, Batch 82/86 [D loss: 0.6621727049350739, acc.: 59.18%] [G loss: 0.8047101497650146]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 83/86 [D loss: 0.6538901925086975, acc.: 61.87%] [G loss: 0.8048431873321533]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 189/200, Batch 84/86 [D loss: 0.6557943820953369, acc.: 60.94%] [G loss: 0.8015391826629639]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 85/86 [D loss: 0.661525696516037, acc.: 59.08%] [G loss: 0.8024234175682068]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 189/200, Batch 86/86 [D loss: 0.6631036996841431, acc.: 59.81%] [G loss: 0.7927654981613159]\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 190/200, Batch 1/86 [D loss: 0.6606461405754089, acc.: 60.06%] [G loss: 0.8023422956466675]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 2/86 [D loss: 0.6564977765083313, acc.: 61.52%] [G loss: 0.7956582903862]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 3/86 [D loss: 0.6568090915679932, acc.: 60.50%] [G loss: 0.7976333498954773]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 4/86 [D loss: 0.6608097851276398, acc.: 60.21%] [G loss: 0.7958274483680725]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 190/200, Batch 5/86 [D loss: 0.6614397168159485, acc.: 59.72%] [G loss: 0.8060914278030396]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 6/86 [D loss: 0.6634263098239899, acc.: 58.98%] [G loss: 0.7921424508094788]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 7/86 [D loss: 0.651504635810852, acc.: 61.38%] [G loss: 0.7991949915885925]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 8/86 [D loss: 0.6673583686351776, acc.: 59.47%] [G loss: 0.8092994689941406]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 9/86 [D loss: 0.6563363671302795, acc.: 61.28%] [G loss: 0.8143781423568726]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 190/200, Batch 10/86 [D loss: 0.6567278802394867, acc.: 61.38%] [G loss: 0.8057935237884521]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 11/86 [D loss: 0.6526148319244385, acc.: 61.62%] [G loss: 0.8044753670692444]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 12/86 [D loss: 0.6571947932243347, acc.: 61.13%] [G loss: 0.7953522205352783]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 13/86 [D loss: 0.6681708991527557, acc.: 58.64%] [G loss: 0.8087339401245117]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 14/86 [D loss: 0.662549614906311, acc.: 58.79%] [G loss: 0.8003296852111816]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 15/86 [D loss: 0.658743679523468, acc.: 60.30%] [G loss: 0.8031136393547058]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 16/86 [D loss: 0.6602666676044464, acc.: 59.38%] [G loss: 0.7996053695678711]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 17/86 [D loss: 0.6597152054309845, acc.: 60.16%] [G loss: 0.7931138277053833]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 18/86 [D loss: 0.6565293967723846, acc.: 61.82%] [G loss: 0.8018288612365723]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 190/200, Batch 19/86 [D loss: 0.6584987640380859, acc.: 61.04%] [G loss: 0.8017450571060181]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 20/86 [D loss: 0.6536320447921753, acc.: 61.28%] [G loss: 0.7977051734924316]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 21/86 [D loss: 0.6548221111297607, acc.: 60.94%] [G loss: 0.7956799268722534]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 22/86 [D loss: 0.665821760892868, acc.: 58.15%] [G loss: 0.798563539981842]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 23/86 [D loss: 0.6667689681053162, acc.: 57.28%] [G loss: 0.8044685125350952]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 190/200, Batch 24/86 [D loss: 0.6581251919269562, acc.: 60.89%] [G loss: 0.8012384176254272]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 25/86 [D loss: 0.6546916961669922, acc.: 61.77%] [G loss: 0.8055111765861511]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 26/86 [D loss: 0.6592907905578613, acc.: 60.25%] [G loss: 0.8038119673728943]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 190/200, Batch 27/86 [D loss: 0.6596100330352783, acc.: 59.57%] [G loss: 0.7967163324356079]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 28/86 [D loss: 0.6627329289913177, acc.: 59.23%] [G loss: 0.8084874749183655]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 29/86 [D loss: 0.6535331010818481, acc.: 62.40%] [G loss: 0.7988777756690979]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 190/200, Batch 30/86 [D loss: 0.6622799634933472, acc.: 59.47%] [G loss: 0.7992404699325562]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 31/86 [D loss: 0.664962112903595, acc.: 58.69%] [G loss: 0.807000458240509]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 32/86 [D loss: 0.6570659875869751, acc.: 60.89%] [G loss: 0.801227331161499]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 190/200, Batch 33/86 [D loss: 0.6602535247802734, acc.: 60.99%] [G loss: 0.8198662996292114]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 34/86 [D loss: 0.6572181880474091, acc.: 60.89%] [G loss: 0.8011756539344788]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 35/86 [D loss: 0.6661409139633179, acc.: 58.79%] [G loss: 0.802128255367279]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 36/86 [D loss: 0.6499956250190735, acc.: 60.99%] [G loss: 0.7883583903312683]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 37/86 [D loss: 0.6591710150241852, acc.: 59.86%] [G loss: 0.7988635301589966]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 38/86 [D loss: 0.661146342754364, acc.: 58.89%] [G loss: 0.8011931777000427]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 190/200, Batch 39/86 [D loss: 0.667864054441452, acc.: 57.76%] [G loss: 0.7920899987220764]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 40/86 [D loss: 0.6617506146430969, acc.: 60.01%] [G loss: 0.8036413788795471]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 41/86 [D loss: 0.6591028869152069, acc.: 59.42%] [G loss: 0.8087208867073059]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 42/86 [D loss: 0.6626192927360535, acc.: 59.81%] [G loss: 0.8023279905319214]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 43/86 [D loss: 0.6581176221370697, acc.: 59.57%] [G loss: 0.8041540384292603]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 44/86 [D loss: 0.6618950366973877, acc.: 59.86%] [G loss: 0.7920957207679749]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 45/86 [D loss: 0.65517258644104, acc.: 61.23%] [G loss: 0.7959458231925964]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 46/86 [D loss: 0.6577236950397491, acc.: 60.45%] [G loss: 0.790012001991272]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 47/86 [D loss: 0.6603552401065826, acc.: 59.33%] [G loss: 0.8049883842468262]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 48/86 [D loss: 0.6603934168815613, acc.: 60.64%] [G loss: 0.8029072284698486]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 49/86 [D loss: 0.6532140374183655, acc.: 62.45%] [G loss: 0.8096312284469604]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 50/86 [D loss: 0.6645366847515106, acc.: 58.54%] [G loss: 0.7849502563476562]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 51/86 [D loss: 0.6620476245880127, acc.: 60.06%] [G loss: 0.8087484240531921]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 52/86 [D loss: 0.662302553653717, acc.: 59.86%] [G loss: 0.7929586172103882]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 53/86 [D loss: 0.6644467711448669, acc.: 58.94%] [G loss: 0.8081001043319702]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 54/86 [D loss: 0.6515701115131378, acc.: 62.21%] [G loss: 0.8142242431640625]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 55/86 [D loss: 0.6648388803005219, acc.: 58.74%] [G loss: 0.8008624911308289]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 56/86 [D loss: 0.6562351882457733, acc.: 60.69%] [G loss: 0.8118732571601868]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 57/86 [D loss: 0.6606633961200714, acc.: 60.55%] [G loss: 0.8109977841377258]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 190/200, Batch 58/86 [D loss: 0.6493978798389435, acc.: 63.04%] [G loss: 0.8206424713134766]\n",
      "32/32 [==============================] - 1s 20ms/step\n",
      "Epoch 190/200, Batch 59/86 [D loss: 0.6601231098175049, acc.: 58.98%] [G loss: 0.8036688566207886]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 190/200, Batch 60/86 [D loss: 0.6605751514434814, acc.: 58.89%] [G loss: 0.8069729804992676]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 61/86 [D loss: 0.6608359217643738, acc.: 59.23%] [G loss: 0.7951153516769409]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 62/86 [D loss: 0.6574091911315918, acc.: 61.52%] [G loss: 0.7875430583953857]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 63/86 [D loss: 0.6605649888515472, acc.: 59.33%] [G loss: 0.7962239980697632]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 64/86 [D loss: 0.6609564423561096, acc.: 59.57%] [G loss: 0.7976149916648865]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 65/86 [D loss: 0.6595602035522461, acc.: 60.79%] [G loss: 0.7873880863189697]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 66/86 [D loss: 0.658296674489975, acc.: 60.60%] [G loss: 0.7998915314674377]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 190/200, Batch 67/86 [D loss: 0.6505319774150848, acc.: 61.72%] [G loss: 0.8078050017356873]\n",
      "32/32 [==============================] - 0s 16ms/step\n",
      "Epoch 190/200, Batch 68/86 [D loss: 0.6632660329341888, acc.: 58.94%] [G loss: 0.8051693439483643]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 69/86 [D loss: 0.6527387797832489, acc.: 62.70%] [G loss: 0.8104609847068787]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 190/200, Batch 70/86 [D loss: 0.6601840853691101, acc.: 61.38%] [G loss: 0.802497386932373]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 71/86 [D loss: 0.6574991643428802, acc.: 59.86%] [G loss: 0.7989246845245361]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 190/200, Batch 72/86 [D loss: 0.6660583019256592, acc.: 58.45%] [G loss: 0.7955699563026428]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 73/86 [D loss: 0.6639999747276306, acc.: 60.45%] [G loss: 0.7975505590438843]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 74/86 [D loss: 0.6597640812397003, acc.: 60.21%] [G loss: 0.7986748218536377]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 75/86 [D loss: 0.6633825302124023, acc.: 61.38%] [G loss: 0.7992657423019409]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 76/86 [D loss: 0.6630538702011108, acc.: 59.38%] [G loss: 0.7988396883010864]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 77/86 [D loss: 0.6535820066928864, acc.: 61.08%] [G loss: 0.8007059693336487]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 78/86 [D loss: 0.6555323302745819, acc.: 62.21%] [G loss: 0.7960019111633301]\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "Epoch 190/200, Batch 79/86 [D loss: 0.6592601239681244, acc.: 60.35%] [G loss: 0.7942763566970825]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 80/86 [D loss: 0.6653071045875549, acc.: 58.74%] [G loss: 0.7941201329231262]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 81/86 [D loss: 0.6557012498378754, acc.: 61.52%] [G loss: 0.7945151329040527]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 190/200, Batch 82/86 [D loss: 0.6577070653438568, acc.: 60.25%] [G loss: 0.8016079068183899]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 83/86 [D loss: 0.6511688232421875, acc.: 63.38%] [G loss: 0.7992473840713501]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 84/86 [D loss: 0.6651893258094788, acc.: 57.91%] [G loss: 0.8000342845916748]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 190/200, Batch 85/86 [D loss: 0.6536658704280853, acc.: 60.35%] [G loss: 0.8054769039154053]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 190/200, Batch 86/86 [D loss: 0.6650541722774506, acc.: 59.42%] [G loss: 0.8028849959373474]\n",
      "4/4 [==============================] - 0s 22ms/step\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 191/200, Batch 1/86 [D loss: 0.6554781794548035, acc.: 60.45%] [G loss: 0.8018198013305664]\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "Epoch 191/200, Batch 2/86 [D loss: 0.6626618206501007, acc.: 60.55%] [G loss: 0.7888261079788208]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 191/200, Batch 3/86 [D loss: 0.6522902250289917, acc.: 62.35%] [G loss: 0.8040746450424194]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 191/200, Batch 4/86 [D loss: 0.6536995768547058, acc.: 60.94%] [G loss: 0.7982577085494995]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 191/200, Batch 5/86 [D loss: 0.6678276360034943, acc.: 58.45%] [G loss: 0.7917911410331726]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 6/86 [D loss: 0.6669295132160187, acc.: 58.84%] [G loss: 0.8002215623855591]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 7/86 [D loss: 0.6625374257564545, acc.: 59.03%] [G loss: 0.7961927056312561]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 191/200, Batch 8/86 [D loss: 0.6564055979251862, acc.: 60.35%] [G loss: 0.7975962162017822]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 9/86 [D loss: 0.6572402715682983, acc.: 60.40%] [G loss: 0.80318284034729]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 10/86 [D loss: 0.6585916876792908, acc.: 60.45%] [G loss: 0.7965174913406372]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 11/86 [D loss: 0.6599068939685822, acc.: 62.21%] [G loss: 0.7908650040626526]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 191/200, Batch 12/86 [D loss: 0.6594478189945221, acc.: 59.77%] [G loss: 0.7906116247177124]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 13/86 [D loss: 0.6531376838684082, acc.: 62.40%] [G loss: 0.8018507361412048]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 14/86 [D loss: 0.6599074900150299, acc.: 60.40%] [G loss: 0.7962978482246399]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 15/86 [D loss: 0.6539335548877716, acc.: 62.79%] [G loss: 0.7976049184799194]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 16/86 [D loss: 0.6544892489910126, acc.: 59.96%] [G loss: 0.8089421987533569]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 17/86 [D loss: 0.6580991744995117, acc.: 60.60%] [G loss: 0.7973405122756958]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 18/86 [D loss: 0.6590595841407776, acc.: 60.50%] [G loss: 0.8110873103141785]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 19/86 [D loss: 0.654838353395462, acc.: 61.72%] [G loss: 0.7970864176750183]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 20/86 [D loss: 0.6623823940753937, acc.: 61.43%] [G loss: 0.8013138771057129]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 21/86 [D loss: 0.6635375618934631, acc.: 58.98%] [G loss: 0.7966852784156799]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 191/200, Batch 22/86 [D loss: 0.6527642607688904, acc.: 61.23%] [G loss: 0.8091710805892944]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 23/86 [D loss: 0.6611371040344238, acc.: 59.28%] [G loss: 0.7978594899177551]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 24/86 [D loss: 0.6568978130817413, acc.: 59.08%] [G loss: 0.816608190536499]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 191/200, Batch 25/86 [D loss: 0.6648282408714294, acc.: 58.45%] [G loss: 0.8058074712753296]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 26/86 [D loss: 0.6553495526313782, acc.: 60.89%] [G loss: 0.7876027822494507]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 27/86 [D loss: 0.6623759567737579, acc.: 59.62%] [G loss: 0.7899388074874878]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 28/86 [D loss: 0.6505476832389832, acc.: 62.84%] [G loss: 0.8021095395088196]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 29/86 [D loss: 0.658231109380722, acc.: 60.50%] [G loss: 0.8074557185173035]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 30/86 [D loss: 0.65915647149086, acc.: 60.11%] [G loss: 0.8009130954742432]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 191/200, Batch 31/86 [D loss: 0.6582926213741302, acc.: 60.64%] [G loss: 0.7969919443130493]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 191/200, Batch 32/86 [D loss: 0.6517838835716248, acc.: 61.33%] [G loss: 0.8067361116409302]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 191/200, Batch 33/86 [D loss: 0.664115309715271, acc.: 60.01%] [G loss: 0.800969123840332]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 34/86 [D loss: 0.6615162789821625, acc.: 59.08%] [G loss: 0.7920914888381958]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 35/86 [D loss: 0.6652637124061584, acc.: 58.45%] [G loss: 0.8129771947860718]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 36/86 [D loss: 0.6650758981704712, acc.: 58.64%] [G loss: 0.7998238205909729]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 37/86 [D loss: 0.6566891074180603, acc.: 61.33%] [G loss: 0.8056137561798096]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 38/86 [D loss: 0.6669744551181793, acc.: 58.45%] [G loss: 0.8027292490005493]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 191/200, Batch 39/86 [D loss: 0.665734589099884, acc.: 58.20%] [G loss: 0.8036441802978516]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 191/200, Batch 40/86 [D loss: 0.6578457951545715, acc.: 60.94%] [G loss: 0.7954967617988586]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 41/86 [D loss: 0.6597419679164886, acc.: 61.08%] [G loss: 0.7979220151901245]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 191/200, Batch 42/86 [D loss: 0.6625062525272369, acc.: 60.06%] [G loss: 0.8022860288619995]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 43/86 [D loss: 0.6563563346862793, acc.: 62.45%] [G loss: 0.7984517812728882]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 44/86 [D loss: 0.6587926149368286, acc.: 61.18%] [G loss: 0.8000004291534424]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 45/86 [D loss: 0.6608685255050659, acc.: 60.50%] [G loss: 0.8067764639854431]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 46/86 [D loss: 0.6582561433315277, acc.: 61.43%] [G loss: 0.801972508430481]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 47/86 [D loss: 0.6558876931667328, acc.: 61.13%] [G loss: 0.8057944774627686]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 48/86 [D loss: 0.6531035006046295, acc.: 62.60%] [G loss: 0.806302547454834]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 49/86 [D loss: 0.6551117300987244, acc.: 61.67%] [G loss: 0.7818346619606018]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 191/200, Batch 50/86 [D loss: 0.6628987491130829, acc.: 58.89%] [G loss: 0.8028308153152466]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 51/86 [D loss: 0.652941882610321, acc.: 60.89%] [G loss: 0.8035512566566467]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 52/86 [D loss: 0.6605729162693024, acc.: 59.52%] [G loss: 0.7909544706344604]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 53/86 [D loss: 0.662841796875, acc.: 59.86%] [G loss: 0.7834072709083557]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 54/86 [D loss: 0.6625392436981201, acc.: 59.96%] [G loss: 0.7889114618301392]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 191/200, Batch 55/86 [D loss: 0.6572990417480469, acc.: 60.16%] [G loss: 0.7996278405189514]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 56/86 [D loss: 0.656028538942337, acc.: 59.42%] [G loss: 0.7994893789291382]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 57/86 [D loss: 0.6626735329627991, acc.: 60.16%] [G loss: 0.7972527742385864]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 191/200, Batch 58/86 [D loss: 0.6623947322368622, acc.: 59.23%] [G loss: 0.8022501468658447]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 59/86 [D loss: 0.6617194414138794, acc.: 60.11%] [G loss: 0.8020023107528687]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 191/200, Batch 60/86 [D loss: 0.6568329632282257, acc.: 60.99%] [G loss: 0.7997928261756897]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 191/200, Batch 61/86 [D loss: 0.6589476764202118, acc.: 59.81%] [G loss: 0.8125073909759521]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 62/86 [D loss: 0.6582516133785248, acc.: 60.64%] [G loss: 0.8036806583404541]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 63/86 [D loss: 0.6694575548171997, acc.: 58.11%] [G loss: 0.8055040240287781]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 64/86 [D loss: 0.6593926548957825, acc.: 60.64%] [G loss: 0.7998450994491577]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 65/86 [D loss: 0.659439355134964, acc.: 59.81%] [G loss: 0.8065264225006104]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 66/86 [D loss: 0.6627703309059143, acc.: 59.91%] [G loss: 0.8057740926742554]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 67/86 [D loss: 0.6535888910293579, acc.: 61.57%] [G loss: 0.8070581555366516]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 68/86 [D loss: 0.655089259147644, acc.: 61.38%] [G loss: 0.7936406135559082]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 69/86 [D loss: 0.6627486348152161, acc.: 60.11%] [G loss: 0.7774061560630798]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 191/200, Batch 70/86 [D loss: 0.6548154652118683, acc.: 60.84%] [G loss: 0.7931171655654907]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 71/86 [D loss: 0.663716584444046, acc.: 59.42%] [G loss: 0.7917095422744751]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 72/86 [D loss: 0.6614033281803131, acc.: 59.38%] [G loss: 0.803185224533081]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 73/86 [D loss: 0.660309374332428, acc.: 59.47%] [G loss: 0.7923527359962463]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 74/86 [D loss: 0.6588091254234314, acc.: 60.35%] [G loss: 0.792178750038147]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 191/200, Batch 75/86 [D loss: 0.6541985273361206, acc.: 61.96%] [G loss: 0.7991316318511963]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 76/86 [D loss: 0.6609953343868256, acc.: 59.96%] [G loss: 0.7916160225868225]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 191/200, Batch 77/86 [D loss: 0.6620997190475464, acc.: 60.89%] [G loss: 0.8014768362045288]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 78/86 [D loss: 0.667829692363739, acc.: 58.40%] [G loss: 0.801629364490509]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 79/86 [D loss: 0.662795901298523, acc.: 58.94%] [G loss: 0.815057635307312]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 191/200, Batch 80/86 [D loss: 0.6593388319015503, acc.: 60.30%] [G loss: 0.7983773946762085]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 81/86 [D loss: 0.6648207008838654, acc.: 59.33%] [G loss: 0.8045380115509033]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 191/200, Batch 82/86 [D loss: 0.6563684046268463, acc.: 60.06%] [G loss: 0.7960125803947449]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 191/200, Batch 83/86 [D loss: 0.6586150527000427, acc.: 60.30%] [G loss: 0.7977886199951172]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 191/200, Batch 84/86 [D loss: 0.6571826636791229, acc.: 60.74%] [G loss: 0.8077342510223389]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 85/86 [D loss: 0.6636581718921661, acc.: 59.03%] [G loss: 0.8090447783470154]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 191/200, Batch 86/86 [D loss: 0.6627798676490784, acc.: 59.86%] [G loss: 0.8020796775817871]\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 192/200, Batch 1/86 [D loss: 0.6620058715343475, acc.: 59.77%] [G loss: 0.8034573793411255]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 2/86 [D loss: 0.6628416478633881, acc.: 59.47%] [G loss: 0.8031666278839111]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 3/86 [D loss: 0.6602976322174072, acc.: 60.30%] [G loss: 0.8006418943405151]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 192/200, Batch 4/86 [D loss: 0.6566324234008789, acc.: 60.89%] [G loss: 0.808102011680603]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 5/86 [D loss: 0.6538446545600891, acc.: 61.38%] [G loss: 0.8032288551330566]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 6/86 [D loss: 0.6640863716602325, acc.: 59.72%] [G loss: 0.8043733835220337]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 192/200, Batch 7/86 [D loss: 0.6616950333118439, acc.: 59.81%] [G loss: 0.7957871556282043]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 8/86 [D loss: 0.6565335392951965, acc.: 60.40%] [G loss: 0.8135607242584229]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 192/200, Batch 9/86 [D loss: 0.6542087495326996, acc.: 61.43%] [G loss: 0.8012213706970215]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 192/200, Batch 10/86 [D loss: 0.6605843007564545, acc.: 60.16%] [G loss: 0.8084962368011475]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 192/200, Batch 11/86 [D loss: 0.6618048846721649, acc.: 59.23%] [G loss: 0.7983101606369019]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 12/86 [D loss: 0.6555669605731964, acc.: 60.60%] [G loss: 0.8030458688735962]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 13/86 [D loss: 0.663118302822113, acc.: 60.50%] [G loss: 0.8028848767280579]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 14/86 [D loss: 0.6571606695652008, acc.: 60.94%] [G loss: 0.7967291474342346]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 15/86 [D loss: 0.6631852090358734, acc.: 59.52%] [G loss: 0.7920753359794617]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 16/86 [D loss: 0.6597526371479034, acc.: 59.52%] [G loss: 0.8157772421836853]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 17/86 [D loss: 0.6612734198570251, acc.: 60.25%] [G loss: 0.8148707151412964]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 192/200, Batch 18/86 [D loss: 0.6581936478614807, acc.: 59.13%] [G loss: 0.8069453239440918]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 19/86 [D loss: 0.6546320021152496, acc.: 60.50%] [G loss: 0.7921391725540161]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 20/86 [D loss: 0.6545000076293945, acc.: 61.82%] [G loss: 0.8087857961654663]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 192/200, Batch 21/86 [D loss: 0.6610526740550995, acc.: 59.91%] [G loss: 0.8116917014122009]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 22/86 [D loss: 0.6585680544376373, acc.: 61.57%] [G loss: 0.8050850033760071]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 192/200, Batch 23/86 [D loss: 0.6620500981807709, acc.: 60.01%] [G loss: 0.800976574420929]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 24/86 [D loss: 0.6595941185951233, acc.: 60.30%] [G loss: 0.8005187511444092]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 25/86 [D loss: 0.6542940139770508, acc.: 61.82%] [G loss: 0.7844678163528442]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 192/200, Batch 26/86 [D loss: 0.6665564179420471, acc.: 59.18%] [G loss: 0.8043158054351807]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 27/86 [D loss: 0.6588972508907318, acc.: 60.11%] [G loss: 0.8032028675079346]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 28/86 [D loss: 0.6512237787246704, acc.: 62.99%] [G loss: 0.8066302537918091]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 29/86 [D loss: 0.6535619795322418, acc.: 61.62%] [G loss: 0.8116568326950073]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 30/86 [D loss: 0.6626958847045898, acc.: 59.38%] [G loss: 0.809872031211853]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 31/86 [D loss: 0.6565814316272736, acc.: 61.28%] [G loss: 0.7988177537918091]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 32/86 [D loss: 0.6586980819702148, acc.: 59.77%] [G loss: 0.8018717765808105]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 192/200, Batch 33/86 [D loss: 0.6638684868812561, acc.: 59.62%] [G loss: 0.8052762746810913]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 34/86 [D loss: 0.6609295010566711, acc.: 58.35%] [G loss: 0.8039910793304443]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 192/200, Batch 35/86 [D loss: 0.6584492325782776, acc.: 60.84%] [G loss: 0.8007293343544006]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 36/86 [D loss: 0.6620540618896484, acc.: 60.40%] [G loss: 0.8079923391342163]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 192/200, Batch 37/86 [D loss: 0.6558524668216705, acc.: 60.45%] [G loss: 0.8194776177406311]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 38/86 [D loss: 0.656341940164566, acc.: 61.04%] [G loss: 0.8047477006912231]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 39/86 [D loss: 0.6527852714061737, acc.: 61.52%] [G loss: 0.8182207345962524]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 40/86 [D loss: 0.6562930643558502, acc.: 60.69%] [G loss: 0.8065043687820435]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 41/86 [D loss: 0.6561239659786224, acc.: 61.77%] [G loss: 0.7945936918258667]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 42/86 [D loss: 0.6629522442817688, acc.: 59.03%] [G loss: 0.7951887249946594]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 192/200, Batch 43/86 [D loss: 0.6588772237300873, acc.: 59.86%] [G loss: 0.7832934260368347]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 44/86 [D loss: 0.6657490134239197, acc.: 58.50%] [G loss: 0.8051138520240784]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 45/86 [D loss: 0.6609665155410767, acc.: 59.77%] [G loss: 0.8134204745292664]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 46/86 [D loss: 0.6616680026054382, acc.: 59.96%] [G loss: 0.8010310530662537]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 47/86 [D loss: 0.6590489745140076, acc.: 60.30%] [G loss: 0.8012313842773438]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 48/86 [D loss: 0.6604732275009155, acc.: 60.64%] [G loss: 0.7943217158317566]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 49/86 [D loss: 0.6625273525714874, acc.: 58.40%] [G loss: 0.8046995401382446]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 50/86 [D loss: 0.659965842962265, acc.: 60.40%] [G loss: 0.8018423318862915]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 192/200, Batch 51/86 [D loss: 0.6667126417160034, acc.: 59.52%] [G loss: 0.8060224652290344]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 52/86 [D loss: 0.6576248407363892, acc.: 61.62%] [G loss: 0.8022326231002808]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 53/86 [D loss: 0.6608021557331085, acc.: 60.69%] [G loss: 0.8157137632369995]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 192/200, Batch 54/86 [D loss: 0.6580956876277924, acc.: 60.30%] [G loss: 0.7972223162651062]\n",
      "32/32 [==============================] - 1s 20ms/step\n",
      "Epoch 192/200, Batch 55/86 [D loss: 0.6618507504463196, acc.: 60.01%] [G loss: 0.8038389682769775]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 56/86 [D loss: 0.6609792113304138, acc.: 59.42%] [G loss: 0.7918152809143066]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 192/200, Batch 57/86 [D loss: 0.6666077971458435, acc.: 58.94%] [G loss: 0.7895541191101074]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 192/200, Batch 58/86 [D loss: 0.6594601571559906, acc.: 60.99%] [G loss: 0.7860178351402283]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 59/86 [D loss: 0.6683149635791779, acc.: 57.52%] [G loss: 0.7974565625190735]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 60/86 [D loss: 0.6599666476249695, acc.: 60.16%] [G loss: 0.8023333549499512]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 61/86 [D loss: 0.6612423062324524, acc.: 58.98%] [G loss: 0.808416485786438]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 62/86 [D loss: 0.6597273945808411, acc.: 60.69%] [G loss: 0.8008820414543152]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 63/86 [D loss: 0.658221423625946, acc.: 59.42%] [G loss: 0.8006514310836792]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 64/86 [D loss: 0.6616986691951752, acc.: 57.52%] [G loss: 0.8037703037261963]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 65/86 [D loss: 0.6595141291618347, acc.: 60.45%] [G loss: 0.809711217880249]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 66/86 [D loss: 0.6593616604804993, acc.: 59.72%] [G loss: 0.8043266534805298]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 67/86 [D loss: 0.6605011820793152, acc.: 60.69%] [G loss: 0.7866442203521729]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 68/86 [D loss: 0.6542450785636902, acc.: 60.69%] [G loss: 0.7961535453796387]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 69/86 [D loss: 0.6685197949409485, acc.: 58.54%] [G loss: 0.7976037263870239]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 70/86 [D loss: 0.6654252707958221, acc.: 59.28%] [G loss: 0.7853477001190186]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 71/86 [D loss: 0.6611400842666626, acc.: 60.35%] [G loss: 0.7926077842712402]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 72/86 [D loss: 0.6636577546596527, acc.: 57.28%] [G loss: 0.7966474890708923]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 73/86 [D loss: 0.6720193326473236, acc.: 57.23%] [G loss: 0.8018614053726196]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 74/86 [D loss: 0.6615065038204193, acc.: 59.47%] [G loss: 0.7955443263053894]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 75/86 [D loss: 0.6574876308441162, acc.: 60.99%] [G loss: 0.8062965869903564]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 76/86 [D loss: 0.6574025750160217, acc.: 60.35%] [G loss: 0.803078293800354]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 77/86 [D loss: 0.6594395339488983, acc.: 59.08%] [G loss: 0.7934830188751221]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 78/86 [D loss: 0.6561056673526764, acc.: 60.74%] [G loss: 0.7940633296966553]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 192/200, Batch 79/86 [D loss: 0.6588990390300751, acc.: 60.16%] [G loss: 0.8049886226654053]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 80/86 [D loss: 0.6656710803508759, acc.: 59.23%] [G loss: 0.7992212176322937]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 192/200, Batch 81/86 [D loss: 0.6632394790649414, acc.: 58.84%] [G loss: 0.8029964566230774]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 82/86 [D loss: 0.657652348279953, acc.: 61.18%] [G loss: 0.7933122515678406]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 192/200, Batch 83/86 [D loss: 0.6548398733139038, acc.: 61.23%] [G loss: 0.8034956455230713]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 192/200, Batch 84/86 [D loss: 0.658486545085907, acc.: 60.11%] [G loss: 0.8100275993347168]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 192/200, Batch 85/86 [D loss: 0.6615199744701385, acc.: 59.77%] [G loss: 0.8016387820243835]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 192/200, Batch 86/86 [D loss: 0.6591707766056061, acc.: 60.45%] [G loss: 0.7997056245803833]\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 193/200, Batch 1/86 [D loss: 0.6618130803108215, acc.: 58.74%] [G loss: 0.8038740158081055]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 193/200, Batch 2/86 [D loss: 0.656629353761673, acc.: 60.94%] [G loss: 0.8027642369270325]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 3/86 [D loss: 0.6598655879497528, acc.: 60.25%] [G loss: 0.8014172315597534]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 193/200, Batch 4/86 [D loss: 0.6650736033916473, acc.: 59.18%] [G loss: 0.8015550971031189]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 193/200, Batch 5/86 [D loss: 0.6632238626480103, acc.: 59.23%] [G loss: 0.8090531229972839]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 6/86 [D loss: 0.6620514392852783, acc.: 57.57%] [G loss: 0.804217517375946]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 7/86 [D loss: 0.6594158113002777, acc.: 61.38%] [G loss: 0.80283522605896]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 8/86 [D loss: 0.6649184823036194, acc.: 59.52%] [G loss: 0.8034204244613647]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 9/86 [D loss: 0.6638552844524384, acc.: 59.03%] [G loss: 0.8151986002922058]\n",
      "32/32 [==============================] - 1s 21ms/step\n",
      "Epoch 193/200, Batch 10/86 [D loss: 0.6563701033592224, acc.: 61.38%] [G loss: 0.8068910837173462]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 193/200, Batch 11/86 [D loss: 0.6598330438137054, acc.: 59.72%] [G loss: 0.8015894889831543]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 193/200, Batch 12/86 [D loss: 0.6557018160820007, acc.: 60.99%] [G loss: 0.8037581443786621]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 13/86 [D loss: 0.6640233099460602, acc.: 59.67%] [G loss: 0.7992273569107056]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 14/86 [D loss: 0.6546074151992798, acc.: 62.01%] [G loss: 0.8073922991752625]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 15/86 [D loss: 0.6600620448589325, acc.: 59.03%] [G loss: 0.8133887648582458]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 16/86 [D loss: 0.6570847630500793, acc.: 61.23%] [G loss: 0.8119138479232788]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 193/200, Batch 17/86 [D loss: 0.6524800360202789, acc.: 62.89%] [G loss: 0.8042536973953247]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 18/86 [D loss: 0.6586116254329681, acc.: 60.60%] [G loss: 0.7933127880096436]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 19/86 [D loss: 0.6650652587413788, acc.: 59.13%] [G loss: 0.7895662784576416]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 20/86 [D loss: 0.6564953625202179, acc.: 59.91%] [G loss: 0.795356273651123]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 21/86 [D loss: 0.6644118130207062, acc.: 58.35%] [G loss: 0.7889869809150696]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 22/86 [D loss: 0.6622535884380341, acc.: 60.01%] [G loss: 0.8019071221351624]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 193/200, Batch 23/86 [D loss: 0.6633005440235138, acc.: 58.98%] [G loss: 0.7990525960922241]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 24/86 [D loss: 0.655819445848465, acc.: 61.23%] [G loss: 0.803943932056427]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 25/86 [D loss: 0.6605605185031891, acc.: 60.06%] [G loss: 0.8094642162322998]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 26/86 [D loss: 0.6627485752105713, acc.: 58.94%] [G loss: 0.8028302192687988]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 27/86 [D loss: 0.6663977801799774, acc.: 58.94%] [G loss: 0.8046072721481323]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 193/200, Batch 28/86 [D loss: 0.6570462882518768, acc.: 60.79%] [G loss: 0.8077349066734314]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 29/86 [D loss: 0.6611590683460236, acc.: 58.84%] [G loss: 0.8095671534538269]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 30/86 [D loss: 0.6548469066619873, acc.: 61.18%] [G loss: 0.8041055202484131]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 31/86 [D loss: 0.6595253944396973, acc.: 59.62%] [G loss: 0.7980960607528687]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 32/86 [D loss: 0.6550639569759369, acc.: 61.28%] [G loss: 0.8078707456588745]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 193/200, Batch 33/86 [D loss: 0.6604717373847961, acc.: 59.96%] [G loss: 0.7958149909973145]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 34/86 [D loss: 0.6616548299789429, acc.: 59.81%] [G loss: 0.7876913547515869]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 35/86 [D loss: 0.6604552865028381, acc.: 60.16%] [G loss: 0.8107142448425293]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 36/86 [D loss: 0.6579343974590302, acc.: 59.96%] [G loss: 0.7929769158363342]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 37/86 [D loss: 0.6545346677303314, acc.: 61.23%] [G loss: 0.8019518256187439]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 193/200, Batch 38/86 [D loss: 0.6514267027378082, acc.: 62.40%] [G loss: 0.7989469766616821]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 39/86 [D loss: 0.6584250330924988, acc.: 61.33%] [G loss: 0.8181543946266174]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 40/86 [D loss: 0.657775342464447, acc.: 60.35%] [G loss: 0.790855884552002]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 41/86 [D loss: 0.6531447768211365, acc.: 60.40%] [G loss: 0.7928136587142944]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 42/86 [D loss: 0.6570873558521271, acc.: 59.67%] [G loss: 0.8047387003898621]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 43/86 [D loss: 0.6569333374500275, acc.: 61.23%] [G loss: 0.7926169037818909]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 44/86 [D loss: 0.6613697409629822, acc.: 60.16%] [G loss: 0.7945709228515625]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 45/86 [D loss: 0.6580715775489807, acc.: 61.08%] [G loss: 0.792820155620575]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 46/86 [D loss: 0.664598286151886, acc.: 58.59%] [G loss: 0.8035573363304138]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 47/86 [D loss: 0.6646234691143036, acc.: 59.03%] [G loss: 0.8070623278617859]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 48/86 [D loss: 0.6646298468112946, acc.: 59.23%] [G loss: 0.7975115180015564]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 49/86 [D loss: 0.6666940152645111, acc.: 58.40%] [G loss: 0.7920690774917603]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 50/86 [D loss: 0.6493166387081146, acc.: 62.50%] [G loss: 0.7922319769859314]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 51/86 [D loss: 0.6607633233070374, acc.: 59.38%] [G loss: 0.7964474558830261]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 52/86 [D loss: 0.6534458994865417, acc.: 62.40%] [G loss: 0.7881267070770264]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 193/200, Batch 53/86 [D loss: 0.6551322937011719, acc.: 61.57%] [G loss: 0.8013259768486023]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 54/86 [D loss: 0.6643615067005157, acc.: 58.89%] [G loss: 0.7958678007125854]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 55/86 [D loss: 0.6579332053661346, acc.: 61.08%] [G loss: 0.793726921081543]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 56/86 [D loss: 0.6539468467235565, acc.: 62.06%] [G loss: 0.8020203113555908]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 57/86 [D loss: 0.6639919579029083, acc.: 59.28%] [G loss: 0.7930852174758911]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 58/86 [D loss: 0.6600668728351593, acc.: 60.79%] [G loss: 0.7958585023880005]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 59/86 [D loss: 0.6576845049858093, acc.: 61.28%] [G loss: 0.8077549338340759]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 60/86 [D loss: 0.6577037572860718, acc.: 60.64%] [G loss: 0.7968409061431885]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 61/86 [D loss: 0.6646229326725006, acc.: 59.38%] [G loss: 0.7960280179977417]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 62/86 [D loss: 0.6582905352115631, acc.: 61.52%] [G loss: 0.7980018258094788]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 63/86 [D loss: 0.6584575176239014, acc.: 60.79%] [G loss: 0.810482382774353]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 193/200, Batch 64/86 [D loss: 0.6519919335842133, acc.: 62.06%] [G loss: 0.7979421615600586]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 65/86 [D loss: 0.66091188788414, acc.: 60.74%] [G loss: 0.8034194707870483]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 66/86 [D loss: 0.6574155390262604, acc.: 59.77%] [G loss: 0.7932363152503967]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 67/86 [D loss: 0.6511847376823425, acc.: 61.67%] [G loss: 0.7986598014831543]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 68/86 [D loss: 0.6583876013755798, acc.: 59.81%] [G loss: 0.7917954325675964]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 69/86 [D loss: 0.6606424152851105, acc.: 60.06%] [G loss: 0.8061885833740234]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 70/86 [D loss: 0.6615632176399231, acc.: 60.45%] [G loss: 0.7992880940437317]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 193/200, Batch 71/86 [D loss: 0.6608997285366058, acc.: 59.72%] [G loss: 0.7949830293655396]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 193/200, Batch 72/86 [D loss: 0.651316225528717, acc.: 63.43%] [G loss: 0.8057153224945068]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 193/200, Batch 73/86 [D loss: 0.6611365079879761, acc.: 60.16%] [G loss: 0.7968090772628784]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 193/200, Batch 74/86 [D loss: 0.6632575690746307, acc.: 58.79%] [G loss: 0.7994214296340942]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 75/86 [D loss: 0.6550059616565704, acc.: 61.18%] [G loss: 0.7999825477600098]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 193/200, Batch 76/86 [D loss: 0.6660991609096527, acc.: 59.62%] [G loss: 0.7892590761184692]\n",
      "32/32 [==============================] - 1s 21ms/step\n",
      "Epoch 193/200, Batch 77/86 [D loss: 0.6622248291969299, acc.: 59.23%] [G loss: 0.7984052896499634]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 78/86 [D loss: 0.6609016358852386, acc.: 60.21%] [G loss: 0.8030719757080078]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 79/86 [D loss: 0.6606634855270386, acc.: 61.18%] [G loss: 0.8023877739906311]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 193/200, Batch 80/86 [D loss: 0.660010576248169, acc.: 59.77%] [G loss: 0.791276216506958]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 81/86 [D loss: 0.656069278717041, acc.: 60.21%] [G loss: 0.8064912557601929]\n",
      "32/32 [==============================] - 1s 19ms/step\n",
      "Epoch 193/200, Batch 82/86 [D loss: 0.6572466790676117, acc.: 60.79%] [G loss: 0.7975967526435852]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 193/200, Batch 83/86 [D loss: 0.6640610098838806, acc.: 59.28%] [G loss: 0.7991440892219543]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 193/200, Batch 84/86 [D loss: 0.6555865705013275, acc.: 61.72%] [G loss: 0.8001194000244141]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 193/200, Batch 85/86 [D loss: 0.6605671048164368, acc.: 59.96%] [G loss: 0.8000501394271851]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 193/200, Batch 86/86 [D loss: 0.6550029218196869, acc.: 61.57%] [G loss: 0.8001039028167725]\n",
      "4/4 [==============================] - 0s 22ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 194/200, Batch 1/86 [D loss: 0.6643193960189819, acc.: 59.86%] [G loss: 0.8089385628700256]\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "Epoch 194/200, Batch 2/86 [D loss: 0.6568291783332825, acc.: 60.45%] [G loss: 0.7820214629173279]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 194/200, Batch 3/86 [D loss: 0.6650914251804352, acc.: 58.94%] [G loss: 0.8021945953369141]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 4/86 [D loss: 0.6562749743461609, acc.: 61.08%] [G loss: 0.7937442064285278]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 5/86 [D loss: 0.65713831782341, acc.: 61.67%] [G loss: 0.8043994903564453]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 6/86 [D loss: 0.658911406993866, acc.: 60.84%] [G loss: 0.7944437265396118]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 7/86 [D loss: 0.6662642657756805, acc.: 58.69%] [G loss: 0.8052537441253662]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 194/200, Batch 8/86 [D loss: 0.6602169871330261, acc.: 59.08%] [G loss: 0.7926035523414612]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 194/200, Batch 9/86 [D loss: 0.6632167100906372, acc.: 59.28%] [G loss: 0.8143866062164307]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 10/86 [D loss: 0.653601199388504, acc.: 61.23%] [G loss: 0.8161939382553101]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 11/86 [D loss: 0.659414529800415, acc.: 60.79%] [G loss: 0.8033743500709534]\n",
      "32/32 [==============================] - 1s 20ms/step\n",
      "Epoch 194/200, Batch 12/86 [D loss: 0.6564200520515442, acc.: 61.96%] [G loss: 0.8026033043861389]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 194/200, Batch 13/86 [D loss: 0.6617641746997833, acc.: 59.72%] [G loss: 0.8188683390617371]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 14/86 [D loss: 0.6582348942756653, acc.: 59.28%] [G loss: 0.8013092875480652]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 15/86 [D loss: 0.6606893539428711, acc.: 59.38%] [G loss: 0.8027386665344238]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 194/200, Batch 16/86 [D loss: 0.6618845164775848, acc.: 59.81%] [G loss: 0.7955040335655212]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 17/86 [D loss: 0.6538378596305847, acc.: 61.62%] [G loss: 0.8136048316955566]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 194/200, Batch 18/86 [D loss: 0.6543532013893127, acc.: 62.21%] [G loss: 0.8011319637298584]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 19/86 [D loss: 0.6583273112773895, acc.: 59.96%] [G loss: 0.7984691262245178]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 20/86 [D loss: 0.6500856280326843, acc.: 61.91%] [G loss: 0.8077356815338135]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 194/200, Batch 21/86 [D loss: 0.6606955826282501, acc.: 60.30%] [G loss: 0.7995298504829407]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 22/86 [D loss: 0.6638191342353821, acc.: 59.42%] [G loss: 0.8138278722763062]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 23/86 [D loss: 0.6526374518871307, acc.: 61.38%] [G loss: 0.8039746880531311]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 24/86 [D loss: 0.6537075936794281, acc.: 60.35%] [G loss: 0.8089537024497986]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 25/86 [D loss: 0.6625604331493378, acc.: 59.81%] [G loss: 0.8026769757270813]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 26/86 [D loss: 0.6582305133342743, acc.: 59.91%] [G loss: 0.7918378114700317]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 194/200, Batch 27/86 [D loss: 0.6618652939796448, acc.: 59.96%] [G loss: 0.792441189289093]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 28/86 [D loss: 0.6570078134536743, acc.: 60.89%] [G loss: 0.8040796518325806]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 29/86 [D loss: 0.658024787902832, acc.: 61.43%] [G loss: 0.7999337911605835]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 30/86 [D loss: 0.6568293869495392, acc.: 61.96%] [G loss: 0.7980195879936218]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 31/86 [D loss: 0.6688838601112366, acc.: 57.52%] [G loss: 0.8033007383346558]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 32/86 [D loss: 0.6566874384880066, acc.: 59.52%] [G loss: 0.8073937296867371]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 194/200, Batch 33/86 [D loss: 0.657147616147995, acc.: 61.04%] [G loss: 0.7991288900375366]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 194/200, Batch 34/86 [D loss: 0.6590699851512909, acc.: 59.62%] [G loss: 0.8084403276443481]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 35/86 [D loss: 0.6581248044967651, acc.: 60.40%] [G loss: 0.8074623346328735]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 36/86 [D loss: 0.6459322571754456, acc.: 63.96%] [G loss: 0.8021626472473145]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 37/86 [D loss: 0.6552846431732178, acc.: 61.23%] [G loss: 0.7976199984550476]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 38/86 [D loss: 0.6626843512058258, acc.: 59.57%] [G loss: 0.8180445432662964]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 39/86 [D loss: 0.6663938462734222, acc.: 58.20%] [G loss: 0.8141966462135315]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 40/86 [D loss: 0.6556877195835114, acc.: 60.06%] [G loss: 0.8005374670028687]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 41/86 [D loss: 0.6640070974826813, acc.: 59.33%] [G loss: 0.8146133422851562]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 42/86 [D loss: 0.6565601825714111, acc.: 60.89%] [G loss: 0.8080293536186218]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 43/86 [D loss: 0.6542705297470093, acc.: 61.62%] [G loss: 0.8068514466285706]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 44/86 [D loss: 0.6538823843002319, acc.: 61.52%] [G loss: 0.8036911487579346]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 45/86 [D loss: 0.6518657803535461, acc.: 61.67%] [G loss: 0.8012260794639587]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 46/86 [D loss: 0.6621796786785126, acc.: 58.98%] [G loss: 0.8008520603179932]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 47/86 [D loss: 0.6559159755706787, acc.: 60.25%] [G loss: 0.80387282371521]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 48/86 [D loss: 0.6608639657497406, acc.: 60.64%] [G loss: 0.804024338722229]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 49/86 [D loss: 0.6618041694164276, acc.: 59.28%] [G loss: 0.7921558618545532]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 50/86 [D loss: 0.6586433053016663, acc.: 60.79%] [G loss: 0.8165773153305054]\n",
      "32/32 [==============================] - 0s 16ms/step\n",
      "Epoch 194/200, Batch 51/86 [D loss: 0.668066680431366, acc.: 57.71%] [G loss: 0.8060271739959717]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 52/86 [D loss: 0.6515014171600342, acc.: 61.87%] [G loss: 0.7958565950393677]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 53/86 [D loss: 0.6584954559803009, acc.: 59.23%] [G loss: 0.8121588230133057]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 54/86 [D loss: 0.6579996049404144, acc.: 59.38%] [G loss: 0.7987826466560364]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 55/86 [D loss: 0.6532393395900726, acc.: 62.55%] [G loss: 0.8048245310783386]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 56/86 [D loss: 0.6524811089038849, acc.: 61.28%] [G loss: 0.8070160746574402]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 57/86 [D loss: 0.6611591577529907, acc.: 59.28%] [G loss: 0.79887855052948]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 58/86 [D loss: 0.655268669128418, acc.: 60.50%] [G loss: 0.7988691926002502]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 59/86 [D loss: 0.6577398180961609, acc.: 60.84%] [G loss: 0.795921802520752]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 194/200, Batch 60/86 [D loss: 0.6573373675346375, acc.: 60.69%] [G loss: 0.8080241084098816]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 194/200, Batch 61/86 [D loss: 0.6584278345108032, acc.: 61.52%] [G loss: 0.8005635142326355]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 62/86 [D loss: 0.6568320989608765, acc.: 61.13%] [G loss: 0.8096879720687866]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 194/200, Batch 63/86 [D loss: 0.6508059799671173, acc.: 61.87%] [G loss: 0.8116436004638672]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 64/86 [D loss: 0.6550204157829285, acc.: 60.84%] [G loss: 0.8008223176002502]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 65/86 [D loss: 0.662035197019577, acc.: 60.40%] [G loss: 0.7985160946846008]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 194/200, Batch 66/86 [D loss: 0.6544029414653778, acc.: 62.55%] [G loss: 0.7929824590682983]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 194/200, Batch 67/86 [D loss: 0.6599843800067902, acc.: 60.35%] [G loss: 0.8001289367675781]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 68/86 [D loss: 0.6597587466239929, acc.: 61.04%] [G loss: 0.8093107342720032]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 69/86 [D loss: 0.6573935151100159, acc.: 60.60%] [G loss: 0.8085366487503052]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 70/86 [D loss: 0.6598770916461945, acc.: 60.25%] [G loss: 0.8021846413612366]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 71/86 [D loss: 0.6559213697910309, acc.: 61.28%] [G loss: 0.8107668161392212]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 72/86 [D loss: 0.6582261621952057, acc.: 59.77%] [G loss: 0.8056831359863281]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 73/86 [D loss: 0.6632171273231506, acc.: 60.21%] [G loss: 0.8078317046165466]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 194/200, Batch 74/86 [D loss: 0.6616587936878204, acc.: 59.57%] [G loss: 0.7990206480026245]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 75/86 [D loss: 0.6616198122501373, acc.: 58.59%] [G loss: 0.8024171590805054]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 194/200, Batch 76/86 [D loss: 0.6512979567050934, acc.: 61.28%] [G loss: 0.8019565343856812]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 77/86 [D loss: 0.6639673411846161, acc.: 58.89%] [G loss: 0.8048449754714966]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 78/86 [D loss: 0.6587219834327698, acc.: 59.67%] [G loss: 0.8152597546577454]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 79/86 [D loss: 0.6640797257423401, acc.: 60.01%] [G loss: 0.7968459129333496]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 80/86 [D loss: 0.6666470170021057, acc.: 58.30%] [G loss: 0.8030174970626831]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 194/200, Batch 81/86 [D loss: 0.657742440700531, acc.: 60.25%] [G loss: 0.8010447025299072]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 82/86 [D loss: 0.6560363173484802, acc.: 60.69%] [G loss: 0.8049463033676147]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 83/86 [D loss: 0.6585205495357513, acc.: 59.03%] [G loss: 0.8126096129417419]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 194/200, Batch 84/86 [D loss: 0.6566819250583649, acc.: 61.52%] [G loss: 0.810012936592102]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 194/200, Batch 85/86 [D loss: 0.6573577225208282, acc.: 60.50%] [G loss: 0.8069435358047485]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 194/200, Batch 86/86 [D loss: 0.6572887003421783, acc.: 60.01%] [G loss: 0.7992395162582397]\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 195/200, Batch 1/86 [D loss: 0.6543389558792114, acc.: 60.45%] [G loss: 0.808691680431366]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 2/86 [D loss: 0.6640938818454742, acc.: 59.38%] [G loss: 0.8055441975593567]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 3/86 [D loss: 0.6566519439220428, acc.: 59.96%] [G loss: 0.7980915307998657]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 195/200, Batch 4/86 [D loss: 0.6590136587619781, acc.: 61.47%] [G loss: 0.8142390847206116]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 5/86 [D loss: 0.6614211797714233, acc.: 60.06%] [G loss: 0.8051596283912659]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 6/86 [D loss: 0.661897212266922, acc.: 58.50%] [G loss: 0.7935764193534851]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 7/86 [D loss: 0.6590953469276428, acc.: 60.55%] [G loss: 0.8054967522621155]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 8/86 [D loss: 0.6546680629253387, acc.: 61.87%] [G loss: 0.799627423286438]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 9/86 [D loss: 0.6642383933067322, acc.: 59.67%] [G loss: 0.8090096712112427]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 10/86 [D loss: 0.6577102541923523, acc.: 61.18%] [G loss: 0.7860361933708191]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 11/86 [D loss: 0.6585677266120911, acc.: 59.72%] [G loss: 0.8159215450286865]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 12/86 [D loss: 0.6669000089168549, acc.: 59.81%] [G loss: 0.7949450612068176]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 13/86 [D loss: 0.6583518385887146, acc.: 60.40%] [G loss: 0.8288872838020325]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 14/86 [D loss: 0.6575100123882294, acc.: 62.01%] [G loss: 0.7970685362815857]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 15/86 [D loss: 0.6611297428607941, acc.: 59.03%] [G loss: 0.8039110898971558]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 16/86 [D loss: 0.6553372442722321, acc.: 60.69%] [G loss: 0.7905160188674927]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 195/200, Batch 17/86 [D loss: 0.6661964356899261, acc.: 58.64%] [G loss: 0.7935084700584412]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 195/200, Batch 18/86 [D loss: 0.6553506255149841, acc.: 60.64%] [G loss: 0.7938892245292664]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 19/86 [D loss: 0.659866988658905, acc.: 60.40%] [G loss: 0.813348650932312]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 20/86 [D loss: 0.6594880521297455, acc.: 60.79%] [G loss: 0.8009875416755676]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 21/86 [D loss: 0.6600154936313629, acc.: 58.94%] [G loss: 0.802898108959198]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 22/86 [D loss: 0.6598941683769226, acc.: 60.06%] [G loss: 0.8036862015724182]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 195/200, Batch 23/86 [D loss: 0.6617040932178497, acc.: 59.13%] [G loss: 0.8039537072181702]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 24/86 [D loss: 0.664395809173584, acc.: 59.67%] [G loss: 0.803612470626831]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 25/86 [D loss: 0.6548463404178619, acc.: 62.06%] [G loss: 0.8018594980239868]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 26/86 [D loss: 0.6537188589572906, acc.: 60.79%] [G loss: 0.8050302267074585]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 195/200, Batch 27/86 [D loss: 0.6578460931777954, acc.: 61.52%] [G loss: 0.8075485229492188]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 28/86 [D loss: 0.6629166901111603, acc.: 60.30%] [G loss: 0.8010053634643555]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 29/86 [D loss: 0.6600419580936432, acc.: 60.40%] [G loss: 0.8128459453582764]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 195/200, Batch 30/86 [D loss: 0.6649160087108612, acc.: 58.69%] [G loss: 0.8035465478897095]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 31/86 [D loss: 0.6594857573509216, acc.: 59.81%] [G loss: 0.8076860308647156]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 32/86 [D loss: 0.6591371595859528, acc.: 60.11%] [G loss: 0.8086920976638794]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 33/86 [D loss: 0.6589341461658478, acc.: 60.45%] [G loss: 0.8014078140258789]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 34/86 [D loss: 0.6566236317157745, acc.: 61.13%] [G loss: 0.8015021085739136]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 35/86 [D loss: 0.6579857766628265, acc.: 58.54%] [G loss: 0.8080703020095825]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 36/86 [D loss: 0.6562801897525787, acc.: 60.21%] [G loss: 0.7898540496826172]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 37/86 [D loss: 0.6642026603221893, acc.: 58.94%] [G loss: 0.8039634227752686]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 38/86 [D loss: 0.6557469666004181, acc.: 60.64%] [G loss: 0.8051964044570923]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 39/86 [D loss: 0.6522440314292908, acc.: 61.04%] [G loss: 0.8085280656814575]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 40/86 [D loss: 0.655753880739212, acc.: 61.77%] [G loss: 0.8021717071533203]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 41/86 [D loss: 0.6646938025951385, acc.: 59.03%] [G loss: 0.803987979888916]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 42/86 [D loss: 0.6598235368728638, acc.: 59.62%] [G loss: 0.8069713711738586]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 43/86 [D loss: 0.66208416223526, acc.: 58.25%] [G loss: 0.8048868775367737]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 44/86 [D loss: 0.6547815501689911, acc.: 60.89%] [G loss: 0.8122633099555969]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 45/86 [D loss: 0.6667919754981995, acc.: 57.86%] [G loss: 0.8073462843894958]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 46/86 [D loss: 0.6602668464183807, acc.: 59.77%] [G loss: 0.8108751177787781]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 47/86 [D loss: 0.6706501245498657, acc.: 56.93%] [G loss: 0.8056684732437134]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 48/86 [D loss: 0.6594714522361755, acc.: 60.40%] [G loss: 0.7944628000259399]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 49/86 [D loss: 0.6629893481731415, acc.: 59.72%] [G loss: 0.7919230461120605]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 195/200, Batch 50/86 [D loss: 0.6594846844673157, acc.: 60.16%] [G loss: 0.8060476183891296]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 51/86 [D loss: 0.6658991575241089, acc.: 58.25%] [G loss: 0.7873082160949707]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 52/86 [D loss: 0.65357705950737, acc.: 61.72%] [G loss: 0.7990028262138367]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 195/200, Batch 53/86 [D loss: 0.6614876091480255, acc.: 59.67%] [G loss: 0.8026624917984009]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 54/86 [D loss: 0.6547853052616119, acc.: 60.35%] [G loss: 0.8054829239845276]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 195/200, Batch 55/86 [D loss: 0.6583698093891144, acc.: 60.84%] [G loss: 0.8056771159172058]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 56/86 [D loss: 0.6557493507862091, acc.: 61.23%] [G loss: 0.8082804083824158]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 195/200, Batch 57/86 [D loss: 0.6600558757781982, acc.: 60.89%] [G loss: 0.8049616813659668]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 58/86 [D loss: 0.650694727897644, acc.: 62.30%] [G loss: 0.8122178316116333]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 195/200, Batch 59/86 [D loss: 0.6579623818397522, acc.: 60.79%] [G loss: 0.7992735505104065]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 60/86 [D loss: 0.6555151045322418, acc.: 61.96%] [G loss: 0.8002630472183228]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 195/200, Batch 61/86 [D loss: 0.6673895120620728, acc.: 59.23%] [G loss: 0.8080282211303711]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 195/200, Batch 62/86 [D loss: 0.651547372341156, acc.: 61.57%] [G loss: 0.7987035512924194]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 63/86 [D loss: 0.6503145396709442, acc.: 62.84%] [G loss: 0.8097467422485352]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 64/86 [D loss: 0.6548927426338196, acc.: 60.69%] [G loss: 0.7992426156997681]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 195/200, Batch 65/86 [D loss: 0.6603032946586609, acc.: 59.42%] [G loss: 0.7942845821380615]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 66/86 [D loss: 0.6583983898162842, acc.: 60.45%] [G loss: 0.8021181225776672]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 67/86 [D loss: 0.6551232635974884, acc.: 61.47%] [G loss: 0.804398775100708]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 68/86 [D loss: 0.6582789123058319, acc.: 60.35%] [G loss: 0.8107962012290955]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 69/86 [D loss: 0.6570369601249695, acc.: 60.64%] [G loss: 0.8006919622421265]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 70/86 [D loss: 0.6650099158287048, acc.: 59.03%] [G loss: 0.8011932373046875]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 71/86 [D loss: 0.6618942618370056, acc.: 59.86%] [G loss: 0.8003195524215698]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 72/86 [D loss: 0.6562418341636658, acc.: 60.35%] [G loss: 0.8189050555229187]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 73/86 [D loss: 0.6591454148292542, acc.: 60.30%] [G loss: 0.8062612414360046]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 74/86 [D loss: 0.6539972126483917, acc.: 61.91%] [G loss: 0.8089630603790283]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 75/86 [D loss: 0.65906623005867, acc.: 60.50%] [G loss: 0.8195136785507202]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 195/200, Batch 76/86 [D loss: 0.6610388457775116, acc.: 59.67%] [G loss: 0.8088318109512329]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 77/86 [D loss: 0.6601483225822449, acc.: 59.81%] [G loss: 0.813036322593689]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 195/200, Batch 78/86 [D loss: 0.6535465121269226, acc.: 61.67%] [G loss: 0.7928160429000854]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 79/86 [D loss: 0.6642798185348511, acc.: 59.03%] [G loss: 0.808639407157898]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 80/86 [D loss: 0.6486212909221649, acc.: 62.89%] [G loss: 0.8026999235153198]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 195/200, Batch 81/86 [D loss: 0.6680821180343628, acc.: 58.06%] [G loss: 0.8030003309249878]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 82/86 [D loss: 0.6570948958396912, acc.: 61.04%] [G loss: 0.7934004068374634]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 83/86 [D loss: 0.6696337461471558, acc.: 58.15%] [G loss: 0.8129634857177734]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 84/86 [D loss: 0.666161298751831, acc.: 58.20%] [G loss: 0.7958452105522156]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 195/200, Batch 85/86 [D loss: 0.6598454415798187, acc.: 60.30%] [G loss: 0.80653977394104]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 195/200, Batch 86/86 [D loss: 0.6574448943138123, acc.: 59.67%] [G loss: 0.7953152656555176]\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 196/200, Batch 1/86 [D loss: 0.6689578294754028, acc.: 57.96%] [G loss: 0.808763861656189]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 2/86 [D loss: 0.666496992111206, acc.: 59.03%] [G loss: 0.7964032888412476]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 196/200, Batch 3/86 [D loss: 0.6630767285823822, acc.: 59.28%] [G loss: 0.8010416030883789]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 4/86 [D loss: 0.6617628931999207, acc.: 59.13%] [G loss: 0.805634617805481]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 5/86 [D loss: 0.6579157412052155, acc.: 60.74%] [G loss: 0.8084524273872375]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 6/86 [D loss: 0.657501757144928, acc.: 60.06%] [G loss: 0.8040136694908142]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 7/86 [D loss: 0.6581855416297913, acc.: 59.28%] [G loss: 0.8128929734230042]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 8/86 [D loss: 0.6592774391174316, acc.: 59.91%] [G loss: 0.8079767227172852]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 9/86 [D loss: 0.6546257138252258, acc.: 62.74%] [G loss: 0.8071087598800659]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 10/86 [D loss: 0.6552874445915222, acc.: 62.16%] [G loss: 0.813992977142334]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 11/86 [D loss: 0.6573730111122131, acc.: 60.55%] [G loss: 0.8061131238937378]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 12/86 [D loss: 0.6571817696094513, acc.: 60.40%] [G loss: 0.8006390333175659]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 13/86 [D loss: 0.648994505405426, acc.: 63.38%] [G loss: 0.7926017045974731]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 196/200, Batch 14/86 [D loss: 0.6571199893951416, acc.: 59.47%] [G loss: 0.7908726930618286]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 15/86 [D loss: 0.6581051647663116, acc.: 59.91%] [G loss: 0.8004258275032043]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 196/200, Batch 16/86 [D loss: 0.6528433263301849, acc.: 61.96%] [G loss: 0.8096126914024353]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 196/200, Batch 17/86 [D loss: 0.6607435941696167, acc.: 58.69%] [G loss: 0.7974928617477417]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 18/86 [D loss: 0.6579990983009338, acc.: 60.21%] [G loss: 0.8134121298789978]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 19/86 [D loss: 0.6577321588993073, acc.: 61.18%] [G loss: 0.8036981225013733]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 196/200, Batch 20/86 [D loss: 0.6578269004821777, acc.: 60.25%] [G loss: 0.8106300830841064]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 21/86 [D loss: 0.6601528227329254, acc.: 59.67%] [G loss: 0.8074688911437988]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 22/86 [D loss: 0.6609373390674591, acc.: 60.01%] [G loss: 0.7915087342262268]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 23/86 [D loss: 0.6633890569210052, acc.: 58.79%] [G loss: 0.8127554059028625]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 196/200, Batch 24/86 [D loss: 0.6584036350250244, acc.: 59.91%] [G loss: 0.7973504066467285]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 25/86 [D loss: 0.6578306257724762, acc.: 60.74%] [G loss: 0.8129270076751709]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 196/200, Batch 26/86 [D loss: 0.6612040996551514, acc.: 60.64%] [G loss: 0.8008171319961548]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 27/86 [D loss: 0.653974711894989, acc.: 62.26%] [G loss: 0.8068216443061829]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 28/86 [D loss: 0.6626389920711517, acc.: 59.77%] [G loss: 0.804582417011261]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 29/86 [D loss: 0.6551057398319244, acc.: 61.33%] [G loss: 0.8062467575073242]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 30/86 [D loss: 0.6509845852851868, acc.: 61.57%] [G loss: 0.8011104464530945]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 31/86 [D loss: 0.6555757224559784, acc.: 59.67%] [G loss: 0.8019537925720215]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 32/86 [D loss: 0.656952977180481, acc.: 60.69%] [G loss: 0.8075933456420898]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 196/200, Batch 33/86 [D loss: 0.6601206660270691, acc.: 60.21%] [G loss: 0.8070136308670044]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 34/86 [D loss: 0.6559551358222961, acc.: 60.55%] [G loss: 0.8019099235534668]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 35/86 [D loss: 0.6613961458206177, acc.: 60.21%] [G loss: 0.7976025342941284]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 36/86 [D loss: 0.657676637172699, acc.: 60.45%] [G loss: 0.8042749762535095]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 37/86 [D loss: 0.667511910200119, acc.: 58.50%] [G loss: 0.8058019280433655]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 38/86 [D loss: 0.6606909930706024, acc.: 61.52%] [G loss: 0.7965059280395508]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 196/200, Batch 39/86 [D loss: 0.6646677851676941, acc.: 58.25%] [G loss: 0.7945169806480408]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 40/86 [D loss: 0.6647453904151917, acc.: 59.42%] [G loss: 0.8069198131561279]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 41/86 [D loss: 0.6571027338504791, acc.: 60.94%] [G loss: 0.7960377335548401]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 42/86 [D loss: 0.6576356887817383, acc.: 61.04%] [G loss: 0.8278728723526001]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 43/86 [D loss: 0.6536605954170227, acc.: 61.38%] [G loss: 0.8148478269577026]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 44/86 [D loss: 0.6516235768795013, acc.: 61.72%] [G loss: 0.8145270347595215]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 45/86 [D loss: 0.6531314849853516, acc.: 62.16%] [G loss: 0.805185079574585]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 46/86 [D loss: 0.6606901288032532, acc.: 60.55%] [G loss: 0.7988438010215759]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 47/86 [D loss: 0.659240335226059, acc.: 61.23%] [G loss: 0.7993065118789673]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 48/86 [D loss: 0.6539342403411865, acc.: 60.69%] [G loss: 0.8018748760223389]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 49/86 [D loss: 0.662810206413269, acc.: 60.60%] [G loss: 0.8051929473876953]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 50/86 [D loss: 0.660338819026947, acc.: 60.40%] [G loss: 0.81023108959198]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 51/86 [D loss: 0.654371976852417, acc.: 61.96%] [G loss: 0.8113753199577332]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 52/86 [D loss: 0.6587222516536713, acc.: 60.06%] [G loss: 0.8018754720687866]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 53/86 [D loss: 0.6550931334495544, acc.: 60.99%] [G loss: 0.8075879216194153]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 54/86 [D loss: 0.6572481095790863, acc.: 60.35%] [G loss: 0.8102878928184509]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 55/86 [D loss: 0.6574043929576874, acc.: 61.04%] [G loss: 0.8178638815879822]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 56/86 [D loss: 0.6521479785442352, acc.: 61.91%] [G loss: 0.8117932677268982]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 57/86 [D loss: 0.6638151407241821, acc.: 58.15%] [G loss: 0.8125237226486206]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 196/200, Batch 58/86 [D loss: 0.6588097214698792, acc.: 60.89%] [G loss: 0.8070971965789795]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 59/86 [D loss: 0.6555761098861694, acc.: 60.25%] [G loss: 0.8029359579086304]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 60/86 [D loss: 0.6634120643138885, acc.: 59.03%] [G loss: 0.8067182302474976]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 61/86 [D loss: 0.6589927673339844, acc.: 59.52%] [G loss: 0.8018339276313782]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 196/200, Batch 62/86 [D loss: 0.6573193967342377, acc.: 60.40%] [G loss: 0.8116178512573242]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 63/86 [D loss: 0.655441552400589, acc.: 59.67%] [G loss: 0.8131406903266907]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 196/200, Batch 64/86 [D loss: 0.6547792553901672, acc.: 60.89%] [G loss: 0.808941125869751]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 65/86 [D loss: 0.655096709728241, acc.: 61.57%] [G loss: 0.80057293176651]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 66/86 [D loss: 0.6664189398288727, acc.: 59.23%] [G loss: 0.7992377281188965]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 67/86 [D loss: 0.6629449129104614, acc.: 60.30%] [G loss: 0.7791844606399536]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 68/86 [D loss: 0.6555138826370239, acc.: 62.16%] [G loss: 0.7934945225715637]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 69/86 [D loss: 0.6551932096481323, acc.: 61.67%] [G loss: 0.8013840317726135]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 196/200, Batch 70/86 [D loss: 0.6650535464286804, acc.: 59.52%] [G loss: 0.8073577880859375]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 196/200, Batch 71/86 [D loss: 0.6629385054111481, acc.: 59.72%] [G loss: 0.8167089223861694]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 72/86 [D loss: 0.6659719347953796, acc.: 57.62%] [G loss: 0.8110171556472778]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 73/86 [D loss: 0.6547758877277374, acc.: 61.18%] [G loss: 0.7991806864738464]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 74/86 [D loss: 0.6480355858802795, acc.: 62.16%] [G loss: 0.808283805847168]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 196/200, Batch 75/86 [D loss: 0.6569722294807434, acc.: 60.21%] [G loss: 0.8087542653083801]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 196/200, Batch 76/86 [D loss: 0.6652592718601227, acc.: 58.59%] [G loss: 0.8049602508544922]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 196/200, Batch 77/86 [D loss: 0.655207097530365, acc.: 61.18%] [G loss: 0.8101301193237305]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 78/86 [D loss: 0.6536712944507599, acc.: 61.38%] [G loss: 0.8079435229301453]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 79/86 [D loss: 0.6597403585910797, acc.: 58.59%] [G loss: 0.8105065822601318]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 80/86 [D loss: 0.6655305027961731, acc.: 58.30%] [G loss: 0.8059715628623962]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 81/86 [D loss: 0.6607180237770081, acc.: 60.69%] [G loss: 0.8072434663772583]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 196/200, Batch 82/86 [D loss: 0.6604768931865692, acc.: 60.45%] [G loss: 0.8063571453094482]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 83/86 [D loss: 0.6573592126369476, acc.: 60.45%] [G loss: 0.8041821122169495]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 84/86 [D loss: 0.6612902879714966, acc.: 60.01%] [G loss: 0.8059698343276978]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 196/200, Batch 85/86 [D loss: 0.6577664315700531, acc.: 60.21%] [G loss: 0.8114359974861145]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 196/200, Batch 86/86 [D loss: 0.6606630682945251, acc.: 60.99%] [G loss: 0.7954927682876587]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 197/200, Batch 1/86 [D loss: 0.6535003185272217, acc.: 60.60%] [G loss: 0.7963838577270508]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 2/86 [D loss: 0.6599353551864624, acc.: 60.50%] [G loss: 0.8003078699111938]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 3/86 [D loss: 0.6573209464550018, acc.: 60.79%] [G loss: 0.8083612322807312]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 4/86 [D loss: 0.6504435539245605, acc.: 62.74%] [G loss: 0.7988232374191284]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 5/86 [D loss: 0.6629758179187775, acc.: 59.52%] [G loss: 0.811107873916626]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 6/86 [D loss: 0.6644848883152008, acc.: 59.28%] [G loss: 0.8100818991661072]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 7/86 [D loss: 0.6537439823150635, acc.: 61.28%] [G loss: 0.8078961968421936]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 197/200, Batch 8/86 [D loss: 0.6563670933246613, acc.: 61.04%] [G loss: 0.8112838268280029]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 9/86 [D loss: 0.6620876789093018, acc.: 58.98%] [G loss: 0.8007918000221252]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 10/86 [D loss: 0.6627628207206726, acc.: 59.81%] [G loss: 0.8098015189170837]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 11/86 [D loss: 0.660557746887207, acc.: 61.47%] [G loss: 0.8044602274894714]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 12/86 [D loss: 0.6485645473003387, acc.: 62.01%] [G loss: 0.8058056831359863]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 13/86 [D loss: 0.6561751365661621, acc.: 60.45%] [G loss: 0.7991352081298828]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 14/86 [D loss: 0.6529110670089722, acc.: 62.21%] [G loss: 0.7983924150466919]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 15/86 [D loss: 0.6618618965148926, acc.: 59.08%] [G loss: 0.8048148155212402]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 16/86 [D loss: 0.6543484330177307, acc.: 61.57%] [G loss: 0.8032718300819397]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 17/86 [D loss: 0.6637823283672333, acc.: 58.89%] [G loss: 0.8033254742622375]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 197/200, Batch 18/86 [D loss: 0.6517245769500732, acc.: 61.72%] [G loss: 0.8033760786056519]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 19/86 [D loss: 0.6588377356529236, acc.: 61.33%] [G loss: 0.8085198402404785]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 20/86 [D loss: 0.6566740274429321, acc.: 60.55%] [G loss: 0.8017920255661011]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 21/86 [D loss: 0.6566825211048126, acc.: 61.52%] [G loss: 0.8026081919670105]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 22/86 [D loss: 0.6590000987052917, acc.: 60.40%] [G loss: 0.8011224865913391]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 23/86 [D loss: 0.6606536209583282, acc.: 58.84%] [G loss: 0.7978284358978271]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 24/86 [D loss: 0.6546568870544434, acc.: 62.45%] [G loss: 0.8134864568710327]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 197/200, Batch 25/86 [D loss: 0.6613189876079559, acc.: 60.40%] [G loss: 0.8025674223899841]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 197/200, Batch 26/86 [D loss: 0.6585903167724609, acc.: 60.74%] [G loss: 0.7963082790374756]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 27/86 [D loss: 0.6585071086883545, acc.: 60.89%] [G loss: 0.8001362681388855]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 28/86 [D loss: 0.6577826738357544, acc.: 61.08%] [G loss: 0.797045886516571]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 29/86 [D loss: 0.6662276983261108, acc.: 58.59%] [G loss: 0.811680257320404]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 30/86 [D loss: 0.6539863049983978, acc.: 60.94%] [G loss: 0.8071951866149902]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 31/86 [D loss: 0.6579740941524506, acc.: 60.60%] [G loss: 0.805382490158081]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 32/86 [D loss: 0.6683040857315063, acc.: 58.89%] [G loss: 0.8080280423164368]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 197/200, Batch 33/86 [D loss: 0.6562593579292297, acc.: 61.18%] [G loss: 0.8020250201225281]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 34/86 [D loss: 0.6645514369010925, acc.: 59.13%] [G loss: 0.8133429288864136]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 35/86 [D loss: 0.6559325158596039, acc.: 61.62%] [G loss: 0.805143415927887]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 197/200, Batch 36/86 [D loss: 0.657831460237503, acc.: 59.42%] [G loss: 0.8199597597122192]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 37/86 [D loss: 0.6513287425041199, acc.: 62.45%] [G loss: 0.7995185256004333]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 38/86 [D loss: 0.6610549688339233, acc.: 59.57%] [G loss: 0.8004347085952759]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 39/86 [D loss: 0.6628055274486542, acc.: 58.40%] [G loss: 0.8086537718772888]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 40/86 [D loss: 0.6598511934280396, acc.: 59.62%] [G loss: 0.8129612803459167]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 41/86 [D loss: 0.6558385789394379, acc.: 60.69%] [G loss: 0.8123400211334229]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 42/86 [D loss: 0.654873788356781, acc.: 61.77%] [G loss: 0.8114327788352966]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 43/86 [D loss: 0.6622743606567383, acc.: 60.64%] [G loss: 0.8171032667160034]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 197/200, Batch 44/86 [D loss: 0.6591202318668365, acc.: 60.06%] [G loss: 0.8100306391716003]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 45/86 [D loss: 0.6641586422920227, acc.: 59.67%] [G loss: 0.8207557797431946]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 46/86 [D loss: 0.6550579071044922, acc.: 60.99%] [G loss: 0.8173201084136963]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 47/86 [D loss: 0.6579973697662354, acc.: 60.55%] [G loss: 0.8176848292350769]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 197/200, Batch 48/86 [D loss: 0.6588023900985718, acc.: 60.21%] [G loss: 0.7995569109916687]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 49/86 [D loss: 0.6608633399009705, acc.: 59.08%] [G loss: 0.8097654581069946]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 50/86 [D loss: 0.6638019382953644, acc.: 58.84%] [G loss: 0.8035913705825806]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 51/86 [D loss: 0.6678731441497803, acc.: 58.35%] [G loss: 0.8016116619110107]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 197/200, Batch 52/86 [D loss: 0.6595427989959717, acc.: 60.45%] [G loss: 0.8030347228050232]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 197/200, Batch 53/86 [D loss: 0.6602978408336639, acc.: 60.06%] [G loss: 0.7941597700119019]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 54/86 [D loss: 0.664519727230072, acc.: 57.37%] [G loss: 0.8085614442825317]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 197/200, Batch 55/86 [D loss: 0.6581573784351349, acc.: 59.77%] [G loss: 0.8094687461853027]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 56/86 [D loss: 0.6549496054649353, acc.: 61.23%] [G loss: 0.8065637350082397]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 57/86 [D loss: 0.6562948226928711, acc.: 61.23%] [G loss: 0.8076486587524414]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 58/86 [D loss: 0.6522266268730164, acc.: 61.52%] [G loss: 0.8018465638160706]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 59/86 [D loss: 0.6608522832393646, acc.: 58.94%] [G loss: 0.8214948177337646]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 60/86 [D loss: 0.6551524102687836, acc.: 61.28%] [G loss: 0.7962641716003418]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 61/86 [D loss: 0.6537860631942749, acc.: 60.30%] [G loss: 0.7914222478866577]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 62/86 [D loss: 0.6585738956928253, acc.: 60.40%] [G loss: 0.7981215715408325]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 63/86 [D loss: 0.6626343727111816, acc.: 59.33%] [G loss: 0.8052608966827393]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 64/86 [D loss: 0.6592035889625549, acc.: 60.30%] [G loss: 0.7995703816413879]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 65/86 [D loss: 0.6578152477741241, acc.: 60.40%] [G loss: 0.8053251504898071]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 66/86 [D loss: 0.6611959338188171, acc.: 61.13%] [G loss: 0.8036553263664246]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 67/86 [D loss: 0.6612882614135742, acc.: 60.01%] [G loss: 0.8116289973258972]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 68/86 [D loss: 0.6562954783439636, acc.: 60.11%] [G loss: 0.8135517239570618]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 69/86 [D loss: 0.6586063206195831, acc.: 59.81%] [G loss: 0.7944478988647461]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 70/86 [D loss: 0.6637416183948517, acc.: 58.59%] [G loss: 0.8124815225601196]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 71/86 [D loss: 0.6630134284496307, acc.: 58.98%] [G loss: 0.8088191151618958]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 72/86 [D loss: 0.6559706032276154, acc.: 62.06%] [G loss: 0.81301349401474]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 73/86 [D loss: 0.6581577360630035, acc.: 60.21%] [G loss: 0.8099336624145508]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 74/86 [D loss: 0.6657001674175262, acc.: 58.11%] [G loss: 0.8171516060829163]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 75/86 [D loss: 0.663103848695755, acc.: 58.84%] [G loss: 0.7919507026672363]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 76/86 [D loss: 0.6547944247722626, acc.: 62.06%] [G loss: 0.8029681444168091]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 77/86 [D loss: 0.6549817025661469, acc.: 61.43%] [G loss: 0.7898828983306885]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 78/86 [D loss: 0.6618363559246063, acc.: 59.67%] [G loss: 0.8054808974266052]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 79/86 [D loss: 0.658849835395813, acc.: 59.42%] [G loss: 0.8055062294006348]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 80/86 [D loss: 0.6577885448932648, acc.: 60.79%] [G loss: 0.8034710884094238]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 81/86 [D loss: 0.6595751941204071, acc.: 59.81%] [G loss: 0.7991659641265869]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 82/86 [D loss: 0.6649776101112366, acc.: 59.33%] [G loss: 0.8148576021194458]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 197/200, Batch 83/86 [D loss: 0.6570141017436981, acc.: 59.47%] [G loss: 0.7936363220214844]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 197/200, Batch 84/86 [D loss: 0.6652449369430542, acc.: 58.20%] [G loss: 0.7952486872673035]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 197/200, Batch 85/86 [D loss: 0.6594876945018768, acc.: 59.52%] [G loss: 0.7930631041526794]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 197/200, Batch 86/86 [D loss: 0.6685346066951752, acc.: 57.96%] [G loss: 0.8268632888793945]\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 198/200, Batch 1/86 [D loss: 0.6575385630130768, acc.: 61.82%] [G loss: 0.7896869778633118]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 2/86 [D loss: 0.661798745393753, acc.: 59.57%] [G loss: 0.8182319402694702]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 3/86 [D loss: 0.6535789966583252, acc.: 60.64%] [G loss: 0.7929474115371704]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 4/86 [D loss: 0.6556456089019775, acc.: 61.04%] [G loss: 0.8236251473426819]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 5/86 [D loss: 0.6581934690475464, acc.: 61.28%] [G loss: 0.7950670719146729]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 6/86 [D loss: 0.6666328012943268, acc.: 57.76%] [G loss: 0.8116217851638794]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 7/86 [D loss: 0.6563756763935089, acc.: 60.69%] [G loss: 0.8061363101005554]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 8/86 [D loss: 0.6627528071403503, acc.: 59.96%] [G loss: 0.8092536330223083]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 9/86 [D loss: 0.6649022996425629, acc.: 58.35%] [G loss: 0.7994368076324463]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 10/86 [D loss: 0.6549348831176758, acc.: 61.08%] [G loss: 0.8155538439750671]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 11/86 [D loss: 0.6578636467456818, acc.: 60.11%] [G loss: 0.7959731221199036]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 12/86 [D loss: 0.6505431234836578, acc.: 61.52%] [G loss: 0.807288408279419]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 198/200, Batch 13/86 [D loss: 0.6701438426971436, acc.: 56.93%] [G loss: 0.8023858070373535]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 14/86 [D loss: 0.663850724697113, acc.: 58.64%] [G loss: 0.8070818781852722]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 15/86 [D loss: 0.6587502062320709, acc.: 60.35%] [G loss: 0.8045223355293274]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 198/200, Batch 16/86 [D loss: 0.6585700213909149, acc.: 61.38%] [G loss: 0.803887128829956]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 17/86 [D loss: 0.662012368440628, acc.: 60.21%] [G loss: 0.807243824005127]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 18/86 [D loss: 0.6580072939395905, acc.: 60.74%] [G loss: 0.8082247376441956]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 19/86 [D loss: 0.6660602986812592, acc.: 57.67%] [G loss: 0.8151683211326599]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 20/86 [D loss: 0.6591251790523529, acc.: 60.35%] [G loss: 0.8103396892547607]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 21/86 [D loss: 0.6609203219413757, acc.: 60.25%] [G loss: 0.8030024170875549]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 22/86 [D loss: 0.6591792702674866, acc.: 59.77%] [G loss: 0.8053793907165527]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 23/86 [D loss: 0.6611602008342743, acc.: 60.16%] [G loss: 0.7988612055778503]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 24/86 [D loss: 0.6643243432044983, acc.: 58.25%] [G loss: 0.797512412071228]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 25/86 [D loss: 0.6496197879314423, acc.: 62.21%] [G loss: 0.8016877174377441]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 26/86 [D loss: 0.6546812653541565, acc.: 61.13%] [G loss: 0.8052668571472168]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 27/86 [D loss: 0.6668570339679718, acc.: 58.30%] [G loss: 0.8017723560333252]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 28/86 [D loss: 0.6572247445583344, acc.: 60.11%] [G loss: 0.8030893802642822]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 29/86 [D loss: 0.6621640026569366, acc.: 58.89%] [G loss: 0.8038941621780396]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 30/86 [D loss: 0.6502939760684967, acc.: 63.48%] [G loss: 0.80391925573349]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 31/86 [D loss: 0.6589047908782959, acc.: 60.50%] [G loss: 0.8201440572738647]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 32/86 [D loss: 0.6580911874771118, acc.: 60.79%] [G loss: 0.7992501258850098]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 198/200, Batch 33/86 [D loss: 0.6663272976875305, acc.: 58.84%] [G loss: 0.8059703707695007]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 34/86 [D loss: 0.6585443913936615, acc.: 60.01%] [G loss: 0.8020580410957336]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 35/86 [D loss: 0.6661618947982788, acc.: 58.45%] [G loss: 0.8040831089019775]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 36/86 [D loss: 0.6574780941009521, acc.: 62.11%] [G loss: 0.8047521114349365]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 37/86 [D loss: 0.6632297337055206, acc.: 59.33%] [G loss: 0.7953464984893799]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 38/86 [D loss: 0.654230147600174, acc.: 61.43%] [G loss: 0.7942956686019897]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 39/86 [D loss: 0.6627065539360046, acc.: 59.67%] [G loss: 0.806447446346283]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 40/86 [D loss: 0.6586886048316956, acc.: 60.45%] [G loss: 0.7964053750038147]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 41/86 [D loss: 0.6591357588768005, acc.: 60.55%] [G loss: 0.7962492108345032]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 42/86 [D loss: 0.658004641532898, acc.: 60.89%] [G loss: 0.8034461140632629]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 43/86 [D loss: 0.6565779447555542, acc.: 60.35%] [G loss: 0.8052732348442078]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 44/86 [D loss: 0.6629487872123718, acc.: 58.89%] [G loss: 0.8044660091400146]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 45/86 [D loss: 0.6615660190582275, acc.: 59.18%] [G loss: 0.7961226105690002]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 198/200, Batch 46/86 [D loss: 0.6576724052429199, acc.: 60.94%] [G loss: 0.8050936460494995]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 198/200, Batch 47/86 [D loss: 0.6591591536998749, acc.: 59.67%] [G loss: 0.8182846307754517]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 198/200, Batch 48/86 [D loss: 0.6615422666072845, acc.: 59.72%] [G loss: 0.8042495846748352]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 49/86 [D loss: 0.6575274169445038, acc.: 59.81%] [G loss: 0.7911737561225891]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 50/86 [D loss: 0.6624034941196442, acc.: 59.67%] [G loss: 0.8048904538154602]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 51/86 [D loss: 0.6687930226325989, acc.: 57.86%] [G loss: 0.8015961647033691]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 52/86 [D loss: 0.6542859971523285, acc.: 62.45%] [G loss: 0.8030823469161987]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 53/86 [D loss: 0.6638480424880981, acc.: 59.62%] [G loss: 0.8004425764083862]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 54/86 [D loss: 0.6566430926322937, acc.: 60.60%] [G loss: 0.8182975053787231]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 55/86 [D loss: 0.6558374166488647, acc.: 60.84%] [G loss: 0.8194757699966431]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 56/86 [D loss: 0.6545422673225403, acc.: 62.60%] [G loss: 0.8157246112823486]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 198/200, Batch 57/86 [D loss: 0.6612966060638428, acc.: 59.81%] [G loss: 0.8013024926185608]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 58/86 [D loss: 0.6614934206008911, acc.: 58.30%] [G loss: 0.8069206476211548]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 59/86 [D loss: 0.6561706066131592, acc.: 61.67%] [G loss: 0.8132110834121704]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 60/86 [D loss: 0.6583288311958313, acc.: 60.89%] [G loss: 0.8046532273292542]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 61/86 [D loss: 0.6535113155841827, acc.: 61.18%] [G loss: 0.8023040294647217]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 62/86 [D loss: 0.6576572954654694, acc.: 60.06%] [G loss: 0.8154181241989136]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 63/86 [D loss: 0.6554861068725586, acc.: 61.38%] [G loss: 0.802449107170105]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 64/86 [D loss: 0.6583277583122253, acc.: 59.86%] [G loss: 0.815836489200592]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 65/86 [D loss: 0.6604092717170715, acc.: 60.74%] [G loss: 0.8012887239456177]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 66/86 [D loss: 0.6606596410274506, acc.: 60.60%] [G loss: 0.8098145723342896]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 198/200, Batch 67/86 [D loss: 0.6572936475276947, acc.: 60.60%] [G loss: 0.8075350522994995]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 68/86 [D loss: 0.661344587802887, acc.: 59.57%] [G loss: 0.815056562423706]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 69/86 [D loss: 0.6607803106307983, acc.: 59.18%] [G loss: 0.8049083948135376]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 70/86 [D loss: 0.6524885296821594, acc.: 62.21%] [G loss: 0.8094408512115479]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 71/86 [D loss: 0.6649621427059174, acc.: 59.52%] [G loss: 0.8128187656402588]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 198/200, Batch 72/86 [D loss: 0.6580147743225098, acc.: 61.77%] [G loss: 0.7855852842330933]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 73/86 [D loss: 0.657468318939209, acc.: 60.79%] [G loss: 0.806796133518219]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 74/86 [D loss: 0.6529309153556824, acc.: 60.06%] [G loss: 0.8149592280387878]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 75/86 [D loss: 0.6510918140411377, acc.: 62.11%] [G loss: 0.7988811135292053]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 198/200, Batch 76/86 [D loss: 0.6622317433357239, acc.: 59.33%] [G loss: 0.7979166507720947]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 198/200, Batch 77/86 [D loss: 0.6562067866325378, acc.: 60.99%] [G loss: 0.8031349182128906]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 78/86 [D loss: 0.6515513360500336, acc.: 61.67%] [G loss: 0.8021349310874939]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 79/86 [D loss: 0.6618013978004456, acc.: 60.25%] [G loss: 0.7981600761413574]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 80/86 [D loss: 0.6576560437679291, acc.: 60.69%] [G loss: 0.800764262676239]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 81/86 [D loss: 0.6540850400924683, acc.: 61.28%] [G loss: 0.8044142127037048]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 198/200, Batch 82/86 [D loss: 0.6546347141265869, acc.: 61.96%] [G loss: 0.8056136965751648]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 83/86 [D loss: 0.6594828963279724, acc.: 58.98%] [G loss: 0.8020344972610474]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 198/200, Batch 84/86 [D loss: 0.6563448011875153, acc.: 60.69%] [G loss: 0.8059034943580627]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 85/86 [D loss: 0.6576939523220062, acc.: 60.16%] [G loss: 0.802660346031189]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 198/200, Batch 86/86 [D loss: 0.6616978645324707, acc.: 60.11%] [G loss: 0.8088114261627197]\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 1/86 [D loss: 0.6603344976902008, acc.: 59.96%] [G loss: 0.8084508776664734]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 2/86 [D loss: 0.6564489006996155, acc.: 61.23%] [G loss: 0.7945744395256042]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 3/86 [D loss: 0.6568773090839386, acc.: 61.43%] [G loss: 0.8168128728866577]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 4/86 [D loss: 0.6509891450405121, acc.: 62.01%] [G loss: 0.798778772354126]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 199/200, Batch 5/86 [D loss: 0.6585695445537567, acc.: 59.81%] [G loss: 0.7995613813400269]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 6/86 [D loss: 0.6578080654144287, acc.: 60.16%] [G loss: 0.7928974628448486]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 7/86 [D loss: 0.6537624895572662, acc.: 62.74%] [G loss: 0.8121342658996582]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 8/86 [D loss: 0.6635924279689789, acc.: 59.47%] [G loss: 0.7931933403015137]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 9/86 [D loss: 0.6612994372844696, acc.: 58.84%] [G loss: 0.7990778684616089]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 10/86 [D loss: 0.6656611859798431, acc.: 59.33%] [G loss: 0.8050436973571777]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 11/86 [D loss: 0.656874805688858, acc.: 60.16%] [G loss: 0.8053829073905945]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 12/86 [D loss: 0.6610598564147949, acc.: 59.72%] [G loss: 0.8014805912971497]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 13/86 [D loss: 0.6688001751899719, acc.: 58.11%] [G loss: 0.8100264072418213]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 14/86 [D loss: 0.6596095561981201, acc.: 59.67%] [G loss: 0.8130601644515991]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 15/86 [D loss: 0.6534052193164825, acc.: 61.08%] [G loss: 0.8141344785690308]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 16/86 [D loss: 0.6549833714962006, acc.: 60.79%] [G loss: 0.8029302358627319]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 17/86 [D loss: 0.653585821390152, acc.: 61.47%] [G loss: 0.8012523651123047]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 18/86 [D loss: 0.6573014557361603, acc.: 61.38%] [G loss: 0.8203451633453369]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 19/86 [D loss: 0.664304792881012, acc.: 59.47%] [G loss: 0.8146108388900757]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 20/86 [D loss: 0.6598168909549713, acc.: 60.55%] [G loss: 0.8019012212753296]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 21/86 [D loss: 0.6620968282222748, acc.: 59.08%] [G loss: 0.8060531616210938]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 199/200, Batch 22/86 [D loss: 0.6572677791118622, acc.: 59.96%] [G loss: 0.8124237060546875]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 23/86 [D loss: 0.655756413936615, acc.: 60.01%] [G loss: 0.8043815493583679]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 24/86 [D loss: 0.6571879088878632, acc.: 61.04%] [G loss: 0.8083151578903198]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 25/86 [D loss: 0.6538049876689911, acc.: 60.74%] [G loss: 0.8199703693389893]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 26/86 [D loss: 0.6642529964447021, acc.: 59.47%] [G loss: 0.8076300621032715]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 27/86 [D loss: 0.6639436483383179, acc.: 57.86%] [G loss: 0.8210062384605408]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 28/86 [D loss: 0.6503063440322876, acc.: 63.13%] [G loss: 0.8062206506729126]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 29/86 [D loss: 0.6583757698535919, acc.: 61.04%] [G loss: 0.8155025839805603]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 30/86 [D loss: 0.6575660407543182, acc.: 59.86%] [G loss: 0.7941355109214783]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 31/86 [D loss: 0.6670842468738556, acc.: 58.54%] [G loss: 0.8086053729057312]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 32/86 [D loss: 0.6610381603240967, acc.: 60.35%] [G loss: 0.8050863146781921]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 33/86 [D loss: 0.660105288028717, acc.: 60.89%] [G loss: 0.8166072964668274]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 34/86 [D loss: 0.6648799777030945, acc.: 59.28%] [G loss: 0.804986834526062]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 35/86 [D loss: 0.6509955525398254, acc.: 62.99%] [G loss: 0.7978200912475586]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 36/86 [D loss: 0.6541750431060791, acc.: 59.77%] [G loss: 0.8104056119918823]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 37/86 [D loss: 0.6554635167121887, acc.: 62.40%] [G loss: 0.8048056364059448]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 38/86 [D loss: 0.6629493236541748, acc.: 59.62%] [G loss: 0.8166196942329407]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 39/86 [D loss: 0.6589058339595795, acc.: 60.21%] [G loss: 0.8104585409164429]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 40/86 [D loss: 0.6538428962230682, acc.: 61.91%] [G loss: 0.8125957250595093]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 199/200, Batch 41/86 [D loss: 0.6632625162601471, acc.: 59.96%] [G loss: 0.814471423625946]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 42/86 [D loss: 0.6554487347602844, acc.: 61.77%] [G loss: 0.8140350580215454]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 43/86 [D loss: 0.6644860506057739, acc.: 59.42%] [G loss: 0.8040594458580017]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 44/86 [D loss: 0.6533905863761902, acc.: 59.86%] [G loss: 0.8000239729881287]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 45/86 [D loss: 0.6634604930877686, acc.: 57.96%] [G loss: 0.8147486448287964]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 46/86 [D loss: 0.6603641510009766, acc.: 60.40%] [G loss: 0.8092559576034546]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 47/86 [D loss: 0.661909282207489, acc.: 58.69%] [G loss: 0.8133851289749146]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 48/86 [D loss: 0.6600181460380554, acc.: 60.06%] [G loss: 0.8057383298873901]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 49/86 [D loss: 0.6547560393810272, acc.: 60.64%] [G loss: 0.7951434850692749]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 50/86 [D loss: 0.6545442938804626, acc.: 60.94%] [G loss: 0.8142932653427124]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 51/86 [D loss: 0.6559745967388153, acc.: 59.86%] [G loss: 0.811089277267456]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 52/86 [D loss: 0.6519787907600403, acc.: 63.33%] [G loss: 0.8055422902107239]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 53/86 [D loss: 0.6499604284763336, acc.: 61.77%] [G loss: 0.806597113609314]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 54/86 [D loss: 0.6521518230438232, acc.: 61.91%] [G loss: 0.8088330626487732]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 55/86 [D loss: 0.6611977517604828, acc.: 58.59%] [G loss: 0.8107186555862427]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 56/86 [D loss: 0.6626179814338684, acc.: 59.52%] [G loss: 0.8213250637054443]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 57/86 [D loss: 0.6550388634204865, acc.: 61.08%] [G loss: 0.8110522031784058]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 58/86 [D loss: 0.6652623414993286, acc.: 60.11%] [G loss: 0.796124279499054]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 59/86 [D loss: 0.658607691526413, acc.: 58.35%] [G loss: 0.8083028793334961]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 60/86 [D loss: 0.6567398011684418, acc.: 61.96%] [G loss: 0.8050349354743958]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 61/86 [D loss: 0.6667435765266418, acc.: 58.69%] [G loss: 0.8007972240447998]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 62/86 [D loss: 0.6553319692611694, acc.: 60.69%] [G loss: 0.8006227016448975]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 63/86 [D loss: 0.6556682288646698, acc.: 60.50%] [G loss: 0.80572909116745]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 64/86 [D loss: 0.6566146612167358, acc.: 60.60%] [G loss: 0.8200976848602295]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 65/86 [D loss: 0.6588936746120453, acc.: 60.25%] [G loss: 0.8210344910621643]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 66/86 [D loss: 0.6602681279182434, acc.: 60.30%] [G loss: 0.7961313128471375]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 67/86 [D loss: 0.6582025289535522, acc.: 60.06%] [G loss: 0.8153128623962402]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 68/86 [D loss: 0.6596845984458923, acc.: 60.21%] [G loss: 0.8137843608856201]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 69/86 [D loss: 0.6568404138088226, acc.: 60.25%] [G loss: 0.8094136714935303]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 70/86 [D loss: 0.6614842116832733, acc.: 59.52%] [G loss: 0.8088423013687134]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 71/86 [D loss: 0.6635986864566803, acc.: 58.15%] [G loss: 0.8077207803726196]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 72/86 [D loss: 0.6560772061347961, acc.: 61.08%] [G loss: 0.8132113218307495]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 199/200, Batch 73/86 [D loss: 0.6586054861545563, acc.: 60.84%] [G loss: 0.8085371255874634]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 74/86 [D loss: 0.6577122211456299, acc.: 60.40%] [G loss: 0.8122321367263794]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 75/86 [D loss: 0.6543555557727814, acc.: 60.79%] [G loss: 0.8026052713394165]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 76/86 [D loss: 0.6608455181121826, acc.: 59.86%] [G loss: 0.7915458083152771]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 199/200, Batch 77/86 [D loss: 0.6552556753158569, acc.: 60.55%] [G loss: 0.8026533126831055]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 78/86 [D loss: 0.6545366048812866, acc.: 60.94%] [G loss: 0.8082396984100342]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 199/200, Batch 79/86 [D loss: 0.6610249578952789, acc.: 59.08%] [G loss: 0.8176682591438293]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 80/86 [D loss: 0.6576820909976959, acc.: 61.18%] [G loss: 0.7919887900352478]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 199/200, Batch 81/86 [D loss: 0.6568767428398132, acc.: 61.08%] [G loss: 0.8064417839050293]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 82/86 [D loss: 0.6603452861309052, acc.: 58.94%] [G loss: 0.8051782846450806]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 199/200, Batch 83/86 [D loss: 0.6639569401741028, acc.: 59.33%] [G loss: 0.8076096177101135]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 84/86 [D loss: 0.6619472503662109, acc.: 58.06%] [G loss: 0.8022082448005676]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 85/86 [D loss: 0.6643905341625214, acc.: 58.59%] [G loss: 0.8096705079078674]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 199/200, Batch 86/86 [D loss: 0.6632083654403687, acc.: 59.72%] [G loss: 0.8094152212142944]\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 200/200, Batch 1/86 [D loss: 0.6551458835601807, acc.: 61.08%] [G loss: 0.8127235770225525]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 2/86 [D loss: 0.6561245620250702, acc.: 60.74%] [G loss: 0.8104273676872253]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 3/86 [D loss: 0.6596351265907288, acc.: 58.98%] [G loss: 0.8052297830581665]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 4/86 [D loss: 0.6644944846630096, acc.: 59.57%] [G loss: 0.8085551857948303]\n",
      "32/32 [==============================] - 1s 15ms/step\n",
      "Epoch 200/200, Batch 5/86 [D loss: 0.6556452810764313, acc.: 61.87%] [G loss: 0.8176946043968201]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 6/86 [D loss: 0.6555438935756683, acc.: 60.11%] [G loss: 0.8174322843551636]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 7/86 [D loss: 0.6649591028690338, acc.: 58.74%] [G loss: 0.8007744550704956]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 8/86 [D loss: 0.6616242825984955, acc.: 59.77%] [G loss: 0.8222643136978149]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 9/86 [D loss: 0.6561633050441742, acc.: 60.45%] [G loss: 0.8056216239929199]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 10/86 [D loss: 0.6554773151874542, acc.: 61.38%] [G loss: 0.8089287877082825]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 11/86 [D loss: 0.6507221460342407, acc.: 61.96%] [G loss: 0.8103305697441101]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 12/86 [D loss: 0.6577454805374146, acc.: 60.35%] [G loss: 0.8145802021026611]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 13/86 [D loss: 0.6555418372154236, acc.: 60.01%] [G loss: 0.8012665510177612]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 14/86 [D loss: 0.65484619140625, acc.: 62.01%] [G loss: 0.802189826965332]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 200/200, Batch 15/86 [D loss: 0.6535926461219788, acc.: 60.06%] [G loss: 0.7920936346054077]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 16/86 [D loss: 0.6614250838756561, acc.: 58.64%] [G loss: 0.8245137929916382]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 200/200, Batch 17/86 [D loss: 0.6641986966133118, acc.: 58.06%] [G loss: 0.7956281304359436]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 18/86 [D loss: 0.6608234345912933, acc.: 60.50%] [G loss: 0.8169378638267517]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 19/86 [D loss: 0.6582874059677124, acc.: 60.21%] [G loss: 0.8081080913543701]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 20/86 [D loss: 0.6598770320415497, acc.: 61.04%] [G loss: 0.8065735101699829]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 21/86 [D loss: 0.6533525586128235, acc.: 61.82%] [G loss: 0.7947617173194885]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 22/86 [D loss: 0.6648490130901337, acc.: 58.45%] [G loss: 0.8068037629127502]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 23/86 [D loss: 0.6596055030822754, acc.: 60.55%] [G loss: 0.8010627031326294]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 24/86 [D loss: 0.661670595407486, acc.: 60.50%] [G loss: 0.8093327283859253]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 200/200, Batch 25/86 [D loss: 0.6589764058589935, acc.: 60.35%] [G loss: 0.798780083656311]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 26/86 [D loss: 0.666704535484314, acc.: 57.76%] [G loss: 0.7989276647567749]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 27/86 [D loss: 0.6595843434333801, acc.: 60.79%] [G loss: 0.809569239616394]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 200/200, Batch 28/86 [D loss: 0.6516209542751312, acc.: 61.77%] [G loss: 0.8117485046386719]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 200/200, Batch 29/86 [D loss: 0.6552646160125732, acc.: 60.64%] [G loss: 0.7952777147293091]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 30/86 [D loss: 0.6585785150527954, acc.: 59.42%] [G loss: 0.8051548600196838]\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 200/200, Batch 31/86 [D loss: 0.6669489145278931, acc.: 58.20%] [G loss: 0.793962299823761]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 32/86 [D loss: 0.6566591560840607, acc.: 60.16%] [G loss: 0.8031127452850342]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 200/200, Batch 33/86 [D loss: 0.6576676964759827, acc.: 60.01%] [G loss: 0.7992881536483765]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 34/86 [D loss: 0.6625340580940247, acc.: 58.79%] [G loss: 0.8109334707260132]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 35/86 [D loss: 0.6596237421035767, acc.: 59.72%] [G loss: 0.7954734563827515]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 200/200, Batch 36/86 [D loss: 0.6618404388427734, acc.: 60.45%] [G loss: 0.8055095076560974]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 37/86 [D loss: 0.6591180860996246, acc.: 59.81%] [G loss: 0.7859524488449097]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 38/86 [D loss: 0.6644754111766815, acc.: 58.20%] [G loss: 0.8024765849113464]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 39/86 [D loss: 0.6563681066036224, acc.: 60.84%] [G loss: 0.801139771938324]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 40/86 [D loss: 0.6661521196365356, acc.: 58.11%] [G loss: 0.8330897688865662]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 41/86 [D loss: 0.6630546748638153, acc.: 59.62%] [G loss: 0.8094111680984497]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 42/86 [D loss: 0.6682086884975433, acc.: 57.91%] [G loss: 0.802213191986084]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 43/86 [D loss: 0.6546015441417694, acc.: 61.38%] [G loss: 0.8218037486076355]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 200/200, Batch 44/86 [D loss: 0.6625340580940247, acc.: 59.03%] [G loss: 0.8016687631607056]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 45/86 [D loss: 0.6533174514770508, acc.: 60.94%] [G loss: 0.8055453300476074]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 46/86 [D loss: 0.66387540102005, acc.: 59.38%] [G loss: 0.8025277256965637]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 200/200, Batch 47/86 [D loss: 0.6525537073612213, acc.: 61.23%] [G loss: 0.7987192869186401]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 48/86 [D loss: 0.6669449806213379, acc.: 58.35%] [G loss: 0.8061946034431458]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 49/86 [D loss: 0.6550061702728271, acc.: 60.69%] [G loss: 0.7890619039535522]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 50/86 [D loss: 0.6717482805252075, acc.: 56.20%] [G loss: 0.8113325834274292]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 51/86 [D loss: 0.6587144732475281, acc.: 60.94%] [G loss: 0.8074942231178284]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 200/200, Batch 52/86 [D loss: 0.660990834236145, acc.: 58.94%] [G loss: 0.8063259124755859]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 200/200, Batch 53/86 [D loss: 0.6551195681095123, acc.: 60.79%] [G loss: 0.8097202777862549]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 54/86 [D loss: 0.662902444601059, acc.: 59.42%] [G loss: 0.8087258338928223]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 55/86 [D loss: 0.65751513838768, acc.: 60.16%] [G loss: 0.8183059692382812]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 200/200, Batch 56/86 [D loss: 0.6643381118774414, acc.: 58.79%] [G loss: 0.8014740347862244]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 57/86 [D loss: 0.6584355235099792, acc.: 60.35%] [G loss: 0.8159599304199219]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 58/86 [D loss: 0.6647756993770599, acc.: 59.08%] [G loss: 0.8083579540252686]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 59/86 [D loss: 0.6591303646564484, acc.: 59.47%] [G loss: 0.8119704127311707]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 60/86 [D loss: 0.6592559218406677, acc.: 61.13%] [G loss: 0.8185513019561768]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 61/86 [D loss: 0.6608882546424866, acc.: 60.74%] [G loss: 0.8218696117401123]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 62/86 [D loss: 0.6616576910018921, acc.: 59.18%] [G loss: 0.7937418222427368]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 63/86 [D loss: 0.6594417691230774, acc.: 60.74%] [G loss: 0.801874577999115]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 64/86 [D loss: 0.6602423787117004, acc.: 59.38%] [G loss: 0.8129737377166748]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 65/86 [D loss: 0.6524490416049957, acc.: 61.87%] [G loss: 0.8162702322006226]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 66/86 [D loss: 0.6633151471614838, acc.: 60.01%] [G loss: 0.8107776641845703]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 67/86 [D loss: 0.6487524807453156, acc.: 61.91%] [G loss: 0.8079077005386353]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 68/86 [D loss: 0.6575803756713867, acc.: 60.06%] [G loss: 0.8125354051589966]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 200/200, Batch 69/86 [D loss: 0.6526957452297211, acc.: 62.70%] [G loss: 0.8088092803955078]\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 200/200, Batch 70/86 [D loss: 0.6614751815795898, acc.: 59.91%] [G loss: 0.8009772896766663]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 71/86 [D loss: 0.6561823785305023, acc.: 60.84%] [G loss: 0.8101065158843994]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 72/86 [D loss: 0.6504469215869904, acc.: 60.50%] [G loss: 0.8084641098976135]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 73/86 [D loss: 0.6538352966308594, acc.: 61.18%] [G loss: 0.8034656047821045]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 74/86 [D loss: 0.6600591242313385, acc.: 60.55%] [G loss: 0.8075780868530273]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 75/86 [D loss: 0.6587269008159637, acc.: 60.25%] [G loss: 0.815837025642395]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 76/86 [D loss: 0.65756955742836, acc.: 59.03%] [G loss: 0.8144310712814331]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 77/86 [D loss: 0.6576623618602753, acc.: 60.11%] [G loss: 0.8157402276992798]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 78/86 [D loss: 0.6633394658565521, acc.: 58.20%] [G loss: 0.8121190071105957]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 79/86 [D loss: 0.6556909680366516, acc.: 61.47%] [G loss: 0.8094056248664856]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 80/86 [D loss: 0.6600550711154938, acc.: 60.84%] [G loss: 0.8067564368247986]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 200/200, Batch 81/86 [D loss: 0.6571227014064789, acc.: 60.01%] [G loss: 0.8121572136878967]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 200/200, Batch 82/86 [D loss: 0.6612724959850311, acc.: 60.64%] [G loss: 0.7975130677223206]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 83/86 [D loss: 0.6543504893779755, acc.: 61.96%] [G loss: 0.8142802715301514]\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 200/200, Batch 84/86 [D loss: 0.6551690101623535, acc.: 61.23%] [G loss: 0.7955723404884338]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 200/200, Batch 85/86 [D loss: 0.6532363891601562, acc.: 61.87%] [G loss: 0.8141725659370422]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 200/200, Batch 86/86 [D loss: 0.6573184132575989, acc.: 60.55%] [G loss: 0.8030178546905518]\n",
      "4/4 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "cgan = CGAN(img_rows, img_cols, channels)\n",
    "cgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = self.define_gan(self.generator,self.discriminator)\n",
    "\n",
    "        \n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        fe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(64, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        out1 = Dense(1, activation='sigmoid')(fe)\n",
    "        out2 = Dense(n_classes, activation='softmax')(fe)\n",
    "        model = Model(in_image, [out1, out2])\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 384 * 7 * 7\n",
    "        gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Reshape((7, 7, 384))(gen)\n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "        gen = BatchNormalization()(gen)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "        out_layer = Activation('tanh')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "    \n",
    "        # define the combined generator and discriminator model, for updating the generator\n",
    "    def define_gan(self, g_model, d_model):\n",
    "        # make weights in the discriminator not trainable\n",
    "        for layer in d_model.layers:\n",
    "            if not isinstance(layer, BatchNormalization):\n",
    "                layer.trainable = False\n",
    "        # connect the outputs of the generator to the inputs of the discriminator\n",
    "        gan_output = d_model(g_model.output)\n",
    "        # define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "        model = Model(g_model.input, gan_output)\n",
    "        # compile model\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                # axs[i, j].set_title(chr(sampled_labels[cnt][0] + 65))\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"ACGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('ACGAN_mnist', exist_ok=True)\n",
    "        fig.savefig(\"ACGAN_mnist/ACGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def generate_latent_points(self, latent_dim, n_samples, n_classes=26):\n",
    "        # generate points in the latent space\n",
    "        x_input = randn(latent_dim * n_samples)\n",
    "        # reshape into a batch of inputs for the network\n",
    "        z_input = x_input.reshape(n_samples, latent_dim)\n",
    "        # generate labels\n",
    "        labels = randint(0, n_classes, n_samples)\n",
    "        return [z_input, labels]\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=1024, save_interval=1, gen_steps=3):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, [labels_real, y_train[idx]])\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [labels_fake, gen_labels])\n",
    "                d_loss_1 = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])\n",
    "                d_loss_2 = 0.5 * np.add(d_loss_real[1], d_loss_fake[1])\n",
    "\n",
    "                for _ in range(gen_steps):\n",
    "                    z_input, z_labels = self.generate_latent_points(self.latent_dim, batch_size)\n",
    "                    y_gan = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss 1: {d_loss_1}, D loss 2: {d_loss_2}, G loss: {g_loss}]\")\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 11ms/step\n",
      "WARNING:tensorflow:5 out of the last 1051 calls to <function Model.make_train_function.<locals>.train_function at 0x000002AF2A09A430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1052 calls to <function Model.make_train_function.<locals>.train_function at 0x000002AF2A09A430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/200, Batch 1/86 [D loss 1: 5.381772041320801, D loss 2: 1.2783698439598083, G loss: [3.94864821434021, 0.6901225447654724, 3.2585256099700928]]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 2/86 [D loss 1: 4.9785637855529785, D loss 2: 0.8746237903833389, G loss: [3.951296806335449, 0.6927280426025391, 3.25856876373291]]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 3/86 [D loss 1: 4.6563920974731445, D loss 2: 0.6023098826408386, G loss: [3.9551031589508057, 0.6965446472167969, 3.258558511734009]]\n",
      "32/32 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model_32/batch_normalization_23/FusedBatchNormGradV3' defined at (most recent call last):\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\windows_events.py\", line 316, in run_forever\n      super().run_forever()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1284\\2451029588.py\", line 6, in <module>\n      acgan.train()\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1284\\2771885421.py\", line 139, in train\n      g_loss = self.combined.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 2381, in train_on_batch\n      logs = self.train_function(iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model_32/batch_normalization_23/FusedBatchNormGradV3'\nOOM when allocating tensor with shape[1024,192,14,14] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_32/batch_normalization_23/FusedBatchNormGradV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7010093]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate and train the DCGAN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m acgan \u001b[38;5;241m=\u001b[39m ACGAN(img_rows, img_cols, channels)\n\u001b[1;32m----> 6\u001b[0m \u001b[43macgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 139\u001b[0m, in \u001b[0;36mACGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval, gen_steps)\u001b[0m\n\u001b[0;32m    137\u001b[0m         z_input, z_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_latent_points(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim, batch_size)\n\u001b[0;32m    138\u001b[0m         y_gan \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 139\u001b[0m         g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_gan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatches_per_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [D loss 1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, D loss 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss_2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2381\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2379\u001b[0m     )\n\u001b[0;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2381\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2383\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model_32/batch_normalization_23/FusedBatchNormGradV3' defined at (most recent call last):\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\windows_events.py\", line 316, in run_forever\n      super().run_forever()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1284\\2451029588.py\", line 6, in <module>\n      acgan.train()\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1284\\2771885421.py\", line 139, in train\n      g_loss = self.combined.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 2381, in train_on_batch\n      logs = self.train_function(iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model_32/batch_normalization_23/FusedBatchNormGradV3'\nOOM when allocating tensor with shape[1024,192,14,14] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_32/batch_normalization_23/FusedBatchNormGradV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_7010093]"
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "acgan = ACGAN(img_rows, img_cols, channels)\n",
    "acgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
