{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\rejey ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, Conv2DTranspose, PReLU\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Concatenate\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, HTML\n",
    "import glob\n",
    "from keras.layers import AveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Add\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import Input\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install scikit-image\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Reshape, UpSampling2D, \\\n",
    "    BatchNormalization, Activation, Input, LeakyReLU, ZeroPadding2D, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "#import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('emnist-letters-train.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51:24 Harry Potter Deathy Hallows 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {1: 0, \n",
    "           2: 1, \n",
    "           3: 2, \n",
    "           4: 3, \n",
    "           5: 4, \n",
    "           6: 5, \n",
    "           7: 6, \n",
    "           8: 7, \n",
    "           9: 8, \n",
    "           10: 9, \n",
    "           11: 10, \n",
    "           12: 11, \n",
    "           13: 12, \n",
    "           14: 13, \n",
    "           15: 14, \n",
    "           16: 15, \n",
    "           17: 16, \n",
    "           18: 17, \n",
    "           19: 18, \n",
    "           20: 19, \n",
    "           21: 20, \n",
    "           22: 21, \n",
    "           23: 22, \n",
    "           24: 23, \n",
    "           25: 24, \n",
    "           26: 25, \n",
    "           27: 26}\n",
    "\n",
    "        # Map the labels column to its corresponding value\n",
    "df[0] = df[0].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = np.array(df.iloc[:,0].values)\n",
    "y_pre = pd.Categorical(y_pre)\n",
    "X = np.array(df.iloc[:,1:].values)\n",
    "X = X.reshape(-1,28,28,1)\n",
    "preprocessed = []\n",
    "for image in X:\n",
    "    rotated_image = rotate(image, 90, reshape=False)\n",
    "    flipped_image = np.flipud(rotated_image)\n",
    "    preprocessed.append(flipped_image)\n",
    "X_pre = np.array(preprocessed)\n",
    "X = X_pre\n",
    "X = X.astype('float32')\n",
    "X_pre = (X - 127.5) / 127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 6, 15, 14, 16, ..., 19, 8, 5, 11, 0]\n",
       "Length: 26\n",
       "Categories (26, int64): [0, 1, 2, 3, ..., 22, 23, 24, 25]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'y_pre\\n{y_pre.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, images, labels_input, epochs=10000, batch_size=256, save_interval=500, gen_steps=1):\n",
    "        os.makedirs('ACGAN_Generated_Images', exist_ok=True)\n",
    "        X_train = images\n",
    "        y_train = labels_input\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                # Train Discriminator\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, [labels_real, y_train[idx]])\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [labels_fake, gen_labels])\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # Train Generator\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.acgan.train_on_batch([noise, gen_labels], [valid_y, gen_labels])\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[3]:.2f}%] [G loss: {g_loss[0]}]\")\n",
    "\n",
    "            # Save generated images at save intervals\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "                self.save_images(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CGAN():\n",
    "\n",
    "#     def __init__(self, rows, cols, channels, z = 100):\n",
    "#         # Input shape\n",
    "#         self.img_rows = rows\n",
    "#         self.img_cols = cols\n",
    "#         self.channels = channels\n",
    "#         self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "#         self.latent_dim = z\n",
    "#         self.classes = 25\n",
    "#         optimizer = Adam(0.00002, 0.5)\n",
    "#         # Build and compile the discriminator\n",
    "#         self.discriminator = self.define_discriminator(self.img_shape,self.classes)\n",
    "#         self.discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "#         # Build the generator\n",
    "#         self.g_model = self.define_generator(self.latent_dim,self.classes)\n",
    "#         # The generator takes noise as input and generates imgs\n",
    "#         z = Input(shape=(self.latent_dim,))\n",
    "#         img = self.g_model(z)\n",
    "#         # For the combined model we will only train the generator\n",
    "#         self.discriminator.trainable = False\n",
    "#         # The discriminator takes generated images as input and\n",
    "#         # determines validity\n",
    "#         valid = self.discriminator(img)\n",
    "#         # The combined model (stacked generator and discriminator)\n",
    "#         # Trains the generator to fool the discriminator\n",
    "#         self.combined = Model(z, valid)\n",
    "#         self.combined.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "\n",
    "#     # define the standalone discriminator model\n",
    "#     def define_discriminator(self, in_shape, n_classes):\n",
    "#         in_label = Input(shape=(1,))\n",
    "#         li = Embedding(n_classes, 50)(in_label)\n",
    "#         n_nodes = in_shape[0] * in_shape[1]\n",
    "#         li = Dense(n_nodes)(li)\n",
    "#         li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "#         in_image = Input(shape=in_shape)\n",
    "#         merge = Concatenate()([in_image, li])\n",
    "#         fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
    "#         fe = LeakyReLU(alpha=0.2)(fe)\n",
    "#         fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
    "#         fe = LeakyReLU(alpha=0.2)(fe)\n",
    "#         fe = Flatten()(fe)\n",
    "#         fe = Dropout(0.4)(fe)\n",
    "#         out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "#         model = Model([in_image, in_label], out_layer)\n",
    "#         return model\n",
    "\n",
    "#     # define the standalone generator model\n",
    "#     def define_generator(self, latent_dim, n_classes):\n",
    "#         in_label = Input(shape=(1,))\n",
    "#         li = Embedding(n_classes, 50)(in_label)\n",
    "#         n_nodes = 7 * 7\n",
    "#         li = Dense(n_nodes)(li)\n",
    "#         li = Reshape((7, 7, 1))(li)\n",
    "#         in_lat = Input(shape=(latent_dim,))\n",
    "#         n_nodes = 128 * 7 * 7\n",
    "#         gen = Dense(n_nodes)(in_lat)\n",
    "#         gen = LeakyReLU(alpha=0.2)(gen)\n",
    "#         gen = Reshape((7, 7, 128))(gen)\n",
    "#         merge = Concatenate()([gen, li])\n",
    "#         gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same')(merge)\n",
    "#         gen = LeakyReLU(alpha=0.2)(gen)\n",
    "#         gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same')(gen)\n",
    "#         gen = LeakyReLU(alpha=0.2)(gen)\n",
    "#         out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
    "#         model = Model([in_lat, in_label], out_layer)\n",
    "#         return model\n",
    "\n",
    "\n",
    "#     def save_imgs(self, epoch):\n",
    "#         r, c = 5, 5\n",
    "#         noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "#         gen_imgs = self.generator.predict(noise)\n",
    "#         gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "#         fig, axs = plt.subplots(r, c)\n",
    "#         cnt = 0\n",
    "#         for i in range(r):\n",
    "#             for j in range(c):\n",
    "#                 axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "#                 axs[i,j].axis('off')\n",
    "#                 cnt += 1\n",
    "#         os.makedirs('generated_mnist_CGAN', exist_ok=True)\n",
    "#         fig.savefig(\"generated_mnist_CGAN/cgan_mnist_improved_%d.png\" % epoch)\n",
    "#         plt.close()\n",
    "\n",
    "#     # train the generator and discriminator\n",
    "#     def train(self, epochs, batch_size=256, save_internal = 500, gen_steps=1):\n",
    "#         X_train = X_pre\n",
    "#         y_train = y_pre\n",
    "\n",
    "#         batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             for batch in range(batches_per_epoch):\n",
    "#                 # Train Discriminator\n",
    "#                 idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "#                 imgs = X_train[idx]\n",
    "#                 labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "#                 noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "#                 gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "#                 gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "#                 labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "#                 d_loss_real = self.discriminator.train_on_batch(imgs, [labels_real, y_train[idx]])\n",
    "#                 d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [labels_fake, gen_labels])\n",
    "#                 d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "#                 # Train Generator\n",
    "#                 g_loss = None\n",
    "#                 for _ in range(gen_steps):\n",
    "#                     noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "#                     gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "#                     valid_y = np.ones((batch_size, 1))\n",
    "#                     g_loss = self.acgan.train_on_batch([noise, gen_labels], [valid_y, gen_labels])\n",
    "\n",
    "#                 # Print the progress\n",
    "#                 print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[3]:.2f}%] [G loss: {g_loss[0]}]\")\n",
    "\n",
    "#             # Save generated images at save intervals\n",
    "#             if (epoch + 1) % save_internal == 0:\n",
    "#                 self.save_images(epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.00002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = Model([z, label], valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = in_shape[0] * in_shape[1]\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        merge = Concatenate()([in_image, li])\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        fe = Dropout(0.4)(fe)\n",
    "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "        model = Model([in_image, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 128 * 7 * 7\n",
    "        gen = Dense(n_nodes)(in_lat)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Reshape((7, 7, 128))(gen)\n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(merge)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes  # Ensure labels are within valid range\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        os.makedirs('generated_mnist_CGAN', exist_ok=True)\n",
    "        fig.savefig(\"generated_mnist_CGAN/cgan_mnist_improved_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def train(self, epochs=100, batch_size=256, save_interval=2, gen_steps=1):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, y_train[idx]], labels_real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, gen_labels], labels_fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([noise, gen_labels], valid_y)\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 1/346 [D loss: 0.694011926651001, acc.: 51.76%] [G loss: 0.6934709548950195]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 2/346 [D loss: 0.6883581578731537, acc.: 58.79%] [G loss: 0.6932325959205627]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 3/346 [D loss: 0.686428964138031, acc.: 54.88%] [G loss: 0.6930594444274902]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 4/346 [D loss: 0.6820458769798279, acc.: 52.73%] [G loss: 0.6926531791687012]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 5/346 [D loss: 0.6763851046562195, acc.: 53.91%] [G loss: 0.692227840423584]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 6/346 [D loss: 0.6749053299427032, acc.: 50.78%] [G loss: 0.6919347047805786]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 7/346 [D loss: 0.6700701415538788, acc.: 50.39%] [G loss: 0.6914581060409546]\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "Epoch 1/100, Batch 8/346 [D loss: 0.6679535806179047, acc.: 48.83%] [G loss: 0.6909193992614746]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 9/346 [D loss: 0.662602424621582, acc.: 50.78%] [G loss: 0.690280556678772]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 10/346 [D loss: 0.6611395478248596, acc.: 49.80%] [G loss: 0.6898636817932129]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 11/346 [D loss: 0.6579989492893219, acc.: 50.00%] [G loss: 0.6893957853317261]\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "Epoch 1/100, Batch 12/346 [D loss: 0.6525456011295319, acc.: 50.20%] [G loss: 0.6885550022125244]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 13/346 [D loss: 0.6499205827713013, acc.: 50.00%] [G loss: 0.6880006790161133]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 14/346 [D loss: 0.6475188136100769, acc.: 50.00%] [G loss: 0.6871968507766724]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 15/346 [D loss: 0.6458257734775543, acc.: 50.00%] [G loss: 0.6865285038948059]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 16/346 [D loss: 0.6413047313690186, acc.: 49.61%] [G loss: 0.6854324340820312]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 17/346 [D loss: 0.6368770003318787, acc.: 50.00%] [G loss: 0.6848059296607971]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 18/346 [D loss: 0.6348203718662262, acc.: 50.00%] [G loss: 0.6838542222976685]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 19/346 [D loss: 0.6300227046012878, acc.: 50.00%] [G loss: 0.6826025247573853]\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "Epoch 1/100, Batch 20/346 [D loss: 0.6291057169437408, acc.: 49.80%] [G loss: 0.6817648410797119]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 21/346 [D loss: 0.624124139547348, acc.: 50.00%] [G loss: 0.6801584362983704]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 22/346 [D loss: 0.6220275163650513, acc.: 50.00%] [G loss: 0.6788584589958191]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 23/346 [D loss: 0.6197009384632111, acc.: 50.00%] [G loss: 0.6771987080574036]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 24/346 [D loss: 0.6166257560253143, acc.: 50.00%] [G loss: 0.6753559112548828]\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "Epoch 1/100, Batch 25/346 [D loss: 0.614718496799469, acc.: 50.00%] [G loss: 0.6740414500236511]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 26/346 [D loss: 0.612551212310791, acc.: 50.00%] [G loss: 0.6721921563148499]\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "Epoch 1/100, Batch 27/346 [D loss: 0.6084203273057938, acc.: 50.00%] [G loss: 0.6700640320777893]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 28/346 [D loss: 0.6053051054477692, acc.: 50.00%] [G loss: 0.6678630113601685]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 29/346 [D loss: 0.6041657030582428, acc.: 50.00%] [G loss: 0.6660771369934082]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 30/346 [D loss: 0.5999669134616852, acc.: 50.00%] [G loss: 0.6639805436134338]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 31/346 [D loss: 0.5991868525743484, acc.: 50.00%] [G loss: 0.6615304946899414]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 32/346 [D loss: 0.5979304611682892, acc.: 50.00%] [G loss: 0.6593109369277954]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 33/346 [D loss: 0.5935690701007843, acc.: 50.00%] [G loss: 0.6570352911949158]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 34/346 [D loss: 0.5954631567001343, acc.: 50.00%] [G loss: 0.654676079750061]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 35/346 [D loss: 0.5913686454296112, acc.: 50.00%] [G loss: 0.6514524817466736]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 36/346 [D loss: 0.5894229114055634, acc.: 50.00%] [G loss: 0.6484776735305786]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 37/346 [D loss: 0.5882250368595123, acc.: 50.00%] [G loss: 0.6466588973999023]\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "Epoch 1/100, Batch 38/346 [D loss: 0.5870433747768402, acc.: 50.00%] [G loss: 0.6439788341522217]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 39/346 [D loss: 0.586372584104538, acc.: 50.00%] [G loss: 0.6407513618469238]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 40/346 [D loss: 0.5846806466579437, acc.: 50.00%] [G loss: 0.6376655697822571]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 41/346 [D loss: 0.5844893753528595, acc.: 50.00%] [G loss: 0.6359348297119141]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 42/346 [D loss: 0.5826271772384644, acc.: 50.00%] [G loss: 0.6329389810562134]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 43/346 [D loss: 0.5822448134422302, acc.: 50.00%] [G loss: 0.629115104675293]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 44/346 [D loss: 0.5817800909280777, acc.: 50.00%] [G loss: 0.6266648769378662]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 45/346 [D loss: 0.5801652669906616, acc.: 50.00%] [G loss: 0.6241734027862549]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 46/346 [D loss: 0.580036073923111, acc.: 50.00%] [G loss: 0.6224814653396606]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 47/346 [D loss: 0.5815375447273254, acc.: 50.00%] [G loss: 0.6179354190826416]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 48/346 [D loss: 0.5794205218553543, acc.: 50.00%] [G loss: 0.6161522269248962]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 49/346 [D loss: 0.580568939447403, acc.: 50.00%] [G loss: 0.6128798723220825]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 50/346 [D loss: 0.5787061154842377, acc.: 50.00%] [G loss: 0.612403154373169]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 51/346 [D loss: 0.5786391496658325, acc.: 50.00%] [G loss: 0.6095392107963562]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 52/346 [D loss: 0.5794494152069092, acc.: 50.00%] [G loss: 0.6064377427101135]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 53/346 [D loss: 0.5789458304643631, acc.: 50.00%] [G loss: 0.607170581817627]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 54/346 [D loss: 0.5792729705572128, acc.: 50.00%] [G loss: 0.6043186187744141]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 55/346 [D loss: 0.5785831809043884, acc.: 50.00%] [G loss: 0.6020858883857727]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 56/346 [D loss: 0.5790222138166428, acc.: 50.00%] [G loss: 0.6007180213928223]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 57/346 [D loss: 0.578608363866806, acc.: 50.00%] [G loss: 0.6000114679336548]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 58/346 [D loss: 0.5778614431619644, acc.: 50.00%] [G loss: 0.6010192632675171]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 59/346 [D loss: 0.5772786438465118, acc.: 50.00%] [G loss: 0.6012211441993713]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 60/346 [D loss: 0.5756827145814896, acc.: 50.00%] [G loss: 0.5990740656852722]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 61/346 [D loss: 0.5764826387166977, acc.: 50.00%] [G loss: 0.6009036898612976]\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "Epoch 1/100, Batch 62/346 [D loss: 0.5735994875431061, acc.: 50.00%] [G loss: 0.6005526185035706]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 63/346 [D loss: 0.5738352090120316, acc.: 50.00%] [G loss: 0.6048518419265747]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 64/346 [D loss: 0.5735385417938232, acc.: 50.00%] [G loss: 0.6046829223632812]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 65/346 [D loss: 0.5720493048429489, acc.: 50.00%] [G loss: 0.6069519519805908]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 66/346 [D loss: 0.5699985474348068, acc.: 50.00%] [G loss: 0.6088422536849976]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 67/346 [D loss: 0.5672807693481445, acc.: 50.00%] [G loss: 0.6103189587593079]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 68/346 [D loss: 0.5669141709804535, acc.: 50.00%] [G loss: 0.6135509014129639]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 69/346 [D loss: 0.5657494515180588, acc.: 50.20%] [G loss: 0.6137477159500122]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 70/346 [D loss: 0.5655111968517303, acc.: 50.00%] [G loss: 0.6200523376464844]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 71/346 [D loss: 0.5611425191164017, acc.: 50.20%] [G loss: 0.6261873245239258]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 72/346 [D loss: 0.5560575276613235, acc.: 50.20%] [G loss: 0.6281226873397827]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 73/346 [D loss: 0.5568519681692123, acc.: 50.39%] [G loss: 0.6349813938140869]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 74/346 [D loss: 0.5546883642673492, acc.: 50.39%] [G loss: 0.6385293006896973]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 75/346 [D loss: 0.5471760630607605, acc.: 50.78%] [G loss: 0.6460024118423462]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 76/346 [D loss: 0.5494763404130936, acc.: 50.39%] [G loss: 0.6510952711105347]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 77/346 [D loss: 0.5447825044393539, acc.: 53.32%] [G loss: 0.6580090522766113]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 78/346 [D loss: 0.5393843054771423, acc.: 56.25%] [G loss: 0.6630011796951294]\n",
      "8/8 [==============================] - 0s 17ms/step\n",
      "Epoch 1/100, Batch 79/346 [D loss: 0.5411163866519928, acc.: 57.42%] [G loss: 0.6664038896560669]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 80/346 [D loss: 0.5385975539684296, acc.: 58.98%] [G loss: 0.6737878918647766]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 81/346 [D loss: 0.5313648581504822, acc.: 61.52%] [G loss: 0.6817935705184937]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 82/346 [D loss: 0.5254002213478088, acc.: 69.53%] [G loss: 0.6891192197799683]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 83/346 [D loss: 0.5220378637313843, acc.: 71.29%] [G loss: 0.6985340714454651]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 84/346 [D loss: 0.5186851173639297, acc.: 75.78%] [G loss: 0.7033897638320923]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 85/346 [D loss: 0.5163699388504028, acc.: 78.32%] [G loss: 0.711193323135376]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 86/346 [D loss: 0.5158363580703735, acc.: 83.98%] [G loss: 0.7262083888053894]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 87/346 [D loss: 0.5076218396425247, acc.: 90.82%] [G loss: 0.7272452712059021]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 88/346 [D loss: 0.502982884645462, acc.: 92.19%] [G loss: 0.7399696111679077]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 89/346 [D loss: 0.5049783289432526, acc.: 92.58%] [G loss: 0.7396036386489868]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 90/346 [D loss: 0.4997026026248932, acc.: 95.51%] [G loss: 0.7483789920806885]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 91/346 [D loss: 0.49245284497737885, acc.: 98.44%] [G loss: 0.7587391138076782]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 92/346 [D loss: 0.48775795102119446, acc.: 98.24%] [G loss: 0.7664589881896973]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 93/346 [D loss: 0.48985788226127625, acc.: 98.05%] [G loss: 0.7696318626403809]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 94/346 [D loss: 0.4841756820678711, acc.: 98.44%] [G loss: 0.7780577540397644]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 95/346 [D loss: 0.47939442098140717, acc.: 99.02%] [G loss: 0.7821888327598572]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 96/346 [D loss: 0.47886523604393005, acc.: 98.83%] [G loss: 0.7900088429450989]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 97/346 [D loss: 0.479152113199234, acc.: 99.80%] [G loss: 0.7940188646316528]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 98/346 [D loss: 0.47741593420505524, acc.: 99.80%] [G loss: 0.8026545643806458]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 99/346 [D loss: 0.4676479399204254, acc.: 99.61%] [G loss: 0.8007044792175293]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 100/346 [D loss: 0.47518467903137207, acc.: 99.61%] [G loss: 0.8056657314300537]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 101/346 [D loss: 0.4804431200027466, acc.: 99.61%] [G loss: 0.8044725060462952]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 102/346 [D loss: 0.4703715443611145, acc.: 99.80%] [G loss: 0.8052957057952881]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 103/346 [D loss: 0.4725150465965271, acc.: 100.00%] [G loss: 0.7953022718429565]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 104/346 [D loss: 0.47019752860069275, acc.: 99.80%] [G loss: 0.7963401675224304]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 105/346 [D loss: 0.4704016000032425, acc.: 99.61%] [G loss: 0.7931682467460632]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 106/346 [D loss: 0.46778707206249237, acc.: 100.00%] [G loss: 0.7838951349258423]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 107/346 [D loss: 0.4693545252084732, acc.: 99.80%] [G loss: 0.7732564806938171]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 108/346 [D loss: 0.47724367678165436, acc.: 98.83%] [G loss: 0.7659733295440674]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 109/346 [D loss: 0.48401302099227905, acc.: 99.22%] [G loss: 0.7608340382575989]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 110/346 [D loss: 0.4811909794807434, acc.: 97.85%] [G loss: 0.7506798505783081]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 111/346 [D loss: 0.48431816697120667, acc.: 96.88%] [G loss: 0.7396093606948853]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 112/346 [D loss: 0.49214619398117065, acc.: 94.92%] [G loss: 0.7325102686882019]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 113/346 [D loss: 0.4840923547744751, acc.: 93.16%] [G loss: 0.7232556939125061]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 114/346 [D loss: 0.4928623139858246, acc.: 89.45%] [G loss: 0.7159687876701355]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 115/346 [D loss: 0.49243107438087463, acc.: 82.23%] [G loss: 0.7103100419044495]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 116/346 [D loss: 0.5020847916603088, acc.: 79.30%] [G loss: 0.7014042139053345]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 117/346 [D loss: 0.5035808980464935, acc.: 67.38%] [G loss: 0.6942354440689087]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 118/346 [D loss: 0.49784278869628906, acc.: 61.33%] [G loss: 0.6875896453857422]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 119/346 [D loss: 0.5036254525184631, acc.: 57.23%] [G loss: 0.6774736642837524]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 120/346 [D loss: 0.5110348463058472, acc.: 51.95%] [G loss: 0.667154848575592]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 121/346 [D loss: 0.5119140595197678, acc.: 50.39%] [G loss: 0.6587496995925903]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 122/346 [D loss: 0.5128092616796494, acc.: 50.59%] [G loss: 0.650099515914917]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 123/346 [D loss: 0.5140245258808136, acc.: 50.00%] [G loss: 0.6452212929725647]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 124/346 [D loss: 0.5195486545562744, acc.: 50.00%] [G loss: 0.637143611907959]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 125/346 [D loss: 0.5204708725214005, acc.: 50.00%] [G loss: 0.6306969523429871]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 126/346 [D loss: 0.5221498310565948, acc.: 50.00%] [G loss: 0.6259672045707703]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 127/346 [D loss: 0.5280759036540985, acc.: 50.00%] [G loss: 0.6181399822235107]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 128/346 [D loss: 0.5294611155986786, acc.: 50.00%] [G loss: 0.6159506440162659]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 129/346 [D loss: 0.5238915681838989, acc.: 50.00%] [G loss: 0.6142433881759644]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 130/346 [D loss: 0.5229360014200211, acc.: 50.00%] [G loss: 0.617590069770813]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 131/346 [D loss: 0.5203322172164917, acc.: 50.00%] [G loss: 0.6173790693283081]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 132/346 [D loss: 0.5160297378897667, acc.: 50.00%] [G loss: 0.6223912835121155]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 133/346 [D loss: 0.5202886462211609, acc.: 50.00%] [G loss: 0.6268019080162048]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 134/346 [D loss: 0.5102448612451553, acc.: 50.00%] [G loss: 0.6293226480484009]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 135/346 [D loss: 0.5051314234733582, acc.: 50.00%] [G loss: 0.6371505260467529]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 136/346 [D loss: 0.49878714978694916, acc.: 50.20%] [G loss: 0.645627498626709]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 137/346 [D loss: 0.48895902931690216, acc.: 50.59%] [G loss: 0.6582018136978149]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 138/346 [D loss: 0.48042576014995575, acc.: 53.91%] [G loss: 0.6676168441772461]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 139/346 [D loss: 0.47397331148386, acc.: 57.03%] [G loss: 0.6797264814376831]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 140/346 [D loss: 0.4638785794377327, acc.: 64.84%] [G loss: 0.6931233406066895]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 141/346 [D loss: 0.45964254438877106, acc.: 73.24%] [G loss: 0.7032968401908875]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 142/346 [D loss: 0.4488494023680687, acc.: 83.40%] [G loss: 0.7189584970474243]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 143/346 [D loss: 0.4421764016151428, acc.: 89.84%] [G loss: 0.7315248250961304]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 144/346 [D loss: 0.4292803779244423, acc.: 96.48%] [G loss: 0.7444992065429688]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 145/346 [D loss: 0.42591820657253265, acc.: 98.63%] [G loss: 0.7607362270355225]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 146/346 [D loss: 0.41568052768707275, acc.: 99.61%] [G loss: 0.7735027074813843]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 147/346 [D loss: 0.4075760990381241, acc.: 100.00%] [G loss: 0.7848716974258423]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 148/346 [D loss: 0.4014078229665756, acc.: 99.80%] [G loss: 0.7959296703338623]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 149/346 [D loss: 0.3960427939891815, acc.: 100.00%] [G loss: 0.8048182725906372]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 150/346 [D loss: 0.3896365761756897, acc.: 100.00%] [G loss: 0.8162797689437866]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 151/346 [D loss: 0.3908696919679642, acc.: 100.00%] [G loss: 0.8219001293182373]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 152/346 [D loss: 0.38389313966035843, acc.: 100.00%] [G loss: 0.8298995494842529]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 153/346 [D loss: 0.3788498342037201, acc.: 100.00%] [G loss: 0.8313079476356506]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 154/346 [D loss: 0.37942183017730713, acc.: 100.00%] [G loss: 0.830791711807251]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 155/346 [D loss: 0.3745267391204834, acc.: 100.00%] [G loss: 0.8387255668640137]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 156/346 [D loss: 0.37333789467811584, acc.: 100.00%] [G loss: 0.8403030037879944]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 157/346 [D loss: 0.3700135573744774, acc.: 100.00%] [G loss: 0.8335316181182861]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 158/346 [D loss: 0.3725201338529587, acc.: 100.00%] [G loss: 0.8332449197769165]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 159/346 [D loss: 0.3721505105495453, acc.: 100.00%] [G loss: 0.8258497714996338]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 160/346 [D loss: 0.36783473938703537, acc.: 100.00%] [G loss: 0.825471043586731]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 161/346 [D loss: 0.3713037967681885, acc.: 100.00%] [G loss: 0.8186575174331665]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 162/346 [D loss: 0.3722599446773529, acc.: 100.00%] [G loss: 0.817274808883667]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 163/346 [D loss: 0.37079016864299774, acc.: 100.00%] [G loss: 0.8146463632583618]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 164/346 [D loss: 0.37083158642053604, acc.: 100.00%] [G loss: 0.8159357905387878]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 165/346 [D loss: 0.36603280156850815, acc.: 100.00%] [G loss: 0.8106006383895874]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 166/346 [D loss: 0.36850691586732864, acc.: 100.00%] [G loss: 0.8186247944831848]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 167/346 [D loss: 0.3618754670023918, acc.: 100.00%] [G loss: 0.814791738986969]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 168/346 [D loss: 0.3619636967778206, acc.: 100.00%] [G loss: 0.821090042591095]\n",
      "8/8 [==============================] - 0s 24ms/step\n",
      "Epoch 1/100, Batch 169/346 [D loss: 0.3568103611469269, acc.: 99.80%] [G loss: 0.8240330219268799]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 170/346 [D loss: 0.35311979800462723, acc.: 99.80%] [G loss: 0.8295649290084839]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 171/346 [D loss: 0.35180823132395744, acc.: 100.00%] [G loss: 0.8385576605796814]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 172/346 [D loss: 0.3459411934018135, acc.: 100.00%] [G loss: 0.8443752527236938]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 173/346 [D loss: 0.34288063645362854, acc.: 100.00%] [G loss: 0.8469156622886658]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 174/346 [D loss: 0.33927688747644424, acc.: 100.00%] [G loss: 0.8580073118209839]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 175/346 [D loss: 0.3340330496430397, acc.: 100.00%] [G loss: 0.86112380027771]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 176/346 [D loss: 0.33063753321766853, acc.: 100.00%] [G loss: 0.8699166178703308]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 177/346 [D loss: 0.3274611867964268, acc.: 100.00%] [G loss: 0.8742501735687256]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 178/346 [D loss: 0.32510727643966675, acc.: 100.00%] [G loss: 0.8798298835754395]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 179/346 [D loss: 0.3181118294596672, acc.: 100.00%] [G loss: 0.8839403390884399]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 180/346 [D loss: 0.319391094148159, acc.: 100.00%] [G loss: 0.8866707682609558]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 181/346 [D loss: 0.31832101196050644, acc.: 100.00%] [G loss: 0.8844059705734253]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 182/346 [D loss: 0.3173798583447933, acc.: 100.00%] [G loss: 0.8812111616134644]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 183/346 [D loss: 0.31928904727101326, acc.: 100.00%] [G loss: 0.8759870529174805]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 184/346 [D loss: 0.32057078182697296, acc.: 100.00%] [G loss: 0.8686531186103821]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 185/346 [D loss: 0.31870774179697037, acc.: 100.00%] [G loss: 0.8644452095031738]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 186/346 [D loss: 0.3218306228518486, acc.: 100.00%] [G loss: 0.8536049127578735]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 187/346 [D loss: 0.32560139149427414, acc.: 100.00%] [G loss: 0.8455082774162292]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 188/346 [D loss: 0.32974883541464806, acc.: 100.00%] [G loss: 0.8343570232391357]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 189/346 [D loss: 0.33249523490667343, acc.: 100.00%] [G loss: 0.8200222253799438]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 190/346 [D loss: 0.33787575364112854, acc.: 100.00%] [G loss: 0.8056378960609436]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 191/346 [D loss: 0.34229521825909615, acc.: 100.00%] [G loss: 0.789208173751831]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 192/346 [D loss: 0.3515210226178169, acc.: 99.80%] [G loss: 0.7694565057754517]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 193/346 [D loss: 0.35998472943902016, acc.: 99.22%] [G loss: 0.7478718161582947]\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "Epoch 1/100, Batch 194/346 [D loss: 0.3707348108291626, acc.: 94.73%] [G loss: 0.7238258123397827]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 195/346 [D loss: 0.3782718628644943, acc.: 77.15%] [G loss: 0.7020760178565979]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 196/346 [D loss: 0.38880692049860954, acc.: 57.62%] [G loss: 0.6801292300224304]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 197/346 [D loss: 0.4011051245033741, acc.: 50.20%] [G loss: 0.6567485332489014]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 198/346 [D loss: 0.4099348150193691, acc.: 50.00%] [G loss: 0.6389695405960083]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 199/346 [D loss: 0.4219542220234871, acc.: 50.00%] [G loss: 0.6188861727714539]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 200/346 [D loss: 0.4331474341452122, acc.: 50.00%] [G loss: 0.6002482175827026]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 201/346 [D loss: 0.4441898427903652, acc.: 50.00%] [G loss: 0.5832809209823608]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 202/346 [D loss: 0.45420425944030285, acc.: 50.00%] [G loss: 0.5683443546295166]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 203/346 [D loss: 0.46336615830659866, acc.: 50.00%] [G loss: 0.5554977655410767]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 204/346 [D loss: 0.4704287387430668, acc.: 50.00%] [G loss: 0.5417520999908447]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 205/346 [D loss: 0.47979697212576866, acc.: 50.00%] [G loss: 0.5352119207382202]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 206/346 [D loss: 0.48529015481472015, acc.: 50.00%] [G loss: 0.5247124433517456]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 207/346 [D loss: 0.49014035798609257, acc.: 50.00%] [G loss: 0.518742024898529]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 208/346 [D loss: 0.49615058302879333, acc.: 50.00%] [G loss: 0.511421799659729]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 209/346 [D loss: 0.49880895763635635, acc.: 50.00%] [G loss: 0.504224956035614]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 210/346 [D loss: 0.5009588822722435, acc.: 50.00%] [G loss: 0.5024036169052124]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 211/346 [D loss: 0.5036937147378922, acc.: 50.00%] [G loss: 0.5030558109283447]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 212/346 [D loss: 0.5075321942567825, acc.: 50.00%] [G loss: 0.5007741451263428]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 213/346 [D loss: 0.5082231946289539, acc.: 50.00%] [G loss: 0.49906185269355774]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 214/346 [D loss: 0.5116448514163494, acc.: 50.00%] [G loss: 0.5010155439376831]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 215/346 [D loss: 0.5083913803100586, acc.: 50.00%] [G loss: 0.4981730580329895]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 216/346 [D loss: 0.5084513500332832, acc.: 50.00%] [G loss: 0.49728769063949585]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 217/346 [D loss: 0.5073206499218941, acc.: 50.00%] [G loss: 0.5026645660400391]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 218/346 [D loss: 0.5058939792215824, acc.: 50.00%] [G loss: 0.5060276985168457]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 219/346 [D loss: 0.5045222640037537, acc.: 50.00%] [G loss: 0.5110338926315308]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 220/346 [D loss: 0.5060325711965561, acc.: 50.00%] [G loss: 0.5139131546020508]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 221/346 [D loss: 0.5010344162583351, acc.: 50.00%] [G loss: 0.5167831778526306]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 222/346 [D loss: 0.4954106956720352, acc.: 50.00%] [G loss: 0.5251560211181641]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 223/346 [D loss: 0.49589400738477707, acc.: 50.00%] [G loss: 0.5279445648193359]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 224/346 [D loss: 0.491010345518589, acc.: 50.00%] [G loss: 0.5346770882606506]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 225/346 [D loss: 0.4892496056854725, acc.: 50.00%] [G loss: 0.5446884632110596]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 226/346 [D loss: 0.48212679848074913, acc.: 50.00%] [G loss: 0.5497260093688965]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 227/346 [D loss: 0.4788189008831978, acc.: 50.00%] [G loss: 0.55808424949646]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 228/346 [D loss: 0.4764462783932686, acc.: 50.00%] [G loss: 0.5684194564819336]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 229/346 [D loss: 0.4698783606290817, acc.: 50.00%] [G loss: 0.5755022764205933]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 230/346 [D loss: 0.46648940443992615, acc.: 50.00%] [G loss: 0.5854021310806274]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 231/346 [D loss: 0.46172134578227997, acc.: 50.00%] [G loss: 0.5932950973510742]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 232/346 [D loss: 0.45669548213481903, acc.: 50.20%] [G loss: 0.6038268208503723]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 1/100, Batch 233/346 [D loss: 0.4509356878697872, acc.: 50.20%] [G loss: 0.6124330759048462]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 234/346 [D loss: 0.44724801182746887, acc.: 50.20%] [G loss: 0.6227010488510132]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 235/346 [D loss: 0.4430004954338074, acc.: 51.17%] [G loss: 0.6322781443595886]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 236/346 [D loss: 0.4413401260972023, acc.: 51.17%] [G loss: 0.64360511302948]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 237/346 [D loss: 0.4345667213201523, acc.: 54.30%] [G loss: 0.652584433555603]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 238/346 [D loss: 0.43086110055446625, acc.: 56.05%] [G loss: 0.6630795001983643]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 239/346 [D loss: 0.42937036231160164, acc.: 57.23%] [G loss: 0.6719380021095276]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 240/346 [D loss: 0.42388980835676193, acc.: 62.50%] [G loss: 0.6829307675361633]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 241/346 [D loss: 0.418090358376503, acc.: 66.41%] [G loss: 0.6940439343452454]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 242/346 [D loss: 0.41138362884521484, acc.: 74.22%] [G loss: 0.7038633823394775]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 243/346 [D loss: 0.4136973023414612, acc.: 73.24%] [G loss: 0.713468611240387]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 244/346 [D loss: 0.40955543518066406, acc.: 78.71%] [G loss: 0.7191026210784912]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 245/346 [D loss: 0.4099651798605919, acc.: 81.05%] [G loss: 0.7297793626785278]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 246/346 [D loss: 0.4082532748579979, acc.: 85.74%] [G loss: 0.7354725003242493]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 247/346 [D loss: 0.4022318571805954, acc.: 89.45%] [G loss: 0.7422096729278564]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 248/346 [D loss: 0.4016343802213669, acc.: 90.23%] [G loss: 0.7533814907073975]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 249/346 [D loss: 0.405318446457386, acc.: 90.82%] [G loss: 0.7539737820625305]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 250/346 [D loss: 0.40224095433950424, acc.: 93.36%] [G loss: 0.7674486637115479]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 251/346 [D loss: 0.4018319249153137, acc.: 93.36%] [G loss: 0.7752074003219604]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 252/346 [D loss: 0.4019251689314842, acc.: 93.95%] [G loss: 0.7755164504051208]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 253/346 [D loss: 0.3985932767391205, acc.: 95.31%] [G loss: 0.7867618799209595]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 254/346 [D loss: 0.3971063941717148, acc.: 96.29%] [G loss: 0.7947396039962769]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 255/346 [D loss: 0.3986513763666153, acc.: 95.90%] [G loss: 0.8110837936401367]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 256/346 [D loss: 0.3978417366743088, acc.: 99.02%] [G loss: 0.8178822994232178]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 257/346 [D loss: 0.39519935846328735, acc.: 98.63%] [G loss: 0.8237037658691406]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 258/346 [D loss: 0.39163506031036377, acc.: 98.83%] [G loss: 0.839925229549408]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 259/346 [D loss: 0.39220863580703735, acc.: 98.83%] [G loss: 0.8554288148880005]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 260/346 [D loss: 0.39353659003973007, acc.: 99.02%] [G loss: 0.8478177785873413]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 261/346 [D loss: 0.38867923617362976, acc.: 99.61%] [G loss: 0.8631250858306885]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 262/346 [D loss: 0.3893759176135063, acc.: 99.22%] [G loss: 0.877280592918396]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 263/346 [D loss: 0.3886980265378952, acc.: 99.41%] [G loss: 0.8908665776252747]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 264/346 [D loss: 0.3910425081849098, acc.: 100.00%] [G loss: 0.8934837579727173]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 265/346 [D loss: 0.3860125467181206, acc.: 100.00%] [G loss: 0.9044342041015625]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 266/346 [D loss: 0.39095883816480637, acc.: 99.61%] [G loss: 0.9036257266998291]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 267/346 [D loss: 0.381853848695755, acc.: 99.61%] [G loss: 0.9142171144485474]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 268/346 [D loss: 0.38792183995246887, acc.: 100.00%] [G loss: 0.9202703237533569]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 1/100, Batch 269/346 [D loss: 0.39002618193626404, acc.: 100.00%] [G loss: 0.930435836315155]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 270/346 [D loss: 0.3826640844345093, acc.: 99.80%] [G loss: 0.929533839225769]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 271/346 [D loss: 0.3866681158542633, acc.: 100.00%] [G loss: 0.9393880367279053]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 272/346 [D loss: 0.38581472635269165, acc.: 100.00%] [G loss: 0.9475780129432678]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 273/346 [D loss: 0.3823460787534714, acc.: 99.61%] [G loss: 0.9517433643341064]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 274/346 [D loss: 0.38305985927581787, acc.: 100.00%] [G loss: 0.9426826238632202]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 275/346 [D loss: 0.3850152939558029, acc.: 99.61%] [G loss: 0.9536234140396118]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 276/346 [D loss: 0.3883233964443207, acc.: 100.00%] [G loss: 0.9529615640640259]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 277/346 [D loss: 0.391658216714859, acc.: 99.61%] [G loss: 0.9509097933769226]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 278/346 [D loss: 0.38685861229896545, acc.: 100.00%] [G loss: 0.943764328956604]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 279/346 [D loss: 0.39928390085697174, acc.: 99.61%] [G loss: 0.9408056735992432]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 280/346 [D loss: 0.39203251898288727, acc.: 99.80%] [G loss: 0.9375133514404297]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 281/346 [D loss: 0.3998493254184723, acc.: 99.61%] [G loss: 0.9305242300033569]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 282/346 [D loss: 0.40374743938446045, acc.: 99.61%] [G loss: 0.9235787391662598]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 283/346 [D loss: 0.4004087746143341, acc.: 99.80%] [G loss: 0.9105192422866821]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 1/100, Batch 284/346 [D loss: 0.398767352104187, acc.: 99.61%] [G loss: 0.9005969762802124]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 285/346 [D loss: 0.40739665925502777, acc.: 99.61%] [G loss: 0.8910679817199707]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 286/346 [D loss: 0.4191192239522934, acc.: 99.22%] [G loss: 0.8680388331413269]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 287/346 [D loss: 0.41843077540397644, acc.: 98.83%] [G loss: 0.8676626682281494]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 288/346 [D loss: 0.4218442142009735, acc.: 99.61%] [G loss: 0.8519307971000671]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 289/346 [D loss: 0.44143158197402954, acc.: 98.44%] [G loss: 0.8270430564880371]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 290/346 [D loss: 0.43612486124038696, acc.: 97.85%] [G loss: 0.8242403864860535]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 291/346 [D loss: 0.4455125331878662, acc.: 96.29%] [G loss: 0.7997031211853027]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 292/346 [D loss: 0.4442891776561737, acc.: 98.05%] [G loss: 0.7891350984573364]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 293/346 [D loss: 0.44900915026664734, acc.: 93.75%] [G loss: 0.7663540244102478]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 294/346 [D loss: 0.45213522017002106, acc.: 93.75%] [G loss: 0.7590478658676147]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 295/346 [D loss: 0.4610895812511444, acc.: 90.23%] [G loss: 0.7480982542037964]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 296/346 [D loss: 0.4658616930246353, acc.: 88.67%] [G loss: 0.7354140281677246]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 297/346 [D loss: 0.4692278951406479, acc.: 85.94%] [G loss: 0.729180097579956]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 298/346 [D loss: 0.46613097190856934, acc.: 85.35%] [G loss: 0.719523549079895]\n",
      "8/8 [==============================] - 0s 24ms/step\n",
      "Epoch 1/100, Batch 299/346 [D loss: 0.47017961740493774, acc.: 78.71%] [G loss: 0.7187992930412292]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 300/346 [D loss: 0.47096721827983856, acc.: 79.88%] [G loss: 0.7131701111793518]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 301/346 [D loss: 0.4765036702156067, acc.: 77.73%] [G loss: 0.7136852741241455]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 302/346 [D loss: 0.4702669382095337, acc.: 78.32%] [G loss: 0.7073354721069336]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 303/346 [D loss: 0.4709320068359375, acc.: 77.15%] [G loss: 0.7073636054992676]\n",
      "8/8 [==============================] - 0s 23ms/step\n",
      "Epoch 1/100, Batch 304/346 [D loss: 0.473814994096756, acc.: 76.95%] [G loss: 0.703334629535675]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 305/346 [D loss: 0.4670928120613098, acc.: 77.93%] [G loss: 0.7043313980102539]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 306/346 [D loss: 0.45891133695840836, acc.: 76.56%] [G loss: 0.7101155519485474]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 307/346 [D loss: 0.46506601572036743, acc.: 77.73%] [G loss: 0.7064676284790039]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 308/346 [D loss: 0.46176137030124664, acc.: 78.71%] [G loss: 0.7107094526290894]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 309/346 [D loss: 0.46223263442516327, acc.: 79.30%] [G loss: 0.7133171558380127]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 310/346 [D loss: 0.4563727602362633, acc.: 78.91%] [G loss: 0.7180285453796387]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 311/346 [D loss: 0.45856714248657227, acc.: 81.25%] [G loss: 0.7137365341186523]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 312/346 [D loss: 0.4512156844139099, acc.: 83.98%] [G loss: 0.7153776288032532]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 313/346 [D loss: 0.4537908360362053, acc.: 81.05%] [G loss: 0.7148252725601196]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 314/346 [D loss: 0.44374930113554, acc.: 85.16%] [G loss: 0.7102760672569275]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 315/346 [D loss: 0.4448845833539963, acc.: 82.81%] [G loss: 0.7149456739425659]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 316/346 [D loss: 0.4433864653110504, acc.: 83.98%] [G loss: 0.7170283794403076]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 317/346 [D loss: 0.44007353484630585, acc.: 82.81%] [G loss: 0.7190403938293457]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 318/346 [D loss: 0.4343516230583191, acc.: 83.40%] [G loss: 0.7182065844535828]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 319/346 [D loss: 0.44211696088314056, acc.: 84.96%] [G loss: 0.7155923843383789]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 320/346 [D loss: 0.4419840797781944, acc.: 85.16%] [G loss: 0.7157884836196899]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 321/346 [D loss: 0.4389524459838867, acc.: 83.01%] [G loss: 0.7197672128677368]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 322/346 [D loss: 0.43729346990585327, acc.: 83.01%] [G loss: 0.7126361131668091]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 323/346 [D loss: 0.44467969238758087, acc.: 79.69%] [G loss: 0.7085322141647339]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 324/346 [D loss: 0.440715990960598, acc.: 80.08%] [G loss: 0.7119799852371216]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 325/346 [D loss: 0.4533274173736572, acc.: 80.08%] [G loss: 0.707927942276001]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 326/346 [D loss: 0.43991146981716156, acc.: 80.47%] [G loss: 0.7044267654418945]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 327/346 [D loss: 0.4454363286495209, acc.: 73.24%] [G loss: 0.7018356323242188]\n",
      "8/8 [==============================] - 0s 23ms/step\n",
      "Epoch 1/100, Batch 328/346 [D loss: 0.4423394799232483, acc.: 77.73%] [G loss: 0.700851559638977]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 329/346 [D loss: 0.43897902965545654, acc.: 74.02%] [G loss: 0.6974186301231384]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 330/346 [D loss: 0.43757594376802444, acc.: 74.02%] [G loss: 0.6969079971313477]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 331/346 [D loss: 0.4446352422237396, acc.: 70.51%] [G loss: 0.6928274035453796]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 332/346 [D loss: 0.440364345908165, acc.: 67.97%] [G loss: 0.686435341835022]\n",
      "8/8 [==============================] - 0s 24ms/step\n",
      "Epoch 1/100, Batch 333/346 [D loss: 0.45121708512306213, acc.: 65.82%] [G loss: 0.6802879571914673]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 1/100, Batch 334/346 [D loss: 0.44226282089948654, acc.: 63.67%] [G loss: 0.6806036233901978]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 335/346 [D loss: 0.4513196721673012, acc.: 62.30%] [G loss: 0.6728920936584473]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 336/346 [D loss: 0.4513092041015625, acc.: 59.96%] [G loss: 0.6693910360336304]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 337/346 [D loss: 0.4499475657939911, acc.: 57.03%] [G loss: 0.6658220291137695]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 1/100, Batch 338/346 [D loss: 0.4507106766104698, acc.: 56.64%] [G loss: 0.657333493232727]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 339/346 [D loss: 0.45996271073818207, acc.: 55.47%] [G loss: 0.655903697013855]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 340/346 [D loss: 0.4571833908557892, acc.: 52.93%] [G loss: 0.6458569169044495]\n",
      "8/8 [==============================] - 0s 23ms/step\n",
      "Epoch 1/100, Batch 341/346 [D loss: 0.4556170105934143, acc.: 51.95%] [G loss: 0.6409896612167358]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 1/100, Batch 342/346 [D loss: 0.46040989458560944, acc.: 52.15%] [G loss: 0.6349199414253235]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 1/100, Batch 343/346 [D loss: 0.465938962996006, acc.: 50.59%] [G loss: 0.6331045627593994]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 344/346 [D loss: 0.46841923147439957, acc.: 50.20%] [G loss: 0.6245002150535583]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 345/346 [D loss: 0.47429487109184265, acc.: 50.20%] [G loss: 0.6178890466690063]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 1/100, Batch 346/346 [D loss: 0.4673277586698532, acc.: 50.00%] [G loss: 0.6156079769134521]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 1/346 [D loss: 0.46705929189920425, acc.: 50.00%] [G loss: 0.6079497337341309]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 2/100, Batch 2/346 [D loss: 0.47507695108652115, acc.: 50.00%] [G loss: 0.5997539162635803]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 3/346 [D loss: 0.4805435985326767, acc.: 50.00%] [G loss: 0.5974749326705933]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 2/100, Batch 4/346 [D loss: 0.4845791161060333, acc.: 50.00%] [G loss: 0.5924737453460693]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 2/100, Batch 5/346 [D loss: 0.4817345440387726, acc.: 50.00%] [G loss: 0.5878190994262695]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 6/346 [D loss: 0.4849569499492645, acc.: 50.00%] [G loss: 0.5810633897781372]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 7/346 [D loss: 0.489297479391098, acc.: 50.00%] [G loss: 0.575469970703125]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 8/346 [D loss: 0.4968497008085251, acc.: 50.00%] [G loss: 0.5706515908241272]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 9/346 [D loss: 0.4958053007721901, acc.: 50.00%] [G loss: 0.5680086016654968]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 2/100, Batch 10/346 [D loss: 0.4983629956841469, acc.: 50.00%] [G loss: 0.5643978118896484]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 2/100, Batch 11/346 [D loss: 0.49983933568000793, acc.: 50.00%] [G loss: 0.5572305917739868]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 12/346 [D loss: 0.5075728893280029, acc.: 50.00%] [G loss: 0.5529963374137878]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 13/346 [D loss: 0.5034070909023285, acc.: 50.00%] [G loss: 0.5509651899337769]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 14/346 [D loss: 0.5082413107156754, acc.: 49.80%] [G loss: 0.5498311519622803]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 15/346 [D loss: 0.5109565258026123, acc.: 50.00%] [G loss: 0.5450207591056824]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 16/346 [D loss: 0.5081539303064346, acc.: 50.00%] [G loss: 0.5412285923957825]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 2/100, Batch 17/346 [D loss: 0.5125242546200752, acc.: 50.00%] [G loss: 0.5361286401748657]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 2/100, Batch 18/346 [D loss: 0.5106310471892357, acc.: 50.00%] [G loss: 0.5351695418357849]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 2/100, Batch 19/346 [D loss: 0.5127709656953812, acc.: 50.00%] [G loss: 0.5352948904037476]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 2/100, Batch 20/346 [D loss: 0.5153435319662094, acc.: 50.00%] [G loss: 0.5308729410171509]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 21/346 [D loss: 0.5130918622016907, acc.: 50.00%] [G loss: 0.5321358442306519]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 2/100, Batch 22/346 [D loss: 0.5207491889595985, acc.: 50.00%] [G loss: 0.5302975177764893]\n",
      "8/8 [==============================] - 0s 24ms/step\n",
      "Epoch 2/100, Batch 23/346 [D loss: 0.5202027708292007, acc.: 50.00%] [G loss: 0.5299582481384277]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 2/100, Batch 24/346 [D loss: 0.5195982530713081, acc.: 50.00%] [G loss: 0.5275174379348755]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 2/100, Batch 25/346 [D loss: 0.5188298374414444, acc.: 50.00%] [G loss: 0.5291612148284912]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 26/346 [D loss: 0.5217326283454895, acc.: 50.00%] [G loss: 0.5226070880889893]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 27/346 [D loss: 0.5205325931310654, acc.: 50.00%] [G loss: 0.5254952311515808]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 2/100, Batch 28/346 [D loss: 0.5243169888854027, acc.: 50.00%] [G loss: 0.5270193815231323]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 2/100, Batch 29/346 [D loss: 0.5207255706191063, acc.: 50.00%] [G loss: 0.5268294811248779]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 2/100, Batch 30/346 [D loss: 0.5271493121981621, acc.: 50.00%] [G loss: 0.5243685245513916]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 2/100, Batch 31/346 [D loss: 0.5310676321387291, acc.: 50.00%] [G loss: 0.5234217643737793]\n",
      "8/8 [==============================] - 0s 21ms/step\n",
      "Epoch 2/100, Batch 32/346 [D loss: 0.5204346999526024, acc.: 50.00%] [G loss: 0.5251177549362183]\n",
      "8/8 [==============================] - 0s 23ms/step\n",
      "Epoch 2/100, Batch 33/346 [D loss: 0.5217080265283585, acc.: 50.00%] [G loss: 0.5226116180419922]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 2/100, Batch 34/346 [D loss: 0.5263611227273941, acc.: 50.00%] [G loss: 0.5249735116958618]\n",
      "8/8 [==============================] - 0s 18ms/step\n",
      "Epoch 2/100, Batch 35/346 [D loss: 0.528303325176239, acc.: 50.00%] [G loss: 0.5269954800605774]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 2/100, Batch 36/346 [D loss: 0.5266172885894775, acc.: 50.00%] [G loss: 0.5233011841773987]\n",
      "8/8 [==============================] - 0s 19ms/step\n",
      "Epoch 2/100, Batch 37/346 [D loss: 0.5245263129472733, acc.: 50.00%] [G loss: 0.5253033638000488]\n",
      "8/8 [==============================] - 0s 20ms/step\n",
      "Epoch 2/100, Batch 38/346 [D loss: 0.5271820724010468, acc.: 50.00%] [G loss: 0.5249777436256409]\n",
      "8/8 [==============================] - 0s 22ms/step\n",
      "Epoch 2/100, Batch 39/346 [D loss: 0.5281146094202995, acc.: 50.00%] [G loss: 0.5209941864013672]\n",
      "8/8 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate and train the DCGAN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m cgan \u001b[38;5;241m=\u001b[39m CGAN(img_rows, img_cols, channels)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 102\u001b[0m, in \u001b[0;36mCGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval, gen_steps)\u001b[0m\n\u001b[0;32m    100\u001b[0m     gen_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, (batch_size, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Ensure valid range\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     valid_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 102\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Print the progress\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatches_per_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc.: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39md_loss[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%] [G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rejey Ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2381\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2379\u001b[0m     )\n\u001b[0;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2381\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2383\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\Rejey Ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Rejey Ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Rejey Ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Rejey Ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rejey Ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Rejey Ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Rejey Ezekiel\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "cgan = CGAN(img_rows, img_cols, channels)\n",
    "cgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
