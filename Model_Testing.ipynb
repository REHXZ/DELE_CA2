{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, Conv2DTranspose, PReLU\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Concatenate\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, HTML\n",
    "import glob\n",
    "from keras.layers import AveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Add\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import Input\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install scikit-image\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Reshape, UpSampling2D, \\\n",
    "    BatchNormalization, Activation, Input, LeakyReLU, ZeroPadding2D, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "#import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('emnist-letters-train.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51:24 Harry Potter Deathy Hallows 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {1: 0, \n",
    "           2: 1, \n",
    "           3: 2, \n",
    "           4: 3, \n",
    "           5: 4, \n",
    "           6: 5, \n",
    "           7: 6, \n",
    "           8: 7, \n",
    "           9: 8, \n",
    "           10: 9, \n",
    "           11: 10, \n",
    "           12: 11, \n",
    "           13: 12, \n",
    "           14: 13, \n",
    "           15: 14, \n",
    "           16: 15, \n",
    "           17: 16, \n",
    "           18: 17, \n",
    "           19: 18, \n",
    "           20: 19, \n",
    "           21: 20, \n",
    "           22: 21, \n",
    "           23: 22, \n",
    "           24: 23, \n",
    "           25: 24, \n",
    "           26: 25, \n",
    "           27: 26}\n",
    "\n",
    "        # Map the labels column to its corresponding value\n",
    "df[0] = df[0].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = np.array(df.iloc[:,0].values)\n",
    "y_pre = pd.Categorical(y_pre)\n",
    "X = np.array(df.iloc[:,1:].values)\n",
    "X = X.reshape(-1,28,28,1)\n",
    "preprocessed = []\n",
    "for image in X:\n",
    "    rotated_image = rotate(image, 90, reshape=False)\n",
    "    flipped_image = np.flipud(rotated_image)\n",
    "    preprocessed.append(flipped_image)\n",
    "X_pre = np.array(preprocessed)\n",
    "X = X_pre\n",
    "X = X.astype('float32')\n",
    "X_pre = (X - 127.5) / 127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pre\n",
      "[22, 6, 15, 14, 16, ..., 19, 8, 5, 11, 0]\n",
      "Length: 26\n",
      "Categories (26, int64): [0, 1, 2, 3, ..., 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f'y_pre\\n{y_pre.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = Model([z, label], valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = in_shape[0] * in_shape[1]\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        merge = Concatenate()([in_image, li])\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        fe = Dropout(0.4)(fe)\n",
    "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "        model = Model([in_image, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 128 * 7 * 7\n",
    "        gen = Dense(n_nodes)(in_lat)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Reshape((7, 7, 128))(gen) \n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(merge)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes  # Ensure labels are within valid range\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"CGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('CGAN_mnist', exist_ok=True)\n",
    "        fig.savefig(\"CGAN_mnist/CGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=1024, save_interval=1, gen_steps=3):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, y_train[idx]], labels_real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, gen_labels], labels_fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([noise, gen_labels], valid_y)\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "            if (epoch) % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 14ms/step\n",
      "Epoch 1/200, Batch 1/86 [D loss: 0.6843744516372681, acc.: 36.13%] [G loss: 0.6843607425689697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 2/86 [D loss: 0.6472448408603668, acc.: 49.95%] [G loss: 0.6532118320465088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 3/86 [D loss: 0.6312700659036636, acc.: 50.00%] [G loss: 0.5709606409072876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 4/86 [D loss: 0.6696633696556091, acc.: 50.00%] [G loss: 0.4677077531814575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 5/86 [D loss: 0.7232813537120819, acc.: 50.00%] [G loss: 0.4666253328323364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 6/86 [D loss: 0.6931503415107727, acc.: 50.00%] [G loss: 0.5488387942314148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 7/86 [D loss: 0.6357947289943695, acc.: 50.00%] [G loss: 0.6639143228530884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 8/86 [D loss: 0.5850180983543396, acc.: 65.97%] [G loss: 0.7854741811752319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 9/86 [D loss: 0.5425160527229309, acc.: 98.83%] [G loss: 0.9078989028930664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 10/86 [D loss: 0.5038425028324127, acc.: 97.66%] [G loss: 1.0349065065383911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 11/86 [D loss: 0.4597938060760498, acc.: 95.80%] [G loss: 1.1641602516174316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 12/86 [D loss: 0.4226006269454956, acc.: 95.17%] [G loss: 1.291378378868103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 13/86 [D loss: 0.37975287437438965, acc.: 94.63%] [G loss: 1.4101618528366089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 14/86 [D loss: 0.3441447764635086, acc.: 95.07%] [G loss: 1.5246968269348145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 15/86 [D loss: 0.3032737225294113, acc.: 95.61%] [G loss: 1.6412315368652344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 16/86 [D loss: 0.27400603890419006, acc.: 95.75%] [G loss: 1.7542510032653809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 17/86 [D loss: 0.23742827773094177, acc.: 96.83%] [G loss: 1.8869426250457764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 18/86 [D loss: 0.2055785059928894, acc.: 97.46%] [G loss: 2.0132811069488525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 19/86 [D loss: 0.17868835479021072, acc.: 97.85%] [G loss: 2.1379661560058594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 20/86 [D loss: 0.16592943668365479, acc.: 97.80%] [G loss: 2.247358798980713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 21/86 [D loss: 0.14511944353580475, acc.: 97.90%] [G loss: 2.3616209030151367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 22/86 [D loss: 0.12883297726511955, acc.: 98.24%] [G loss: 2.4597182273864746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 23/86 [D loss: 0.11740795150399208, acc.: 98.39%] [G loss: 2.5688869953155518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 24/86 [D loss: 0.10012057423591614, acc.: 98.97%] [G loss: 2.6741292476654053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 25/86 [D loss: 0.09996860474348068, acc.: 98.68%] [G loss: 2.7626945972442627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 26/86 [D loss: 0.08389072120189667, acc.: 98.83%] [G loss: 2.8687360286712646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 27/86 [D loss: 0.08171207830309868, acc.: 98.93%] [G loss: 2.9271657466888428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 28/86 [D loss: 0.07350500300526619, acc.: 99.12%] [G loss: 3.0070440769195557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 29/86 [D loss: 0.07256411015987396, acc.: 98.78%] [G loss: 3.0514919757843018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 30/86 [D loss: 0.0678798258304596, acc.: 99.02%] [G loss: 3.117177963256836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 31/86 [D loss: 0.05459318868815899, acc.: 99.46%] [G loss: 3.2003307342529297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 32/86 [D loss: 0.05900950729846954, acc.: 99.22%] [G loss: 3.268747091293335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 33/86 [D loss: 0.04969479702413082, acc.: 99.46%] [G loss: 3.346803903579712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 34/86 [D loss: 0.04414178989827633, acc.: 99.46%] [G loss: 3.4336462020874023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 35/86 [D loss: 0.04308951087296009, acc.: 99.37%] [G loss: 3.484795093536377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 36/86 [D loss: 0.04095294140279293, acc.: 99.32%] [G loss: 3.5616447925567627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 37/86 [D loss: 0.0387207567691803, acc.: 99.61%] [G loss: 3.6294283866882324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 38/86 [D loss: 0.036865889094769955, acc.: 99.22%] [G loss: 3.682116746902466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 39/86 [D loss: 0.03432478755712509, acc.: 99.61%] [G loss: 3.7498106956481934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 40/86 [D loss: 0.030515982769429684, acc.: 99.85%] [G loss: 3.8097898960113525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 41/86 [D loss: 0.030354971066117287, acc.: 99.56%] [G loss: 3.8426096439361572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 42/86 [D loss: 0.025042014196515083, acc.: 99.80%] [G loss: 3.931689739227295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 43/86 [D loss: 0.027507374063134193, acc.: 99.71%] [G loss: 3.9825072288513184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 44/86 [D loss: 0.026540763676166534, acc.: 99.51%] [G loss: 4.022153377532959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 45/86 [D loss: 0.0245148204267025, acc.: 99.51%] [G loss: 4.064769744873047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 46/86 [D loss: 0.02213070821017027, acc.: 99.76%] [G loss: 4.132532596588135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 47/86 [D loss: 0.022009918466210365, acc.: 99.66%] [G loss: 4.153740406036377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 48/86 [D loss: 0.020495052449405193, acc.: 99.76%] [G loss: 4.197888374328613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 49/86 [D loss: 0.023612369783222675, acc.: 99.61%] [G loss: 4.22275447845459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 50/86 [D loss: 0.019400100223720074, acc.: 99.61%] [G loss: 4.271001815795898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 51/86 [D loss: 0.021695095114409924, acc.: 99.80%] [G loss: 4.277644634246826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 52/86 [D loss: 0.016008672770112753, acc.: 99.90%] [G loss: 4.336175918579102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 53/86 [D loss: 0.016430001240223646, acc.: 99.90%] [G loss: 4.384870529174805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 54/86 [D loss: 0.016663344111293554, acc.: 99.85%] [G loss: 4.4175519943237305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 55/86 [D loss: 0.01690288633108139, acc.: 99.76%] [G loss: 4.46785831451416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 56/86 [D loss: 0.014909102581441402, acc.: 99.76%] [G loss: 4.521358013153076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 57/86 [D loss: 0.013350313995033503, acc.: 99.85%] [G loss: 4.580972194671631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 58/86 [D loss: 0.014981928747147322, acc.: 99.85%] [G loss: 4.586328029632568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 59/86 [D loss: 0.014197521843016148, acc.: 99.85%] [G loss: 4.641758441925049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 60/86 [D loss: 0.011466732248663902, acc.: 99.80%] [G loss: 4.700331211090088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 61/86 [D loss: 0.010636062826961279, acc.: 99.95%] [G loss: 4.760668754577637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 62/86 [D loss: 0.012714382726699114, acc.: 99.76%] [G loss: 4.761786460876465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 63/86 [D loss: 0.010428211651742458, acc.: 99.90%] [G loss: 4.815974235534668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 64/86 [D loss: 0.011236957274377346, acc.: 99.80%] [G loss: 4.862083435058594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 65/86 [D loss: 0.009809961076825857, acc.: 99.85%] [G loss: 4.8758440017700195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 66/86 [D loss: 0.009422156028449535, acc.: 99.90%] [G loss: 4.935000419616699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 67/86 [D loss: 0.008270870428532362, acc.: 100.00%] [G loss: 5.009754180908203]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 1/200, Batch 68/86 [D loss: 0.007164902985095978, acc.: 99.90%] [G loss: 5.065746784210205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 69/86 [D loss: 0.009095206391066313, acc.: 99.85%] [G loss: 5.048742294311523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 70/86 [D loss: 0.01050555519759655, acc.: 99.85%] [G loss: 5.022345542907715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 71/86 [D loss: 0.00733816740103066, acc.: 99.95%] [G loss: 5.105802536010742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 72/86 [D loss: 0.007994797313585877, acc.: 99.90%] [G loss: 5.138138771057129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 73/86 [D loss: 0.00818606885150075, acc.: 99.90%] [G loss: 5.195821762084961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 74/86 [D loss: 0.007799332495778799, acc.: 99.95%] [G loss: 5.204370021820068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 75/86 [D loss: 0.0072398202028125525, acc.: 99.90%] [G loss: 5.218662261962891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 76/86 [D loss: 0.0063089788891375065, acc.: 99.95%] [G loss: 5.254570960998535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 77/86 [D loss: 0.004968453664332628, acc.: 100.00%] [G loss: 5.3659868240356445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 78/86 [D loss: 0.007733698934316635, acc.: 99.80%] [G loss: 5.296210289001465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 79/86 [D loss: 0.00751259783282876, acc.: 99.90%] [G loss: 5.339451789855957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 80/86 [D loss: 0.006476095179095864, acc.: 100.00%] [G loss: 5.345212936401367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 81/86 [D loss: 0.00693338830024004, acc.: 99.95%] [G loss: 5.372895240783691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 82/86 [D loss: 0.008449672721326351, acc.: 99.80%] [G loss: 5.3966383934021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 83/86 [D loss: 0.006916114827618003, acc.: 99.90%] [G loss: 5.411507606506348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 84/86 [D loss: 0.005672375904396176, acc.: 99.95%] [G loss: 5.460792541503906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 1/200, Batch 85/86 [D loss: 0.0064325216226279736, acc.: 99.90%] [G loss: 5.4629106521606445]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 1/200, Batch 86/86 [D loss: 0.00438577588647604, acc.: 100.00%] [G loss: 5.556465148925781]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 1/86 [D loss: 0.006322537548840046, acc.: 99.90%] [G loss: 5.526697158813477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 2/86 [D loss: 0.0066340044140815735, acc.: 99.85%] [G loss: 5.520747184753418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 3/86 [D loss: 0.0050969182047992945, acc.: 99.90%] [G loss: 5.59625768661499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 4/86 [D loss: 0.00407660286873579, acc.: 99.95%] [G loss: 5.675650596618652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 5/86 [D loss: 0.004202851559966803, acc.: 99.95%] [G loss: 5.73507022857666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 6/86 [D loss: 0.006527571473270655, acc.: 99.85%] [G loss: 5.676181793212891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 7/86 [D loss: 0.005147137213498354, acc.: 99.95%] [G loss: 5.693634033203125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 8/86 [D loss: 0.00507733877748251, acc.: 99.95%] [G loss: 5.708062171936035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 9/86 [D loss: 0.00424381229095161, acc.: 100.00%] [G loss: 5.752164840698242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 10/86 [D loss: 0.004648477770388126, acc.: 100.00%] [G loss: 5.767320156097412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 11/86 [D loss: 0.005467241397127509, acc.: 99.85%] [G loss: 5.76779317855835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 12/86 [D loss: 0.005303374491631985, acc.: 99.95%] [G loss: 5.740049362182617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 13/86 [D loss: 0.004757708637043834, acc.: 99.95%] [G loss: 5.775241374969482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 14/86 [D loss: 0.003666760283522308, acc.: 99.95%] [G loss: 5.8300957679748535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 15/86 [D loss: 0.003779844380915165, acc.: 100.00%] [G loss: 5.894669532775879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 16/86 [D loss: 0.004488426726311445, acc.: 99.95%] [G loss: 5.9150190353393555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 17/86 [D loss: 0.0039269214030355215, acc.: 99.90%] [G loss: 5.936250686645508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 18/86 [D loss: 0.003482108237221837, acc.: 100.00%] [G loss: 5.9561285972595215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 19/86 [D loss: 0.0030260594794526696, acc.: 100.00%] [G loss: 5.99251127243042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 20/86 [D loss: 0.0035678547574207187, acc.: 100.00%] [G loss: 6.013273239135742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 21/86 [D loss: 0.00447773071937263, acc.: 99.95%] [G loss: 5.995145797729492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 22/86 [D loss: 0.004252942744642496, acc.: 100.00%] [G loss: 5.9269866943359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 23/86 [D loss: 0.002661807229742408, acc.: 100.00%] [G loss: 6.042697906494141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 24/86 [D loss: 0.0030424357391893864, acc.: 99.95%] [G loss: 6.10184907913208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 25/86 [D loss: 0.003137686289846897, acc.: 100.00%] [G loss: 6.120270729064941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 26/86 [D loss: 0.003004079684615135, acc.: 100.00%] [G loss: 6.1313252449035645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 27/86 [D loss: 0.0025827428326010704, acc.: 100.00%] [G loss: 6.171393394470215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 28/86 [D loss: 0.0027477353578433394, acc.: 99.95%] [G loss: 6.186715126037598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 29/86 [D loss: 0.0029498660005629063, acc.: 100.00%] [G loss: 6.241786956787109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 30/86 [D loss: 0.0028793533565476537, acc.: 99.95%] [G loss: 6.225800037384033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 31/86 [D loss: 0.0023078357335180044, acc.: 100.00%] [G loss: 6.258840084075928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 32/86 [D loss: 0.0023420125944539905, acc.: 100.00%] [G loss: 6.318511486053467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 33/86 [D loss: 0.0026289602974429727, acc.: 100.00%] [G loss: 6.309065818786621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 34/86 [D loss: 0.0026938183000311255, acc.: 99.95%] [G loss: 6.315834045410156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 35/86 [D loss: 0.002395699848420918, acc.: 99.95%] [G loss: 6.363033771514893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 36/86 [D loss: 0.0027207332896068692, acc.: 100.00%] [G loss: 6.347765922546387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 37/86 [D loss: 0.0016703151050023735, acc.: 100.00%] [G loss: 6.393945693969727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 38/86 [D loss: 0.004096998833119869, acc.: 99.95%] [G loss: 6.357494354248047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 39/86 [D loss: 0.002009156800340861, acc.: 100.00%] [G loss: 6.389778137207031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 40/86 [D loss: 0.002828907803632319, acc.: 99.95%] [G loss: 6.428722381591797]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 41/86 [D loss: 0.002877450780943036, acc.: 100.00%] [G loss: 6.329463958740234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 42/86 [D loss: 0.0029724370688199997, acc.: 99.90%] [G loss: 6.337357997894287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 43/86 [D loss: 0.002422029501758516, acc.: 99.95%] [G loss: 6.3952717781066895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 44/86 [D loss: 0.001824502949602902, acc.: 100.00%] [G loss: 6.447620391845703]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 45/86 [D loss: 0.0021237987093627453, acc.: 99.95%] [G loss: 6.509335041046143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 46/86 [D loss: 0.002798969973810017, acc.: 99.90%] [G loss: 6.4690141677856445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 47/86 [D loss: 0.0023993435315787792, acc.: 99.95%] [G loss: 6.4876861572265625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 48/86 [D loss: 0.00253215862903744, acc.: 99.95%] [G loss: 6.4803571701049805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 49/86 [D loss: 0.001340879243798554, acc.: 100.00%] [G loss: 6.592660903930664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 50/86 [D loss: 0.0019154767505824566, acc.: 100.00%] [G loss: 6.617110252380371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 51/86 [D loss: 0.003464498440735042, acc.: 99.90%] [G loss: 6.512373447418213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 52/86 [D loss: 0.001982104149647057, acc.: 100.00%] [G loss: 6.571736812591553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 53/86 [D loss: 0.0019114435999654233, acc.: 100.00%] [G loss: 6.650727272033691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 54/86 [D loss: 0.0022417972795665264, acc.: 100.00%] [G loss: 6.606807708740234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 55/86 [D loss: 0.0013621701509691775, acc.: 100.00%] [G loss: 6.654891490936279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 56/86 [D loss: 0.00344599672826007, acc.: 99.90%] [G loss: 6.646916389465332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 57/86 [D loss: 0.0017647637869231403, acc.: 99.95%] [G loss: 6.6484880447387695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 58/86 [D loss: 0.0018457093392498791, acc.: 99.90%] [G loss: 6.7154459953308105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 59/86 [D loss: 0.0017567038303241134, acc.: 99.95%] [G loss: 6.74012565612793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 60/86 [D loss: 0.0023679790319874883, acc.: 99.90%] [G loss: 6.6918625831604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 61/86 [D loss: 0.0016625682474114, acc.: 99.95%] [G loss: 6.730464935302734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 62/86 [D loss: 0.0014775313320569694, acc.: 100.00%] [G loss: 6.758740425109863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 63/86 [D loss: 0.0018263686797581613, acc.: 99.95%] [G loss: 6.782157897949219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 64/86 [D loss: 0.0024083027383312583, acc.: 99.90%] [G loss: 6.7867279052734375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 65/86 [D loss: 0.0016748880152590573, acc.: 100.00%] [G loss: 6.798181533813477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 66/86 [D loss: 0.0014956267550587654, acc.: 100.00%] [G loss: 6.777487277984619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 67/86 [D loss: 0.0015917517594061792, acc.: 100.00%] [G loss: 6.834196090698242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 68/86 [D loss: 0.0010598924709483981, acc.: 100.00%] [G loss: 6.886941432952881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 69/86 [D loss: 0.001435745507478714, acc.: 100.00%] [G loss: 6.859969139099121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 70/86 [D loss: 0.0016691519413143396, acc.: 99.90%] [G loss: 6.9057207107543945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 71/86 [D loss: 0.0015613748109899461, acc.: 99.95%] [G loss: 6.926985740661621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 72/86 [D loss: 0.0013970783329568803, acc.: 100.00%] [G loss: 6.892761707305908]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 2/200, Batch 73/86 [D loss: 0.0011718064779415727, acc.: 100.00%] [G loss: 6.943523406982422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 74/86 [D loss: 0.0023799369228072464, acc.: 99.95%] [G loss: 6.86308479309082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 75/86 [D loss: 0.0013397273723967373, acc.: 100.00%] [G loss: 6.875626087188721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 76/86 [D loss: 0.0011933247442357242, acc.: 100.00%] [G loss: 6.947142601013184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 77/86 [D loss: 0.001539583783596754, acc.: 99.95%] [G loss: 7.024969100952148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 78/86 [D loss: 0.0010888771212194115, acc.: 100.00%] [G loss: 7.007329940795898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 79/86 [D loss: 0.0016873724525794387, acc.: 99.95%] [G loss: 7.004203796386719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 80/86 [D loss: 0.0014658705913461745, acc.: 99.95%] [G loss: 6.973730087280273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 81/86 [D loss: 0.0010494523157831281, acc.: 100.00%] [G loss: 7.038501739501953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 82/86 [D loss: 0.0010361687745898962, acc.: 100.00%] [G loss: 7.100917816162109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 83/86 [D loss: 0.002085805172100663, acc.: 99.95%] [G loss: 6.991379737854004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 84/86 [D loss: 0.0014808604028075933, acc.: 100.00%] [G loss: 7.004693508148193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 85/86 [D loss: 0.0015812055789865553, acc.: 100.00%] [G loss: 7.007798194885254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 2/200, Batch 86/86 [D loss: 0.0013877626624889672, acc.: 100.00%] [G loss: 7.048687934875488]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 1/86 [D loss: 0.001025052013574168, acc.: 100.00%] [G loss: 7.107865333557129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 2/86 [D loss: 0.0024900443386286497, acc.: 99.90%] [G loss: 6.976144790649414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 3/86 [D loss: 0.0012269251747056842, acc.: 100.00%] [G loss: 6.987934589385986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 4/86 [D loss: 0.0014377945335581899, acc.: 100.00%] [G loss: 7.014926910400391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 5/86 [D loss: 0.001015008514514193, acc.: 100.00%] [G loss: 7.105334758758545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 6/86 [D loss: 0.0010862076887860894, acc.: 100.00%] [G loss: 7.141167640686035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 7/86 [D loss: 0.0008161104342434555, acc.: 100.00%] [G loss: 7.2266926765441895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 8/86 [D loss: 0.0013912241556681693, acc.: 99.95%] [G loss: 7.186036109924316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 9/86 [D loss: 0.0012934441328980029, acc.: 99.95%] [G loss: 7.198051929473877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 10/86 [D loss: 0.0019478301983326674, acc.: 99.95%] [G loss: 7.088866710662842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 11/86 [D loss: 0.0008679563761688769, acc.: 100.00%] [G loss: 7.143706321716309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 12/86 [D loss: 0.0020233229151926935, acc.: 99.95%] [G loss: 7.106462478637695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 13/86 [D loss: 0.0014250145759433508, acc.: 99.95%] [G loss: 7.189169406890869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 14/86 [D loss: 0.0013474835432134569, acc.: 100.00%] [G loss: 7.175421714782715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 15/86 [D loss: 0.0015699403011240065, acc.: 99.95%] [G loss: 7.116225242614746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 16/86 [D loss: 0.0009348848543595523, acc.: 100.00%] [G loss: 7.213132858276367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 17/86 [D loss: 0.001282010751310736, acc.: 100.00%] [G loss: 7.243024826049805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 18/86 [D loss: 0.0008953935757745057, acc.: 100.00%] [G loss: 7.26475191116333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 19/86 [D loss: 0.000771305407397449, acc.: 100.00%] [G loss: 7.3618927001953125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 20/86 [D loss: 0.001449880248401314, acc.: 99.95%] [G loss: 7.295546531677246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 21/86 [D loss: 0.002475498942658305, acc.: 99.95%] [G loss: 7.230152606964111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 22/86 [D loss: 0.0008083920110948384, acc.: 100.00%] [G loss: 7.257115840911865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 23/86 [D loss: 0.0014545412850566208, acc.: 99.95%] [G loss: 7.227184772491455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 24/86 [D loss: 0.0008815325563773513, acc.: 100.00%] [G loss: 7.312597751617432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 25/86 [D loss: 0.0019640253158286214, acc.: 99.95%] [G loss: 7.208128929138184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 26/86 [D loss: 0.0022100298083387315, acc.: 99.90%] [G loss: 7.121264934539795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 27/86 [D loss: 0.002200581890065223, acc.: 99.95%] [G loss: 7.032689094543457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 28/86 [D loss: 0.0008484953141305596, acc.: 100.00%] [G loss: 7.1802802085876465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 29/86 [D loss: 0.000646288288407959, acc.: 100.00%] [G loss: 7.327549934387207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 30/86 [D loss: 0.0007103768002707511, acc.: 100.00%] [G loss: 7.4417572021484375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 31/86 [D loss: 0.001427671842975542, acc.: 99.95%] [G loss: 7.400232791900635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 32/86 [D loss: 0.0006016638653818518, acc.: 100.00%] [G loss: 7.454308032989502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 33/86 [D loss: 0.0006383330910466611, acc.: 100.00%] [G loss: 7.477501392364502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 3/200, Batch 34/86 [D loss: 0.002672271919436753, acc.: 99.90%] [G loss: 7.352287769317627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 35/86 [D loss: 0.0014252533437684178, acc.: 99.95%] [G loss: 7.260993957519531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 36/86 [D loss: 0.0006502985197585076, acc.: 100.00%] [G loss: 7.305091857910156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 37/86 [D loss: 0.0014258176379371434, acc.: 99.90%] [G loss: 7.319857120513916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 38/86 [D loss: 0.0007615031208842993, acc.: 100.00%] [G loss: 7.360860824584961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 39/86 [D loss: 0.0017504314891994, acc.: 99.90%] [G loss: 7.3054094314575195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 40/86 [D loss: 0.000635681688436307, acc.: 100.00%] [G loss: 7.39210319519043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 41/86 [D loss: 0.0006312261248240247, acc.: 100.00%] [G loss: 7.486988544464111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 42/86 [D loss: 0.00088569286162965, acc.: 100.00%] [G loss: 7.5002522468566895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 43/86 [D loss: 0.001462300802813843, acc.: 99.95%] [G loss: 7.4686198234558105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 44/86 [D loss: 0.000707228435203433, acc.: 100.00%] [G loss: 7.510146617889404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 45/86 [D loss: 0.0007036018650978804, acc.: 100.00%] [G loss: 7.497005462646484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 46/86 [D loss: 0.0008618577558081597, acc.: 100.00%] [G loss: 7.577625274658203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 47/86 [D loss: 0.0006511015526484698, acc.: 100.00%] [G loss: 7.604902267456055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 48/86 [D loss: 0.0005794811440864578, acc.: 100.00%] [G loss: 7.664900302886963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 49/86 [D loss: 0.0006244134856387973, acc.: 100.00%] [G loss: 7.719455718994141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 50/86 [D loss: 0.0006195041642058641, acc.: 100.00%] [G loss: 7.641685485839844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 51/86 [D loss: 0.0005259981116978452, acc.: 100.00%] [G loss: 7.761628150939941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 52/86 [D loss: 0.0008048945455811918, acc.: 100.00%] [G loss: 7.668691158294678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 53/86 [D loss: 0.0010276298562530428, acc.: 100.00%] [G loss: 7.628453731536865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 54/86 [D loss: 0.000693062727805227, acc.: 100.00%] [G loss: 7.627018451690674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 55/86 [D loss: 0.000740483490517363, acc.: 100.00%] [G loss: 7.666400909423828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 56/86 [D loss: 0.0006115062860772014, acc.: 100.00%] [G loss: 7.681948661804199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 57/86 [D loss: 0.0015608783869538456, acc.: 99.95%] [G loss: 7.5293450355529785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 58/86 [D loss: 0.0007804887136444449, acc.: 100.00%] [G loss: 7.502202033996582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 59/86 [D loss: 0.0005235186399659142, acc.: 100.00%] [G loss: 7.603847026824951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 60/86 [D loss: 0.00044617279490921646, acc.: 100.00%] [G loss: 7.685579299926758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 61/86 [D loss: 0.0004582309629768133, acc.: 100.00%] [G loss: 7.775516033172607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 62/86 [D loss: 0.0006762343109585345, acc.: 100.00%] [G loss: 7.795411109924316]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 63/86 [D loss: 0.000669585628202185, acc.: 100.00%] [G loss: 7.792068958282471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 64/86 [D loss: 0.001004313409794122, acc.: 99.95%] [G loss: 7.726524353027344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 65/86 [D loss: 0.0012808788742404431, acc.: 99.95%] [G loss: 7.621917724609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 66/86 [D loss: 0.0005049618921475485, acc.: 100.00%] [G loss: 7.663170337677002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 67/86 [D loss: 0.0009513366385363042, acc.: 99.95%] [G loss: 7.641522407531738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 68/86 [D loss: 0.0014540210831910372, acc.: 99.95%] [G loss: 7.598517417907715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 69/86 [D loss: 0.0005084555450594053, acc.: 100.00%] [G loss: 7.690768241882324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 70/86 [D loss: 0.0006787510646972805, acc.: 100.00%] [G loss: 7.744787216186523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 71/86 [D loss: 0.0005864079867023975, acc.: 100.00%] [G loss: 7.740763187408447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 72/86 [D loss: 0.0003625777389970608, acc.: 100.00%] [G loss: 7.798297882080078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 73/86 [D loss: 0.0004683477454818785, acc.: 100.00%] [G loss: 7.854727268218994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 74/86 [D loss: 0.0009103106276597828, acc.: 100.00%] [G loss: 7.824126243591309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 75/86 [D loss: 0.0005589151987805963, acc.: 100.00%] [G loss: 7.822296142578125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 76/86 [D loss: 0.0005064354627393186, acc.: 100.00%] [G loss: 7.8270697593688965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 77/86 [D loss: 0.00034980815689777955, acc.: 100.00%] [G loss: 7.902528762817383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 78/86 [D loss: 0.0008169718785211444, acc.: 99.95%] [G loss: 7.854394435882568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 79/86 [D loss: 0.00038261468580458313, acc.: 100.00%] [G loss: 7.90531587600708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 80/86 [D loss: 0.0006947205984033644, acc.: 100.00%] [G loss: 7.865934371948242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 81/86 [D loss: 0.0008283419883809984, acc.: 100.00%] [G loss: 7.828268051147461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 82/86 [D loss: 0.0006357722741086036, acc.: 100.00%] [G loss: 7.8548583984375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 83/86 [D loss: 0.0003119581742794253, acc.: 100.00%] [G loss: 7.948520660400391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 84/86 [D loss: 0.0013767104246653616, acc.: 99.95%] [G loss: 7.749872207641602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 85/86 [D loss: 0.0006325393915176392, acc.: 100.00%] [G loss: 7.735695838928223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 3/200, Batch 86/86 [D loss: 0.0005152336088940501, acc.: 100.00%] [G loss: 7.827827453613281]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 1/86 [D loss: 0.00095370999770239, acc.: 100.00%] [G loss: 7.845944404602051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 2/86 [D loss: 0.0004102754028281197, acc.: 100.00%] [G loss: 7.848820209503174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 3/86 [D loss: 0.0005955726664979011, acc.: 100.00%] [G loss: 7.86616849899292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 4/86 [D loss: 0.00043790112249553204, acc.: 100.00%] [G loss: 7.97750186920166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 5/86 [D loss: 0.00045264937216416, acc.: 100.00%] [G loss: 7.971776008605957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 6/86 [D loss: 0.002220686583314091, acc.: 99.90%] [G loss: 7.662679672241211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 7/86 [D loss: 0.0007910136773716658, acc.: 100.00%] [G loss: 7.657337665557861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 8/86 [D loss: 0.0005484654539031908, acc.: 100.00%] [G loss: 7.737407207489014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 9/86 [D loss: 0.00042097789992112666, acc.: 100.00%] [G loss: 7.853851318359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 10/86 [D loss: 0.0003914155167876743, acc.: 100.00%] [G loss: 7.970659255981445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 11/86 [D loss: 0.0005273477290757, acc.: 100.00%] [G loss: 7.962586879730225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 12/86 [D loss: 0.0008978697005659342, acc.: 99.95%] [G loss: 7.957464218139648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 13/86 [D loss: 0.00030604194034822285, acc.: 100.00%] [G loss: 7.971641540527344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 14/86 [D loss: 0.001215168711496517, acc.: 99.95%] [G loss: 7.928713321685791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 15/86 [D loss: 0.0003980968031100929, acc.: 100.00%] [G loss: 7.98323917388916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 16/86 [D loss: 0.0003306219878140837, acc.: 100.00%] [G loss: 8.048721313476562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 17/86 [D loss: 0.0018109214724972844, acc.: 99.95%] [G loss: 8.012142181396484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 18/86 [D loss: 0.0008082939893938601, acc.: 100.00%] [G loss: 7.935809135437012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 19/86 [D loss: 0.0008392947784159333, acc.: 100.00%] [G loss: 7.889301300048828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 20/86 [D loss: 0.0003764994180528447, acc.: 100.00%] [G loss: 7.959567070007324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 21/86 [D loss: 0.0005204635963309556, acc.: 100.00%] [G loss: 7.97237491607666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 22/86 [D loss: 0.0003667824203148484, acc.: 100.00%] [G loss: 8.019882202148438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 23/86 [D loss: 0.00034700640389928594, acc.: 100.00%] [G loss: 8.051371574401855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 24/86 [D loss: 0.0008699546742718667, acc.: 99.95%] [G loss: 8.042387962341309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 25/86 [D loss: 0.00030554065233445726, acc.: 100.00%] [G loss: 8.053495407104492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 26/86 [D loss: 0.0011655447888188064, acc.: 99.95%] [G loss: 8.00916576385498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 27/86 [D loss: 0.0017328400281257927, acc.: 99.95%] [G loss: 7.8938679695129395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 28/86 [D loss: 0.0013083252124488354, acc.: 99.95%] [G loss: 7.806698322296143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 29/86 [D loss: 0.001596647489350289, acc.: 99.95%] [G loss: 7.808686256408691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 30/86 [D loss: 0.0008306643285322934, acc.: 100.00%] [G loss: 7.710535526275635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 31/86 [D loss: 0.0006245144759304821, acc.: 100.00%] [G loss: 7.825827121734619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 32/86 [D loss: 0.00037084268842590973, acc.: 100.00%] [G loss: 7.915348052978516]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 33/86 [D loss: 0.0003010964937857352, acc.: 100.00%] [G loss: 7.996933937072754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 34/86 [D loss: 0.0002616283509269124, acc.: 100.00%] [G loss: 8.115067481994629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 35/86 [D loss: 0.001478524529375136, acc.: 99.90%] [G loss: 7.879274845123291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 36/86 [D loss: 0.00035455413853924256, acc.: 100.00%] [G loss: 7.93132209777832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 37/86 [D loss: 0.0004236494714859873, acc.: 100.00%] [G loss: 8.044143676757812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 38/86 [D loss: 0.00025014638595166616, acc.: 100.00%] [G loss: 8.084763526916504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 39/86 [D loss: 0.0015638259064871818, acc.: 99.90%] [G loss: 7.930688381195068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 40/86 [D loss: 0.00045122579467715696, acc.: 100.00%] [G loss: 7.966796875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 41/86 [D loss: 0.0003552993803168647, acc.: 100.00%] [G loss: 8.065719604492188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 42/86 [D loss: 0.00028162196394987404, acc.: 100.00%] [G loss: 8.169292449951172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 43/86 [D loss: 0.0002846502684406005, acc.: 100.00%] [G loss: 8.29202651977539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 44/86 [D loss: 0.000254585003858665, acc.: 100.00%] [G loss: 8.301311492919922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 45/86 [D loss: 0.00026509635063121095, acc.: 100.00%] [G loss: 8.391141891479492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 46/86 [D loss: 0.00035814834700431675, acc.: 100.00%] [G loss: 8.364880561828613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 47/86 [D loss: 0.00017968052634387277, acc.: 100.00%] [G loss: 8.408273696899414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 48/86 [D loss: 0.0004281736182747409, acc.: 100.00%] [G loss: 8.4281644821167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 49/86 [D loss: 0.000554313519387506, acc.: 100.00%] [G loss: 8.322531700134277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 50/86 [D loss: 0.0007400668109767139, acc.: 99.95%] [G loss: 8.244341850280762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 51/86 [D loss: 0.0003216269251424819, acc.: 100.00%] [G loss: 8.279147148132324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 52/86 [D loss: 0.00024173127530957572, acc.: 100.00%] [G loss: 8.304877281188965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 53/86 [D loss: 0.00022604981495533139, acc.: 100.00%] [G loss: 8.397744178771973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 54/86 [D loss: 0.0002443958292133175, acc.: 100.00%] [G loss: 8.382036209106445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 55/86 [D loss: 0.00029930695018265396, acc.: 100.00%] [G loss: 8.457448959350586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 56/86 [D loss: 0.00046988940448500216, acc.: 100.00%] [G loss: 8.417400360107422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 57/86 [D loss: 0.0009925049089360982, acc.: 99.95%] [G loss: 8.277997016906738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 58/86 [D loss: 0.00037691496254410595, acc.: 100.00%] [G loss: 8.277606964111328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 59/86 [D loss: 0.00026850214271689765, acc.: 100.00%] [G loss: 8.33662223815918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 60/86 [D loss: 0.00023160648925113492, acc.: 100.00%] [G loss: 8.41856861114502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 61/86 [D loss: 0.0011197864077985287, acc.: 99.95%] [G loss: 8.305337905883789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 62/86 [D loss: 0.0003578682226361707, acc.: 100.00%] [G loss: 8.303194999694824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 63/86 [D loss: 0.0002538992775953375, acc.: 100.00%] [G loss: 8.364142417907715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 64/86 [D loss: 0.00036230676050763577, acc.: 100.00%] [G loss: 8.343633651733398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 65/86 [D loss: 0.000506757220136933, acc.: 100.00%] [G loss: 8.33631706237793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 66/86 [D loss: 0.00022169445946929045, acc.: 100.00%] [G loss: 8.42172908782959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 67/86 [D loss: 0.00025563351664459333, acc.: 100.00%] [G loss: 8.48023509979248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 68/86 [D loss: 0.00017275218488066457, acc.: 100.00%] [G loss: 8.509848594665527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 69/86 [D loss: 0.00034979262272827327, acc.: 100.00%] [G loss: 8.488791465759277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 70/86 [D loss: 0.00017178595226141624, acc.: 100.00%] [G loss: 8.540722846984863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 71/86 [D loss: 0.00026183306908933446, acc.: 100.00%] [G loss: 8.577152252197266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 72/86 [D loss: 0.0005208405491430312, acc.: 100.00%] [G loss: 8.50772476196289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 73/86 [D loss: 0.0001887166581582278, acc.: 100.00%] [G loss: 8.541253089904785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 74/86 [D loss: 0.0005079589318484068, acc.: 100.00%] [G loss: 8.476247787475586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 75/86 [D loss: 0.0002962149737868458, acc.: 100.00%] [G loss: 8.45883846282959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 76/86 [D loss: 0.0002122303267242387, acc.: 100.00%] [G loss: 8.517061233520508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 77/86 [D loss: 0.00034581124782562256, acc.: 100.00%] [G loss: 8.526515007019043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 78/86 [D loss: 0.0001934660722326953, acc.: 100.00%] [G loss: 8.549379348754883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 79/86 [D loss: 0.0008206715137930587, acc.: 99.95%] [G loss: 8.4453706741333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 80/86 [D loss: 0.00023890019656391814, acc.: 100.00%] [G loss: 8.43980884552002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 81/86 [D loss: 0.00019313157099531963, acc.: 100.00%] [G loss: 8.53933048248291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 82/86 [D loss: 0.0006049406219972298, acc.: 99.95%] [G loss: 8.515238761901855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 83/86 [D loss: 0.00019408811203902587, acc.: 100.00%] [G loss: 8.521071434020996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 84/86 [D loss: 0.0003471442323643714, acc.: 100.00%] [G loss: 8.48906421661377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 85/86 [D loss: 0.0002863724221242592, acc.: 100.00%] [G loss: 8.525178909301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 4/200, Batch 86/86 [D loss: 0.0002564229289419018, acc.: 100.00%] [G loss: 8.566169738769531]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 1/86 [D loss: 0.000328935420839116, acc.: 100.00%] [G loss: 8.57240104675293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 2/86 [D loss: 0.00022849823290016502, acc.: 100.00%] [G loss: 8.653210639953613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 3/86 [D loss: 0.0005472596239997074, acc.: 99.95%] [G loss: 8.5345458984375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 4/86 [D loss: 0.00023408764536725357, acc.: 100.00%] [G loss: 8.59139633178711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 5/86 [D loss: 0.00047964148689061403, acc.: 100.00%] [G loss: 8.582413673400879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 6/86 [D loss: 0.00020498467347351834, acc.: 100.00%] [G loss: 8.598248481750488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 7/86 [D loss: 0.00034836219856515527, acc.: 100.00%] [G loss: 8.585832595825195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 8/86 [D loss: 0.00017751624545780942, acc.: 100.00%] [G loss: 8.61557674407959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 9/86 [D loss: 0.00025568944693077356, acc.: 100.00%] [G loss: 8.632200241088867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 10/86 [D loss: 0.0003347193996887654, acc.: 100.00%] [G loss: 8.632912635803223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 11/86 [D loss: 0.00037160434294492006, acc.: 100.00%] [G loss: 8.569116592407227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 12/86 [D loss: 0.00020722484987345524, acc.: 100.00%] [G loss: 8.603236198425293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 13/86 [D loss: 0.00019136860282742418, acc.: 100.00%] [G loss: 8.63966178894043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 14/86 [D loss: 0.0012756745127262548, acc.: 99.95%] [G loss: 8.53956413269043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 15/86 [D loss: 0.00035953018232248724, acc.: 100.00%] [G loss: 8.47075080871582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 16/86 [D loss: 0.00020414228856679983, acc.: 100.00%] [G loss: 8.523356437683105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 17/86 [D loss: 0.00023330518888542429, acc.: 100.00%] [G loss: 8.578792572021484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 18/86 [D loss: 0.0001948402168636676, acc.: 100.00%] [G loss: 8.640506744384766]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 5/200, Batch 19/86 [D loss: 0.000181504638021579, acc.: 100.00%] [G loss: 8.676408767700195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 20/86 [D loss: 0.0001689218588580843, acc.: 100.00%] [G loss: 8.764678955078125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 21/86 [D loss: 0.0001536821946501732, acc.: 100.00%] [G loss: 8.742122650146484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 22/86 [D loss: 0.0006883060996187851, acc.: 99.95%] [G loss: 8.642865180969238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 23/86 [D loss: 0.0006463806057581678, acc.: 99.95%] [G loss: 8.518431663513184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 24/86 [D loss: 0.00031926050723996013, acc.: 100.00%] [G loss: 8.47834300994873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 25/86 [D loss: 0.00023882300592958927, acc.: 100.00%] [G loss: 8.58365249633789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 26/86 [D loss: 0.00020458573999349028, acc.: 100.00%] [G loss: 8.619824409484863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 27/86 [D loss: 0.00021476184338098392, acc.: 100.00%] [G loss: 8.663421630859375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 28/86 [D loss: 0.0002738455368671566, acc.: 100.00%] [G loss: 8.665624618530273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 29/86 [D loss: 0.00015581579646095634, acc.: 100.00%] [G loss: 8.806336402893066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 30/86 [D loss: 0.00016796885029179975, acc.: 100.00%] [G loss: 8.826839447021484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 31/86 [D loss: 0.00022997530322754756, acc.: 100.00%] [G loss: 8.77444839477539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 32/86 [D loss: 0.00017128718172898516, acc.: 100.00%] [G loss: 8.850601196289062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 33/86 [D loss: 0.0002085494779748842, acc.: 100.00%] [G loss: 8.857034683227539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 34/86 [D loss: 0.00026043527032015845, acc.: 100.00%] [G loss: 8.842105865478516]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 35/86 [D loss: 0.00017240610031876713, acc.: 100.00%] [G loss: 8.862459182739258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 36/86 [D loss: 0.00011274656844761921, acc.: 100.00%] [G loss: 8.8590726852417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 37/86 [D loss: 0.00015478529894608073, acc.: 100.00%] [G loss: 8.93376636505127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 38/86 [D loss: 0.00020003148529212922, acc.: 100.00%] [G loss: 8.920013427734375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 39/86 [D loss: 0.00024961492454167455, acc.: 100.00%] [G loss: 8.9404935836792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 40/86 [D loss: 0.0015188427350949496, acc.: 99.95%] [G loss: 8.654184341430664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 41/86 [D loss: 0.00022664490825263783, acc.: 100.00%] [G loss: 8.581329345703125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 42/86 [D loss: 0.0001577558014105307, acc.: 100.00%] [G loss: 8.702516555786133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 43/86 [D loss: 0.00018129741147276945, acc.: 100.00%] [G loss: 8.782369613647461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 44/86 [D loss: 0.0004653903015423566, acc.: 100.00%] [G loss: 8.707355499267578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 45/86 [D loss: 0.00021414089133031666, acc.: 100.00%] [G loss: 8.735897064208984]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 46/86 [D loss: 0.00027216151647735387, acc.: 100.00%] [G loss: 8.712179183959961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 47/86 [D loss: 0.0001839666801970452, acc.: 100.00%] [G loss: 8.802720069885254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 48/86 [D loss: 0.00017857500643003732, acc.: 100.00%] [G loss: 8.833402633666992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 49/86 [D loss: 0.00012835191728299833, acc.: 100.00%] [G loss: 8.881564140319824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 50/86 [D loss: 0.00011034735643988824, acc.: 100.00%] [G loss: 8.954432487487793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 51/86 [D loss: 0.0005288225947879255, acc.: 100.00%] [G loss: 8.813765525817871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 52/86 [D loss: 0.00048342140507884324, acc.: 100.00%] [G loss: 8.72838306427002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 53/86 [D loss: 0.003084482275880873, acc.: 99.90%] [G loss: 6.931099891662598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 54/86 [D loss: 4.67923463810439, acc.: 82.03%] [G loss: 5.513712264537673e-10]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 55/86 [D loss: 16.129744611680508, acc.: 48.68%] [G loss: 1.3088537343719508e-05]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 56/86 [D loss: 5.378089874982834, acc.: 42.58%] [G loss: 2.5752399324119324e-06]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 57/86 [D loss: 5.440449878573418, acc.: 47.71%] [G loss: 0.10506229102611542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 58/86 [D loss: 0.11643738579005003, acc.: 98.68%] [G loss: 2.9593021869659424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 59/86 [D loss: 0.08262539003044367, acc.: 99.90%] [G loss: 0.15977345407009125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 60/86 [D loss: 1.8182751387357712, acc.: 50.05%] [G loss: 0.11049206554889679]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 5/200, Batch 61/86 [D loss: 0.9124861024320126, acc.: 50.15%] [G loss: 0.6880348920822144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 62/86 [D loss: 0.8830775618553162, acc.: 35.25%] [G loss: 0.6490174531936646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 63/86 [D loss: 0.834776908159256, acc.: 40.87%] [G loss: 0.6401664018630981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 64/86 [D loss: 0.7504842430353165, acc.: 43.41%] [G loss: 0.41296273469924927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 65/86 [D loss: 1.022341936826706, acc.: 39.45%] [G loss: 0.37107598781585693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 66/86 [D loss: 1.0240072906017303, acc.: 36.13%] [G loss: 0.5065934062004089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 67/86 [D loss: 0.9743789732456207, acc.: 26.56%] [G loss: 0.6292171478271484]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 68/86 [D loss: 0.8836184144020081, acc.: 23.00%] [G loss: 0.7049891948699951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 69/86 [D loss: 0.8041408360004425, acc.: 29.54%] [G loss: 0.712921142578125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 70/86 [D loss: 0.7986633777618408, acc.: 31.15%] [G loss: 0.6764807105064392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 71/86 [D loss: 0.809000551700592, acc.: 31.84%] [G loss: 0.6778702139854431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 72/86 [D loss: 0.7965993881225586, acc.: 30.76%] [G loss: 0.6909490823745728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 73/86 [D loss: 0.7772140502929688, acc.: 30.57%] [G loss: 0.6913356184959412]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 5/200, Batch 74/86 [D loss: 0.7616634666919708, acc.: 31.49%] [G loss: 0.6859670877456665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 75/86 [D loss: 0.741452544927597, acc.: 37.21%] [G loss: 0.6618365049362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 76/86 [D loss: 0.7568790316581726, acc.: 39.60%] [G loss: 0.6066824793815613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 77/86 [D loss: 0.8023898303508759, acc.: 37.11%] [G loss: 0.7136309146881104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 78/86 [D loss: 0.7357547283172607, acc.: 43.26%] [G loss: 0.7255679965019226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 79/86 [D loss: 0.7800320386886597, acc.: 32.62%] [G loss: 0.6152973771095276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 80/86 [D loss: 0.8060762882232666, acc.: 27.00%] [G loss: 0.6534273028373718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 81/86 [D loss: 0.7375167906284332, acc.: 39.60%] [G loss: 0.6469097137451172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 82/86 [D loss: 0.7713287770748138, acc.: 39.89%] [G loss: 0.5051464438438416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 83/86 [D loss: 0.885929137468338, acc.: 35.94%] [G loss: 0.5823061466217041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 84/86 [D loss: 0.7963565587997437, acc.: 31.93%] [G loss: 0.6714027523994446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 85/86 [D loss: 0.8063388764858246, acc.: 23.58%] [G loss: 0.6509683132171631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 5/200, Batch 86/86 [D loss: 0.815616101026535, acc.: 20.21%] [G loss: 0.668405294418335]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 6/200, Batch 1/86 [D loss: 0.7843489944934845, acc.: 25.39%] [G loss: 0.7370272874832153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 2/86 [D loss: 0.734550952911377, acc.: 40.14%] [G loss: 0.8029500246047974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 3/86 [D loss: 0.7021917402744293, acc.: 51.81%] [G loss: 0.7880886793136597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 4/86 [D loss: 0.7118901014328003, acc.: 47.90%] [G loss: 0.7036144137382507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 5/86 [D loss: 0.7574913501739502, acc.: 36.96%] [G loss: 0.6680169701576233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 6/86 [D loss: 0.7711029052734375, acc.: 32.71%] [G loss: 0.6862295269966125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 7/86 [D loss: 0.7760708332061768, acc.: 31.25%] [G loss: 0.7059506177902222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 8/86 [D loss: 0.7926318347454071, acc.: 25.68%] [G loss: 0.695828914642334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 9/86 [D loss: 0.7902995645999908, acc.: 26.17%] [G loss: 0.6865439414978027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 10/86 [D loss: 0.7920242846012115, acc.: 23.49%] [G loss: 0.6681590676307678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 11/86 [D loss: 0.8000234663486481, acc.: 24.17%] [G loss: 0.6541889309883118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 12/86 [D loss: 0.8056654930114746, acc.: 24.32%] [G loss: 0.642769992351532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 13/86 [D loss: 0.8195734024047852, acc.: 21.88%] [G loss: 0.6419216394424438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 14/86 [D loss: 0.8144127428531647, acc.: 20.80%] [G loss: 0.6678332090377808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 15/86 [D loss: 0.7976718544960022, acc.: 23.10%] [G loss: 0.715790331363678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 16/86 [D loss: 0.7698773145675659, acc.: 29.15%] [G loss: 0.7547265291213989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 17/86 [D loss: 0.73832768201828, acc.: 39.01%] [G loss: 0.7731531262397766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 18/86 [D loss: 0.714439332485199, acc.: 46.34%] [G loss: 0.7583814263343811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 19/86 [D loss: 0.6993915736675262, acc.: 49.22%] [G loss: 0.745011031627655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 20/86 [D loss: 0.6882547438144684, acc.: 52.29%] [G loss: 0.7011807560920715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 21/86 [D loss: 0.7141807973384857, acc.: 46.09%] [G loss: 0.6484462022781372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 22/86 [D loss: 0.7454731464385986, acc.: 43.07%] [G loss: 0.6622388958930969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 23/86 [D loss: 0.7239475846290588, acc.: 47.07%] [G loss: 0.7046816349029541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 24/86 [D loss: 0.734218418598175, acc.: 43.07%] [G loss: 0.7085394859313965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 25/86 [D loss: 0.7598830461502075, acc.: 35.06%] [G loss: 0.6751818060874939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 26/86 [D loss: 0.7717798352241516, acc.: 33.69%] [G loss: 0.6490412950515747]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 6/200, Batch 27/86 [D loss: 0.7796362936496735, acc.: 30.42%] [G loss: 0.6419944167137146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 28/86 [D loss: 0.768513023853302, acc.: 33.11%] [G loss: 0.6331685185432434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 29/86 [D loss: 0.7689737379550934, acc.: 33.59%] [G loss: 0.6428366303443909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 30/86 [D loss: 0.7757772207260132, acc.: 33.01%] [G loss: 0.6835417747497559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 31/86 [D loss: 0.7566533386707306, acc.: 33.74%] [G loss: 0.714968740940094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 32/86 [D loss: 0.7519344091415405, acc.: 34.38%] [G loss: 0.7301730513572693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 33/86 [D loss: 0.7430253326892853, acc.: 37.01%] [G loss: 0.7350707650184631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 34/86 [D loss: 0.7429034113883972, acc.: 34.96%] [G loss: 0.7149732112884521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 35/86 [D loss: 0.7429687082767487, acc.: 35.74%] [G loss: 0.7130064368247986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 36/86 [D loss: 0.7360548973083496, acc.: 37.11%] [G loss: 0.7085148096084595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 37/86 [D loss: 0.7347031533718109, acc.: 36.77%] [G loss: 0.7001481652259827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 38/86 [D loss: 0.7409813404083252, acc.: 34.81%] [G loss: 0.6906954646110535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 39/86 [D loss: 0.7395075857639313, acc.: 35.60%] [G loss: 0.6876379251480103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 40/86 [D loss: 0.7407616376876831, acc.: 35.40%] [G loss: 0.7074315547943115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 41/86 [D loss: 0.7382977902889252, acc.: 36.23%] [G loss: 0.7296594977378845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 42/86 [D loss: 0.7308882474899292, acc.: 37.84%] [G loss: 0.7485617399215698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 43/86 [D loss: 0.7329543232917786, acc.: 37.11%] [G loss: 0.7490620613098145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 44/86 [D loss: 0.7386700809001923, acc.: 35.84%] [G loss: 0.7256208062171936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 45/86 [D loss: 0.7419416010379791, acc.: 32.81%] [G loss: 0.7062652111053467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 46/86 [D loss: 0.7372832894325256, acc.: 34.03%] [G loss: 0.7050576210021973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 47/86 [D loss: 0.7346533536911011, acc.: 35.35%] [G loss: 0.7042950391769409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 48/86 [D loss: 0.7318398058414459, acc.: 35.60%] [G loss: 0.6907656192779541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 49/86 [D loss: 0.7342721819877625, acc.: 35.21%] [G loss: 0.6784676313400269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 50/86 [D loss: 0.7359273135662079, acc.: 37.01%] [G loss: 0.6820701360702515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 51/86 [D loss: 0.7365586459636688, acc.: 38.18%] [G loss: 0.6932498812675476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 52/86 [D loss: 0.7402185797691345, acc.: 36.13%] [G loss: 0.7265899777412415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 53/86 [D loss: 0.7320113182067871, acc.: 38.92%] [G loss: 0.758675754070282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 54/86 [D loss: 0.7234064936637878, acc.: 40.72%] [G loss: 0.7565486431121826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 55/86 [D loss: 0.7238967716693878, acc.: 40.28%] [G loss: 0.7349848747253418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 56/86 [D loss: 0.7271120250225067, acc.: 39.16%] [G loss: 0.727024257183075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 57/86 [D loss: 0.7251092791557312, acc.: 37.35%] [G loss: 0.7211667895317078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 58/86 [D loss: 0.716264933347702, acc.: 41.75%] [G loss: 0.7160419821739197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 6/200, Batch 59/86 [D loss: 0.7041080296039581, acc.: 46.88%] [G loss: 0.7173779010772705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 60/86 [D loss: 0.6969449818134308, acc.: 50.78%] [G loss: 0.6981619596481323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 61/86 [D loss: 0.7113897800445557, acc.: 45.17%] [G loss: 0.6697685718536377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 62/86 [D loss: 0.7243821024894714, acc.: 45.12%] [G loss: 0.6605644822120667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 63/86 [D loss: 0.728548139333725, acc.: 41.89%] [G loss: 0.7075662612915039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 64/86 [D loss: 0.713081419467926, acc.: 45.12%] [G loss: 0.795777440071106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 65/86 [D loss: 0.6934005618095398, acc.: 49.80%] [G loss: 0.8329238295555115]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 6/200, Batch 66/86 [D loss: 0.6927379667758942, acc.: 51.61%] [G loss: 0.7965829968452454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 67/86 [D loss: 0.7214208543300629, acc.: 40.77%] [G loss: 0.7323678135871887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 68/86 [D loss: 0.7339690625667572, acc.: 34.67%] [G loss: 0.7075958847999573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 69/86 [D loss: 0.725334107875824, acc.: 36.28%] [G loss: 0.7064318656921387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 70/86 [D loss: 0.7049249708652496, acc.: 44.43%] [G loss: 0.7222974300384521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 71/86 [D loss: 0.6865586638450623, acc.: 54.49%] [G loss: 0.7179186940193176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 72/86 [D loss: 0.6848740577697754, acc.: 54.59%] [G loss: 0.6740062832832336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 73/86 [D loss: 0.7043309211730957, acc.: 47.80%] [G loss: 0.6297411918640137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 74/86 [D loss: 0.7417231500148773, acc.: 40.58%] [G loss: 0.6039097905158997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 75/86 [D loss: 0.7600930631160736, acc.: 38.23%] [G loss: 0.6684423685073853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 76/86 [D loss: 0.7270522117614746, acc.: 39.70%] [G loss: 0.7884541153907776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 77/86 [D loss: 0.6943262815475464, acc.: 50.34%] [G loss: 0.8265029788017273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 78/86 [D loss: 0.705740749835968, acc.: 47.31%] [G loss: 0.7855181694030762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 79/86 [D loss: 0.7194617986679077, acc.: 42.72%] [G loss: 0.7416879534721375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 80/86 [D loss: 0.7317591905593872, acc.: 38.33%] [G loss: 0.7131284475326538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 81/86 [D loss: 0.7268674075603485, acc.: 37.06%] [G loss: 0.706346333026886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 82/86 [D loss: 0.7149326503276825, acc.: 42.14%] [G loss: 0.7121504545211792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 83/86 [D loss: 0.7029838860034943, acc.: 46.34%] [G loss: 0.7013522982597351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 84/86 [D loss: 0.6966249942779541, acc.: 48.68%] [G loss: 0.6796889305114746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 85/86 [D loss: 0.7126166224479675, acc.: 44.97%] [G loss: 0.6451179385185242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 6/200, Batch 86/86 [D loss: 0.735410749912262, acc.: 39.94%] [G loss: 0.6340001821517944]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 1/86 [D loss: 0.7386612892150879, acc.: 38.72%] [G loss: 0.6870777010917664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 2/86 [D loss: 0.7207909822463989, acc.: 42.48%] [G loss: 0.7682023644447327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 3/86 [D loss: 0.7052383422851562, acc.: 46.48%] [G loss: 0.7964796423912048]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 4/86 [D loss: 0.7032665908336639, acc.: 48.10%] [G loss: 0.769244372844696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 5/86 [D loss: 0.7188715934753418, acc.: 41.70%] [G loss: 0.741104245185852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 6/86 [D loss: 0.7332549393177032, acc.: 37.70%] [G loss: 0.721174418926239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 7/86 [D loss: 0.7241978049278259, acc.: 38.48%] [G loss: 0.7177598476409912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 8/86 [D loss: 0.7084831893444061, acc.: 43.51%] [G loss: 0.7071139216423035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 9/86 [D loss: 0.7058519124984741, acc.: 44.09%] [G loss: 0.6935381293296814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 10/86 [D loss: 0.7143969535827637, acc.: 41.16%] [G loss: 0.6632802486419678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 11/86 [D loss: 0.7271320819854736, acc.: 38.23%] [G loss: 0.6515697836875916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 12/86 [D loss: 0.7356036603450775, acc.: 38.13%] [G loss: 0.6788363456726074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 13/86 [D loss: 0.7290902733802795, acc.: 38.72%] [G loss: 0.7354788184165955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 14/86 [D loss: 0.7152468860149384, acc.: 42.33%] [G loss: 0.763117253780365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 15/86 [D loss: 0.7171734571456909, acc.: 41.36%] [G loss: 0.7536446452140808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 16/86 [D loss: 0.7244633436203003, acc.: 40.72%] [G loss: 0.7386722564697266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 17/86 [D loss: 0.7270491123199463, acc.: 37.99%] [G loss: 0.7328590154647827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 18/86 [D loss: 0.716608464717865, acc.: 40.38%] [G loss: 0.734770655632019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 19/86 [D loss: 0.7079002261161804, acc.: 44.58%] [G loss: 0.7296990752220154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 20/86 [D loss: 0.7044278979301453, acc.: 45.85%] [G loss: 0.7228268384933472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 21/86 [D loss: 0.707452803850174, acc.: 45.17%] [G loss: 0.7059725522994995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 22/86 [D loss: 0.7078951597213745, acc.: 44.38%] [G loss: 0.7127923369407654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 23/86 [D loss: 0.7070591747760773, acc.: 47.27%] [G loss: 0.7355698347091675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 24/86 [D loss: 0.7055144608020782, acc.: 45.65%] [G loss: 0.7545242309570312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 25/86 [D loss: 0.7006419003009796, acc.: 49.37%] [G loss: 0.7568258047103882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 26/86 [D loss: 0.7053482830524445, acc.: 47.27%] [G loss: 0.7381495833396912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 27/86 [D loss: 0.7178157567977905, acc.: 41.89%] [G loss: 0.7190220952033997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 28/86 [D loss: 0.7209257781505585, acc.: 39.55%] [G loss: 0.7024949193000793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 29/86 [D loss: 0.7240145802497864, acc.: 37.21%] [G loss: 0.6916411519050598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 30/86 [D loss: 0.72048619389534, acc.: 38.77%] [G loss: 0.677940309047699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 31/86 [D loss: 0.7277442812919617, acc.: 35.94%] [G loss: 0.6625522375106812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 32/86 [D loss: 0.7285104990005493, acc.: 37.26%] [G loss: 0.6606583595275879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 33/86 [D loss: 0.7328998446464539, acc.: 36.18%] [G loss: 0.6697374582290649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 34/86 [D loss: 0.7272043228149414, acc.: 36.87%] [G loss: 0.686939537525177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 35/86 [D loss: 0.7183151543140411, acc.: 39.84%] [G loss: 0.7192305326461792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 7/200, Batch 36/86 [D loss: 0.7155066728591919, acc.: 41.85%] [G loss: 0.729393720626831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 37/86 [D loss: 0.7072462737560272, acc.: 45.21%] [G loss: 0.738056480884552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 38/86 [D loss: 0.707588404417038, acc.: 46.00%] [G loss: 0.7322154641151428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 39/86 [D loss: 0.6981906890869141, acc.: 50.98%] [G loss: 0.7311154007911682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 40/86 [D loss: 0.6988429725170135, acc.: 48.63%] [G loss: 0.7166502475738525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 41/86 [D loss: 0.6977483630180359, acc.: 48.49%] [G loss: 0.7105557918548584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 42/86 [D loss: 0.7008745968341827, acc.: 46.83%] [G loss: 0.694808304309845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 43/86 [D loss: 0.7054418921470642, acc.: 46.09%] [G loss: 0.6838490962982178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 44/86 [D loss: 0.712630957365036, acc.: 41.46%] [G loss: 0.6795538067817688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 45/86 [D loss: 0.7151502668857574, acc.: 42.63%] [G loss: 0.6839450597763062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 46/86 [D loss: 0.7185731530189514, acc.: 41.65%] [G loss: 0.6978037357330322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 47/86 [D loss: 0.7194381952285767, acc.: 41.11%] [G loss: 0.7093038558959961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 48/86 [D loss: 0.7221965193748474, acc.: 38.82%] [G loss: 0.7048402428627014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 49/86 [D loss: 0.7290470898151398, acc.: 35.99%] [G loss: 0.6936130523681641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 50/86 [D loss: 0.7351408302783966, acc.: 32.91%] [G loss: 0.6903938055038452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 51/86 [D loss: 0.7321158051490784, acc.: 33.84%] [G loss: 0.6866283416748047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 52/86 [D loss: 0.7302876114845276, acc.: 33.54%] [G loss: 0.6864371299743652]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 7/200, Batch 53/86 [D loss: 0.7225092053413391, acc.: 38.28%] [G loss: 0.697540819644928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 54/86 [D loss: 0.7138764262199402, acc.: 42.72%] [G loss: 0.7137365341186523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 55/86 [D loss: 0.7053819894790649, acc.: 46.29%] [G loss: 0.7422263026237488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 56/86 [D loss: 0.6847656071186066, acc.: 55.32%] [G loss: 0.7662244439125061]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 57/86 [D loss: 0.6833710968494415, acc.: 56.79%] [G loss: 0.7787110805511475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 58/86 [D loss: 0.6768536269664764, acc.: 57.62%] [G loss: 0.7754318118095398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 59/86 [D loss: 0.6826068758964539, acc.: 56.59%] [G loss: 0.7555297613143921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 60/86 [D loss: 0.6938478648662567, acc.: 49.90%] [G loss: 0.7394994497299194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 61/86 [D loss: 0.6997113823890686, acc.: 48.88%] [G loss: 0.7184395790100098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 62/86 [D loss: 0.7054004669189453, acc.: 46.92%] [G loss: 0.701600193977356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 63/86 [D loss: 0.7147716581821442, acc.: 41.60%] [G loss: 0.6850407123565674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 64/86 [D loss: 0.7189333736896515, acc.: 40.48%] [G loss: 0.6745673418045044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 65/86 [D loss: 0.7270746827125549, acc.: 35.69%] [G loss: 0.6625540256500244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 66/86 [D loss: 0.7304176688194275, acc.: 35.40%] [G loss: 0.6479230523109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 67/86 [D loss: 0.7390972971916199, acc.: 30.62%] [G loss: 0.645393967628479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 68/86 [D loss: 0.7388493418693542, acc.: 32.28%] [G loss: 0.6498709917068481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 69/86 [D loss: 0.7294147908687592, acc.: 35.16%] [G loss: 0.6734906435012817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 70/86 [D loss: 0.7176161110401154, acc.: 38.23%] [G loss: 0.7120943665504456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 71/86 [D loss: 0.6903619766235352, acc.: 53.08%] [G loss: 0.7581323385238647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 72/86 [D loss: 0.6712948679924011, acc.: 64.94%] [G loss: 0.7789916396141052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 73/86 [D loss: 0.6593851149082184, acc.: 69.68%] [G loss: 0.7755659222602844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 74/86 [D loss: 0.659545361995697, acc.: 68.65%] [G loss: 0.7581142783164978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 75/86 [D loss: 0.659866452217102, acc.: 68.21%] [G loss: 0.732719361782074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 76/86 [D loss: 0.6705620288848877, acc.: 60.99%] [G loss: 0.6871013641357422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 77/86 [D loss: 0.6958029866218567, acc.: 52.00%] [G loss: 0.6403795480728149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 78/86 [D loss: 0.7208026349544525, acc.: 46.88%] [G loss: 0.6468498706817627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 79/86 [D loss: 0.7143925726413727, acc.: 46.53%] [G loss: 0.7065653800964355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 80/86 [D loss: 0.6937538981437683, acc.: 51.32%] [G loss: 0.7422046065330505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 81/86 [D loss: 0.6916669011116028, acc.: 52.69%] [G loss: 0.7203967571258545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 82/86 [D loss: 0.7207094132900238, acc.: 41.11%] [G loss: 0.6862242221832275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 83/86 [D loss: 0.7330330908298492, acc.: 33.06%] [G loss: 0.6705089807510376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 84/86 [D loss: 0.7315706610679626, acc.: 32.81%] [G loss: 0.662002444267273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 85/86 [D loss: 0.7289077937602997, acc.: 35.21%] [G loss: 0.6519669890403748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 7/200, Batch 86/86 [D loss: 0.7226645350456238, acc.: 39.06%] [G loss: 0.6434545516967773]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 8/200, Batch 1/86 [D loss: 0.7329729497432709, acc.: 37.35%] [G loss: 0.623208224773407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 2/86 [D loss: 0.7428292632102966, acc.: 37.70%] [G loss: 0.6225378513336182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 3/86 [D loss: 0.7394639253616333, acc.: 37.30%] [G loss: 0.6555072069168091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 4/86 [D loss: 0.7266861498355865, acc.: 39.31%] [G loss: 0.7006876468658447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 5/86 [D loss: 0.7097684144973755, acc.: 43.26%] [G loss: 0.7517991065979004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 6/86 [D loss: 0.6971345841884613, acc.: 48.39%] [G loss: 0.781853199005127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 7/86 [D loss: 0.6928597688674927, acc.: 51.95%] [G loss: 0.7834967374801636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 8/86 [D loss: 0.6938169896602631, acc.: 52.64%] [G loss: 0.7813947796821594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 9/86 [D loss: 0.6943078935146332, acc.: 53.37%] [G loss: 0.7633604407310486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 10/86 [D loss: 0.6992237567901611, acc.: 49.56%] [G loss: 0.7389824986457825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 11/86 [D loss: 0.7035373151302338, acc.: 47.31%] [G loss: 0.7193112969398499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 12/86 [D loss: 0.710013210773468, acc.: 43.02%] [G loss: 0.705195963382721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 13/86 [D loss: 0.7136213481426239, acc.: 42.33%] [G loss: 0.6946601867675781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 14/86 [D loss: 0.7143877148628235, acc.: 40.92%] [G loss: 0.6932135224342346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 15/86 [D loss: 0.7199400663375854, acc.: 38.23%] [G loss: 0.6860880255699158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 16/86 [D loss: 0.724162369966507, acc.: 36.28%] [G loss: 0.6814837455749512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 17/86 [D loss: 0.7273947298526764, acc.: 35.55%] [G loss: 0.6763139367103577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 18/86 [D loss: 0.7300411760807037, acc.: 35.89%] [G loss: 0.6738298535346985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 19/86 [D loss: 0.7251231372356415, acc.: 35.69%] [G loss: 0.6837652921676636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 20/86 [D loss: 0.7201856672763824, acc.: 38.28%] [G loss: 0.69412761926651]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 21/86 [D loss: 0.7070541679859161, acc.: 43.41%] [G loss: 0.7172859311103821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 22/86 [D loss: 0.6956987380981445, acc.: 50.73%] [G loss: 0.7389715909957886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 23/86 [D loss: 0.6832903027534485, acc.: 58.11%] [G loss: 0.7430669665336609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 24/86 [D loss: 0.6795973181724548, acc.: 58.84%] [G loss: 0.7434635758399963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 25/86 [D loss: 0.6829763650894165, acc.: 56.40%] [G loss: 0.7273101806640625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 26/86 [D loss: 0.685779333114624, acc.: 54.54%] [G loss: 0.7153239846229553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 27/86 [D loss: 0.6931082904338837, acc.: 51.76%] [G loss: 0.7058468461036682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 28/86 [D loss: 0.7037701606750488, acc.: 45.95%] [G loss: 0.6888297200202942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 29/86 [D loss: 0.7052482962608337, acc.: 44.53%] [G loss: 0.6875772476196289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 30/86 [D loss: 0.7132847011089325, acc.: 42.19%] [G loss: 0.681084156036377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 31/86 [D loss: 0.7180849015712738, acc.: 39.89%] [G loss: 0.6761340498924255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 32/86 [D loss: 0.7175519466400146, acc.: 39.40%] [G loss: 0.6748927235603333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 33/86 [D loss: 0.7194958925247192, acc.: 39.11%] [G loss: 0.6788511872291565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 34/86 [D loss: 0.7180615067481995, acc.: 39.21%] [G loss: 0.6788762807846069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 35/86 [D loss: 0.7198777496814728, acc.: 39.31%] [G loss: 0.6869181990623474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 36/86 [D loss: 0.7165632545948029, acc.: 42.09%] [G loss: 0.6963316202163696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 37/86 [D loss: 0.7114339172840118, acc.: 43.26%] [G loss: 0.7081074118614197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 38/86 [D loss: 0.7115786969661713, acc.: 41.70%] [G loss: 0.7121411561965942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 39/86 [D loss: 0.7046380341053009, acc.: 46.29%] [G loss: 0.7212234139442444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 40/86 [D loss: 0.7038524746894836, acc.: 47.51%] [G loss: 0.7304518222808838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 41/86 [D loss: 0.7017135620117188, acc.: 46.04%] [G loss: 0.7317960262298584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 8/200, Batch 42/86 [D loss: 0.7022371888160706, acc.: 47.85%] [G loss: 0.7279418110847473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 43/86 [D loss: 0.7024583518505096, acc.: 48.05%] [G loss: 0.7228975892066956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 44/86 [D loss: 0.7023960053920746, acc.: 47.66%] [G loss: 0.7180138826370239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 45/86 [D loss: 0.7042026817798615, acc.: 45.85%] [G loss: 0.7108882665634155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 46/86 [D loss: 0.7068986892700195, acc.: 44.78%] [G loss: 0.7016682624816895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 47/86 [D loss: 0.7086777091026306, acc.: 42.33%] [G loss: 0.7009938955307007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 48/86 [D loss: 0.712136834859848, acc.: 42.72%] [G loss: 0.6977502703666687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 49/86 [D loss: 0.7148309648036957, acc.: 40.19%] [G loss: 0.7004814743995667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 50/86 [D loss: 0.714728593826294, acc.: 39.84%] [G loss: 0.7057929635047913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 51/86 [D loss: 0.7089081406593323, acc.: 43.85%] [G loss: 0.70882248878479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 52/86 [D loss: 0.7110022604465485, acc.: 41.60%] [G loss: 0.7097243070602417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 53/86 [D loss: 0.706974983215332, acc.: 44.24%] [G loss: 0.708813488483429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 54/86 [D loss: 0.7089737355709076, acc.: 43.21%] [G loss: 0.708389163017273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 55/86 [D loss: 0.7077818214893341, acc.: 42.19%] [G loss: 0.7028439044952393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 56/86 [D loss: 0.7046575248241425, acc.: 43.26%] [G loss: 0.6982250213623047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 57/86 [D loss: 0.7043558955192566, acc.: 45.17%] [G loss: 0.6968110799789429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 58/86 [D loss: 0.7048738300800323, acc.: 45.12%] [G loss: 0.686663806438446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 59/86 [D loss: 0.7032346725463867, acc.: 44.87%] [G loss: 0.67802894115448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 60/86 [D loss: 0.7071192562580109, acc.: 45.51%] [G loss: 0.6753668785095215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 61/86 [D loss: 0.7124887108802795, acc.: 42.53%] [G loss: 0.680774986743927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 62/86 [D loss: 0.7110174596309662, acc.: 43.46%] [G loss: 0.6917719841003418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 63/86 [D loss: 0.7095524668693542, acc.: 43.70%] [G loss: 0.7118996381759644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 64/86 [D loss: 0.7026465833187103, acc.: 45.70%] [G loss: 0.7217663526535034]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 65/86 [D loss: 0.700049638748169, acc.: 48.05%] [G loss: 0.7257434725761414]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 8/200, Batch 66/86 [D loss: 0.7013272643089294, acc.: 45.80%] [G loss: 0.7170040607452393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 67/86 [D loss: 0.6979019045829773, acc.: 47.90%] [G loss: 0.7071191072463989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 68/86 [D loss: 0.6940982043743134, acc.: 50.49%] [G loss: 0.7004424929618835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 69/86 [D loss: 0.689011812210083, acc.: 51.56%] [G loss: 0.6933016180992126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 70/86 [D loss: 0.6824747920036316, acc.: 56.98%] [G loss: 0.687855064868927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 71/86 [D loss: 0.6933082342147827, acc.: 52.64%] [G loss: 0.6539531350135803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 72/86 [D loss: 0.7106750309467316, acc.: 47.36%] [G loss: 0.6205547451972961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 73/86 [D loss: 0.7304661273956299, acc.: 42.19%] [G loss: 0.6267995834350586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 74/86 [D loss: 0.7330518364906311, acc.: 39.60%] [G loss: 0.671642541885376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 75/86 [D loss: 0.7120345532894135, acc.: 44.19%] [G loss: 0.71803879737854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 76/86 [D loss: 0.7052172124385834, acc.: 44.38%] [G loss: 0.718117892742157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 77/86 [D loss: 0.7166031897068024, acc.: 38.09%] [G loss: 0.7037163376808167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 78/86 [D loss: 0.721294105052948, acc.: 35.64%] [G loss: 0.6918274164199829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 79/86 [D loss: 0.7212927937507629, acc.: 34.28%] [G loss: 0.6956106424331665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 80/86 [D loss: 0.7138889133930206, acc.: 38.48%] [G loss: 0.7027239799499512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 81/86 [D loss: 0.7044914960861206, acc.: 44.97%] [G loss: 0.7082412242889404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 82/86 [D loss: 0.6962786912918091, acc.: 49.41%] [G loss: 0.7030007839202881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 83/86 [D loss: 0.6935014128684998, acc.: 52.20%] [G loss: 0.6940432786941528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 84/86 [D loss: 0.6958112120628357, acc.: 50.93%] [G loss: 0.6928576827049255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 85/86 [D loss: 0.7011958360671997, acc.: 48.78%] [G loss: 0.6964828372001648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 8/200, Batch 86/86 [D loss: 0.6964023113250732, acc.: 50.88%] [G loss: 0.7256040573120117]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 1/86 [D loss: 0.694321483373642, acc.: 49.56%] [G loss: 0.7427380084991455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 2/86 [D loss: 0.6889537572860718, acc.: 53.03%] [G loss: 0.7376493811607361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 3/86 [D loss: 0.6968032717704773, acc.: 47.75%] [G loss: 0.7272990942001343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 4/86 [D loss: 0.7007144093513489, acc.: 47.07%] [G loss: 0.709145724773407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 5/86 [D loss: 0.7092005014419556, acc.: 41.80%] [G loss: 0.6982507109642029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 6/86 [D loss: 0.710283637046814, acc.: 41.80%] [G loss: 0.6880602836608887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 7/86 [D loss: 0.7102563381195068, acc.: 40.23%] [G loss: 0.6806718707084656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 8/86 [D loss: 0.7111206352710724, acc.: 41.50%] [G loss: 0.666262149810791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 9/86 [D loss: 0.7163135409355164, acc.: 41.16%] [G loss: 0.6581701636314392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 10/86 [D loss: 0.7182523012161255, acc.: 39.79%] [G loss: 0.6631404161453247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 11/86 [D loss: 0.719278872013092, acc.: 39.55%] [G loss: 0.6808359026908875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 12/86 [D loss: 0.7124070227146149, acc.: 42.63%] [G loss: 0.7052385807037354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 13/86 [D loss: 0.7043699622154236, acc.: 46.09%] [G loss: 0.7310059070587158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 14/86 [D loss: 0.6965529024600983, acc.: 48.97%] [G loss: 0.7395588159561157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 15/86 [D loss: 0.6975604891777039, acc.: 48.49%] [G loss: 0.739514946937561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 16/86 [D loss: 0.6971400380134583, acc.: 49.80%] [G loss: 0.7301310896873474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 17/86 [D loss: 0.697769284248352, acc.: 49.80%] [G loss: 0.7176494002342224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 18/86 [D loss: 0.6985197365283966, acc.: 48.10%] [G loss: 0.7105531692504883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 19/86 [D loss: 0.6954725086688995, acc.: 49.37%] [G loss: 0.6991913318634033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 20/86 [D loss: 0.6986952424049377, acc.: 48.19%] [G loss: 0.6829909086227417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 21/86 [D loss: 0.7058872580528259, acc.: 45.70%] [G loss: 0.6661547422409058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 22/86 [D loss: 0.7146724462509155, acc.: 41.80%] [G loss: 0.6606225371360779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 23/86 [D loss: 0.7173624038696289, acc.: 41.46%] [G loss: 0.6720954775810242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 24/86 [D loss: 0.7140542566776276, acc.: 42.53%] [G loss: 0.6860500574111938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 25/86 [D loss: 0.7098718881607056, acc.: 42.77%] [G loss: 0.6998688578605652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 26/86 [D loss: 0.713228702545166, acc.: 40.14%] [G loss: 0.706561267375946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 27/86 [D loss: 0.70777428150177, acc.: 43.21%] [G loss: 0.7132485508918762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 28/86 [D loss: 0.7060319185256958, acc.: 43.60%] [G loss: 0.7137628197669983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 29/86 [D loss: 0.7003017961978912, acc.: 48.10%] [G loss: 0.7208056449890137]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 9/200, Batch 30/86 [D loss: 0.6951811015605927, acc.: 50.54%] [G loss: 0.7149764895439148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 31/86 [D loss: 0.6986566781997681, acc.: 47.27%] [G loss: 0.711570680141449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 32/86 [D loss: 0.6947706043720245, acc.: 51.17%] [G loss: 0.7000146508216858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 33/86 [D loss: 0.6988324522972107, acc.: 47.75%] [G loss: 0.6905362010002136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 34/86 [D loss: 0.7016497850418091, acc.: 46.44%] [G loss: 0.6859134435653687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 35/86 [D loss: 0.7044359445571899, acc.: 43.80%] [G loss: 0.6895358562469482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 36/86 [D loss: 0.7099276781082153, acc.: 41.11%] [G loss: 0.6887456774711609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 37/86 [D loss: 0.7117548286914825, acc.: 40.58%] [G loss: 0.6914348602294922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 38/86 [D loss: 0.7119252979755402, acc.: 41.06%] [G loss: 0.6953750848770142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 39/86 [D loss: 0.711310088634491, acc.: 39.45%] [G loss: 0.7043110728263855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 40/86 [D loss: 0.7091203033924103, acc.: 41.21%] [G loss: 0.705768346786499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 41/86 [D loss: 0.7061158716678619, acc.: 43.46%] [G loss: 0.7097223401069641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 42/86 [D loss: 0.7034288644790649, acc.: 46.00%] [G loss: 0.7180884480476379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 43/86 [D loss: 0.7001423239707947, acc.: 46.92%] [G loss: 0.7261264324188232]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 44/86 [D loss: 0.6962784826755524, acc.: 49.07%] [G loss: 0.7389849424362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 45/86 [D loss: 0.6911814212799072, acc.: 53.76%] [G loss: 0.7339669466018677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 46/86 [D loss: 0.6890966296195984, acc.: 54.74%] [G loss: 0.7380972504615784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 47/86 [D loss: 0.6867187023162842, acc.: 55.13%] [G loss: 0.7402172684669495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 48/86 [D loss: 0.6889399290084839, acc.: 54.74%] [G loss: 0.7337299585342407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 49/86 [D loss: 0.6899006962776184, acc.: 52.98%] [G loss: 0.7269189953804016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 50/86 [D loss: 0.6958314776420593, acc.: 49.02%] [G loss: 0.7202560901641846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 51/86 [D loss: 0.6990845799446106, acc.: 47.85%] [G loss: 0.7075759172439575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 52/86 [D loss: 0.6999286413192749, acc.: 46.78%] [G loss: 0.7022911310195923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 53/86 [D loss: 0.7043817937374115, acc.: 43.31%] [G loss: 0.6956334114074707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 54/86 [D loss: 0.7080814242362976, acc.: 42.24%] [G loss: 0.695853590965271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 55/86 [D loss: 0.7084399163722992, acc.: 43.21%] [G loss: 0.695392906665802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 56/86 [D loss: 0.7089675962924957, acc.: 41.41%] [G loss: 0.6936935186386108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 57/86 [D loss: 0.7061757743358612, acc.: 43.31%] [G loss: 0.6957172155380249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 58/86 [D loss: 0.7036111652851105, acc.: 44.43%] [G loss: 0.6990153789520264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 59/86 [D loss: 0.7054310739040375, acc.: 42.43%] [G loss: 0.7015855312347412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 60/86 [D loss: 0.7025107741355896, acc.: 44.14%] [G loss: 0.7049068212509155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 61/86 [D loss: 0.6998481154441833, acc.: 45.61%] [G loss: 0.7067933678627014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 62/86 [D loss: 0.6973092257976532, acc.: 47.51%] [G loss: 0.7104906439781189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 63/86 [D loss: 0.6947671473026276, acc.: 49.37%] [G loss: 0.7078448534011841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 64/86 [D loss: 0.6893959939479828, acc.: 52.64%] [G loss: 0.7115451097488403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 65/86 [D loss: 0.6920289397239685, acc.: 52.05%] [G loss: 0.707679271697998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 66/86 [D loss: 0.693597137928009, acc.: 50.68%] [G loss: 0.7020790576934814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 67/86 [D loss: 0.6955073177814484, acc.: 49.41%] [G loss: 0.7020284533500671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 68/86 [D loss: 0.6986860930919647, acc.: 47.56%] [G loss: 0.6960152983665466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 69/86 [D loss: 0.6973874270915985, acc.: 48.78%] [G loss: 0.6902931332588196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 70/86 [D loss: 0.697599858045578, acc.: 48.39%] [G loss: 0.6879331469535828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 71/86 [D loss: 0.7021211981773376, acc.: 46.44%] [G loss: 0.6839936971664429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 72/86 [D loss: 0.7062325775623322, acc.: 44.87%] [G loss: 0.6851938962936401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 73/86 [D loss: 0.7077434659004211, acc.: 42.33%] [G loss: 0.6806206703186035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 74/86 [D loss: 0.7089907824993134, acc.: 43.85%] [G loss: 0.681122899055481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 75/86 [D loss: 0.7089400887489319, acc.: 42.63%] [G loss: 0.6792759895324707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 76/86 [D loss: 0.7101955413818359, acc.: 42.53%] [G loss: 0.6823717951774597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 77/86 [D loss: 0.7115562856197357, acc.: 41.89%] [G loss: 0.6831463575363159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 78/86 [D loss: 0.707778126001358, acc.: 42.97%] [G loss: 0.6860412359237671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 79/86 [D loss: 0.7070594131946564, acc.: 44.63%] [G loss: 0.6881863474845886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 80/86 [D loss: 0.7093375623226166, acc.: 42.77%] [G loss: 0.6964241862297058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 81/86 [D loss: 0.7054937481880188, acc.: 44.48%] [G loss: 0.6950165629386902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 82/86 [D loss: 0.7056312561035156, acc.: 44.19%] [G loss: 0.7001458406448364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 83/86 [D loss: 0.7024756968021393, acc.: 45.41%] [G loss: 0.7027156352996826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 84/86 [D loss: 0.7007694244384766, acc.: 47.71%] [G loss: 0.7036978006362915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 85/86 [D loss: 0.6983267068862915, acc.: 48.14%] [G loss: 0.7058790326118469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 9/200, Batch 86/86 [D loss: 0.7009140253067017, acc.: 46.73%] [G loss: 0.7109785079956055]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 1/86 [D loss: 0.6995630860328674, acc.: 46.88%] [G loss: 0.7074161767959595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 2/86 [D loss: 0.70054891705513, acc.: 46.97%] [G loss: 0.7069122195243835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 3/86 [D loss: 0.7024169564247131, acc.: 45.31%] [G loss: 0.7050086259841919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 4/86 [D loss: 0.698891669511795, acc.: 47.85%] [G loss: 0.7076303362846375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 5/86 [D loss: 0.7030036151409149, acc.: 46.00%] [G loss: 0.7050623893737793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 6/86 [D loss: 0.7019992470741272, acc.: 46.44%] [G loss: 0.7045846581459045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 7/86 [D loss: 0.6995497047901154, acc.: 47.07%] [G loss: 0.7033460140228271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 8/86 [D loss: 0.7027262449264526, acc.: 45.31%] [G loss: 0.7032878994941711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 9/86 [D loss: 0.7056103050708771, acc.: 44.14%] [G loss: 0.7067016959190369]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 10/200, Batch 10/86 [D loss: 0.7038258016109467, acc.: 43.75%] [G loss: 0.7076133489608765]\n",
      "32/32 [==============================] - 2s 60ms/step\n",
      "Epoch 10/200, Batch 11/86 [D loss: 0.7041929066181183, acc.: 45.36%] [G loss: 0.7039856910705566]\n",
      "32/32 [==============================] - 2s 58ms/step\n",
      "Epoch 10/200, Batch 12/86 [D loss: 0.7033000290393829, acc.: 45.17%] [G loss: 0.7073473930358887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 13/86 [D loss: 0.7055693864822388, acc.: 43.80%] [G loss: 0.7082622051239014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 14/86 [D loss: 0.7020625174045563, acc.: 45.85%] [G loss: 0.7063277959823608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 15/86 [D loss: 0.6994374096393585, acc.: 47.22%] [G loss: 0.7070775032043457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 16/86 [D loss: 0.6992773413658142, acc.: 47.22%] [G loss: 0.709243893623352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 17/86 [D loss: 0.7014337182044983, acc.: 45.21%] [G loss: 0.7124485373497009]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 18/86 [D loss: 0.702081948518753, acc.: 45.75%] [G loss: 0.7133119106292725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 19/86 [D loss: 0.6987132728099823, acc.: 48.83%] [G loss: 0.7128466963768005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 20/86 [D loss: 0.7003255784511566, acc.: 47.17%] [G loss: 0.713761031627655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 21/86 [D loss: 0.6964805424213409, acc.: 49.85%] [G loss: 0.7105048894882202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 22/86 [D loss: 0.6990861296653748, acc.: 47.12%] [G loss: 0.7105642557144165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 23/86 [D loss: 0.701409786939621, acc.: 46.24%] [G loss: 0.7108978033065796]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 24/86 [D loss: 0.7003372311592102, acc.: 47.95%] [G loss: 0.7087738513946533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 25/86 [D loss: 0.6992431282997131, acc.: 47.46%] [G loss: 0.7108681201934814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 26/86 [D loss: 0.7012373208999634, acc.: 45.56%] [G loss: 0.7116385698318481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 27/86 [D loss: 0.703591912984848, acc.: 42.87%] [G loss: 0.7077733278274536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 28/86 [D loss: 0.7024245858192444, acc.: 45.21%] [G loss: 0.7099374532699585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 29/86 [D loss: 0.6999199986457825, acc.: 46.48%] [G loss: 0.7131068110466003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 30/86 [D loss: 0.6970163881778717, acc.: 49.07%] [G loss: 0.7126748561859131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 31/86 [D loss: 0.6983227431774139, acc.: 47.27%] [G loss: 0.7143281698226929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 32/86 [D loss: 0.6996820867061615, acc.: 48.88%] [G loss: 0.7172487378120422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 33/86 [D loss: 0.6987020075321198, acc.: 46.78%] [G loss: 0.7178063988685608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 34/86 [D loss: 0.7007512152194977, acc.: 46.68%] [G loss: 0.714728832244873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 35/86 [D loss: 0.6984733939170837, acc.: 48.39%] [G loss: 0.7163432836532593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 36/86 [D loss: 0.6983102560043335, acc.: 48.00%] [G loss: 0.7178618907928467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 37/86 [D loss: 0.7005418241024017, acc.: 47.31%] [G loss: 0.7153017520904541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 38/86 [D loss: 0.6997734308242798, acc.: 47.51%] [G loss: 0.713385283946991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 39/86 [D loss: 0.7002542614936829, acc.: 47.12%] [G loss: 0.7128976583480835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 40/86 [D loss: 0.7024770975112915, acc.: 45.65%] [G loss: 0.7107288837432861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 41/86 [D loss: 0.7031796276569366, acc.: 43.41%] [G loss: 0.7078059911727905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 42/86 [D loss: 0.7043278813362122, acc.: 44.58%] [G loss: 0.707697331905365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 43/86 [D loss: 0.7014959752559662, acc.: 45.41%] [G loss: 0.7104066014289856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 44/86 [D loss: 0.705230712890625, acc.: 42.77%] [G loss: 0.7037639021873474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 45/86 [D loss: 0.702444463968277, acc.: 45.46%] [G loss: 0.7021656036376953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 46/86 [D loss: 0.7042303085327148, acc.: 44.73%] [G loss: 0.7010545134544373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 47/86 [D loss: 0.7034801840782166, acc.: 43.90%] [G loss: 0.7063833475112915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 48/86 [D loss: 0.7030700445175171, acc.: 44.14%] [G loss: 0.7087690830230713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 49/86 [D loss: 0.7026038765907288, acc.: 43.60%] [G loss: 0.7068005204200745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 50/86 [D loss: 0.6998341083526611, acc.: 46.48%] [G loss: 0.7051100730895996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 51/86 [D loss: 0.7004857063293457, acc.: 45.95%] [G loss: 0.7092934846878052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 52/86 [D loss: 0.7000470459461212, acc.: 46.00%] [G loss: 0.7111577391624451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 53/86 [D loss: 0.7013912796974182, acc.: 44.97%] [G loss: 0.7088354229927063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 54/86 [D loss: 0.6998324692249298, acc.: 46.73%] [G loss: 0.7090907096862793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 55/86 [D loss: 0.6975668370723724, acc.: 48.78%] [G loss: 0.7103002071380615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 56/86 [D loss: 0.6982063353061676, acc.: 47.71%] [G loss: 0.7061548829078674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 57/86 [D loss: 0.7006930708885193, acc.: 44.24%] [G loss: 0.7077500224113464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 58/86 [D loss: 0.6999541521072388, acc.: 46.68%] [G loss: 0.7095103859901428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 59/86 [D loss: 0.697391927242279, acc.: 48.97%] [G loss: 0.7139236927032471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 60/86 [D loss: 0.699187695980072, acc.: 46.24%] [G loss: 0.7063589096069336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 61/86 [D loss: 0.700916975736618, acc.: 46.34%] [G loss: 0.7057281136512756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 62/86 [D loss: 0.7018623352050781, acc.: 45.46%] [G loss: 0.7032459378242493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 63/86 [D loss: 0.7004635334014893, acc.: 46.58%] [G loss: 0.7032065987586975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 64/86 [D loss: 0.7025181651115417, acc.: 45.21%] [G loss: 0.6959387063980103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 65/86 [D loss: 0.6996127068996429, acc.: 47.07%] [G loss: 0.6958792209625244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 66/86 [D loss: 0.703930139541626, acc.: 45.90%] [G loss: 0.6948839426040649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 67/86 [D loss: 0.7021072208881378, acc.: 46.00%] [G loss: 0.6973450779914856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 68/86 [D loss: 0.7011395692825317, acc.: 46.63%] [G loss: 0.6968555450439453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 69/86 [D loss: 0.7040361166000366, acc.: 45.12%] [G loss: 0.7007680535316467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 70/86 [D loss: 0.7047854363918304, acc.: 43.36%] [G loss: 0.7021650075912476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 71/86 [D loss: 0.7025843858718872, acc.: 44.82%] [G loss: 0.7030947208404541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 72/86 [D loss: 0.7022465169429779, acc.: 45.75%] [G loss: 0.7064080238342285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 73/86 [D loss: 0.7035209238529205, acc.: 43.90%] [G loss: 0.700311005115509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 74/86 [D loss: 0.702060341835022, acc.: 46.68%] [G loss: 0.6994511485099792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 75/86 [D loss: 0.7025206685066223, acc.: 45.07%] [G loss: 0.6996261477470398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 76/86 [D loss: 0.7004658281803131, acc.: 47.36%] [G loss: 0.7025235295295715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 77/86 [D loss: 0.7028810679912567, acc.: 43.60%] [G loss: 0.6967459917068481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 78/86 [D loss: 0.7019818723201752, acc.: 45.56%] [G loss: 0.7055106163024902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 79/86 [D loss: 0.6997296512126923, acc.: 47.02%] [G loss: 0.7022768259048462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 80/86 [D loss: 0.701411098241806, acc.: 45.90%] [G loss: 0.7047160267829895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 81/86 [D loss: 0.6986913681030273, acc.: 47.56%] [G loss: 0.7036942839622498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 82/86 [D loss: 0.7009730637073517, acc.: 46.83%] [G loss: 0.7050747871398926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 83/86 [D loss: 0.7024860382080078, acc.: 45.65%] [G loss: 0.70578533411026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 84/86 [D loss: 0.7030442953109741, acc.: 44.24%] [G loss: 0.7026642560958862]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 85/86 [D loss: 0.7011873424053192, acc.: 45.31%] [G loss: 0.7003480792045593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 10/200, Batch 86/86 [D loss: 0.7026530504226685, acc.: 44.38%] [G loss: 0.6977207064628601]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 11/200, Batch 1/86 [D loss: 0.7033281922340393, acc.: 43.95%] [G loss: 0.6970577836036682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 2/86 [D loss: 0.701416552066803, acc.: 45.17%] [G loss: 0.696946918964386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 3/86 [D loss: 0.7019715309143066, acc.: 45.56%] [G loss: 0.6994670629501343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 4/86 [D loss: 0.703578919172287, acc.: 44.58%] [G loss: 0.6971958875656128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 5/86 [D loss: 0.7039009034633636, acc.: 44.48%] [G loss: 0.6997293829917908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 6/86 [D loss: 0.7016987502574921, acc.: 45.36%] [G loss: 0.7030150890350342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 7/86 [D loss: 0.6998928189277649, acc.: 48.00%] [G loss: 0.706136167049408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 8/86 [D loss: 0.7026731371879578, acc.: 44.82%] [G loss: 0.7003025412559509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 9/86 [D loss: 0.7037895321846008, acc.: 43.95%] [G loss: 0.7007194757461548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 10/86 [D loss: 0.6986425518989563, acc.: 46.29%] [G loss: 0.7041722536087036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 11/86 [D loss: 0.7017573118209839, acc.: 46.09%] [G loss: 0.6989466547966003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 12/86 [D loss: 0.7033914923667908, acc.: 44.14%] [G loss: 0.7050068378448486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 13/86 [D loss: 0.7009359300136566, acc.: 45.70%] [G loss: 0.7032891511917114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 14/86 [D loss: 0.7017487585544586, acc.: 45.75%] [G loss: 0.7048016786575317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 15/86 [D loss: 0.7018245160579681, acc.: 44.97%] [G loss: 0.7045430541038513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 16/86 [D loss: 0.7006012499332428, acc.: 45.65%] [G loss: 0.7007420063018799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 17/86 [D loss: 0.702490895986557, acc.: 43.80%] [G loss: 0.7005679607391357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 18/86 [D loss: 0.7027062177658081, acc.: 44.73%] [G loss: 0.7010046243667603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 19/86 [D loss: 0.7001837193965912, acc.: 47.02%] [G loss: 0.7043272256851196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 20/86 [D loss: 0.7033757567405701, acc.: 43.95%] [G loss: 0.7075209617614746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 21/86 [D loss: 0.7029996514320374, acc.: 43.65%] [G loss: 0.7029638886451721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 22/86 [D loss: 0.7020392715930939, acc.: 45.41%] [G loss: 0.704614520072937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 23/86 [D loss: 0.6981802582740784, acc.: 49.12%] [G loss: 0.7020168900489807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 24/86 [D loss: 0.7017079889774323, acc.: 45.85%] [G loss: 0.705115795135498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 25/86 [D loss: 0.7004383504390717, acc.: 46.44%] [G loss: 0.7083428502082825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 26/86 [D loss: 0.6984475553035736, acc.: 47.07%] [G loss: 0.706790566444397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 27/86 [D loss: 0.6993346214294434, acc.: 46.58%] [G loss: 0.7080005407333374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 28/86 [D loss: 0.6990669071674347, acc.: 46.97%] [G loss: 0.703972578048706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 29/86 [D loss: 0.700869232416153, acc.: 46.48%] [G loss: 0.7075761556625366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 30/86 [D loss: 0.6973659694194794, acc.: 47.36%] [G loss: 0.7067996859550476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 31/86 [D loss: 0.7032630741596222, acc.: 43.65%] [G loss: 0.7017458081245422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 32/86 [D loss: 0.698427826166153, acc.: 48.14%] [G loss: 0.7076401114463806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 33/86 [D loss: 0.7005880177021027, acc.: 46.39%] [G loss: 0.7035694718360901]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 34/86 [D loss: 0.699579119682312, acc.: 46.88%] [G loss: 0.703415036201477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 35/86 [D loss: 0.7008353769779205, acc.: 46.29%] [G loss: 0.7014645338058472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 36/86 [D loss: 0.7008143067359924, acc.: 45.46%] [G loss: 0.6992334127426147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 37/86 [D loss: 0.703416496515274, acc.: 44.04%] [G loss: 0.7028694152832031]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 38/86 [D loss: 0.7014731168746948, acc.: 46.83%] [G loss: 0.7050827741622925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 39/86 [D loss: 0.7017516493797302, acc.: 44.97%] [G loss: 0.7054047584533691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 40/86 [D loss: 0.7010959386825562, acc.: 46.04%] [G loss: 0.7068955302238464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 41/86 [D loss: 0.6983376443386078, acc.: 46.58%] [G loss: 0.7013903856277466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 42/86 [D loss: 0.7001032829284668, acc.: 46.88%] [G loss: 0.7023757696151733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 43/86 [D loss: 0.7017586529254913, acc.: 45.90%] [G loss: 0.7019039392471313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 44/86 [D loss: 0.6971665918827057, acc.: 49.51%] [G loss: 0.7032296061515808]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 45/86 [D loss: 0.7007216215133667, acc.: 47.17%] [G loss: 0.705989420413971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 46/86 [D loss: 0.701244443655014, acc.: 45.51%] [G loss: 0.705188512802124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 47/86 [D loss: 0.7014445066452026, acc.: 46.19%] [G loss: 0.7069011330604553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 48/86 [D loss: 0.7020805478096008, acc.: 45.70%] [G loss: 0.7014554142951965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 49/86 [D loss: 0.7018904387950897, acc.: 46.19%] [G loss: 0.705642580986023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 50/86 [D loss: 0.7029414474964142, acc.: 43.95%] [G loss: 0.7019087672233582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 51/86 [D loss: 0.7019264101982117, acc.: 45.36%] [G loss: 0.7019715309143066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 52/86 [D loss: 0.7018373310565948, acc.: 45.31%] [G loss: 0.704382061958313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 53/86 [D loss: 0.6989255845546722, acc.: 47.07%] [G loss: 0.7064073085784912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 54/86 [D loss: 0.6989932656288147, acc.: 47.12%] [G loss: 0.7070842385292053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 55/86 [D loss: 0.6997170746326447, acc.: 46.14%] [G loss: 0.7059327960014343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 56/86 [D loss: 0.6984895467758179, acc.: 46.78%] [G loss: 0.7071046233177185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 57/86 [D loss: 0.6996640563011169, acc.: 47.12%] [G loss: 0.7065390348434448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 58/86 [D loss: 0.6939189434051514, acc.: 50.20%] [G loss: 0.7085907459259033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 59/86 [D loss: 0.6962372064590454, acc.: 50.15%] [G loss: 0.7089939713478088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 60/86 [D loss: 0.6975938677787781, acc.: 48.34%] [G loss: 0.7060061097145081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 61/86 [D loss: 0.6969262957572937, acc.: 49.46%] [G loss: 0.7079519033432007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 62/86 [D loss: 0.6965034306049347, acc.: 50.34%] [G loss: 0.7048748731613159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 63/86 [D loss: 0.6953162848949432, acc.: 49.41%] [G loss: 0.7043635249137878]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 64/86 [D loss: 0.6955331265926361, acc.: 48.97%] [G loss: 0.7043111324310303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 65/86 [D loss: 0.6972818374633789, acc.: 48.05%] [G loss: 0.7061630487442017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 66/86 [D loss: 0.6964053213596344, acc.: 48.49%] [G loss: 0.7037534713745117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 67/86 [D loss: 0.6951791346073151, acc.: 50.78%] [G loss: 0.7025052905082703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 68/86 [D loss: 0.6976014375686646, acc.: 46.68%] [G loss: 0.7049376964569092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 69/86 [D loss: 0.700348436832428, acc.: 45.61%] [G loss: 0.7012980580329895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 70/86 [D loss: 0.6973917186260223, acc.: 48.83%] [G loss: 0.6998296976089478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 71/86 [D loss: 0.695830225944519, acc.: 49.80%] [G loss: 0.6985898017883301]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 72/86 [D loss: 0.6963843703269958, acc.: 49.02%] [G loss: 0.6982617974281311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 73/86 [D loss: 0.6982443928718567, acc.: 47.95%] [G loss: 0.6970188617706299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 74/86 [D loss: 0.6980533301830292, acc.: 47.80%] [G loss: 0.7001315951347351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 75/86 [D loss: 0.6989138424396515, acc.: 46.92%] [G loss: 0.7012798190116882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 76/86 [D loss: 0.6971572935581207, acc.: 48.34%] [G loss: 0.6965872049331665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 77/86 [D loss: 0.6998071074485779, acc.: 46.00%] [G loss: 0.6983070969581604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 78/86 [D loss: 0.6991763114929199, acc.: 46.88%] [G loss: 0.7012009620666504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 79/86 [D loss: 0.7006209194660187, acc.: 44.78%] [G loss: 0.6973254680633545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 80/86 [D loss: 0.6996193528175354, acc.: 45.70%] [G loss: 0.6994779109954834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 81/86 [D loss: 0.7012260258197784, acc.: 46.00%] [G loss: 0.6937539577484131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 82/86 [D loss: 0.7007302939891815, acc.: 45.02%] [G loss: 0.6964665055274963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 83/86 [D loss: 0.7011922597885132, acc.: 45.26%] [G loss: 0.6961924433708191]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 11/200, Batch 84/86 [D loss: 0.7004522085189819, acc.: 47.51%] [G loss: 0.6974858045578003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 85/86 [D loss: 0.7001366913318634, acc.: 46.39%] [G loss: 0.6988404989242554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 11/200, Batch 86/86 [D loss: 0.7016837894916534, acc.: 45.31%] [G loss: 0.6994875073432922]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 1/86 [D loss: 0.6986111402511597, acc.: 46.63%] [G loss: 0.7015718221664429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 2/86 [D loss: 0.7022460103034973, acc.: 45.80%] [G loss: 0.7031537294387817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 3/86 [D loss: 0.6996124982833862, acc.: 47.22%] [G loss: 0.7014219760894775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 4/86 [D loss: 0.7034489512443542, acc.: 43.07%] [G loss: 0.6974545121192932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 5/86 [D loss: 0.7018760144710541, acc.: 44.53%] [G loss: 0.6984193921089172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 6/86 [D loss: 0.7003399133682251, acc.: 45.56%] [G loss: 0.6989037394523621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 7/86 [D loss: 0.7020159959793091, acc.: 45.21%] [G loss: 0.7000221014022827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 8/86 [D loss: 0.6990828812122345, acc.: 45.95%] [G loss: 0.7029075026512146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 9/86 [D loss: 0.7012490928173065, acc.: 45.85%] [G loss: 0.7001747488975525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 10/86 [D loss: 0.7009192109107971, acc.: 44.48%] [G loss: 0.7026663422584534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 11/86 [D loss: 0.7024838924407959, acc.: 43.85%] [G loss: 0.7018099427223206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 12/86 [D loss: 0.7014475464820862, acc.: 44.87%] [G loss: 0.7024151682853699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 13/86 [D loss: 0.6997603178024292, acc.: 46.88%] [G loss: 0.7010316848754883]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 14/86 [D loss: 0.7004221379756927, acc.: 45.90%] [G loss: 0.7052172422409058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 15/86 [D loss: 0.6993699669837952, acc.: 47.41%] [G loss: 0.703808605670929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 16/86 [D loss: 0.6989193558692932, acc.: 45.56%] [G loss: 0.7021923065185547]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 17/86 [D loss: 0.7003768980503082, acc.: 45.61%] [G loss: 0.7023862600326538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 18/86 [D loss: 0.6990326642990112, acc.: 47.56%] [G loss: 0.7059832811355591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 19/86 [D loss: 0.6995702981948853, acc.: 46.44%] [G loss: 0.7025079131126404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 20/86 [D loss: 0.6981735527515411, acc.: 47.51%] [G loss: 0.7035729885101318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 21/86 [D loss: 0.6981581151485443, acc.: 45.90%] [G loss: 0.7023848295211792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 22/86 [D loss: 0.6991413235664368, acc.: 46.78%] [G loss: 0.702362060546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 23/86 [D loss: 0.701214998960495, acc.: 45.31%] [G loss: 0.7056332230567932]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 24/86 [D loss: 0.6979618966579437, acc.: 49.07%] [G loss: 0.7050115466117859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 25/86 [D loss: 0.7019200623035431, acc.: 44.63%] [G loss: 0.7036521434783936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 26/86 [D loss: 0.6981277167797089, acc.: 47.02%] [G loss: 0.7006410360336304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 27/86 [D loss: 0.6991210579872131, acc.: 46.88%] [G loss: 0.7029509544372559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 28/86 [D loss: 0.6999168992042542, acc.: 46.34%] [G loss: 0.7007052898406982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 29/86 [D loss: 0.6998418271541595, acc.: 46.09%] [G loss: 0.7041347622871399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 30/86 [D loss: 0.6992238759994507, acc.: 48.00%] [G loss: 0.7071912884712219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 31/86 [D loss: 0.7013961672782898, acc.: 44.97%] [G loss: 0.7051757574081421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 32/86 [D loss: 0.6991747617721558, acc.: 46.44%] [G loss: 0.704695463180542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 33/86 [D loss: 0.6983008980751038, acc.: 48.24%] [G loss: 0.7032313346862793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 34/86 [D loss: 0.697214663028717, acc.: 48.29%] [G loss: 0.7082390785217285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 35/86 [D loss: 0.696582704782486, acc.: 49.51%] [G loss: 0.7034944891929626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 36/86 [D loss: 0.6992168724536896, acc.: 47.90%] [G loss: 0.7040368318557739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 37/86 [D loss: 0.6981644630432129, acc.: 47.56%] [G loss: 0.7062983512878418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 38/86 [D loss: 0.7000032961368561, acc.: 46.44%] [G loss: 0.7027884721755981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 39/86 [D loss: 0.6985578536987305, acc.: 47.61%] [G loss: 0.7041857242584229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 40/86 [D loss: 0.6979463398456573, acc.: 47.41%] [G loss: 0.7039657831192017]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 41/86 [D loss: 0.699240118265152, acc.: 46.78%] [G loss: 0.70135098695755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 42/86 [D loss: 0.6992773413658142, acc.: 47.36%] [G loss: 0.701496422290802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 43/86 [D loss: 0.6989427804946899, acc.: 46.19%] [G loss: 0.702018678188324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 44/86 [D loss: 0.7007355988025665, acc.: 46.34%] [G loss: 0.7041639685630798]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 45/86 [D loss: 0.7000078856945038, acc.: 46.83%] [G loss: 0.699246883392334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 46/86 [D loss: 0.6993762850761414, acc.: 45.75%] [G loss: 0.702060341835022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 47/86 [D loss: 0.698093056678772, acc.: 48.29%] [G loss: 0.6972905397415161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 48/86 [D loss: 0.6997994184494019, acc.: 46.63%] [G loss: 0.7018529176712036]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 49/86 [D loss: 0.7008909285068512, acc.: 45.85%] [G loss: 0.6997745633125305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 50/86 [D loss: 0.7005562484264374, acc.: 46.78%] [G loss: 0.7037465572357178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 51/86 [D loss: 0.6993387937545776, acc.: 47.85%] [G loss: 0.7031159400939941]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 52/86 [D loss: 0.6997535824775696, acc.: 45.61%] [G loss: 0.6993642449378967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 53/86 [D loss: 0.7006455063819885, acc.: 45.51%] [G loss: 0.6990522742271423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 54/86 [D loss: 0.6978001296520233, acc.: 47.07%] [G loss: 0.7010257840156555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 55/86 [D loss: 0.700158566236496, acc.: 45.75%] [G loss: 0.7030528783798218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 56/86 [D loss: 0.698358565568924, acc.: 47.56%] [G loss: 0.7053409814834595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 57/86 [D loss: 0.6986202299594879, acc.: 47.75%] [G loss: 0.704828679561615]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 58/86 [D loss: 0.7009957134723663, acc.: 43.85%] [G loss: 0.7041086554527283]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 59/86 [D loss: 0.6992196440696716, acc.: 46.04%] [G loss: 0.7044690847396851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 60/86 [D loss: 0.7004478275775909, acc.: 45.46%] [G loss: 0.6980429291725159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 61/86 [D loss: 0.6979309618473053, acc.: 46.44%] [G loss: 0.704541802406311]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 62/86 [D loss: 0.6983993649482727, acc.: 46.48%] [G loss: 0.7095968723297119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 63/86 [D loss: 0.6964633762836456, acc.: 47.80%] [G loss: 0.7070350646972656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 64/86 [D loss: 0.6982006728649139, acc.: 47.31%] [G loss: 0.7094042301177979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 65/86 [D loss: 0.6963994204998016, acc.: 48.44%] [G loss: 0.706055223941803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 66/86 [D loss: 0.6984462738037109, acc.: 46.53%] [G loss: 0.7060062885284424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 67/86 [D loss: 0.6982649266719818, acc.: 47.07%] [G loss: 0.7046065330505371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 68/86 [D loss: 0.6977695226669312, acc.: 46.88%] [G loss: 0.701793372631073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 69/86 [D loss: 0.698973149061203, acc.: 47.17%] [G loss: 0.7044543027877808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 70/86 [D loss: 0.697721540927887, acc.: 47.27%] [G loss: 0.7090763449668884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 71/86 [D loss: 0.6952590644359589, acc.: 48.05%] [G loss: 0.7104650735855103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 72/86 [D loss: 0.697719156742096, acc.: 47.46%] [G loss: 0.7077640295028687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 73/86 [D loss: 0.6963202357292175, acc.: 48.68%] [G loss: 0.7077366709709167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 74/86 [D loss: 0.6981225609779358, acc.: 48.19%] [G loss: 0.7040125727653503]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 75/86 [D loss: 0.6976079642772675, acc.: 47.71%] [G loss: 0.7042093276977539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 76/86 [D loss: 0.6986233294010162, acc.: 47.07%] [G loss: 0.7013764381408691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 77/86 [D loss: 0.7001945972442627, acc.: 45.36%] [G loss: 0.7056376338005066]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 12/200, Batch 78/86 [D loss: 0.7008131444454193, acc.: 44.24%] [G loss: 0.7012221217155457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 79/86 [D loss: 0.6984944343566895, acc.: 46.97%] [G loss: 0.702653169631958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 80/86 [D loss: 0.7004894316196442, acc.: 44.97%] [G loss: 0.7011947631835938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 81/86 [D loss: 0.6985785961151123, acc.: 47.61%] [G loss: 0.7004990577697754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 82/86 [D loss: 0.6997538208961487, acc.: 45.65%] [G loss: 0.7032507061958313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 83/86 [D loss: 0.6997762620449066, acc.: 46.39%] [G loss: 0.7058966755867004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 84/86 [D loss: 0.6998796164989471, acc.: 46.04%] [G loss: 0.6994717121124268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 85/86 [D loss: 0.6990821361541748, acc.: 46.19%] [G loss: 0.700359582901001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 12/200, Batch 86/86 [D loss: 0.6989651620388031, acc.: 46.63%] [G loss: 0.7022233605384827]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 1/86 [D loss: 0.7007230222225189, acc.: 44.73%] [G loss: 0.7045252323150635]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 2/86 [D loss: 0.698569655418396, acc.: 46.83%] [G loss: 0.7017102241516113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 3/86 [D loss: 0.6969620585441589, acc.: 48.58%] [G loss: 0.7006178498268127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 4/86 [D loss: 0.701583206653595, acc.: 44.34%] [G loss: 0.6991284489631653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 5/86 [D loss: 0.6989199817180634, acc.: 46.58%] [G loss: 0.7028907537460327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 6/86 [D loss: 0.6981055438518524, acc.: 48.19%] [G loss: 0.6980356574058533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 7/86 [D loss: 0.7007980048656464, acc.: 45.51%] [G loss: 0.7032275199890137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 8/86 [D loss: 0.6992764472961426, acc.: 46.34%] [G loss: 0.7039005160331726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 9/86 [D loss: 0.6980485916137695, acc.: 46.63%] [G loss: 0.6990702152252197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 10/86 [D loss: 0.6976599991321564, acc.: 47.75%] [G loss: 0.7016102075576782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 11/86 [D loss: 0.7009157538414001, acc.: 45.26%] [G loss: 0.7019911408424377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 12/86 [D loss: 0.6994068026542664, acc.: 46.14%] [G loss: 0.6993398666381836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 13/86 [D loss: 0.7000510692596436, acc.: 45.65%] [G loss: 0.704419732093811]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 14/86 [D loss: 0.6994507014751434, acc.: 46.78%] [G loss: 0.699386715888977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 15/86 [D loss: 0.6986513435840607, acc.: 47.71%] [G loss: 0.7007519602775574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 16/86 [D loss: 0.6997427642345428, acc.: 46.09%] [G loss: 0.7036600708961487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 17/86 [D loss: 0.7006005942821503, acc.: 46.58%] [G loss: 0.7010733485221863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 18/86 [D loss: 0.7027620375156403, acc.: 43.75%] [G loss: 0.6995006799697876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 19/86 [D loss: 0.7003688216209412, acc.: 44.09%] [G loss: 0.704644501209259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 20/86 [D loss: 0.7006160616874695, acc.: 46.04%] [G loss: 0.7030155062675476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 21/86 [D loss: 0.6981633901596069, acc.: 46.58%] [G loss: 0.7057443857192993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 22/86 [D loss: 0.6964699625968933, acc.: 47.22%] [G loss: 0.7098627686500549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 23/86 [D loss: 0.6987706124782562, acc.: 46.58%] [G loss: 0.7096115350723267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 24/86 [D loss: 0.6995434165000916, acc.: 45.85%] [G loss: 0.7110320329666138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 25/86 [D loss: 0.697813093662262, acc.: 47.12%] [G loss: 0.7108592391014099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 26/86 [D loss: 0.6982917785644531, acc.: 47.56%] [G loss: 0.7119359374046326]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 27/86 [D loss: 0.6987664997577667, acc.: 47.12%] [G loss: 0.7069135308265686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 28/86 [D loss: 0.6972997486591339, acc.: 47.61%] [G loss: 0.7077445983886719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 29/86 [D loss: 0.7000087201595306, acc.: 46.53%] [G loss: 0.7081706523895264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 30/86 [D loss: 0.6979303956031799, acc.: 46.39%] [G loss: 0.7083752751350403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 31/86 [D loss: 0.6979727149009705, acc.: 47.31%] [G loss: 0.7055267691612244]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 32/86 [D loss: 0.6961842775344849, acc.: 49.56%] [G loss: 0.7111725211143494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 33/86 [D loss: 0.6958873271942139, acc.: 49.17%] [G loss: 0.70926833152771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 34/86 [D loss: 0.6961833238601685, acc.: 48.54%] [G loss: 0.712058424949646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 35/86 [D loss: 0.6989333629608154, acc.: 44.78%] [G loss: 0.7114838361740112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 36/86 [D loss: 0.6986785531044006, acc.: 46.73%] [G loss: 0.7099058032035828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 37/86 [D loss: 0.6993289589881897, acc.: 46.19%] [G loss: 0.7052640318870544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 38/86 [D loss: 0.698992520570755, acc.: 47.07%] [G loss: 0.7053990364074707]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 39/86 [D loss: 0.6971131563186646, acc.: 49.37%] [G loss: 0.7056006789207458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 40/86 [D loss: 0.6983166933059692, acc.: 47.56%] [G loss: 0.7053120732307434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 41/86 [D loss: 0.6994925439357758, acc.: 45.56%] [G loss: 0.706108808517456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 42/86 [D loss: 0.6994720697402954, acc.: 45.90%] [G loss: 0.7061761021614075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 43/86 [D loss: 0.700778067111969, acc.: 45.70%] [G loss: 0.7044205665588379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 44/86 [D loss: 0.6990830302238464, acc.: 47.75%] [G loss: 0.7034739851951599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 45/86 [D loss: 0.6977332532405853, acc.: 47.17%] [G loss: 0.7000295519828796]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 46/86 [D loss: 0.6984796822071075, acc.: 46.19%] [G loss: 0.7067042589187622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 47/86 [D loss: 0.7002849280834198, acc.: 45.26%] [G loss: 0.7058308124542236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 48/86 [D loss: 0.7011221647262573, acc.: 45.41%] [G loss: 0.7020807266235352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 49/86 [D loss: 0.6998420357704163, acc.: 45.90%] [G loss: 0.6996183395385742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 50/86 [D loss: 0.7016249895095825, acc.: 44.82%] [G loss: 0.7027550339698792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 51/86 [D loss: 0.702000081539154, acc.: 43.12%] [G loss: 0.7017658948898315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 52/86 [D loss: 0.6982958018779755, acc.: 46.97%] [G loss: 0.7015297412872314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 53/86 [D loss: 0.7010540962219238, acc.: 44.63%] [G loss: 0.6971014738082886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 54/86 [D loss: 0.6975729465484619, acc.: 48.10%] [G loss: 0.7036826610565186]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 55/86 [D loss: 0.6991877257823944, acc.: 46.83%] [G loss: 0.701546311378479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 56/86 [D loss: 0.697778970003128, acc.: 46.92%] [G loss: 0.705333411693573]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 57/86 [D loss: 0.6982226669788361, acc.: 48.58%] [G loss: 0.7072191834449768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 58/86 [D loss: 0.6976033747196198, acc.: 46.63%] [G loss: 0.7042334079742432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 59/86 [D loss: 0.6991167962551117, acc.: 46.63%] [G loss: 0.7039969563484192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 60/86 [D loss: 0.6976273655891418, acc.: 46.58%] [G loss: 0.7021957635879517]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 61/86 [D loss: 0.6962849199771881, acc.: 48.78%] [G loss: 0.7027252912521362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 62/86 [D loss: 0.6985260546207428, acc.: 46.04%] [G loss: 0.7017232179641724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 63/86 [D loss: 0.6976893842220306, acc.: 48.24%] [G loss: 0.7032342553138733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 64/86 [D loss: 0.6971194744110107, acc.: 46.48%] [G loss: 0.7082366347312927]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 65/86 [D loss: 0.6968971788883209, acc.: 48.19%] [G loss: 0.7084806561470032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 66/86 [D loss: 0.6978965103626251, acc.: 46.83%] [G loss: 0.7068912982940674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 67/86 [D loss: 0.6986528635025024, acc.: 46.83%] [G loss: 0.7054846286773682]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 68/86 [D loss: 0.6979588568210602, acc.: 47.51%] [G loss: 0.7048962116241455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 69/86 [D loss: 0.698565810918808, acc.: 47.41%] [G loss: 0.7045212984085083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 70/86 [D loss: 0.6968798637390137, acc.: 47.31%] [G loss: 0.7028111219406128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 71/86 [D loss: 0.6983572244644165, acc.: 47.31%] [G loss: 0.7033387422561646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 72/86 [D loss: 0.6982057094573975, acc.: 46.68%] [G loss: 0.7031093835830688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 73/86 [D loss: 0.6987890899181366, acc.: 46.14%] [G loss: 0.7046563029289246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 74/86 [D loss: 0.6990414559841156, acc.: 46.44%] [G loss: 0.7030938863754272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 75/86 [D loss: 0.6974457800388336, acc.: 47.22%] [G loss: 0.7057845592498779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 76/86 [D loss: 0.6994937062263489, acc.: 45.80%] [G loss: 0.7018755674362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 77/86 [D loss: 0.6983686089515686, acc.: 47.66%] [G loss: 0.7028062343597412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 78/86 [D loss: 0.6997521221637726, acc.: 45.21%] [G loss: 0.7003949880599976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 79/86 [D loss: 0.6981463730335236, acc.: 46.48%] [G loss: 0.7002636194229126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 80/86 [D loss: 0.6986328363418579, acc.: 47.46%] [G loss: 0.70208340883255]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 13/200, Batch 81/86 [D loss: 0.6962327063083649, acc.: 50.00%] [G loss: 0.7047019004821777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 82/86 [D loss: 0.6991427540779114, acc.: 46.39%] [G loss: 0.7008421421051025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 83/86 [D loss: 0.6987746357917786, acc.: 46.04%] [G loss: 0.6960210800170898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 84/86 [D loss: 0.6978915631771088, acc.: 47.56%] [G loss: 0.6979706287384033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 85/86 [D loss: 0.6977404654026031, acc.: 47.66%] [G loss: 0.7020623683929443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 13/200, Batch 86/86 [D loss: 0.6995657086372375, acc.: 46.68%] [G loss: 0.7026642560958862]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 14/200, Batch 1/86 [D loss: 0.6986300945281982, acc.: 46.83%] [G loss: 0.7015112638473511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 2/86 [D loss: 0.698479413986206, acc.: 47.66%] [G loss: 0.7041595578193665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 3/86 [D loss: 0.6974312663078308, acc.: 48.19%] [G loss: 0.7077649831771851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 4/86 [D loss: 0.6946792602539062, acc.: 49.46%] [G loss: 0.7067893743515015]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 5/86 [D loss: 0.697492390871048, acc.: 48.78%] [G loss: 0.70722496509552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 6/86 [D loss: 0.6970573663711548, acc.: 48.78%] [G loss: 0.7092421650886536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 7/86 [D loss: 0.698852002620697, acc.: 45.51%] [G loss: 0.7106910347938538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 8/86 [D loss: 0.69708052277565, acc.: 47.17%] [G loss: 0.7087469100952148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 9/86 [D loss: 0.6995545625686646, acc.: 45.61%] [G loss: 0.7079026699066162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 10/86 [D loss: 0.696365624666214, acc.: 48.10%] [G loss: 0.713092029094696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 11/86 [D loss: 0.6971211433410645, acc.: 48.54%] [G loss: 0.70918869972229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 12/86 [D loss: 0.6970374882221222, acc.: 48.00%] [G loss: 0.7099342346191406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 13/86 [D loss: 0.6982378959655762, acc.: 46.44%] [G loss: 0.7107423543930054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 14/86 [D loss: 0.6990640461444855, acc.: 47.46%] [G loss: 0.7076735496520996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 15/86 [D loss: 0.6975178718566895, acc.: 47.46%] [G loss: 0.7111278176307678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 16/86 [D loss: 0.6957306861877441, acc.: 49.17%] [G loss: 0.7124876379966736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 17/86 [D loss: 0.6950612664222717, acc.: 50.54%] [G loss: 0.7085649967193604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 18/86 [D loss: 0.6964022815227509, acc.: 48.68%] [G loss: 0.708763599395752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 19/86 [D loss: 0.6971904635429382, acc.: 48.93%] [G loss: 0.7083039283752441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 20/86 [D loss: 0.6986807286739349, acc.: 45.41%] [G loss: 0.7101491093635559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 21/86 [D loss: 0.6991293132305145, acc.: 47.12%] [G loss: 0.7086260318756104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 22/86 [D loss: 0.6984283924102783, acc.: 46.19%] [G loss: 0.7117395997047424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 23/86 [D loss: 0.6997140944004059, acc.: 44.92%] [G loss: 0.707116425037384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 24/86 [D loss: 0.6957187056541443, acc.: 48.54%] [G loss: 0.7117425203323364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 25/86 [D loss: 0.6966174840927124, acc.: 47.46%] [G loss: 0.7086439728736877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 26/86 [D loss: 0.6956881880760193, acc.: 47.71%] [G loss: 0.7059152722358704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 27/86 [D loss: 0.6971204280853271, acc.: 47.07%] [G loss: 0.7059888243675232]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 28/86 [D loss: 0.6978006362915039, acc.: 47.90%] [G loss: 0.7091976404190063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 29/86 [D loss: 0.6961270868778229, acc.: 47.61%] [G loss: 0.7069894075393677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 30/86 [D loss: 0.698111891746521, acc.: 47.27%] [G loss: 0.7090550065040588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 31/86 [D loss: 0.6972408592700958, acc.: 46.97%] [G loss: 0.7082079648971558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 32/86 [D loss: 0.6956658959388733, acc.: 50.24%] [G loss: 0.7113550305366516]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 33/86 [D loss: 0.6977330148220062, acc.: 47.36%] [G loss: 0.7063044309616089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 34/86 [D loss: 0.6973109245300293, acc.: 47.07%] [G loss: 0.7023188471794128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 35/86 [D loss: 0.6963794231414795, acc.: 48.29%] [G loss: 0.7047870755195618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 36/86 [D loss: 0.6963823735713959, acc.: 48.58%] [G loss: 0.7052880525588989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 37/86 [D loss: 0.6970155835151672, acc.: 48.54%] [G loss: 0.7063117623329163]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 38/86 [D loss: 0.6944291889667511, acc.: 49.66%] [G loss: 0.7062953114509583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 39/86 [D loss: 0.6979519426822662, acc.: 47.56%] [G loss: 0.7039021253585815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 40/86 [D loss: 0.6972894668579102, acc.: 47.71%] [G loss: 0.7056207656860352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 41/86 [D loss: 0.6981241405010223, acc.: 47.27%] [G loss: 0.7036638259887695]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 42/86 [D loss: 0.6954576969146729, acc.: 49.12%] [G loss: 0.7025591135025024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 43/86 [D loss: 0.6977708339691162, acc.: 46.88%] [G loss: 0.7037401795387268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 44/86 [D loss: 0.696012020111084, acc.: 48.68%] [G loss: 0.7036980390548706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 45/86 [D loss: 0.698016881942749, acc.: 46.97%] [G loss: 0.7035058736801147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 46/86 [D loss: 0.6978961229324341, acc.: 47.07%] [G loss: 0.704762876033783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 47/86 [D loss: 0.696893721818924, acc.: 47.80%] [G loss: 0.7019141316413879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 48/86 [D loss: 0.6991480886936188, acc.: 46.39%] [G loss: 0.7043877243995667]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 49/86 [D loss: 0.6971751749515533, acc.: 47.95%] [G loss: 0.7028031945228577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 50/86 [D loss: 0.6980015635490417, acc.: 47.46%] [G loss: 0.7042946219444275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 51/86 [D loss: 0.6989628970623016, acc.: 46.29%] [G loss: 0.7083144187927246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 52/86 [D loss: 0.696609228849411, acc.: 48.24%] [G loss: 0.7079476118087769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 53/86 [D loss: 0.6970618069171906, acc.: 47.95%] [G loss: 0.7077370882034302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 54/86 [D loss: 0.6980451941490173, acc.: 48.19%] [G loss: 0.7072064876556396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 55/86 [D loss: 0.6962895393371582, acc.: 48.24%] [G loss: 0.7040511965751648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 56/86 [D loss: 0.7004727721214294, acc.: 43.90%] [G loss: 0.7072994112968445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 57/86 [D loss: 0.6986431777477264, acc.: 46.09%] [G loss: 0.7054044008255005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 58/86 [D loss: 0.7000102698802948, acc.: 46.09%] [G loss: 0.706584632396698]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 59/86 [D loss: 0.6977782249450684, acc.: 45.95%] [G loss: 0.7052738666534424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 60/86 [D loss: 0.6986573338508606, acc.: 47.71%] [G loss: 0.7041576504707336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 61/86 [D loss: 0.6991383135318756, acc.: 46.09%] [G loss: 0.7070544958114624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 62/86 [D loss: 0.6975354850292206, acc.: 48.44%] [G loss: 0.708326518535614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 63/86 [D loss: 0.6981163024902344, acc.: 47.36%] [G loss: 0.7068917155265808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 64/86 [D loss: 0.697152316570282, acc.: 47.80%] [G loss: 0.7050755023956299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 65/86 [D loss: 0.6983451247215271, acc.: 46.44%] [G loss: 0.703425407409668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 66/86 [D loss: 0.6996408700942993, acc.: 45.12%] [G loss: 0.7035229802131653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 67/86 [D loss: 0.6975610554218292, acc.: 48.24%] [G loss: 0.7026481032371521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 68/86 [D loss: 0.6981514096260071, acc.: 47.22%] [G loss: 0.704159140586853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 69/86 [D loss: 0.6981364488601685, acc.: 45.75%] [G loss: 0.7065441012382507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 70/86 [D loss: 0.6986739039421082, acc.: 46.92%] [G loss: 0.7099165916442871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 71/86 [D loss: 0.6976202428340912, acc.: 47.22%] [G loss: 0.7048646211624146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 72/86 [D loss: 0.6975536942481995, acc.: 48.54%] [G loss: 0.7059815526008606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 73/86 [D loss: 0.6995227634906769, acc.: 45.90%] [G loss: 0.7029227018356323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 74/86 [D loss: 0.6973224878311157, acc.: 48.97%] [G loss: 0.7056871652603149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 75/86 [D loss: 0.6976270079612732, acc.: 46.63%] [G loss: 0.706897497177124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 76/86 [D loss: 0.6986408829689026, acc.: 46.00%] [G loss: 0.7074525952339172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 77/86 [D loss: 0.6993181109428406, acc.: 45.61%] [G loss: 0.7065304517745972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 78/86 [D loss: 0.6982783675193787, acc.: 47.22%] [G loss: 0.70798259973526]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 79/86 [D loss: 0.6974433362483978, acc.: 47.80%] [G loss: 0.708451509475708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 80/86 [D loss: 0.6987610161304474, acc.: 46.53%] [G loss: 0.7069322466850281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 81/86 [D loss: 0.700418621301651, acc.: 44.68%] [G loss: 0.7071707248687744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 82/86 [D loss: 0.697313666343689, acc.: 48.83%] [G loss: 0.708834707736969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 83/86 [D loss: 0.6978846192359924, acc.: 47.51%] [G loss: 0.7107448577880859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 84/86 [D loss: 0.6975434124469757, acc.: 46.83%] [G loss: 0.7104910612106323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 14/200, Batch 85/86 [D loss: 0.6975483298301697, acc.: 46.63%] [G loss: 0.7088000178337097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 14/200, Batch 86/86 [D loss: 0.6976208388805389, acc.: 46.63%] [G loss: 0.7123507261276245]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 1/86 [D loss: 0.6987632215023041, acc.: 46.24%] [G loss: 0.7110409736633301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 2/86 [D loss: 0.6969544589519501, acc.: 47.95%] [G loss: 0.711781919002533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 3/86 [D loss: 0.6972898542881012, acc.: 48.44%] [G loss: 0.7098172307014465]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 4/86 [D loss: 0.6976149678230286, acc.: 47.07%] [G loss: 0.7117033004760742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 5/86 [D loss: 0.6957378685474396, acc.: 48.54%] [G loss: 0.7136248350143433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 6/86 [D loss: 0.6970629990100861, acc.: 46.88%] [G loss: 0.7104848623275757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 7/86 [D loss: 0.6978842914104462, acc.: 47.66%] [G loss: 0.7118057012557983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 8/86 [D loss: 0.6990240514278412, acc.: 45.46%] [G loss: 0.7091577053070068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 9/86 [D loss: 0.6975877285003662, acc.: 47.75%] [G loss: 0.7094006538391113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 10/86 [D loss: 0.6973102390766144, acc.: 46.48%] [G loss: 0.7086220979690552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 11/86 [D loss: 0.6986383497714996, acc.: 45.70%] [G loss: 0.7105159163475037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 12/86 [D loss: 0.6982286274433136, acc.: 46.34%] [G loss: 0.7078733444213867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 13/86 [D loss: 0.6983204483985901, acc.: 46.92%] [G loss: 0.7097582817077637]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 14/86 [D loss: 0.6977346837520599, acc.: 48.24%] [G loss: 0.7082762718200684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 15/86 [D loss: 0.6960800588130951, acc.: 48.73%] [G loss: 0.708226203918457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 16/86 [D loss: 0.698523074388504, acc.: 46.24%] [G loss: 0.7036747336387634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 17/86 [D loss: 0.6977447867393494, acc.: 47.61%] [G loss: 0.7029021382331848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 18/86 [D loss: 0.6968439817428589, acc.: 48.34%] [G loss: 0.7055252194404602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 19/86 [D loss: 0.6980621218681335, acc.: 47.41%] [G loss: 0.7044463753700256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 20/86 [D loss: 0.6989395320415497, acc.: 46.68%] [G loss: 0.7045769691467285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 21/86 [D loss: 0.6970367431640625, acc.: 47.95%] [G loss: 0.7037948966026306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 22/86 [D loss: 0.6988368630409241, acc.: 45.56%] [G loss: 0.702160656452179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 23/86 [D loss: 0.6986435055732727, acc.: 46.58%] [G loss: 0.7019233703613281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 24/86 [D loss: 0.6979960203170776, acc.: 47.36%] [G loss: 0.7056494951248169]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 25/86 [D loss: 0.6974854469299316, acc.: 48.83%] [G loss: 0.7056472301483154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 26/86 [D loss: 0.6981540620326996, acc.: 47.17%] [G loss: 0.7050896883010864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 27/86 [D loss: 0.6964814364910126, acc.: 48.10%] [G loss: 0.7030467987060547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 28/86 [D loss: 0.698749303817749, acc.: 46.19%] [G loss: 0.7010205984115601]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 29/86 [D loss: 0.7001327872276306, acc.: 45.12%] [G loss: 0.7040410041809082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 30/86 [D loss: 0.6964639127254486, acc.: 47.75%] [G loss: 0.7057030200958252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 31/86 [D loss: 0.698731929063797, acc.: 46.48%] [G loss: 0.7035756707191467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 32/86 [D loss: 0.698305606842041, acc.: 46.39%] [G loss: 0.7031003832817078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 33/86 [D loss: 0.6987135410308838, acc.: 47.02%] [G loss: 0.7024809718132019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 34/86 [D loss: 0.6981582939624786, acc.: 45.56%] [G loss: 0.7074285745620728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 35/86 [D loss: 0.6976496875286102, acc.: 46.73%] [G loss: 0.7059119343757629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 36/86 [D loss: 0.6994211673736572, acc.: 47.36%] [G loss: 0.7052742838859558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 37/86 [D loss: 0.6958514750003815, acc.: 49.37%] [G loss: 0.7091926336288452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 38/86 [D loss: 0.697546124458313, acc.: 47.61%] [G loss: 0.7063335180282593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 39/86 [D loss: 0.6979503035545349, acc.: 46.24%] [G loss: 0.7082995176315308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 40/86 [D loss: 0.6980322599411011, acc.: 47.36%] [G loss: 0.709432065486908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 41/86 [D loss: 0.696956217288971, acc.: 48.19%] [G loss: 0.7069839835166931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 42/86 [D loss: 0.6968375146389008, acc.: 48.10%] [G loss: 0.7091540098190308]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 43/86 [D loss: 0.6977109313011169, acc.: 46.97%] [G loss: 0.706721305847168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 44/86 [D loss: 0.6981484293937683, acc.: 44.53%] [G loss: 0.7076340913772583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 45/86 [D loss: 0.6975581347942352, acc.: 45.85%] [G loss: 0.7118450403213501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 46/86 [D loss: 0.6972205638885498, acc.: 47.22%] [G loss: 0.7072570323944092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 47/86 [D loss: 0.6987249851226807, acc.: 46.00%] [G loss: 0.707720160484314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 48/86 [D loss: 0.6972515285015106, acc.: 47.75%] [G loss: 0.7113661766052246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 49/86 [D loss: 0.6977607011795044, acc.: 46.68%] [G loss: 0.7088520526885986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 50/86 [D loss: 0.6986953914165497, acc.: 45.90%] [G loss: 0.7080824971199036]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 51/86 [D loss: 0.6989201009273529, acc.: 46.68%] [G loss: 0.7049050331115723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 52/86 [D loss: 0.6971166431903839, acc.: 46.34%] [G loss: 0.7059783935546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 53/86 [D loss: 0.6963799595832825, acc.: 47.12%] [G loss: 0.7085757255554199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 54/86 [D loss: 0.6968169212341309, acc.: 47.56%] [G loss: 0.7043407559394836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 55/86 [D loss: 0.6973760724067688, acc.: 47.66%] [G loss: 0.7069775462150574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 56/86 [D loss: 0.6974837481975555, acc.: 46.00%] [G loss: 0.7072345018386841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 57/86 [D loss: 0.6999680399894714, acc.: 46.24%] [G loss: 0.7056117653846741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 58/86 [D loss: 0.6964861154556274, acc.: 47.12%] [G loss: 0.705228328704834]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 59/86 [D loss: 0.697799801826477, acc.: 46.78%] [G loss: 0.7065580487251282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 60/86 [D loss: 0.6975165009498596, acc.: 47.02%] [G loss: 0.7062865495681763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 61/86 [D loss: 0.6966160535812378, acc.: 49.41%] [G loss: 0.7056581974029541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 62/86 [D loss: 0.6966190934181213, acc.: 48.49%] [G loss: 0.7052094340324402]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 63/86 [D loss: 0.6962219178676605, acc.: 48.14%] [G loss: 0.7041434049606323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 64/86 [D loss: 0.7000924050807953, acc.: 46.44%] [G loss: 0.7080036401748657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 65/86 [D loss: 0.698510468006134, acc.: 46.92%] [G loss: 0.7074195146560669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 66/86 [D loss: 0.6975043416023254, acc.: 47.90%] [G loss: 0.7054926156997681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 67/86 [D loss: 0.6969550549983978, acc.: 48.54%] [G loss: 0.7037181854248047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 68/86 [D loss: 0.6978407502174377, acc.: 47.07%] [G loss: 0.7049670219421387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 69/86 [D loss: 0.6988397538661957, acc.: 46.04%] [G loss: 0.7086620330810547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 70/86 [D loss: 0.6987218856811523, acc.: 46.29%] [G loss: 0.7064107060432434]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 71/86 [D loss: 0.6967902183532715, acc.: 46.92%] [G loss: 0.7070903778076172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 72/86 [D loss: 0.6973379552364349, acc.: 46.29%] [G loss: 0.7034763097763062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 73/86 [D loss: 0.6980287134647369, acc.: 46.68%] [G loss: 0.7052355408668518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 74/86 [D loss: 0.6978217959403992, acc.: 47.22%] [G loss: 0.7034618854522705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 75/86 [D loss: 0.7005136907100677, acc.: 45.36%] [G loss: 0.7043017745018005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 76/86 [D loss: 0.6990121006965637, acc.: 46.44%] [G loss: 0.7047221660614014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 77/86 [D loss: 0.6972098052501678, acc.: 47.46%] [G loss: 0.7065140008926392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 78/86 [D loss: 0.6969240605831146, acc.: 47.31%] [G loss: 0.7067786455154419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 79/86 [D loss: 0.6974901258945465, acc.: 47.12%] [G loss: 0.7053608894348145]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 80/86 [D loss: 0.6973984837532043, acc.: 48.29%] [G loss: 0.7025097012519836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 81/86 [D loss: 0.6994935274124146, acc.: 45.90%] [G loss: 0.700161337852478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 82/86 [D loss: 0.6991444528102875, acc.: 45.95%] [G loss: 0.70653235912323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 83/86 [D loss: 0.6972235441207886, acc.: 47.61%] [G loss: 0.7045295238494873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 84/86 [D loss: 0.697366863489151, acc.: 46.83%] [G loss: 0.7042436599731445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 15/200, Batch 85/86 [D loss: 0.6988576650619507, acc.: 45.70%] [G loss: 0.7049951553344727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 15/200, Batch 86/86 [D loss: 0.6983380615711212, acc.: 45.70%] [G loss: 0.7017682790756226]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 1/86 [D loss: 0.6991772949695587, acc.: 45.56%] [G loss: 0.7045168280601501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 2/86 [D loss: 0.6990072131156921, acc.: 47.41%] [G loss: 0.7014698386192322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 3/86 [D loss: 0.6971404552459717, acc.: 46.63%] [G loss: 0.7032781839370728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 4/86 [D loss: 0.697581946849823, acc.: 47.90%] [G loss: 0.7008834481239319]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 5/86 [D loss: 0.6990650296211243, acc.: 43.90%] [G loss: 0.7034854888916016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 6/86 [D loss: 0.6989061832427979, acc.: 46.53%] [G loss: 0.7008232474327087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 7/86 [D loss: 0.697699099779129, acc.: 46.58%] [G loss: 0.6992613077163696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 8/86 [D loss: 0.6999421417713165, acc.: 44.78%] [G loss: 0.7003353834152222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 9/86 [D loss: 0.6970944106578827, acc.: 46.58%] [G loss: 0.7042301297187805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 10/86 [D loss: 0.6981422305107117, acc.: 45.90%] [G loss: 0.7047039270401001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 11/86 [D loss: 0.699613630771637, acc.: 45.46%] [G loss: 0.7008805871009827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 12/86 [D loss: 0.6999377906322479, acc.: 45.17%] [G loss: 0.7030197381973267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 13/86 [D loss: 0.6985859870910645, acc.: 45.80%] [G loss: 0.7045294046401978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 14/86 [D loss: 0.6975104212760925, acc.: 47.02%] [G loss: 0.7041832208633423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 15/86 [D loss: 0.6983849406242371, acc.: 46.44%] [G loss: 0.7073677182197571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 16/86 [D loss: 0.6969887018203735, acc.: 47.56%] [G loss: 0.7030384540557861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 17/86 [D loss: 0.6984007954597473, acc.: 46.58%] [G loss: 0.7034301161766052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 18/86 [D loss: 0.6974146366119385, acc.: 47.31%] [G loss: 0.7054275870323181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 19/86 [D loss: 0.6968564391136169, acc.: 48.44%] [G loss: 0.7025229930877686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 20/86 [D loss: 0.6982307136058807, acc.: 46.34%] [G loss: 0.7074623703956604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 21/86 [D loss: 0.697145938873291, acc.: 47.56%] [G loss: 0.7079432606697083]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 22/86 [D loss: 0.6979818046092987, acc.: 46.09%] [G loss: 0.7044155597686768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 23/86 [D loss: 0.6978359520435333, acc.: 46.04%] [G loss: 0.7070893049240112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 24/86 [D loss: 0.698072075843811, acc.: 46.39%] [G loss: 0.7046670317649841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 25/86 [D loss: 0.6980486810207367, acc.: 47.75%] [G loss: 0.70534747838974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 26/86 [D loss: 0.6979798972606659, acc.: 47.75%] [G loss: 0.7060310244560242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 27/86 [D loss: 0.6987270712852478, acc.: 44.97%] [G loss: 0.705594003200531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 28/86 [D loss: 0.6981765925884247, acc.: 46.48%] [G loss: 0.7060147523880005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 29/86 [D loss: 0.6976290643215179, acc.: 46.39%] [G loss: 0.7033646702766418]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 30/86 [D loss: 0.6976039111614227, acc.: 47.02%] [G loss: 0.7062504291534424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 31/86 [D loss: 0.696558266878128, acc.: 48.05%] [G loss: 0.7050380706787109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 32/86 [D loss: 0.6983168125152588, acc.: 46.88%] [G loss: 0.7033594846725464]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 33/86 [D loss: 0.6994539201259613, acc.: 44.78%] [G loss: 0.7033048272132874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 34/86 [D loss: 0.6974767744541168, acc.: 46.58%] [G loss: 0.7032088041305542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 35/86 [D loss: 0.6970259547233582, acc.: 47.41%] [G loss: 0.7035258412361145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 36/86 [D loss: 0.6973161697387695, acc.: 45.85%] [G loss: 0.7026599645614624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 37/86 [D loss: 0.6983141005039215, acc.: 46.14%] [G loss: 0.7006649971008301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 38/86 [D loss: 0.6996996104717255, acc.: 45.85%] [G loss: 0.7052751183509827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 39/86 [D loss: 0.6992955207824707, acc.: 45.61%] [G loss: 0.7046319842338562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 40/86 [D loss: 0.696447342634201, acc.: 47.90%] [G loss: 0.7021900415420532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 41/86 [D loss: 0.6989125609397888, acc.: 46.88%] [G loss: 0.7028087377548218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 42/86 [D loss: 0.698406994342804, acc.: 44.63%] [G loss: 0.6993046402931213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 43/86 [D loss: 0.6983157396316528, acc.: 46.24%] [G loss: 0.7006137371063232]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 44/86 [D loss: 0.6968556642532349, acc.: 48.29%] [G loss: 0.698539674282074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 45/86 [D loss: 0.6992088556289673, acc.: 45.36%] [G loss: 0.7001274824142456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 46/86 [D loss: 0.6993376314640045, acc.: 44.78%] [G loss: 0.6987380981445312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 47/86 [D loss: 0.6993865370750427, acc.: 46.00%] [G loss: 0.701508641242981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 48/86 [D loss: 0.6990628242492676, acc.: 45.17%] [G loss: 0.7003048062324524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 49/86 [D loss: 0.6992116272449493, acc.: 45.65%] [G loss: 0.7009016871452332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 50/86 [D loss: 0.697441041469574, acc.: 46.04%] [G loss: 0.699776291847229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 51/86 [D loss: 0.6994274854660034, acc.: 45.21%] [G loss: 0.7003099322319031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 52/86 [D loss: 0.7001534998416901, acc.: 46.00%] [G loss: 0.7024747729301453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 53/86 [D loss: 0.6992901861667633, acc.: 45.65%] [G loss: 0.7014251947402954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 54/86 [D loss: 0.6978138089179993, acc.: 45.85%] [G loss: 0.7032522559165955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 55/86 [D loss: 0.6980507671833038, acc.: 46.88%] [G loss: 0.7002443671226501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 56/86 [D loss: 0.6992294490337372, acc.: 45.41%] [G loss: 0.7005364298820496]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 57/86 [D loss: 0.6973719596862793, acc.: 46.39%] [G loss: 0.7026641368865967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 58/86 [D loss: 0.6972230076789856, acc.: 47.51%] [G loss: 0.7001340389251709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 59/86 [D loss: 0.6976824700832367, acc.: 47.56%] [G loss: 0.7003554105758667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 60/86 [D loss: 0.6992155611515045, acc.: 44.53%] [G loss: 0.7035255432128906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 61/86 [D loss: 0.6995564997196198, acc.: 43.65%] [G loss: 0.6979513764381409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 62/86 [D loss: 0.6991045475006104, acc.: 45.07%] [G loss: 0.7002241015434265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 63/86 [D loss: 0.699191153049469, acc.: 45.41%] [G loss: 0.6984716653823853]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 64/86 [D loss: 0.6983238458633423, acc.: 45.26%] [G loss: 0.7010864019393921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 65/86 [D loss: 0.6995531320571899, acc.: 45.17%] [G loss: 0.6993274688720703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 66/86 [D loss: 0.6995751857757568, acc.: 46.19%] [G loss: 0.7031485438346863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 67/86 [D loss: 0.6976602077484131, acc.: 46.09%] [G loss: 0.7008234262466431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 68/86 [D loss: 0.6978136897087097, acc.: 47.75%] [G loss: 0.7019327878952026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 69/86 [D loss: 0.6979497373104095, acc.: 47.02%] [G loss: 0.7023043632507324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 70/86 [D loss: 0.697567343711853, acc.: 47.46%] [G loss: 0.7010817527770996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 71/86 [D loss: 0.6987778544425964, acc.: 46.24%] [G loss: 0.7013081908226013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 72/86 [D loss: 0.6984145939350128, acc.: 45.56%] [G loss: 0.7027051448822021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 73/86 [D loss: 0.6991878747940063, acc.: 47.07%] [G loss: 0.7037076950073242]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 74/86 [D loss: 0.7001106142997742, acc.: 43.99%] [G loss: 0.7008544206619263]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 75/86 [D loss: 0.6994956433773041, acc.: 43.95%] [G loss: 0.701378345489502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 76/86 [D loss: 0.6997798979282379, acc.: 43.46%] [G loss: 0.7055652737617493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 77/86 [D loss: 0.6977827847003937, acc.: 46.92%] [G loss: 0.7042757868766785]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 16/200, Batch 78/86 [D loss: 0.6979205310344696, acc.: 46.34%] [G loss: 0.7039340734481812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 79/86 [D loss: 0.6995013356208801, acc.: 43.95%] [G loss: 0.7050524950027466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 80/86 [D loss: 0.6983447968959808, acc.: 45.95%] [G loss: 0.6998063325881958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 81/86 [D loss: 0.6991824805736542, acc.: 45.07%] [G loss: 0.7050096392631531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 82/86 [D loss: 0.6993832588195801, acc.: 45.07%] [G loss: 0.7077739834785461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 83/86 [D loss: 0.698833167552948, acc.: 44.48%] [G loss: 0.7055891156196594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 84/86 [D loss: 0.697516143321991, acc.: 45.90%] [G loss: 0.703176736831665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 85/86 [D loss: 0.6982482671737671, acc.: 46.34%] [G loss: 0.7008812427520752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 16/200, Batch 86/86 [D loss: 0.69777911901474, acc.: 46.00%] [G loss: 0.7038140296936035]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 1/86 [D loss: 0.699517011642456, acc.: 45.21%] [G loss: 0.7070785164833069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 2/86 [D loss: 0.6981417238712311, acc.: 46.53%] [G loss: 0.7065884470939636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 3/86 [D loss: 0.6976865828037262, acc.: 46.83%] [G loss: 0.7031447291374207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 4/86 [D loss: 0.6976653039455414, acc.: 46.14%] [G loss: 0.6980358958244324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 5/86 [D loss: 0.7003459334373474, acc.: 44.63%] [G loss: 0.7013636827468872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 6/86 [D loss: 0.699182391166687, acc.: 45.85%] [G loss: 0.7040737867355347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 7/86 [D loss: 0.6966147422790527, acc.: 47.02%] [G loss: 0.7078313827514648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 8/86 [D loss: 0.6992913484573364, acc.: 45.12%] [G loss: 0.7044394612312317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 9/86 [D loss: 0.6980840563774109, acc.: 45.02%] [G loss: 0.6977834105491638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 10/86 [D loss: 0.7003847062587738, acc.: 44.58%] [G loss: 0.6971519589424133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 11/86 [D loss: 0.7001111805438995, acc.: 44.63%] [G loss: 0.7036470770835876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 12/86 [D loss: 0.7007418572902679, acc.: 44.63%] [G loss: 0.7018521428108215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 13/86 [D loss: 0.6971765756607056, acc.: 48.24%] [G loss: 0.7021144032478333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 14/86 [D loss: 0.6980630159378052, acc.: 46.24%] [G loss: 0.7007248401641846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 15/86 [D loss: 0.6994422078132629, acc.: 44.24%] [G loss: 0.6984168291091919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 16/86 [D loss: 0.6991605162620544, acc.: 46.19%] [G loss: 0.7026647329330444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 17/86 [D loss: 0.6985483765602112, acc.: 45.21%] [G loss: 0.7040582895278931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 18/86 [D loss: 0.6987113952636719, acc.: 45.07%] [G loss: 0.703092634677887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 19/86 [D loss: 0.6980274319648743, acc.: 45.85%] [G loss: 0.7010810971260071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 20/86 [D loss: 0.6989135444164276, acc.: 45.46%] [G loss: 0.7010588049888611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 21/86 [D loss: 0.6995630860328674, acc.: 44.63%] [G loss: 0.702010989189148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 22/86 [D loss: 0.6973112225532532, acc.: 46.58%] [G loss: 0.7033309936523438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 23/86 [D loss: 0.6973274052143097, acc.: 46.39%] [G loss: 0.7023882269859314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 24/86 [D loss: 0.6982079148292542, acc.: 47.27%] [G loss: 0.701420783996582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 25/86 [D loss: 0.6982385516166687, acc.: 46.63%] [G loss: 0.7025071382522583]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 26/86 [D loss: 0.6964754462242126, acc.: 47.90%] [G loss: 0.706086277961731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 27/86 [D loss: 0.6992656886577606, acc.: 44.24%] [G loss: 0.704578697681427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 28/86 [D loss: 0.6973766088485718, acc.: 46.73%] [G loss: 0.7016662955284119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 29/86 [D loss: 0.6989800035953522, acc.: 44.92%] [G loss: 0.7019467353820801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 30/86 [D loss: 0.6994199156761169, acc.: 44.78%] [G loss: 0.6994471549987793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 31/86 [D loss: 0.6995209157466888, acc.: 44.34%] [G loss: 0.7021567821502686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 32/86 [D loss: 0.6979009509086609, acc.: 45.17%] [G loss: 0.7022906541824341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 33/86 [D loss: 0.6973276138305664, acc.: 47.36%] [G loss: 0.7002781629562378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 34/86 [D loss: 0.6972415447235107, acc.: 47.07%] [G loss: 0.7022944092750549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 35/86 [D loss: 0.6991377174854279, acc.: 44.78%] [G loss: 0.7007772326469421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 36/86 [D loss: 0.699570894241333, acc.: 45.56%] [G loss: 0.7009967565536499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 37/86 [D loss: 0.6973831355571747, acc.: 47.22%] [G loss: 0.6998416781425476]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 38/86 [D loss: 0.699511706829071, acc.: 45.02%] [G loss: 0.7025434970855713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 39/86 [D loss: 0.6978378891944885, acc.: 46.14%] [G loss: 0.7019422054290771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 40/86 [D loss: 0.6981271207332611, acc.: 45.61%] [G loss: 0.7011370658874512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 41/86 [D loss: 0.6985988020896912, acc.: 44.24%] [G loss: 0.7014030814170837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 42/86 [D loss: 0.698614776134491, acc.: 46.09%] [G loss: 0.700821042060852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 43/86 [D loss: 0.6997739374637604, acc.: 44.53%] [G loss: 0.7002480030059814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 44/86 [D loss: 0.6981441676616669, acc.: 46.14%] [G loss: 0.7033014297485352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 45/86 [D loss: 0.697751522064209, acc.: 46.24%] [G loss: 0.7015708088874817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 46/86 [D loss: 0.6978855133056641, acc.: 46.53%] [G loss: 0.7013232707977295]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 47/86 [D loss: 0.7008211016654968, acc.: 43.41%] [G loss: 0.6995185613632202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 48/86 [D loss: 0.6974307596683502, acc.: 45.65%] [G loss: 0.7014617919921875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 49/86 [D loss: 0.6982187330722809, acc.: 44.82%] [G loss: 0.7032047510147095]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 50/86 [D loss: 0.6969780623912811, acc.: 47.90%] [G loss: 0.7014473676681519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 51/86 [D loss: 0.6984014213085175, acc.: 44.82%] [G loss: 0.7022762894630432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 52/86 [D loss: 0.6991026401519775, acc.: 45.26%] [G loss: 0.7016695141792297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 53/86 [D loss: 0.698270320892334, acc.: 46.19%] [G loss: 0.7003012895584106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 54/86 [D loss: 0.6988065838813782, acc.: 46.29%] [G loss: 0.7004818916320801]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 55/86 [D loss: 0.6963615715503693, acc.: 47.85%] [G loss: 0.7025890350341797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 56/86 [D loss: 0.6986058354377747, acc.: 45.51%] [G loss: 0.7015416622161865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 57/86 [D loss: 0.6986491084098816, acc.: 44.38%] [G loss: 0.702479898929596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 58/86 [D loss: 0.6983362436294556, acc.: 45.21%] [G loss: 0.7044634819030762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 59/86 [D loss: 0.697860062122345, acc.: 46.58%] [G loss: 0.70133376121521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 60/86 [D loss: 0.6998481452465057, acc.: 46.00%] [G loss: 0.7018465399742126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 61/86 [D loss: 0.7004962861537933, acc.: 44.48%] [G loss: 0.7061030864715576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 62/86 [D loss: 0.6990762650966644, acc.: 44.97%] [G loss: 0.70014888048172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 63/86 [D loss: 0.6988031268119812, acc.: 44.97%] [G loss: 0.7013786435127258]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 64/86 [D loss: 0.6968649327754974, acc.: 47.02%] [G loss: 0.7031562924385071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 65/86 [D loss: 0.6987980902194977, acc.: 44.73%] [G loss: 0.7064216136932373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 66/86 [D loss: 0.6972484886646271, acc.: 45.65%] [G loss: 0.7046716213226318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 67/86 [D loss: 0.6973288059234619, acc.: 47.27%] [G loss: 0.7035315036773682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 68/86 [D loss: 0.6992588043212891, acc.: 44.63%] [G loss: 0.7037570476531982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 69/86 [D loss: 0.6985632181167603, acc.: 44.04%] [G loss: 0.7029809355735779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 70/86 [D loss: 0.6973214447498322, acc.: 46.09%] [G loss: 0.7033780217170715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 71/86 [D loss: 0.6985029876232147, acc.: 45.75%] [G loss: 0.703567385673523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 72/86 [D loss: 0.6991652250289917, acc.: 44.68%] [G loss: 0.7050513029098511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 73/86 [D loss: 0.6991503834724426, acc.: 45.61%] [G loss: 0.6999726295471191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 74/86 [D loss: 0.6984285414218903, acc.: 45.90%] [G loss: 0.7031307816505432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 75/86 [D loss: 0.6989723742008209, acc.: 45.90%] [G loss: 0.7019214630126953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 76/86 [D loss: 0.6977306604385376, acc.: 46.00%] [G loss: 0.7023360729217529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 77/86 [D loss: 0.7005151510238647, acc.: 43.21%] [G loss: 0.7021757960319519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 78/86 [D loss: 0.6973573863506317, acc.: 47.61%] [G loss: 0.7026734352111816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 79/86 [D loss: 0.6972449123859406, acc.: 46.92%] [G loss: 0.7008600234985352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 80/86 [D loss: 0.6989416480064392, acc.: 45.02%] [G loss: 0.7011510133743286]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 81/86 [D loss: 0.6979875266551971, acc.: 46.19%] [G loss: 0.7019425630569458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 82/86 [D loss: 0.6980539858341217, acc.: 45.41%] [G loss: 0.6998465657234192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 83/86 [D loss: 0.7004271149635315, acc.: 43.95%] [G loss: 0.7021616697311401]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 84/86 [D loss: 0.6983073353767395, acc.: 45.75%] [G loss: 0.7026652097702026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 17/200, Batch 85/86 [D loss: 0.6975915431976318, acc.: 48.19%] [G loss: 0.6988565921783447]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 17/200, Batch 86/86 [D loss: 0.6993881464004517, acc.: 43.41%] [G loss: 0.6991029977798462]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 18/200, Batch 1/86 [D loss: 0.6996645033359528, acc.: 45.61%] [G loss: 0.699725866317749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 2/86 [D loss: 0.6978113949298859, acc.: 45.95%] [G loss: 0.7007485628128052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 3/86 [D loss: 0.698266476392746, acc.: 43.70%] [G loss: 0.6999747157096863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 4/86 [D loss: 0.6985687911510468, acc.: 44.78%] [G loss: 0.6999579668045044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 5/86 [D loss: 0.6983029544353485, acc.: 44.14%] [G loss: 0.6994068026542664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 6/86 [D loss: 0.6978170871734619, acc.: 45.21%] [G loss: 0.7027503848075867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 7/86 [D loss: 0.7002871930599213, acc.: 43.55%] [G loss: 0.7028740644454956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 8/86 [D loss: 0.6978688836097717, acc.: 45.46%] [G loss: 0.7007099986076355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 9/86 [D loss: 0.6984816193580627, acc.: 45.17%] [G loss: 0.7010471224784851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 10/86 [D loss: 0.6993403136730194, acc.: 45.41%] [G loss: 0.6994525194168091]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 11/86 [D loss: 0.6982318460941315, acc.: 45.80%] [G loss: 0.6987630128860474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 12/86 [D loss: 0.6986376643180847, acc.: 44.34%] [G loss: 0.7031402587890625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 13/86 [D loss: 0.6982826292514801, acc.: 45.65%] [G loss: 0.7005208134651184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 14/86 [D loss: 0.6977512836456299, acc.: 44.92%] [G loss: 0.6986101269721985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 15/86 [D loss: 0.6990609765052795, acc.: 44.87%] [G loss: 0.6988220810890198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 16/86 [D loss: 0.6993290781974792, acc.: 44.43%] [G loss: 0.7007132172584534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 17/86 [D loss: 0.6977552175521851, acc.: 47.31%] [G loss: 0.7011011838912964]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 18/86 [D loss: 0.6979784369468689, acc.: 45.61%] [G loss: 0.7021576166152954]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 19/86 [D loss: 0.6983928084373474, acc.: 44.48%] [G loss: 0.6976915001869202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 20/86 [D loss: 0.7009865641593933, acc.: 43.60%] [G loss: 0.6981213688850403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 21/86 [D loss: 0.6997874081134796, acc.: 45.12%] [G loss: 0.7016423940658569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 22/86 [D loss: 0.6974453926086426, acc.: 46.48%] [G loss: 0.7016940116882324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 23/86 [D loss: 0.6994850933551788, acc.: 44.24%] [G loss: 0.7011781930923462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 24/86 [D loss: 0.6993271112442017, acc.: 43.99%] [G loss: 0.7001258134841919]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 25/86 [D loss: 0.7004033327102661, acc.: 43.51%] [G loss: 0.6953691244125366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 26/86 [D loss: 0.698029488325119, acc.: 45.41%] [G loss: 0.6993919014930725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 27/86 [D loss: 0.6979537308216095, acc.: 45.61%] [G loss: 0.7023835182189941]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 28/86 [D loss: 0.69715616106987, acc.: 46.53%] [G loss: 0.6989672183990479]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 29/86 [D loss: 0.6977652013301849, acc.: 46.09%] [G loss: 0.6983530521392822]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 30/86 [D loss: 0.6996704936027527, acc.: 43.95%] [G loss: 0.6961923837661743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 31/86 [D loss: 0.7003122866153717, acc.: 45.56%] [G loss: 0.7016352415084839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 32/86 [D loss: 0.6977260112762451, acc.: 46.73%] [G loss: 0.7026928067207336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 33/86 [D loss: 0.6988584995269775, acc.: 46.58%] [G loss: 0.7020798325538635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 34/86 [D loss: 0.6980913281440735, acc.: 45.65%] [G loss: 0.6986779570579529]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 35/86 [D loss: 0.7007901072502136, acc.: 43.51%] [G loss: 0.6977823972702026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 36/86 [D loss: 0.7004808783531189, acc.: 44.87%] [G loss: 0.7044559121131897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 37/86 [D loss: 0.6973251104354858, acc.: 46.04%] [G loss: 0.7002565264701843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 38/86 [D loss: 0.6978021562099457, acc.: 45.65%] [G loss: 0.7027061581611633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 39/86 [D loss: 0.6988964080810547, acc.: 44.63%] [G loss: 0.696563720703125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 40/86 [D loss: 0.6995627284049988, acc.: 45.02%] [G loss: 0.6981943249702454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 41/86 [D loss: 0.6985491812229156, acc.: 46.19%] [G loss: 0.7045510411262512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 42/86 [D loss: 0.6988138854503632, acc.: 44.78%] [G loss: 0.7054872512817383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 43/86 [D loss: 0.6971902251243591, acc.: 46.34%] [G loss: 0.7041422128677368]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 44/86 [D loss: 0.6989467740058899, acc.: 44.63%] [G loss: 0.6995874643325806]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 45/86 [D loss: 0.7002457976341248, acc.: 42.87%] [G loss: 0.7018241286277771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 46/86 [D loss: 0.6986055672168732, acc.: 45.85%] [G loss: 0.7030434608459473]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 47/86 [D loss: 0.6979663074016571, acc.: 45.51%] [G loss: 0.7048307657241821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 48/86 [D loss: 0.6973532140254974, acc.: 46.24%] [G loss: 0.7026352286338806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 49/86 [D loss: 0.698161780834198, acc.: 44.38%] [G loss: 0.6996674537658691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 50/86 [D loss: 0.6995201706886292, acc.: 45.31%] [G loss: 0.7025861144065857]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 51/86 [D loss: 0.6990551650524139, acc.: 45.36%] [G loss: 0.7064767479896545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 52/86 [D loss: 0.6975246965885162, acc.: 46.83%] [G loss: 0.7019802331924438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 53/86 [D loss: 0.6962564289569855, acc.: 48.19%] [G loss: 0.7005866765975952]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 54/86 [D loss: 0.6987640559673309, acc.: 45.31%] [G loss: 0.6972986459732056]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 55/86 [D loss: 0.6994317471981049, acc.: 44.53%] [G loss: 0.7027378082275391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 56/86 [D loss: 0.6987424790859222, acc.: 46.14%] [G loss: 0.703938364982605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 57/86 [D loss: 0.6978494226932526, acc.: 44.58%] [G loss: 0.7044312953948975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 58/86 [D loss: 0.6966586709022522, acc.: 48.10%] [G loss: 0.7006667852401733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 59/86 [D loss: 0.7001293897628784, acc.: 43.46%] [G loss: 0.6980002522468567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 60/86 [D loss: 0.7013725340366364, acc.: 42.68%] [G loss: 0.7004229426383972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 61/86 [D loss: 0.6988945007324219, acc.: 45.26%] [G loss: 0.7047407031059265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 62/86 [D loss: 0.696731835603714, acc.: 47.12%] [G loss: 0.7023835778236389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 63/86 [D loss: 0.6997600197792053, acc.: 45.61%] [G loss: 0.7009290456771851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 64/86 [D loss: 0.6984034776687622, acc.: 46.39%] [G loss: 0.6962045431137085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 65/86 [D loss: 0.6999499797821045, acc.: 44.43%] [G loss: 0.6945585012435913]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 66/86 [D loss: 0.7012375593185425, acc.: 44.29%] [G loss: 0.703324019908905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 67/86 [D loss: 0.6970925033092499, acc.: 45.75%] [G loss: 0.7038207650184631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 68/86 [D loss: 0.6973586082458496, acc.: 45.46%] [G loss: 0.7025482654571533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 69/86 [D loss: 0.6987152695655823, acc.: 44.68%] [G loss: 0.6989849805831909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 70/86 [D loss: 0.6989214420318604, acc.: 47.12%] [G loss: 0.6950664520263672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 71/86 [D loss: 0.699300080537796, acc.: 45.61%] [G loss: 0.7051171064376831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 72/86 [D loss: 0.6962820589542389, acc.: 47.66%] [G loss: 0.7031415700912476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 73/86 [D loss: 0.6973105669021606, acc.: 46.48%] [G loss: 0.7045223712921143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 74/86 [D loss: 0.6984211802482605, acc.: 45.46%] [G loss: 0.7024260759353638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 75/86 [D loss: 0.6994675397872925, acc.: 43.95%] [G loss: 0.697584331035614]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 76/86 [D loss: 0.6981328725814819, acc.: 44.82%] [G loss: 0.7031187415122986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 77/86 [D loss: 0.6990168988704681, acc.: 44.97%] [G loss: 0.7037461400032043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 78/86 [D loss: 0.6982050836086273, acc.: 45.51%] [G loss: 0.7047119140625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 79/86 [D loss: 0.697626382112503, acc.: 46.78%] [G loss: 0.7042152285575867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 80/86 [D loss: 0.7003097236156464, acc.: 44.38%] [G loss: 0.6999711394309998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 81/86 [D loss: 0.6994679272174835, acc.: 44.38%] [G loss: 0.6988481283187866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 82/86 [D loss: 0.6979172825813293, acc.: 45.95%] [G loss: 0.7038490772247314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 83/86 [D loss: 0.6971602141857147, acc.: 47.22%] [G loss: 0.7038662433624268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 84/86 [D loss: 0.6993399560451508, acc.: 43.60%] [G loss: 0.7011429071426392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 18/200, Batch 85/86 [D loss: 0.699419766664505, acc.: 44.78%] [G loss: 0.7009751796722412]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 18/200, Batch 86/86 [D loss: 0.6978047490119934, acc.: 46.44%] [G loss: 0.7007246017456055]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 1/86 [D loss: 0.6978490352630615, acc.: 46.78%] [G loss: 0.7016857862472534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 2/86 [D loss: 0.6977448463439941, acc.: 45.85%] [G loss: 0.7023954391479492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 3/86 [D loss: 0.6981105208396912, acc.: 45.56%] [G loss: 0.7004555463790894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 4/86 [D loss: 0.6976024806499481, acc.: 45.70%] [G loss: 0.6988126039505005]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 5/86 [D loss: 0.6990506649017334, acc.: 43.99%] [G loss: 0.7011622190475464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 6/86 [D loss: 0.7004857063293457, acc.: 43.55%] [G loss: 0.7029728889465332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 7/86 [D loss: 0.6992021203041077, acc.: 43.65%] [G loss: 0.7038300037384033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 8/86 [D loss: 0.6991870105266571, acc.: 45.02%] [G loss: 0.6985906362533569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 9/86 [D loss: 0.6991665065288544, acc.: 44.68%] [G loss: 0.7003646492958069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 10/86 [D loss: 0.6988269090652466, acc.: 45.41%] [G loss: 0.7046072483062744]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 11/86 [D loss: 0.6967534720897675, acc.: 46.97%] [G loss: 0.7049189209938049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 12/86 [D loss: 0.6966164112091064, acc.: 45.95%] [G loss: 0.703584611415863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 13/86 [D loss: 0.6977525353431702, acc.: 47.56%] [G loss: 0.7007354497909546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 14/86 [D loss: 0.7002420425415039, acc.: 43.99%] [G loss: 0.6994920372962952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 15/86 [D loss: 0.698377251625061, acc.: 46.97%] [G loss: 0.7058670520782471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 16/86 [D loss: 0.699178546667099, acc.: 44.24%] [G loss: 0.7047823071479797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 17/86 [D loss: 0.6977637112140656, acc.: 45.80%] [G loss: 0.7042898535728455]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 18/86 [D loss: 0.6977787017822266, acc.: 46.29%] [G loss: 0.6989470720291138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 19/86 [D loss: 0.7010743319988251, acc.: 44.48%] [G loss: 0.6980539560317993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 20/86 [D loss: 0.6998492181301117, acc.: 44.82%] [G loss: 0.7054383754730225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 21/86 [D loss: 0.6976669132709503, acc.: 46.29%] [G loss: 0.7033738493919373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 22/86 [D loss: 0.6996727883815765, acc.: 44.24%] [G loss: 0.7012247443199158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 23/86 [D loss: 0.6971464455127716, acc.: 45.80%] [G loss: 0.697475790977478]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 24/86 [D loss: 0.7015025019645691, acc.: 44.09%] [G loss: 0.689541220664978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 25/86 [D loss: 0.7006178498268127, acc.: 45.21%] [G loss: 0.7068314552307129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 26/86 [D loss: 0.6946022212505341, acc.: 48.24%] [G loss: 0.700505256652832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 27/86 [D loss: 0.7010886371135712, acc.: 42.04%] [G loss: 0.7000018358230591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 28/86 [D loss: 0.6989828050136566, acc.: 43.80%] [G loss: 0.6982476711273193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 29/86 [D loss: 0.6977440714836121, acc.: 45.36%] [G loss: 0.6865275502204895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 30/86 [D loss: 0.7039072215557098, acc.: 42.72%] [G loss: 0.6969566345214844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 31/86 [D loss: 0.6951094269752502, acc.: 49.07%] [G loss: 0.7044833302497864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 32/86 [D loss: 0.6993028223514557, acc.: 43.36%] [G loss: 0.7010815143585205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 33/86 [D loss: 0.6997136473655701, acc.: 45.02%] [G loss: 0.7005345225334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 34/86 [D loss: 0.6981372237205505, acc.: 46.83%] [G loss: 0.6917714476585388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 35/86 [D loss: 0.700760155916214, acc.: 43.95%] [G loss: 0.6924445629119873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 36/86 [D loss: 0.6988163888454437, acc.: 44.58%] [G loss: 0.7054930925369263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 37/86 [D loss: 0.6975786685943604, acc.: 46.14%] [G loss: 0.702457845211029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 38/86 [D loss: 0.698582798242569, acc.: 45.56%] [G loss: 0.7003641724586487]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 39/86 [D loss: 0.6985326409339905, acc.: 44.73%] [G loss: 0.6966051459312439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 40/86 [D loss: 0.700878769159317, acc.: 43.95%] [G loss: 0.6943712830543518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 41/86 [D loss: 0.7003738284111023, acc.: 45.02%] [G loss: 0.7010138630867004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 42/86 [D loss: 0.6954023241996765, acc.: 47.61%] [G loss: 0.7031227946281433]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 43/86 [D loss: 0.6976318061351776, acc.: 45.56%] [G loss: 0.7014641165733337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 44/86 [D loss: 0.699463278055191, acc.: 43.55%] [G loss: 0.7028242945671082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 45/86 [D loss: 0.6985255181789398, acc.: 44.78%] [G loss: 0.6986784934997559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 46/86 [D loss: 0.6999513804912567, acc.: 44.63%] [G loss: 0.7021012306213379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 47/86 [D loss: 0.6995596289634705, acc.: 45.07%] [G loss: 0.7044524550437927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 48/86 [D loss: 0.6976848244667053, acc.: 45.26%] [G loss: 0.704758882522583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 49/86 [D loss: 0.698188066482544, acc.: 45.46%] [G loss: 0.7026536464691162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 50/86 [D loss: 0.6996899843215942, acc.: 43.95%] [G loss: 0.7002153992652893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 51/86 [D loss: 0.7004713714122772, acc.: 41.70%] [G loss: 0.7028377056121826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 52/86 [D loss: 0.697986900806427, acc.: 45.61%] [G loss: 0.7056463360786438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 53/86 [D loss: 0.6979226768016815, acc.: 45.17%] [G loss: 0.7043785452842712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 54/86 [D loss: 0.6991772651672363, acc.: 45.21%] [G loss: 0.7032556533813477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 55/86 [D loss: 0.6977637708187103, acc.: 46.14%] [G loss: 0.7002476453781128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 56/86 [D loss: 0.6987016499042511, acc.: 44.24%] [G loss: 0.7028508186340332]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 57/86 [D loss: 0.6993902027606964, acc.: 44.53%] [G loss: 0.7026749849319458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 58/86 [D loss: 0.696925550699234, acc.: 45.36%] [G loss: 0.7035006880760193]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 59/86 [D loss: 0.697757363319397, acc.: 45.70%] [G loss: 0.7054197192192078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 60/86 [D loss: 0.6983542144298553, acc.: 44.19%] [G loss: 0.7053394317626953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 61/86 [D loss: 0.6984519958496094, acc.: 46.00%] [G loss: 0.7029966711997986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 62/86 [D loss: 0.6973069608211517, acc.: 46.88%] [G loss: 0.7015571594238281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 63/86 [D loss: 0.6991516053676605, acc.: 43.85%] [G loss: 0.7036989331245422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 64/86 [D loss: 0.6990927755832672, acc.: 44.73%] [G loss: 0.7042421698570251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 65/86 [D loss: 0.6984068751335144, acc.: 45.70%] [G loss: 0.7016503214836121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 66/86 [D loss: 0.696957916021347, acc.: 46.34%] [G loss: 0.7017099857330322]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 67/86 [D loss: 0.698261171579361, acc.: 45.36%] [G loss: 0.7024375796318054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 68/86 [D loss: 0.699979841709137, acc.: 43.16%] [G loss: 0.7029047012329102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 69/86 [D loss: 0.6977812349796295, acc.: 47.22%] [G loss: 0.7040180563926697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 70/86 [D loss: 0.6967889666557312, acc.: 47.22%] [G loss: 0.7022086381912231]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 71/86 [D loss: 0.6971619725227356, acc.: 47.41%] [G loss: 0.6997137069702148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 72/86 [D loss: 0.6979185044765472, acc.: 46.00%] [G loss: 0.6992840766906738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 73/86 [D loss: 0.6985773146152496, acc.: 44.97%] [G loss: 0.7042540907859802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 74/86 [D loss: 0.6981601715087891, acc.: 44.09%] [G loss: 0.7030745148658752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 75/86 [D loss: 0.6984694004058838, acc.: 44.14%] [G loss: 0.7028969526290894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 76/86 [D loss: 0.6976224184036255, acc.: 46.48%] [G loss: 0.6981640458106995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 77/86 [D loss: 0.6989411115646362, acc.: 45.02%] [G loss: 0.7010858058929443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 78/86 [D loss: 0.6990495920181274, acc.: 43.02%] [G loss: 0.7043476104736328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 79/86 [D loss: 0.6956536173820496, acc.: 48.73%] [G loss: 0.7016417980194092]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 80/86 [D loss: 0.6968701481819153, acc.: 47.02%] [G loss: 0.7026088237762451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 81/86 [D loss: 0.6972706317901611, acc.: 45.46%] [G loss: 0.6965973973274231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 82/86 [D loss: 0.6999769508838654, acc.: 43.95%] [G loss: 0.6953569650650024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 83/86 [D loss: 0.70103919506073, acc.: 44.78%] [G loss: 0.7024399042129517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 84/86 [D loss: 0.697666734457016, acc.: 44.34%] [G loss: 0.7023369073867798]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 19/200, Batch 85/86 [D loss: 0.6979377865791321, acc.: 44.92%] [G loss: 0.7024059891700745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 19/200, Batch 86/86 [D loss: 0.6987200379371643, acc.: 45.02%] [G loss: 0.7002612948417664]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 1/86 [D loss: 0.6996003091335297, acc.: 44.73%] [G loss: 0.6950069665908813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 2/86 [D loss: 0.7002163529396057, acc.: 44.24%] [G loss: 0.7017542719841003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 3/86 [D loss: 0.6974258720874786, acc.: 46.78%] [G loss: 0.7015337347984314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 4/86 [D loss: 0.6987908482551575, acc.: 44.48%] [G loss: 0.7023473381996155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 5/86 [D loss: 0.697676032781601, acc.: 45.75%] [G loss: 0.6962090730667114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 6/86 [D loss: 0.7012318670749664, acc.: 41.94%] [G loss: 0.6941241025924683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 7/86 [D loss: 0.7001471817493439, acc.: 45.21%] [G loss: 0.6991284489631653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 8/86 [D loss: 0.6960834264755249, acc.: 47.17%] [G loss: 0.7013819813728333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 9/86 [D loss: 0.6962748765945435, acc.: 47.36%] [G loss: 0.7000353932380676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 10/86 [D loss: 0.6987428069114685, acc.: 44.53%] [G loss: 0.697394609451294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 11/86 [D loss: 0.698859304189682, acc.: 44.24%] [G loss: 0.6983458995819092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 12/86 [D loss: 0.699155867099762, acc.: 46.14%] [G loss: 0.6979271769523621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 13/86 [D loss: 0.6976643800735474, acc.: 45.17%] [G loss: 0.7024767398834229]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 14/86 [D loss: 0.6991557478904724, acc.: 43.07%] [G loss: 0.7022554874420166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 15/86 [D loss: 0.6977234184741974, acc.: 47.36%] [G loss: 0.7002969980239868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 16/86 [D loss: 0.6976401209831238, acc.: 45.26%] [G loss: 0.6964560747146606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 17/86 [D loss: 0.7006067633628845, acc.: 43.75%] [G loss: 0.6985041499137878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 18/86 [D loss: 0.6958474516868591, acc.: 48.29%] [G loss: 0.7009280920028687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 19/86 [D loss: 0.697148859500885, acc.: 47.02%] [G loss: 0.7028412222862244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 20/86 [D loss: 0.6970078349113464, acc.: 46.88%] [G loss: 0.6990443468093872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 21/86 [D loss: 0.6973443925380707, acc.: 46.68%] [G loss: 0.6994485855102539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 22/86 [D loss: 0.7004311382770538, acc.: 43.31%] [G loss: 0.7001591324806213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 23/86 [D loss: 0.6975221335887909, acc.: 47.12%] [G loss: 0.7026661038398743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 24/86 [D loss: 0.698830246925354, acc.: 45.41%] [G loss: 0.7009361982345581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 25/86 [D loss: 0.6991391181945801, acc.: 45.65%] [G loss: 0.701429545879364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 26/86 [D loss: 0.6974653005599976, acc.: 46.24%] [G loss: 0.7009099125862122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 27/86 [D loss: 0.6982080340385437, acc.: 45.51%] [G loss: 0.7016888856887817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 28/86 [D loss: 0.6985822021961212, acc.: 45.70%] [G loss: 0.700753927230835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 29/86 [D loss: 0.6989506781101227, acc.: 44.19%] [G loss: 0.7008141875267029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 30/86 [D loss: 0.6958662569522858, acc.: 48.34%] [G loss: 0.701511025428772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 31/86 [D loss: 0.6970938742160797, acc.: 46.58%] [G loss: 0.7000053524971008]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 32/86 [D loss: 0.6996555924415588, acc.: 43.07%] [G loss: 0.6980077028274536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 33/86 [D loss: 0.6984860002994537, acc.: 44.19%] [G loss: 0.7013906240463257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 34/86 [D loss: 0.6974993050098419, acc.: 45.70%] [G loss: 0.7017607688903809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 35/86 [D loss: 0.69835364818573, acc.: 46.04%] [G loss: 0.7009754180908203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 36/86 [D loss: 0.6980636417865753, acc.: 45.56%] [G loss: 0.7011754512786865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 37/86 [D loss: 0.6994243860244751, acc.: 44.87%] [G loss: 0.6997100710868835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 38/86 [D loss: 0.6980307400226593, acc.: 44.04%] [G loss: 0.702120304107666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 39/86 [D loss: 0.6977305114269257, acc.: 45.51%] [G loss: 0.701164186000824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 40/86 [D loss: 0.6976374089717865, acc.: 44.87%] [G loss: 0.6989059448242188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 41/86 [D loss: 0.6966060101985931, acc.: 46.44%] [G loss: 0.6989549994468689]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 42/86 [D loss: 0.6981830894947052, acc.: 45.90%] [G loss: 0.700244128704071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 43/86 [D loss: 0.6975359916687012, acc.: 47.31%] [G loss: 0.7027151584625244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 44/86 [D loss: 0.6985874772071838, acc.: 44.87%] [G loss: 0.7020179629325867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 45/86 [D loss: 0.6969419717788696, acc.: 46.73%] [G loss: 0.7004377841949463]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 46/86 [D loss: 0.6973219513893127, acc.: 46.83%] [G loss: 0.6987223625183105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 47/86 [D loss: 0.6982395052909851, acc.: 46.14%] [G loss: 0.6998688578605652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 48/86 [D loss: 0.6982065141201019, acc.: 46.83%] [G loss: 0.7020564675331116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 49/86 [D loss: 0.6969736516475677, acc.: 48.34%] [G loss: 0.7016831636428833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 50/86 [D loss: 0.6972425580024719, acc.: 47.31%] [G loss: 0.700752854347229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 51/86 [D loss: 0.6984237730503082, acc.: 46.39%] [G loss: 0.7005112171173096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 52/86 [D loss: 0.6986124515533447, acc.: 44.63%] [G loss: 0.6994199752807617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 53/86 [D loss: 0.6973700821399689, acc.: 45.95%] [G loss: 0.7009328007698059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 54/86 [D loss: 0.6970311403274536, acc.: 46.92%] [G loss: 0.6997413635253906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 55/86 [D loss: 0.6965019702911377, acc.: 46.83%] [G loss: 0.6996177434921265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 56/86 [D loss: 0.6980219483375549, acc.: 45.41%] [G loss: 0.6999369859695435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 57/86 [D loss: 0.6981185972690582, acc.: 45.80%] [G loss: 0.6992068290710449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 58/86 [D loss: 0.6978503167629242, acc.: 46.00%] [G loss: 0.6987046003341675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 59/86 [D loss: 0.6980473399162292, acc.: 45.17%] [G loss: 0.7002991437911987]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 60/86 [D loss: 0.6968500316143036, acc.: 45.75%] [G loss: 0.6996051669120789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 61/86 [D loss: 0.6981363594532013, acc.: 44.78%] [G loss: 0.6982332468032837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 62/86 [D loss: 0.6974563002586365, acc.: 46.68%] [G loss: 0.7009152770042419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 63/86 [D loss: 0.6966218054294586, acc.: 47.17%] [G loss: 0.6997440457344055]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 64/86 [D loss: 0.6982341706752777, acc.: 44.73%] [G loss: 0.7007309198379517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 65/86 [D loss: 0.6977647542953491, acc.: 44.82%] [G loss: 0.7001141905784607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 66/86 [D loss: 0.6959353089332581, acc.: 48.44%] [G loss: 0.6990194320678711]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 67/86 [D loss: 0.6983573734760284, acc.: 46.14%] [G loss: 0.6980903744697571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 68/86 [D loss: 0.699009358882904, acc.: 45.70%] [G loss: 0.7003283500671387]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 69/86 [D loss: 0.6987218856811523, acc.: 45.36%] [G loss: 0.6992999315261841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 70/86 [D loss: 0.6967057287693024, acc.: 47.56%] [G loss: 0.6993475556373596]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 71/86 [D loss: 0.6962209045886993, acc.: 47.85%] [G loss: 0.6983349919319153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 72/86 [D loss: 0.6977694034576416, acc.: 45.80%] [G loss: 0.696465015411377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 73/86 [D loss: 0.6980702877044678, acc.: 44.87%] [G loss: 0.6999197006225586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 74/86 [D loss: 0.6977055370807648, acc.: 44.78%] [G loss: 0.7000499963760376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 75/86 [D loss: 0.6971107423305511, acc.: 46.04%] [G loss: 0.698195219039917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 76/86 [D loss: 0.6976476907730103, acc.: 44.87%] [G loss: 0.6975130438804626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 77/86 [D loss: 0.6972261667251587, acc.: 47.71%] [G loss: 0.6960406303405762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 78/86 [D loss: 0.6993091702461243, acc.: 45.21%] [G loss: 0.6963099241256714]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 79/86 [D loss: 0.695555180311203, acc.: 47.22%] [G loss: 0.6990489959716797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 80/86 [D loss: 0.6952523291110992, acc.: 48.78%] [G loss: 0.7017463445663452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 81/86 [D loss: 0.696277529001236, acc.: 47.61%] [G loss: 0.7004718780517578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 20/200, Batch 82/86 [D loss: 0.6972029507160187, acc.: 46.58%] [G loss: 0.6967952847480774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 83/86 [D loss: 0.698968917131424, acc.: 44.53%] [G loss: 0.6970233917236328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 84/86 [D loss: 0.6978346705436707, acc.: 45.26%] [G loss: 0.7017560005187988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 85/86 [D loss: 0.6963966190814972, acc.: 46.58%] [G loss: 0.7000846266746521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 20/200, Batch 86/86 [D loss: 0.6970667243003845, acc.: 46.39%] [G loss: 0.698523223400116]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 1/86 [D loss: 0.6970346570014954, acc.: 46.58%] [G loss: 0.6981525421142578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 2/86 [D loss: 0.6985265016555786, acc.: 45.36%] [G loss: 0.6988657116889954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 3/86 [D loss: 0.6979061663150787, acc.: 45.90%] [G loss: 0.7020477056503296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 4/86 [D loss: 0.6982200741767883, acc.: 45.51%] [G loss: 0.7000836133956909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 5/86 [D loss: 0.6961441338062286, acc.: 47.31%] [G loss: 0.7018877863883972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 6/86 [D loss: 0.6971514225006104, acc.: 45.36%] [G loss: 0.6995421051979065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 7/86 [D loss: 0.6980277299880981, acc.: 46.00%] [G loss: 0.6988744139671326]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 8/86 [D loss: 0.6979009509086609, acc.: 47.71%] [G loss: 0.6998395919799805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 9/86 [D loss: 0.6974615156650543, acc.: 44.78%] [G loss: 0.7024375200271606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 10/86 [D loss: 0.6966783702373505, acc.: 46.97%] [G loss: 0.7017332315444946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 11/86 [D loss: 0.6982114911079407, acc.: 45.80%] [G loss: 0.6982248425483704]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 12/86 [D loss: 0.6996662020683289, acc.: 43.90%] [G loss: 0.699207603931427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 13/86 [D loss: 0.6993821561336517, acc.: 43.95%] [G loss: 0.7008477449417114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 14/86 [D loss: 0.6974211931228638, acc.: 47.07%] [G loss: 0.700209379196167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 15/86 [D loss: 0.6966311633586884, acc.: 46.34%] [G loss: 0.6999585628509521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 16/86 [D loss: 0.6977630853652954, acc.: 46.58%] [G loss: 0.6993198394775391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 17/86 [D loss: 0.6968503296375275, acc.: 46.58%] [G loss: 0.6987176537513733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 18/86 [D loss: 0.697528213262558, acc.: 47.22%] [G loss: 0.700249195098877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 19/86 [D loss: 0.6963109970092773, acc.: 47.27%] [G loss: 0.6985222697257996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 20/86 [D loss: 0.6977641582489014, acc.: 45.51%] [G loss: 0.7014341950416565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 21/86 [D loss: 0.6961746513843536, acc.: 47.85%] [G loss: 0.7004361748695374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 22/86 [D loss: 0.6977806389331818, acc.: 45.31%] [G loss: 0.6993637681007385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 23/86 [D loss: 0.6993772089481354, acc.: 44.63%] [G loss: 0.701686441898346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 24/86 [D loss: 0.6971602737903595, acc.: 46.39%] [G loss: 0.7045904397964478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 25/86 [D loss: 0.6970802843570709, acc.: 45.95%] [G loss: 0.7013388276100159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 26/86 [D loss: 0.6969499588012695, acc.: 46.24%] [G loss: 0.7011980414390564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 27/86 [D loss: 0.6984991431236267, acc.: 44.92%] [G loss: 0.6992901563644409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 28/86 [D loss: 0.6981946527957916, acc.: 45.61%] [G loss: 0.7042173147201538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 29/86 [D loss: 0.6977092027664185, acc.: 46.24%] [G loss: 0.7017545104026794]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 30/86 [D loss: 0.6978403329849243, acc.: 45.80%] [G loss: 0.7022470831871033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 31/86 [D loss: 0.6972240507602692, acc.: 46.14%] [G loss: 0.6995600461959839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 32/86 [D loss: 0.6962770521640778, acc.: 47.71%] [G loss: 0.7003377079963684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 33/86 [D loss: 0.6976663172245026, acc.: 45.85%] [G loss: 0.6997365951538086]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 34/86 [D loss: 0.6972211003303528, acc.: 46.39%] [G loss: 0.7010979056358337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 35/86 [D loss: 0.6976266503334045, acc.: 46.63%] [G loss: 0.699611246585846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 36/86 [D loss: 0.6958320140838623, acc.: 47.85%] [G loss: 0.6980369687080383]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 37/86 [D loss: 0.6989164054393768, acc.: 43.90%] [G loss: 0.6976912021636963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 38/86 [D loss: 0.6968256235122681, acc.: 45.61%] [G loss: 0.7005811333656311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 39/86 [D loss: 0.6981669366359711, acc.: 45.51%] [G loss: 0.7030901908874512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 40/86 [D loss: 0.6974913477897644, acc.: 45.90%] [G loss: 0.7004414796829224]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 41/86 [D loss: 0.6974228024482727, acc.: 45.95%] [G loss: 0.6977266669273376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 42/86 [D loss: 0.6979663074016571, acc.: 46.29%] [G loss: 0.6975847482681274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 43/86 [D loss: 0.6990126371383667, acc.: 44.58%] [G loss: 0.7013570070266724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 44/86 [D loss: 0.6949390172958374, acc.: 48.14%] [G loss: 0.7029732465744019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 45/86 [D loss: 0.6957411170005798, acc.: 48.05%] [G loss: 0.7013263702392578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 46/86 [D loss: 0.6986723244190216, acc.: 44.97%] [G loss: 0.6995763182640076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 47/86 [D loss: 0.6980929970741272, acc.: 44.92%] [G loss: 0.6987847089767456]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 48/86 [D loss: 0.6975250244140625, acc.: 45.36%] [G loss: 0.7018905282020569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 49/86 [D loss: 0.6974480748176575, acc.: 46.39%] [G loss: 0.7008209228515625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 50/86 [D loss: 0.6970674693584442, acc.: 46.00%] [G loss: 0.6994966864585876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 51/86 [D loss: 0.6974894106388092, acc.: 46.29%] [G loss: 0.6977067589759827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 52/86 [D loss: 0.6987300217151642, acc.: 44.14%] [G loss: 0.7004681825637817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 53/86 [D loss: 0.6987354159355164, acc.: 43.75%] [G loss: 0.699815571308136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 54/86 [D loss: 0.6976963579654694, acc.: 45.31%] [G loss: 0.6977423429489136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 55/86 [D loss: 0.698384553194046, acc.: 45.46%] [G loss: 0.6989585757255554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 56/86 [D loss: 0.6979925632476807, acc.: 44.14%] [G loss: 0.6973285675048828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 57/86 [D loss: 0.6977494359016418, acc.: 45.61%] [G loss: 0.7014007568359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 58/86 [D loss: 0.696467787027359, acc.: 47.66%] [G loss: 0.7002170085906982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 59/86 [D loss: 0.696575254201889, acc.: 46.09%] [G loss: 0.6992637515068054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 60/86 [D loss: 0.6965664625167847, acc.: 46.48%] [G loss: 0.6958560347557068]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 61/86 [D loss: 0.6972417831420898, acc.: 47.80%] [G loss: 0.6985799074172974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 62/86 [D loss: 0.6982617080211639, acc.: 46.04%] [G loss: 0.7018789649009705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 63/86 [D loss: 0.697736918926239, acc.: 44.87%] [G loss: 0.7000953555107117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 64/86 [D loss: 0.6967373788356781, acc.: 47.36%] [G loss: 0.7000017166137695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 65/86 [D loss: 0.6976548433303833, acc.: 45.56%] [G loss: 0.6984201073646545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 66/86 [D loss: 0.6967900991439819, acc.: 45.31%] [G loss: 0.6993303298950195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 67/86 [D loss: 0.6965261995792389, acc.: 47.36%] [G loss: 0.7023652791976929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 68/86 [D loss: 0.6965707838535309, acc.: 48.49%] [G loss: 0.7009291052818298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 69/86 [D loss: 0.6970668137073517, acc.: 47.51%] [G loss: 0.6993532776832581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 70/86 [D loss: 0.6975646913051605, acc.: 47.07%] [G loss: 0.6975471377372742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 71/86 [D loss: 0.6990398466587067, acc.: 43.80%] [G loss: 0.699303388595581]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 72/86 [D loss: 0.6976649165153503, acc.: 47.07%] [G loss: 0.7021863460540771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 73/86 [D loss: 0.6982189416885376, acc.: 45.85%] [G loss: 0.7009372115135193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 74/86 [D loss: 0.6984878480434418, acc.: 43.75%] [G loss: 0.6999384164810181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 75/86 [D loss: 0.6966403722763062, acc.: 46.83%] [G loss: 0.6993879079818726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 76/86 [D loss: 0.697473406791687, acc.: 45.85%] [G loss: 0.6992897391319275]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 77/86 [D loss: 0.6978742778301239, acc.: 46.14%] [G loss: 0.7031131386756897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 78/86 [D loss: 0.6969918012619019, acc.: 45.85%] [G loss: 0.703089714050293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 79/86 [D loss: 0.6967014074325562, acc.: 48.00%] [G loss: 0.698231041431427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 80/86 [D loss: 0.6976478099822998, acc.: 45.85%] [G loss: 0.6974431872367859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 81/86 [D loss: 0.696191668510437, acc.: 48.68%] [G loss: 0.701424241065979]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 21/200, Batch 82/86 [D loss: 0.6965348422527313, acc.: 47.07%] [G loss: 0.699641764163971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 83/86 [D loss: 0.6971181333065033, acc.: 46.92%] [G loss: 0.6999511122703552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 84/86 [D loss: 0.697319358587265, acc.: 45.70%] [G loss: 0.6984500288963318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 85/86 [D loss: 0.6980010867118835, acc.: 44.78%] [G loss: 0.7001190185546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 21/200, Batch 86/86 [D loss: 0.6964532732963562, acc.: 47.75%] [G loss: 0.7021823525428772]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 22/200, Batch 1/86 [D loss: 0.6964541673660278, acc.: 47.22%] [G loss: 0.7030397653579712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 2/86 [D loss: 0.6970846652984619, acc.: 46.44%] [G loss: 0.6975839138031006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 3/86 [D loss: 0.6974799036979675, acc.: 48.10%] [G loss: 0.6979257464408875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 4/86 [D loss: 0.697810024023056, acc.: 46.19%] [G loss: 0.6995594501495361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 5/86 [D loss: 0.6968058943748474, acc.: 45.75%] [G loss: 0.6994889378547668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 6/86 [D loss: 0.6968781352043152, acc.: 47.66%] [G loss: 0.6986681222915649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 7/86 [D loss: 0.6970263719558716, acc.: 46.83%] [G loss: 0.6971136331558228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 8/86 [D loss: 0.6982125341892242, acc.: 45.80%] [G loss: 0.6977109313011169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 9/86 [D loss: 0.6970260441303253, acc.: 46.92%] [G loss: 0.7027446627616882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 10/86 [D loss: 0.6962333917617798, acc.: 48.05%] [G loss: 0.6999849677085876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 11/86 [D loss: 0.6966589391231537, acc.: 47.75%] [G loss: 0.7003932595252991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 12/86 [D loss: 0.697285532951355, acc.: 46.48%] [G loss: 0.6962057948112488]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 13/86 [D loss: 0.696876734495163, acc.: 45.56%] [G loss: 0.6984990239143372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 14/86 [D loss: 0.69883131980896, acc.: 45.85%] [G loss: 0.7038032412528992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 15/86 [D loss: 0.6969993710517883, acc.: 46.04%] [G loss: 0.6995603442192078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 16/86 [D loss: 0.6985985338687897, acc.: 43.85%] [G loss: 0.7000682950019836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 17/86 [D loss: 0.6975928544998169, acc.: 45.80%] [G loss: 0.6977386474609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 18/86 [D loss: 0.6997037827968597, acc.: 44.24%] [G loss: 0.6984801888465881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 19/86 [D loss: 0.6960912346839905, acc.: 47.12%] [G loss: 0.7030961513519287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 20/86 [D loss: 0.6959527432918549, acc.: 47.66%] [G loss: 0.7013946175575256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 21/86 [D loss: 0.696145623922348, acc.: 46.78%] [G loss: 0.6985395550727844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 22/86 [D loss: 0.6973317861557007, acc.: 47.07%] [G loss: 0.6952612400054932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 23/86 [D loss: 0.697420746088028, acc.: 45.21%] [G loss: 0.6994502544403076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 24/86 [D loss: 0.6976523697376251, acc.: 44.97%] [G loss: 0.7016661167144775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 25/86 [D loss: 0.6958726644515991, acc.: 46.83%] [G loss: 0.7013081312179565]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 26/86 [D loss: 0.6965495347976685, acc.: 46.53%] [G loss: 0.6976834535598755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 27/86 [D loss: 0.6983615756034851, acc.: 46.14%] [G loss: 0.6989302039146423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 28/86 [D loss: 0.6983547210693359, acc.: 44.38%] [G loss: 0.6999645829200745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 29/86 [D loss: 0.6962986588478088, acc.: 47.17%] [G loss: 0.6988599300384521]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 30/86 [D loss: 0.6972529590129852, acc.: 45.26%] [G loss: 0.6991393566131592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 31/86 [D loss: 0.6965344250202179, acc.: 47.12%] [G loss: 0.6986890435218811]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 32/86 [D loss: 0.6968891024589539, acc.: 46.73%] [G loss: 0.6986526250839233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 33/86 [D loss: 0.69709911942482, acc.: 45.02%] [G loss: 0.7014550566673279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 34/86 [D loss: 0.6961453258991241, acc.: 47.27%] [G loss: 0.7018251419067383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 35/86 [D loss: 0.6966367661952972, acc.: 46.00%] [G loss: 0.698850154876709]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 36/86 [D loss: 0.6975226700305939, acc.: 45.80%] [G loss: 0.6980555057525635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 37/86 [D loss: 0.6985701620578766, acc.: 44.43%] [G loss: 0.6989786624908447]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 38/86 [D loss: 0.6972213387489319, acc.: 46.19%] [G loss: 0.7029292583465576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 39/86 [D loss: 0.695999950170517, acc.: 47.12%] [G loss: 0.7013965845108032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 40/86 [D loss: 0.6964833736419678, acc.: 47.27%] [G loss: 0.6992913484573364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 41/86 [D loss: 0.698511391878128, acc.: 45.95%] [G loss: 0.6986902356147766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 42/86 [D loss: 0.6974675357341766, acc.: 46.00%] [G loss: 0.703183114528656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 43/86 [D loss: 0.6964228749275208, acc.: 47.46%] [G loss: 0.7026359438896179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 44/86 [D loss: 0.6963959634304047, acc.: 45.41%] [G loss: 0.701856255531311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 45/86 [D loss: 0.6956060528755188, acc.: 48.63%] [G loss: 0.6972615718841553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 46/86 [D loss: 0.6990955770015717, acc.: 43.55%] [G loss: 0.6981569528579712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 47/86 [D loss: 0.6976754665374756, acc.: 45.65%] [G loss: 0.7006271481513977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 48/86 [D loss: 0.6963797211647034, acc.: 48.39%] [G loss: 0.700751543045044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 49/86 [D loss: 0.6959218382835388, acc.: 47.46%] [G loss: 0.7002506852149963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 50/86 [D loss: 0.6969951689243317, acc.: 44.38%] [G loss: 0.6969857811927795]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 51/86 [D loss: 0.6995733976364136, acc.: 43.80%] [G loss: 0.6968218088150024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 52/86 [D loss: 0.6972368955612183, acc.: 47.17%] [G loss: 0.7015068531036377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 53/86 [D loss: 0.6966820657253265, acc.: 46.83%] [G loss: 0.6982569694519043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 54/86 [D loss: 0.6964786350727081, acc.: 46.68%] [G loss: 0.698814868927002]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 55/86 [D loss: 0.6987411379814148, acc.: 45.51%] [G loss: 0.6989899277687073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 56/86 [D loss: 0.6987550556659698, acc.: 45.95%] [G loss: 0.6995788216590881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 57/86 [D loss: 0.6962198913097382, acc.: 47.07%] [G loss: 0.7014282941818237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 58/86 [D loss: 0.6970426142215729, acc.: 46.63%] [G loss: 0.6995570659637451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 59/86 [D loss: 0.6965427100658417, acc.: 47.36%] [G loss: 0.6993257403373718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 60/86 [D loss: 0.6973977982997894, acc.: 46.29%] [G loss: 0.6974749565124512]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 61/86 [D loss: 0.6967461109161377, acc.: 45.26%] [G loss: 0.700289249420166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 62/86 [D loss: 0.6951694190502167, acc.: 49.07%] [G loss: 0.7008520364761353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 63/86 [D loss: 0.6955638527870178, acc.: 48.54%] [G loss: 0.7009680867195129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 64/86 [D loss: 0.6968837380409241, acc.: 46.14%] [G loss: 0.6989554166793823]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 65/86 [D loss: 0.6967772543430328, acc.: 47.41%] [G loss: 0.6995024085044861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 66/86 [D loss: 0.6970600187778473, acc.: 47.90%] [G loss: 0.7014390826225281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 67/86 [D loss: 0.6970441341400146, acc.: 45.61%] [G loss: 0.700951099395752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 68/86 [D loss: 0.6967491805553436, acc.: 46.68%] [G loss: 0.6990736126899719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 69/86 [D loss: 0.6967921853065491, acc.: 45.80%] [G loss: 0.701371431350708]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 70/86 [D loss: 0.6965630054473877, acc.: 46.09%] [G loss: 0.7002314329147339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 71/86 [D loss: 0.6966369152069092, acc.: 46.09%] [G loss: 0.6996192932128906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 72/86 [D loss: 0.6954028308391571, acc.: 47.80%] [G loss: 0.6984897255897522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 73/86 [D loss: 0.6966294348239899, acc.: 47.02%] [G loss: 0.699923038482666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 74/86 [D loss: 0.6962157189846039, acc.: 48.05%] [G loss: 0.6987264752388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 75/86 [D loss: 0.696925163269043, acc.: 46.78%] [G loss: 0.6990671157836914]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 76/86 [D loss: 0.6960894465446472, acc.: 48.93%] [G loss: 0.7002979516983032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 77/86 [D loss: 0.6967134773731232, acc.: 45.80%] [G loss: 0.699790358543396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 78/86 [D loss: 0.6975502371788025, acc.: 45.75%] [G loss: 0.69817054271698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 79/86 [D loss: 0.6996550559997559, acc.: 42.87%] [G loss: 0.6990631818771362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 80/86 [D loss: 0.6958224773406982, acc.: 48.24%] [G loss: 0.7019059658050537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 81/86 [D loss: 0.6956867873668671, acc.: 46.92%] [G loss: 0.6992961168289185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 82/86 [D loss: 0.6972421407699585, acc.: 47.61%] [G loss: 0.6976898312568665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 83/86 [D loss: 0.6977023482322693, acc.: 46.63%] [G loss: 0.7002776265144348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 84/86 [D loss: 0.6976073682308197, acc.: 45.56%] [G loss: 0.7018478512763977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 22/200, Batch 85/86 [D loss: 0.695588231086731, acc.: 47.46%] [G loss: 0.7001051902770996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 22/200, Batch 86/86 [D loss: 0.6967455148696899, acc.: 47.61%] [G loss: 0.6990712881088257]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 1/86 [D loss: 0.6970303058624268, acc.: 45.70%] [G loss: 0.6980147361755371]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 2/86 [D loss: 0.6984975636005402, acc.: 45.02%] [G loss: 0.6986738443374634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 3/86 [D loss: 0.6953091323375702, acc.: 48.24%] [G loss: 0.6999037265777588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 4/86 [D loss: 0.6965388059616089, acc.: 46.44%] [G loss: 0.6993105411529541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 5/86 [D loss: 0.6964944005012512, acc.: 47.61%] [G loss: 0.6998847126960754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 6/86 [D loss: 0.6976743936538696, acc.: 44.63%] [G loss: 0.6987171173095703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 7/86 [D loss: 0.6968773007392883, acc.: 47.02%] [G loss: 0.6986925601959229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 8/86 [D loss: 0.6970691680908203, acc.: 45.17%] [G loss: 0.6983837485313416]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 9/86 [D loss: 0.69754359126091, acc.: 45.12%] [G loss: 0.6971913576126099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 10/86 [D loss: 0.6964631676673889, acc.: 47.41%] [G loss: 0.6958457231521606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 11/86 [D loss: 0.6970526874065399, acc.: 46.73%] [G loss: 0.6984531879425049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 12/86 [D loss: 0.6961063146591187, acc.: 47.90%] [G loss: 0.698015034198761]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 13/86 [D loss: 0.6971253454685211, acc.: 45.56%] [G loss: 0.6971867680549622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 14/86 [D loss: 0.6971578001976013, acc.: 46.58%] [G loss: 0.6978705525398254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 15/86 [D loss: 0.6972644329071045, acc.: 45.21%] [G loss: 0.6993439197540283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 16/86 [D loss: 0.6963106691837311, acc.: 48.14%] [G loss: 0.699813187122345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 17/86 [D loss: 0.6962290108203888, acc.: 46.78%] [G loss: 0.6985134482383728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 18/86 [D loss: 0.6964218616485596, acc.: 46.04%] [G loss: 0.6987168788909912]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 19/86 [D loss: 0.6974355280399323, acc.: 45.36%] [G loss: 0.6957554221153259]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 20/86 [D loss: 0.6984843909740448, acc.: 44.48%] [G loss: 0.6977259516716003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 21/86 [D loss: 0.6953967809677124, acc.: 48.24%] [G loss: 0.7004091143608093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 22/86 [D loss: 0.6959341168403625, acc.: 48.78%] [G loss: 0.6999232769012451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 23/86 [D loss: 0.6955642104148865, acc.: 48.58%] [G loss: 0.698657214641571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 24/86 [D loss: 0.698311448097229, acc.: 44.97%] [G loss: 0.6983513236045837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 25/86 [D loss: 0.6981095373630524, acc.: 44.48%] [G loss: 0.7017086148262024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 26/86 [D loss: 0.6968146562576294, acc.: 45.75%] [G loss: 0.7007074952125549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 27/86 [D loss: 0.6947396099567413, acc.: 49.27%] [G loss: 0.6992405652999878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 28/86 [D loss: 0.6974062919616699, acc.: 44.92%] [G loss: 0.6978864669799805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 29/86 [D loss: 0.6974330544471741, acc.: 45.75%] [G loss: 0.6975233554840088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 30/86 [D loss: 0.696967750787735, acc.: 46.63%] [G loss: 0.7018199563026428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 31/86 [D loss: 0.6959964036941528, acc.: 48.10%] [G loss: 0.701077938079834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 32/86 [D loss: 0.6955545246601105, acc.: 48.44%] [G loss: 0.7008038759231567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 33/86 [D loss: 0.6971677243709564, acc.: 45.85%] [G loss: 0.6976369619369507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 34/86 [D loss: 0.6977881193161011, acc.: 46.00%] [G loss: 0.6994336843490601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 35/86 [D loss: 0.6974033713340759, acc.: 45.12%] [G loss: 0.7025370001792908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 36/86 [D loss: 0.696362316608429, acc.: 46.78%] [G loss: 0.7009287476539612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 37/86 [D loss: 0.6957159042358398, acc.: 47.56%] [G loss: 0.6992882490158081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 38/86 [D loss: 0.6968196928501129, acc.: 46.00%] [G loss: 0.6953830718994141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 39/86 [D loss: 0.6993538737297058, acc.: 44.53%] [G loss: 0.6993089914321899]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 40/86 [D loss: 0.6968012154102325, acc.: 46.04%] [G loss: 0.702491044998169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 41/86 [D loss: 0.6964750289916992, acc.: 46.48%] [G loss: 0.7004262208938599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 42/86 [D loss: 0.6963052749633789, acc.: 47.85%] [G loss: 0.6968137621879578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 43/86 [D loss: 0.6971331834793091, acc.: 46.97%] [G loss: 0.6922826766967773]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 44/86 [D loss: 0.7006831467151642, acc.: 43.46%] [G loss: 0.6962926983833313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 45/86 [D loss: 0.6942303478717804, acc.: 49.27%] [G loss: 0.7014598846435547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 46/86 [D loss: 0.6979367434978485, acc.: 44.78%] [G loss: 0.7006466388702393]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 47/86 [D loss: 0.6966277360916138, acc.: 46.68%] [G loss: 0.6985539793968201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 48/86 [D loss: 0.697111576795578, acc.: 46.04%] [G loss: 0.6928722262382507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 49/86 [D loss: 0.7002114951610565, acc.: 42.29%] [G loss: 0.6982298493385315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 50/86 [D loss: 0.6941810548305511, acc.: 50.54%] [G loss: 0.6996944546699524]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 51/86 [D loss: 0.6979396939277649, acc.: 43.41%] [G loss: 0.7000631093978882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 52/86 [D loss: 0.6969757080078125, acc.: 44.78%] [G loss: 0.6990157961845398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 53/86 [D loss: 0.6983310878276825, acc.: 45.85%] [G loss: 0.6943280696868896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 54/86 [D loss: 0.6990222930908203, acc.: 43.75%] [G loss: 0.7002913355827332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 55/86 [D loss: 0.6959426403045654, acc.: 47.80%] [G loss: 0.7009482979774475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 56/86 [D loss: 0.6962807476520538, acc.: 47.41%] [G loss: 0.6990929245948792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 57/86 [D loss: 0.696828305721283, acc.: 45.46%] [G loss: 0.6988004446029663]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 58/86 [D loss: 0.6979159712791443, acc.: 46.14%] [G loss: 0.69401615858078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 59/86 [D loss: 0.6999310255050659, acc.: 43.85%] [G loss: 0.701018214225769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 60/86 [D loss: 0.6954213976860046, acc.: 47.02%] [G loss: 0.7012269496917725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 61/86 [D loss: 0.6980366110801697, acc.: 44.63%] [G loss: 0.7001886367797852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 62/86 [D loss: 0.6969919800758362, acc.: 46.24%] [G loss: 0.6989975571632385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 63/86 [D loss: 0.6973957717418671, acc.: 46.19%] [G loss: 0.6988930106163025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 64/86 [D loss: 0.6978841423988342, acc.: 45.02%] [G loss: 0.6992987394332886]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 65/86 [D loss: 0.6973454356193542, acc.: 46.58%] [G loss: 0.6994092464447021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 66/86 [D loss: 0.6966709792613983, acc.: 45.26%] [G loss: 0.7003986239433289]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 67/86 [D loss: 0.6962926089763641, acc.: 47.61%] [G loss: 0.6999971270561218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 68/86 [D loss: 0.6970634460449219, acc.: 46.48%] [G loss: 0.699910581111908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 69/86 [D loss: 0.6959381997585297, acc.: 47.07%] [G loss: 0.7015411257743835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 70/86 [D loss: 0.6961731314659119, acc.: 47.12%] [G loss: 0.7014578580856323]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 71/86 [D loss: 0.6974330246448517, acc.: 45.12%] [G loss: 0.6982194781303406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 72/86 [D loss: 0.6960374414920807, acc.: 46.44%] [G loss: 0.7005606889724731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 73/86 [D loss: 0.6958852112293243, acc.: 47.56%] [G loss: 0.7005577683448792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 74/86 [D loss: 0.6971511542797089, acc.: 45.85%] [G loss: 0.702782928943634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 75/86 [D loss: 0.6950499713420868, acc.: 48.34%] [G loss: 0.7016212940216064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 76/86 [D loss: 0.6960079669952393, acc.: 46.48%] [G loss: 0.7014220952987671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 77/86 [D loss: 0.6970608234405518, acc.: 45.46%] [G loss: 0.6997436285018921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 78/86 [D loss: 0.6970526874065399, acc.: 44.78%] [G loss: 0.7014241218566895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 79/86 [D loss: 0.6965299844741821, acc.: 45.56%] [G loss: 0.7012521624565125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 23/200, Batch 80/86 [D loss: 0.6963033080101013, acc.: 46.63%] [G loss: 0.7029832005500793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 81/86 [D loss: 0.6966663300991058, acc.: 46.14%] [G loss: 0.6993283629417419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 82/86 [D loss: 0.6969527304172516, acc.: 45.65%] [G loss: 0.7010228633880615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 83/86 [D loss: 0.696119487285614, acc.: 46.34%] [G loss: 0.7005900144577026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 84/86 [D loss: 0.6960798501968384, acc.: 47.17%] [G loss: 0.7012139558792114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 85/86 [D loss: 0.6971246600151062, acc.: 46.97%] [G loss: 0.7008872032165527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 23/200, Batch 86/86 [D loss: 0.6969256699085236, acc.: 46.92%] [G loss: 0.7004542946815491]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 1/86 [D loss: 0.6955753862857819, acc.: 47.31%] [G loss: 0.7011101245880127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 2/86 [D loss: 0.6960816383361816, acc.: 48.39%] [G loss: 0.7017993330955505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 3/86 [D loss: 0.6960113048553467, acc.: 46.53%] [G loss: 0.6983950138092041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 4/86 [D loss: 0.6974507868289948, acc.: 45.07%] [G loss: 0.6993987560272217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 5/86 [D loss: 0.6967804133892059, acc.: 45.90%] [G loss: 0.6991549134254456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 6/86 [D loss: 0.6983484029769897, acc.: 44.19%] [G loss: 0.6993481516838074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 7/86 [D loss: 0.6961537301540375, acc.: 46.73%] [G loss: 0.6991050243377686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 8/86 [D loss: 0.6961172223091125, acc.: 46.39%] [G loss: 0.700282096862793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 9/86 [D loss: 0.6966642737388611, acc.: 44.48%] [G loss: 0.6983844637870789]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 10/86 [D loss: 0.695950835943222, acc.: 48.78%] [G loss: 0.6985538601875305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 11/86 [D loss: 0.6965213716030121, acc.: 47.17%] [G loss: 0.6981362104415894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 12/86 [D loss: 0.6961342692375183, acc.: 48.00%] [G loss: 0.6988804340362549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 13/86 [D loss: 0.6957427859306335, acc.: 48.05%] [G loss: 0.6965832114219666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 14/86 [D loss: 0.6983862817287445, acc.: 43.99%] [G loss: 0.6957182288169861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 15/86 [D loss: 0.6973085701465607, acc.: 46.09%] [G loss: 0.6983993649482727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 16/86 [D loss: 0.6969112157821655, acc.: 45.90%] [G loss: 0.6986367702484131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 17/86 [D loss: 0.6958471238613129, acc.: 46.19%] [G loss: 0.6976686120033264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 18/86 [D loss: 0.6959609091281891, acc.: 47.27%] [G loss: 0.6961718797683716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 19/86 [D loss: 0.6985151171684265, acc.: 44.68%] [G loss: 0.6970211267471313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 20/86 [D loss: 0.6972573697566986, acc.: 47.22%] [G loss: 0.696961522102356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 21/86 [D loss: 0.6966817677021027, acc.: 48.39%] [G loss: 0.6998311281204224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 22/86 [D loss: 0.6961873769760132, acc.: 46.39%] [G loss: 0.6978374123573303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 23/86 [D loss: 0.6968265175819397, acc.: 46.19%] [G loss: 0.696714460849762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 24/86 [D loss: 0.697054922580719, acc.: 45.85%] [G loss: 0.6983076333999634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 25/86 [D loss: 0.6961954534053802, acc.: 46.68%] [G loss: 0.6979338526725769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 26/86 [D loss: 0.6971833109855652, acc.: 45.90%] [G loss: 0.6999196410179138]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 27/86 [D loss: 0.6974832713603973, acc.: 45.31%] [G loss: 0.6982102394104004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 28/86 [D loss: 0.6960726082324982, acc.: 47.36%] [G loss: 0.6972441673278809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 29/86 [D loss: 0.6957012116909027, acc.: 47.71%] [G loss: 0.6987466812133789]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 30/86 [D loss: 0.6965154707431793, acc.: 47.02%] [G loss: 0.7001189589500427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 31/86 [D loss: 0.695019394159317, acc.: 49.02%] [G loss: 0.6991354823112488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 32/86 [D loss: 0.6953085064888, acc.: 47.85%] [G loss: 0.6992218494415283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 33/86 [D loss: 0.6962960660457611, acc.: 45.46%] [G loss: 0.6963075995445251]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 34/86 [D loss: 0.6983589828014374, acc.: 44.58%] [G loss: 0.6968818306922913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 35/86 [D loss: 0.6957471072673798, acc.: 47.31%] [G loss: 0.6997634768486023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 36/86 [D loss: 0.6958024501800537, acc.: 47.46%] [G loss: 0.6998552680015564]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 37/86 [D loss: 0.6976288557052612, acc.: 46.68%] [G loss: 0.6987545490264893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 38/86 [D loss: 0.6971939504146576, acc.: 44.82%] [G loss: 0.6983900666236877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 39/86 [D loss: 0.6969306170940399, acc.: 46.68%] [G loss: 0.6997027397155762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 40/86 [D loss: 0.6960205137729645, acc.: 46.68%] [G loss: 0.6986513733863831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 41/86 [D loss: 0.6957288086414337, acc.: 48.00%] [G loss: 0.6975680589675903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 42/86 [D loss: 0.6953431963920593, acc.: 47.95%] [G loss: 0.6963638663291931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 43/86 [D loss: 0.6964768469333649, acc.: 47.02%] [G loss: 0.69843590259552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 44/86 [D loss: 0.6963132321834564, acc.: 45.26%] [G loss: 0.7009112238883972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 45/86 [D loss: 0.6955176293849945, acc.: 46.83%] [G loss: 0.699455976486206]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 46/86 [D loss: 0.6960105001926422, acc.: 47.61%] [G loss: 0.6989975571632385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 47/86 [D loss: 0.6970301568508148, acc.: 45.41%] [G loss: 0.6979272365570068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 48/86 [D loss: 0.6976287066936493, acc.: 44.87%] [G loss: 0.6977035403251648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 49/86 [D loss: 0.6961245536804199, acc.: 47.36%] [G loss: 0.7001082897186279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 50/86 [D loss: 0.696562260389328, acc.: 46.09%] [G loss: 0.7000585794448853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 51/86 [D loss: 0.6968861818313599, acc.: 45.80%] [G loss: 0.6966833472251892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 52/86 [D loss: 0.6979119777679443, acc.: 45.41%] [G loss: 0.6968116760253906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 53/86 [D loss: 0.6965481340885162, acc.: 46.14%] [G loss: 0.7010168433189392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 54/86 [D loss: 0.6960960030555725, acc.: 46.04%] [G loss: 0.700320839881897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 55/86 [D loss: 0.6959617137908936, acc.: 47.71%] [G loss: 0.6980103850364685]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 56/86 [D loss: 0.6957575380802155, acc.: 45.95%] [G loss: 0.6993712186813354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 57/86 [D loss: 0.6967341899871826, acc.: 46.34%] [G loss: 0.699493408203125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 58/86 [D loss: 0.6968847215175629, acc.: 46.83%] [G loss: 0.7003641724586487]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 59/86 [D loss: 0.6957927346229553, acc.: 47.02%] [G loss: 0.7001223564147949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 60/86 [D loss: 0.6967666149139404, acc.: 45.17%] [G loss: 0.6976077556610107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 61/86 [D loss: 0.6988543570041656, acc.: 43.36%] [G loss: 0.6983066201210022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 62/86 [D loss: 0.6966659426689148, acc.: 47.61%] [G loss: 0.699705958366394]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 63/86 [D loss: 0.6960876882076263, acc.: 45.95%] [G loss: 0.6984407305717468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 64/86 [D loss: 0.6960805952548981, acc.: 46.29%] [G loss: 0.6978806257247925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 65/86 [D loss: 0.6967217028141022, acc.: 46.78%] [G loss: 0.6960963606834412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 66/86 [D loss: 0.6964878439903259, acc.: 46.63%] [G loss: 0.6975377202033997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 67/86 [D loss: 0.6958915293216705, acc.: 48.00%] [G loss: 0.7000359296798706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 68/86 [D loss: 0.6955130398273468, acc.: 46.73%] [G loss: 0.6985989809036255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 69/86 [D loss: 0.6967585682868958, acc.: 47.12%] [G loss: 0.6986715793609619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 70/86 [D loss: 0.6977318525314331, acc.: 45.07%] [G loss: 0.6958981156349182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 71/86 [D loss: 0.695919394493103, acc.: 48.73%] [G loss: 0.7012665867805481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 72/86 [D loss: 0.6954454481601715, acc.: 48.58%] [G loss: 0.7000966668128967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 73/86 [D loss: 0.6961765289306641, acc.: 45.85%] [G loss: 0.6991231441497803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 74/86 [D loss: 0.6950439810752869, acc.: 48.49%] [G loss: 0.6969089508056641]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 75/86 [D loss: 0.6994915306568146, acc.: 44.09%] [G loss: 0.6990674734115601]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 76/86 [D loss: 0.6970738470554352, acc.: 45.85%] [G loss: 0.700907289981842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 77/86 [D loss: 0.6965208053588867, acc.: 45.31%] [G loss: 0.7003446221351624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 78/86 [D loss: 0.6958688497543335, acc.: 46.00%] [G loss: 0.6990234851837158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 79/86 [D loss: 0.6986321210861206, acc.: 43.80%] [G loss: 0.6946158409118652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 80/86 [D loss: 0.69801065325737, acc.: 43.75%] [G loss: 0.6978465914726257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 81/86 [D loss: 0.6955963671207428, acc.: 48.10%] [G loss: 0.6994682550430298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 82/86 [D loss: 0.6965139806270599, acc.: 46.29%] [G loss: 0.6997639536857605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 83/86 [D loss: 0.69510218501091, acc.: 47.95%] [G loss: 0.6981703042984009]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 84/86 [D loss: 0.6949933469295502, acc.: 49.76%] [G loss: 0.6936612129211426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 24/200, Batch 85/86 [D loss: 0.7000196278095245, acc.: 43.51%] [G loss: 0.7012126445770264]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 24/200, Batch 86/86 [D loss: 0.6942412853240967, acc.: 48.97%] [G loss: 0.6979725360870361]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 1/86 [D loss: 0.6971895694732666, acc.: 44.43%] [G loss: 0.7002313137054443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 2/86 [D loss: 0.6950414180755615, acc.: 47.85%] [G loss: 0.6967858672142029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 3/86 [D loss: 0.6970177590847015, acc.: 46.00%] [G loss: 0.6896556615829468]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 4/86 [D loss: 0.7007884085178375, acc.: 42.33%] [G loss: 0.7025104761123657]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 5/86 [D loss: 0.6949406266212463, acc.: 48.63%] [G loss: 0.6980777978897095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 6/86 [D loss: 0.6981258690357208, acc.: 42.24%] [G loss: 0.6988989114761353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 7/86 [D loss: 0.6962163746356964, acc.: 47.22%] [G loss: 0.6972192525863647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 8/86 [D loss: 0.6988524198532104, acc.: 43.80%] [G loss: 0.6884820461273193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 9/86 [D loss: 0.700815349817276, acc.: 43.95%] [G loss: 0.7015868425369263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 10/86 [D loss: 0.6945014595985413, acc.: 48.97%] [G loss: 0.6979990601539612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 11/86 [D loss: 0.6983107030391693, acc.: 43.90%] [G loss: 0.6977965235710144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 12/86 [D loss: 0.6971840262413025, acc.: 45.90%] [G loss: 0.6946855187416077]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 13/86 [D loss: 0.6977702677249908, acc.: 44.43%] [G loss: 0.6899276971817017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 14/86 [D loss: 0.7003675997257233, acc.: 43.55%] [G loss: 0.6988727450370789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 15/86 [D loss: 0.6937289237976074, acc.: 49.07%] [G loss: 0.7004197835922241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 16/86 [D loss: 0.6978761553764343, acc.: 45.26%] [G loss: 0.6997295618057251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 17/86 [D loss: 0.6963849365711212, acc.: 46.92%] [G loss: 0.6970496773719788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 18/86 [D loss: 0.6977485716342926, acc.: 43.46%] [G loss: 0.691849946975708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 19/86 [D loss: 0.6995601058006287, acc.: 42.63%] [G loss: 0.6971820592880249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 20/86 [D loss: 0.6957464814186096, acc.: 46.68%] [G loss: 0.7007219791412354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 21/86 [D loss: 0.6954815089702606, acc.: 48.34%] [G loss: 0.6995047926902771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 22/86 [D loss: 0.6951628923416138, acc.: 48.10%] [G loss: 0.6991310715675354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 23/86 [D loss: 0.6978604197502136, acc.: 43.80%] [G loss: 0.6974665522575378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 24/86 [D loss: 0.6973563730716705, acc.: 44.58%] [G loss: 0.6985293626785278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 25/86 [D loss: 0.6968501508235931, acc.: 46.73%] [G loss: 0.7019127607345581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 26/86 [D loss: 0.6954026520252228, acc.: 48.39%] [G loss: 0.7020900845527649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 27/86 [D loss: 0.696694552898407, acc.: 46.09%] [G loss: 0.7024607062339783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 28/86 [D loss: 0.6965646147727966, acc.: 46.92%] [G loss: 0.6974217891693115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 29/86 [D loss: 0.69814732670784, acc.: 44.78%] [G loss: 0.700671911239624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 30/86 [D loss: 0.6961942315101624, acc.: 47.51%] [G loss: 0.7012684345245361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 31/86 [D loss: 0.6958083510398865, acc.: 48.19%] [G loss: 0.7021266222000122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 32/86 [D loss: 0.696600615978241, acc.: 47.36%] [G loss: 0.7011797428131104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 33/86 [D loss: 0.6952179670333862, acc.: 47.56%] [G loss: 0.7002418041229248]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 34/86 [D loss: 0.6984047889709473, acc.: 43.26%] [G loss: 0.7005603313446045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 35/86 [D loss: 0.6950525343418121, acc.: 48.44%] [G loss: 0.7024160623550415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 36/86 [D loss: 0.6960485279560089, acc.: 46.68%] [G loss: 0.7019397616386414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 37/86 [D loss: 0.6961857676506042, acc.: 46.44%] [G loss: 0.7004036903381348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 38/86 [D loss: 0.6961078643798828, acc.: 45.95%] [G loss: 0.7000927925109863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 39/86 [D loss: 0.6971422433853149, acc.: 46.00%] [G loss: 0.7006512880325317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 40/86 [D loss: 0.6956750154495239, acc.: 47.80%] [G loss: 0.7018734216690063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 41/86 [D loss: 0.6966094672679901, acc.: 47.31%] [G loss: 0.7014176249504089]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 42/86 [D loss: 0.6949514150619507, acc.: 48.44%] [G loss: 0.7000662088394165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 43/86 [D loss: 0.6955403089523315, acc.: 47.75%] [G loss: 0.7001066207885742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 44/86 [D loss: 0.696059376001358, acc.: 46.88%] [G loss: 0.7016369104385376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 45/86 [D loss: 0.6967705190181732, acc.: 45.17%] [G loss: 0.7018190026283264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 46/86 [D loss: 0.6962129771709442, acc.: 46.83%] [G loss: 0.7009447813034058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 47/86 [D loss: 0.6960841715335846, acc.: 45.41%] [G loss: 0.7008390426635742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 48/86 [D loss: 0.6963496506214142, acc.: 46.04%] [G loss: 0.7008622884750366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 49/86 [D loss: 0.69615238904953, acc.: 48.14%] [G loss: 0.7017592191696167]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 50/86 [D loss: 0.6967711448669434, acc.: 45.51%] [G loss: 0.7020201683044434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 51/86 [D loss: 0.6966610848903656, acc.: 46.09%] [G loss: 0.6990413665771484]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 52/86 [D loss: 0.6963709592819214, acc.: 46.73%] [G loss: 0.7004200220108032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 53/86 [D loss: 0.6969522833824158, acc.: 45.41%] [G loss: 0.6994596719741821]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 54/86 [D loss: 0.6970553398132324, acc.: 45.90%] [G loss: 0.6994117498397827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 55/86 [D loss: 0.6960912346839905, acc.: 46.19%] [G loss: 0.700042724609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 56/86 [D loss: 0.6961149871349335, acc.: 47.27%] [G loss: 0.7012889981269836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 57/86 [D loss: 0.6951928436756134, acc.: 47.75%] [G loss: 0.6979376673698425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 58/86 [D loss: 0.6963222622871399, acc.: 46.34%] [G loss: 0.6956139802932739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 59/86 [D loss: 0.6975412964820862, acc.: 46.19%] [G loss: 0.7001193761825562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 60/86 [D loss: 0.6955394446849823, acc.: 46.39%] [G loss: 0.6995620131492615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 61/86 [D loss: 0.6961388289928436, acc.: 46.73%] [G loss: 0.6989533305168152]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 62/86 [D loss: 0.6967302560806274, acc.: 45.12%] [G loss: 0.696805477142334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 63/86 [D loss: 0.6964648962020874, acc.: 47.27%] [G loss: 0.6948873996734619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 64/86 [D loss: 0.6974895298480988, acc.: 45.41%] [G loss: 0.7004515528678894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 65/86 [D loss: 0.695546567440033, acc.: 47.07%] [G loss: 0.6992541551589966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 66/86 [D loss: 0.6969550251960754, acc.: 44.48%] [G loss: 0.6998684406280518]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 67/86 [D loss: 0.696437656879425, acc.: 46.53%] [G loss: 0.6963997483253479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 68/86 [D loss: 0.696265459060669, acc.: 47.71%] [G loss: 0.6909924149513245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 69/86 [D loss: 0.699436604976654, acc.: 44.24%] [G loss: 0.7032341957092285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 70/86 [D loss: 0.6930888593196869, acc.: 50.49%] [G loss: 0.6945807337760925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 71/86 [D loss: 0.6979705393314362, acc.: 43.60%] [G loss: 0.6967127323150635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 72/86 [D loss: 0.6960800290107727, acc.: 47.12%] [G loss: 0.6957768797874451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 73/86 [D loss: 0.6977805197238922, acc.: 44.29%] [G loss: 0.6873365640640259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 74/86 [D loss: 0.702177882194519, acc.: 41.70%] [G loss: 0.6982725858688354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 75/86 [D loss: 0.6926559209823608, acc.: 51.07%] [G loss: 0.6951834559440613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 76/86 [D loss: 0.7015663087368011, acc.: 37.16%] [G loss: 0.6928175687789917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 77/86 [D loss: 0.6979634761810303, acc.: 42.92%] [G loss: 0.6968333721160889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 78/86 [D loss: 0.6942036747932434, acc.: 49.71%] [G loss: 0.687152087688446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 79/86 [D loss: 0.7001394033432007, acc.: 45.07%] [G loss: 0.6844518780708313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 80/86 [D loss: 0.69992396235466, acc.: 43.31%] [G loss: 0.7018817663192749]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 81/86 [D loss: 0.692771703004837, acc.: 51.46%] [G loss: 0.6951450109481812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 25/200, Batch 82/86 [D loss: 0.6986996531486511, acc.: 42.19%] [G loss: 0.6946092844009399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 83/86 [D loss: 0.6976475417613983, acc.: 44.87%] [G loss: 0.6954997181892395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 84/86 [D loss: 0.6983532607555389, acc.: 44.68%] [G loss: 0.6885970234870911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 85/86 [D loss: 0.6997115612030029, acc.: 45.12%] [G loss: 0.6944800019264221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 25/200, Batch 86/86 [D loss: 0.6964044868946075, acc.: 46.00%] [G loss: 0.6990880370140076]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 1/86 [D loss: 0.6966104805469513, acc.: 45.75%] [G loss: 0.6982591152191162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 2/86 [D loss: 0.6967937648296356, acc.: 46.00%] [G loss: 0.698316216468811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 3/86 [D loss: 0.6962481439113617, acc.: 47.71%] [G loss: 0.696282148361206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 4/86 [D loss: 0.6970473229885101, acc.: 45.75%] [G loss: 0.6968477964401245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 5/86 [D loss: 0.6958483755588531, acc.: 47.12%] [G loss: 0.7010611295700073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 6/86 [D loss: 0.6958141922950745, acc.: 46.68%] [G loss: 0.7012923955917358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 7/86 [D loss: 0.6962161064147949, acc.: 47.27%] [G loss: 0.6996541619300842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 8/86 [D loss: 0.6964940130710602, acc.: 45.21%] [G loss: 0.7010789513587952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 9/86 [D loss: 0.6969953775405884, acc.: 45.90%] [G loss: 0.7004551291465759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 26/200, Batch 10/86 [D loss: 0.6953958570957184, acc.: 48.63%] [G loss: 0.7027242183685303]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 11/86 [D loss: 0.6955358684062958, acc.: 46.78%] [G loss: 0.7034845948219299]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 12/86 [D loss: 0.6972518265247345, acc.: 46.04%] [G loss: 0.7025850415229797]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 13/86 [D loss: 0.6959955096244812, acc.: 45.75%] [G loss: 0.7021288871765137]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 14/86 [D loss: 0.6954391002655029, acc.: 48.39%] [G loss: 0.7033683657646179]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 15/86 [D loss: 0.6962916254997253, acc.: 46.78%] [G loss: 0.7013154029846191]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 16/86 [D loss: 0.6973433196544647, acc.: 45.31%] [G loss: 0.7018439173698425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 17/86 [D loss: 0.695334792137146, acc.: 47.61%] [G loss: 0.701543927192688]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 18/86 [D loss: 0.6948380768299103, acc.: 48.00%] [G loss: 0.7020034790039062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 19/86 [D loss: 0.6944863200187683, acc.: 49.12%] [G loss: 0.7006407976150513]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 20/86 [D loss: 0.6961721181869507, acc.: 45.90%] [G loss: 0.7010411024093628]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 21/86 [D loss: 0.6948275566101074, acc.: 48.44%] [G loss: 0.7016779780387878]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 22/86 [D loss: 0.6954656541347504, acc.: 46.53%] [G loss: 0.7001780271530151]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 23/86 [D loss: 0.6963871419429779, acc.: 46.09%] [G loss: 0.7015417218208313]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 24/86 [D loss: 0.6963981986045837, acc.: 45.85%] [G loss: 0.7018437385559082]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 25/86 [D loss: 0.6954491436481476, acc.: 47.22%] [G loss: 0.7008469700813293]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 26/86 [D loss: 0.6945125758647919, acc.: 48.54%] [G loss: 0.7015813589096069]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 27/86 [D loss: 0.6952149271965027, acc.: 48.73%] [G loss: 0.6998581886291504]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 28/86 [D loss: 0.6957785487174988, acc.: 46.73%] [G loss: 0.6997617483139038]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 29/86 [D loss: 0.6958727240562439, acc.: 46.44%] [G loss: 0.6975365877151489]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 30/86 [D loss: 0.6965653002262115, acc.: 48.00%] [G loss: 0.7006405591964722]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 31/86 [D loss: 0.6951683163642883, acc.: 47.41%] [G loss: 0.7003558874130249]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 32/86 [D loss: 0.6953316628932953, acc.: 47.41%] [G loss: 0.7009256482124329]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 33/86 [D loss: 0.6952103674411774, acc.: 47.90%] [G loss: 0.696712076663971]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 34/86 [D loss: 0.6960592567920685, acc.: 46.68%] [G loss: 0.6907753348350525]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 35/86 [D loss: 0.6996257603168488, acc.: 44.09%] [G loss: 0.7013199925422668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 36/86 [D loss: 0.6948646008968353, acc.: 48.00%] [G loss: 0.6984854936599731]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 37/86 [D loss: 0.6987859308719635, acc.: 42.33%] [G loss: 0.6961479187011719]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 38/86 [D loss: 0.6957433521747589, acc.: 46.83%] [G loss: 0.6964111924171448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 39/86 [D loss: 0.6963837146759033, acc.: 46.29%] [G loss: 0.6836631298065186]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 40/86 [D loss: 0.7032508254051208, acc.: 41.16%] [G loss: 0.6863526105880737]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 41/86 [D loss: 0.6952100992202759, acc.: 47.75%] [G loss: 0.7045720219612122]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 42/86 [D loss: 0.6960318088531494, acc.: 45.95%] [G loss: 0.6872382164001465]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 43/86 [D loss: 0.7031185626983643, acc.: 34.67%] [G loss: 0.6946519017219543]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 44/86 [D loss: 0.6924188435077667, acc.: 51.86%] [G loss: 0.6954249739646912]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 45/86 [D loss: 0.6947767734527588, acc.: 48.54%] [G loss: 0.6721732020378113]\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 26/200, Batch 46/86 [D loss: 0.7087530195713043, acc.: 40.33%] [G loss: 0.6804851293563843]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 47/86 [D loss: 0.6953956186771393, acc.: 48.39%] [G loss: 0.7045929431915283]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 48/86 [D loss: 0.6960964500904083, acc.: 45.85%] [G loss: 0.6879990100860596]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 49/86 [D loss: 0.7026335895061493, acc.: 36.08%] [G loss: 0.691266655921936]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 50/86 [D loss: 0.6980018615722656, acc.: 44.43%] [G loss: 0.6928485035896301]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 51/86 [D loss: 0.6949902474880219, acc.: 49.02%] [G loss: 0.6872032284736633]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 52/86 [D loss: 0.6986560225486755, acc.: 44.58%] [G loss: 0.684840202331543]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 53/86 [D loss: 0.6986281275749207, acc.: 44.48%] [G loss: 0.6936206817626953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 54/86 [D loss: 0.6956336200237274, acc.: 46.78%] [G loss: 0.6975637078285217]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 55/86 [D loss: 0.6962794363498688, acc.: 45.70%] [G loss: 0.697420597076416]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 56/86 [D loss: 0.6960678398609161, acc.: 46.78%] [G loss: 0.6981840133666992]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 57/86 [D loss: 0.6952923834323883, acc.: 48.24%] [G loss: 0.6970638036727905]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 58/86 [D loss: 0.6959179043769836, acc.: 46.29%] [G loss: 0.6969046592712402]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 59/86 [D loss: 0.6952817738056183, acc.: 48.49%] [G loss: 0.6988493204116821]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 60/86 [D loss: 0.6964620351791382, acc.: 45.12%] [G loss: 0.7016099691390991]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 61/86 [D loss: 0.6965069472789764, acc.: 46.29%] [G loss: 0.7015904188156128]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 62/86 [D loss: 0.6952422857284546, acc.: 47.56%] [G loss: 0.7025360465049744]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 26/200, Batch 63/86 [D loss: 0.696072906255722, acc.: 45.95%] [G loss: 0.7023805379867554]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 64/86 [D loss: 0.6964339911937714, acc.: 46.88%] [G loss: 0.7034502029418945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 65/86 [D loss: 0.695784866809845, acc.: 46.97%] [G loss: 0.7027220726013184]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 66/86 [D loss: 0.6959308981895447, acc.: 46.88%] [G loss: 0.702048659324646]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 67/86 [D loss: 0.6943228840827942, acc.: 49.56%] [G loss: 0.705691397190094]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 68/86 [D loss: 0.6947331428527832, acc.: 47.17%] [G loss: 0.7039818167686462]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 69/86 [D loss: 0.6953641772270203, acc.: 49.51%] [G loss: 0.7053572535514832]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 70/86 [D loss: 0.6945008337497711, acc.: 49.51%] [G loss: 0.7046024203300476]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 71/86 [D loss: 0.6955165863037109, acc.: 47.12%] [G loss: 0.7054203748703003]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 72/86 [D loss: 0.6951129734516144, acc.: 48.58%] [G loss: 0.7043043375015259]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 73/86 [D loss: 0.6954007744789124, acc.: 47.71%] [G loss: 0.7034814357757568]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 74/86 [D loss: 0.6949154436588287, acc.: 49.22%] [G loss: 0.7008004188537598]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 75/86 [D loss: 0.6945516467094421, acc.: 48.54%] [G loss: 0.7006632089614868]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 76/86 [D loss: 0.6966496109962463, acc.: 45.51%] [G loss: 0.7024394273757935]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 77/86 [D loss: 0.6945782005786896, acc.: 49.17%] [G loss: 0.7048189043998718]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 78/86 [D loss: 0.6942765712738037, acc.: 49.32%] [G loss: 0.7016411423683167]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 79/86 [D loss: 0.6963402628898621, acc.: 45.85%] [G loss: 0.7018017172813416]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 80/86 [D loss: 0.6956557929515839, acc.: 47.80%] [G loss: 0.7003462314605713]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 81/86 [D loss: 0.6959773004055023, acc.: 46.44%] [G loss: 0.6974034905433655]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 82/86 [D loss: 0.6956126689910889, acc.: 47.46%] [G loss: 0.7015163898468018]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 83/86 [D loss: 0.6937501430511475, acc.: 50.93%] [G loss: 0.7012409567832947]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 84/86 [D loss: 0.6954746544361115, acc.: 46.44%] [G loss: 0.6993719339370728]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 26/200, Batch 85/86 [D loss: 0.6960123479366302, acc.: 47.07%] [G loss: 0.6985912322998047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 26/200, Batch 86/86 [D loss: 0.6955534815788269, acc.: 47.75%] [G loss: 0.694773256778717]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 1/86 [D loss: 0.6986742317676544, acc.: 44.78%] [G loss: 0.69635009765625]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 2/86 [D loss: 0.6947857737541199, acc.: 48.29%] [G loss: 0.7024523019790649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 3/86 [D loss: 0.694507509469986, acc.: 47.75%] [G loss: 0.6980119347572327]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 4/86 [D loss: 0.6964819133281708, acc.: 47.71%] [G loss: 0.6975420117378235]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 5/86 [D loss: 0.6952225267887115, acc.: 47.07%] [G loss: 0.6956788301467896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 6/86 [D loss: 0.6957151889801025, acc.: 48.05%] [G loss: 0.6820145845413208]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 7/86 [D loss: 0.7028382122516632, acc.: 42.72%] [G loss: 0.6983547210693359]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 8/86 [D loss: 0.6908928453922272, acc.: 54.83%] [G loss: 0.7049123048782349]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 9/86 [D loss: 0.6993890106678009, acc.: 42.33%] [G loss: 0.6895612478256226]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 10/86 [D loss: 0.7005750238895416, acc.: 37.21%] [G loss: 0.6968330144882202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 11/86 [D loss: 0.6919856071472168, acc.: 52.15%] [G loss: 0.6957833766937256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 12/86 [D loss: 0.6939291656017303, acc.: 50.88%] [G loss: 0.6693863272666931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 13/86 [D loss: 0.7107594609260559, acc.: 39.31%] [G loss: 0.6818439960479736]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 14/86 [D loss: 0.6928234398365021, acc.: 52.00%] [G loss: 0.7095518708229065]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 15/86 [D loss: 0.6924936175346375, acc.: 52.29%] [G loss: 0.686386227607727]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 16/86 [D loss: 0.7041256427764893, acc.: 32.32%] [G loss: 0.6896501183509827]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 27/200, Batch 17/86 [D loss: 0.6980819404125214, acc.: 43.41%] [G loss: 0.6960803866386414]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 18/86 [D loss: 0.6942947208881378, acc.: 49.76%] [G loss: 0.6894391179084778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 19/86 [D loss: 0.6966035068035126, acc.: 47.27%] [G loss: 0.679321825504303]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 20/86 [D loss: 0.7015206515789032, acc.: 43.26%] [G loss: 0.6898789405822754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 21/86 [D loss: 0.6947458982467651, acc.: 49.76%] [G loss: 0.6955997347831726]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 22/86 [D loss: 0.6966538727283478, acc.: 45.75%] [G loss: 0.6963807940483093]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 23/86 [D loss: 0.6973840594291687, acc.: 44.63%] [G loss: 0.6959108710289001]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 24/86 [D loss: 0.6963604688644409, acc.: 48.14%] [G loss: 0.6967426538467407]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 25/86 [D loss: 0.6956015229225159, acc.: 47.31%] [G loss: 0.6962257027626038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 26/86 [D loss: 0.6970283389091492, acc.: 44.14%] [G loss: 0.696570873260498]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 27/86 [D loss: 0.6952924132347107, acc.: 46.58%] [G loss: 0.7003797888755798]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 28/86 [D loss: 0.6946116089820862, acc.: 48.14%] [G loss: 0.701557457447052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 29/86 [D loss: 0.6959445476531982, acc.: 46.88%] [G loss: 0.7010486721992493]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 30/86 [D loss: 0.6947658956050873, acc.: 48.97%] [G loss: 0.7016770839691162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 31/86 [D loss: 0.6957828998565674, acc.: 47.17%] [G loss: 0.7016025185585022]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 32/86 [D loss: 0.6958490312099457, acc.: 46.04%] [G loss: 0.6999698877334595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 33/86 [D loss: 0.6957425177097321, acc.: 46.83%] [G loss: 0.7023523449897766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 34/86 [D loss: 0.6950750946998596, acc.: 48.00%] [G loss: 0.7053967714309692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 35/86 [D loss: 0.6944926977157593, acc.: 48.05%] [G loss: 0.7050222158432007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 36/86 [D loss: 0.6951448917388916, acc.: 47.46%] [G loss: 0.7034748792648315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 37/86 [D loss: 0.6950569450855255, acc.: 47.66%] [G loss: 0.7026185393333435]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 38/86 [D loss: 0.6940697431564331, acc.: 49.22%] [G loss: 0.704737663269043]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 39/86 [D loss: 0.6946998536586761, acc.: 48.24%] [G loss: 0.7037898302078247]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 40/86 [D loss: 0.6947504580020905, acc.: 48.97%] [G loss: 0.7050603628158569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 41/86 [D loss: 0.6940940320491791, acc.: 48.68%] [G loss: 0.7037891149520874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 42/86 [D loss: 0.6943142414093018, acc.: 49.61%] [G loss: 0.7042503952980042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 43/86 [D loss: 0.6954315006732941, acc.: 48.10%] [G loss: 0.7030589580535889]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 44/86 [D loss: 0.694345086812973, acc.: 50.63%] [G loss: 0.7032802104949951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 45/86 [D loss: 0.6951135694980621, acc.: 47.41%] [G loss: 0.7018246054649353]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 46/86 [D loss: 0.6950638592243195, acc.: 47.61%] [G loss: 0.7027807235717773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 47/86 [D loss: 0.6954405903816223, acc.: 46.24%] [G loss: 0.7027515769004822]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 48/86 [D loss: 0.6945931017398834, acc.: 48.39%] [G loss: 0.7025785446166992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 49/86 [D loss: 0.6953462064266205, acc.: 47.12%] [G loss: 0.7011337280273438]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 50/86 [D loss: 0.6952562630176544, acc.: 47.61%] [G loss: 0.6984180212020874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 51/86 [D loss: 0.6944847106933594, acc.: 47.85%] [G loss: 0.6999943256378174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 52/86 [D loss: 0.6961510479450226, acc.: 46.78%] [G loss: 0.701149582862854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 53/86 [D loss: 0.6958278119564056, acc.: 47.02%] [G loss: 0.70048987865448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 54/86 [D loss: 0.6949244439601898, acc.: 48.19%] [G loss: 0.6998905539512634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 55/86 [D loss: 0.6956517398357391, acc.: 46.29%] [G loss: 0.6987406611442566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 56/86 [D loss: 0.6942503750324249, acc.: 49.17%] [G loss: 0.6973440647125244]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 57/86 [D loss: 0.6953616738319397, acc.: 47.46%] [G loss: 0.6981369256973267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 58/86 [D loss: 0.6948347389698029, acc.: 48.78%] [G loss: 0.7002201080322266]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 59/86 [D loss: 0.6942978501319885, acc.: 50.00%] [G loss: 0.6985660195350647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 60/86 [D loss: 0.6956481337547302, acc.: 47.27%] [G loss: 0.6969588398933411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 61/86 [D loss: 0.695676863193512, acc.: 47.07%] [G loss: 0.6973492503166199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 62/86 [D loss: 0.6956621706485748, acc.: 46.58%] [G loss: 0.6967887282371521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 63/86 [D loss: 0.6964133381843567, acc.: 46.24%] [G loss: 0.6989624500274658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 64/86 [D loss: 0.6945449113845825, acc.: 49.56%] [G loss: 0.7018036246299744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 65/86 [D loss: 0.6955862939357758, acc.: 47.90%] [G loss: 0.6981043815612793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 66/86 [D loss: 0.6951446831226349, acc.: 47.46%] [G loss: 0.6970858573913574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 67/86 [D loss: 0.6951178908348083, acc.: 47.22%] [G loss: 0.6942965984344482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 68/86 [D loss: 0.6964178383350372, acc.: 46.19%] [G loss: 0.6893925666809082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 69/86 [D loss: 0.6991873681545258, acc.: 44.43%] [G loss: 0.6979517936706543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 70/86 [D loss: 0.6924904882907867, acc.: 51.27%] [G loss: 0.700855016708374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 71/86 [D loss: 0.6965716183185577, acc.: 44.82%] [G loss: 0.6934348940849304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 72/86 [D loss: 0.6989979147911072, acc.: 40.82%] [G loss: 0.6958494782447815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 73/86 [D loss: 0.6939603686332703, acc.: 49.41%] [G loss: 0.6949548125267029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 74/86 [D loss: 0.6947961747646332, acc.: 51.37%] [G loss: 0.6776719689369202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 75/86 [D loss: 0.7041656076908112, acc.: 42.48%] [G loss: 0.6873303651809692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 76/86 [D loss: 0.692011684179306, acc.: 52.88%] [G loss: 0.7104114294052124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 77/86 [D loss: 0.6925645172595978, acc.: 51.81%] [G loss: 0.6870505809783936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 78/86 [D loss: 0.7032140791416168, acc.: 33.69%] [G loss: 0.690142810344696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 79/86 [D loss: 0.6952365338802338, acc.: 45.85%] [G loss: 0.6948606967926025]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 80/86 [D loss: 0.692767858505249, acc.: 52.15%] [G loss: 0.6860306262969971]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 81/86 [D loss: 0.6995278894901276, acc.: 45.51%] [G loss: 0.6752125024795532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 82/86 [D loss: 0.7025258243083954, acc.: 43.31%] [G loss: 0.6973413825035095]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 27/200, Batch 83/86 [D loss: 0.690838634967804, acc.: 54.64%] [G loss: 0.6995323300361633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 84/86 [D loss: 0.6986860930919647, acc.: 41.46%] [G loss: 0.6923018097877502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 85/86 [D loss: 0.6976402103900909, acc.: 44.24%] [G loss: 0.6940778493881226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 27/200, Batch 86/86 [D loss: 0.6957590579986572, acc.: 46.73%] [G loss: 0.694679856300354]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 28/200, Batch 1/86 [D loss: 0.6950562000274658, acc.: 47.61%] [G loss: 0.691918134689331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 2/86 [D loss: 0.6969692707061768, acc.: 44.53%] [G loss: 0.689094066619873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 3/86 [D loss: 0.6980930268764496, acc.: 44.82%] [G loss: 0.6949307918548584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 4/86 [D loss: 0.6953066885471344, acc.: 47.85%] [G loss: 0.6997254490852356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 5/86 [D loss: 0.6948711574077606, acc.: 48.24%] [G loss: 0.7009913921356201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 6/86 [D loss: 0.6968309879302979, acc.: 45.90%] [G loss: 0.7002205848693848]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 7/86 [D loss: 0.696203887462616, acc.: 46.09%] [G loss: 0.6975361704826355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 8/86 [D loss: 0.6951895654201508, acc.: 47.02%] [G loss: 0.6971450448036194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 9/86 [D loss: 0.6959876418113708, acc.: 45.95%] [G loss: 0.6974308490753174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 10/86 [D loss: 0.6946049332618713, acc.: 48.88%] [G loss: 0.7012007236480713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 11/86 [D loss: 0.6942649781703949, acc.: 49.32%] [G loss: 0.7026239037513733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 12/86 [D loss: 0.6950746178627014, acc.: 47.61%] [G loss: 0.7020970582962036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 13/86 [D loss: 0.6956081390380859, acc.: 45.90%] [G loss: 0.7018992900848389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 14/86 [D loss: 0.6947071254253387, acc.: 48.34%] [G loss: 0.7026015520095825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 15/86 [D loss: 0.6949321329593658, acc.: 46.78%] [G loss: 0.7006244659423828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 16/86 [D loss: 0.6945267915725708, acc.: 48.83%] [G loss: 0.7041405439376831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 17/86 [D loss: 0.6945136785507202, acc.: 48.00%] [G loss: 0.702812671661377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 18/86 [D loss: 0.6951365768909454, acc.: 47.61%] [G loss: 0.7031334638595581]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 19/86 [D loss: 0.695805013179779, acc.: 46.09%] [G loss: 0.7034968733787537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 20/86 [D loss: 0.6949409246444702, acc.: 48.29%] [G loss: 0.7036305665969849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 21/86 [D loss: 0.6949899196624756, acc.: 47.95%] [G loss: 0.7025889754295349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 22/86 [D loss: 0.6945953965187073, acc.: 47.12%] [G loss: 0.7008230686187744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 23/86 [D loss: 0.6945600211620331, acc.: 49.12%] [G loss: 0.7025820016860962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 24/86 [D loss: 0.6938124299049377, acc.: 49.61%] [G loss: 0.7028693556785583]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 25/86 [D loss: 0.6945314407348633, acc.: 47.71%] [G loss: 0.703700840473175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 26/86 [D loss: 0.6948548257350922, acc.: 47.95%] [G loss: 0.7029980421066284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 27/86 [D loss: 0.6943720579147339, acc.: 47.90%] [G loss: 0.7010631561279297]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 28/86 [D loss: 0.6943704187870026, acc.: 48.73%] [G loss: 0.7018026113510132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 29/86 [D loss: 0.6951701045036316, acc.: 47.61%] [G loss: 0.700312077999115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 30/86 [D loss: 0.6953850388526917, acc.: 47.56%] [G loss: 0.6990532875061035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 31/86 [D loss: 0.6941713392734528, acc.: 48.68%] [G loss: 0.7012959122657776]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 32/86 [D loss: 0.6941406726837158, acc.: 49.76%] [G loss: 0.7011702060699463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 33/86 [D loss: 0.6962122619152069, acc.: 45.65%] [G loss: 0.6999821662902832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 34/86 [D loss: 0.6955433487892151, acc.: 47.41%] [G loss: 0.6981428265571594]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 35/86 [D loss: 0.6955229043960571, acc.: 46.48%] [G loss: 0.696959912776947]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 36/86 [D loss: 0.6955201923847198, acc.: 48.93%] [G loss: 0.7000384330749512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 37/86 [D loss: 0.6947342455387115, acc.: 48.54%] [G loss: 0.7021269202232361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 38/86 [D loss: 0.6948484778404236, acc.: 47.56%] [G loss: 0.6982322335243225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 39/86 [D loss: 0.6949003040790558, acc.: 48.05%] [G loss: 0.6986626386642456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 40/86 [D loss: 0.6947931051254272, acc.: 48.44%] [G loss: 0.6959531307220459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 41/86 [D loss: 0.6957080066204071, acc.: 46.39%] [G loss: 0.6881337761878967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 42/86 [D loss: 0.6983374953269958, acc.: 44.92%] [G loss: 0.6954584121704102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 43/86 [D loss: 0.6926307380199432, acc.: 51.22%] [G loss: 0.7038554549217224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 44/86 [D loss: 0.6953570544719696, acc.: 46.48%] [G loss: 0.693266749382019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 45/86 [D loss: 0.6983650624752045, acc.: 41.55%] [G loss: 0.6950991153717041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 46/86 [D loss: 0.6938466727733612, acc.: 48.78%] [G loss: 0.6964374780654907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 47/86 [D loss: 0.6938064992427826, acc.: 49.80%] [G loss: 0.6830757260322571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 48/86 [D loss: 0.7004714608192444, acc.: 43.80%] [G loss: 0.6811382174491882]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 49/86 [D loss: 0.6997350454330444, acc.: 44.43%] [G loss: 0.7057338356971741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 50/86 [D loss: 0.689513087272644, acc.: 54.88%] [G loss: 0.6941521167755127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 51/86 [D loss: 0.7008683085441589, acc.: 37.55%] [G loss: 0.6905124187469482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 52/86 [D loss: 0.6980287432670593, acc.: 41.50%] [G loss: 0.6937339305877686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 53/86 [D loss: 0.692705363035202, acc.: 50.83%] [G loss: 0.6905829906463623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 54/86 [D loss: 0.6952710449695587, acc.: 49.12%] [G loss: 0.6812175512313843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 55/86 [D loss: 0.700659304857254, acc.: 42.63%] [G loss: 0.6878793835639954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 56/86 [D loss: 0.6952491104602814, acc.: 48.88%] [G loss: 0.7000082731246948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 57/86 [D loss: 0.6931668519973755, acc.: 50.10%] [G loss: 0.6947358250617981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 58/86 [D loss: 0.6968187987804413, acc.: 45.51%] [G loss: 0.6951736807823181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 59/86 [D loss: 0.6958300769329071, acc.: 45.90%] [G loss: 0.6949227452278137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 60/86 [D loss: 0.6941079497337341, acc.: 49.56%] [G loss: 0.6930228471755981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 61/86 [D loss: 0.6953620314598083, acc.: 47.12%] [G loss: 0.6898231506347656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 62/86 [D loss: 0.6966309547424316, acc.: 46.53%] [G loss: 0.6948795914649963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 63/86 [D loss: 0.6944172978401184, acc.: 49.32%] [G loss: 0.699496865272522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 64/86 [D loss: 0.6938584446907043, acc.: 49.51%] [G loss: 0.6993663907051086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 65/86 [D loss: 0.6956528127193451, acc.: 46.34%] [G loss: 0.6987453103065491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 66/86 [D loss: 0.6948525607585907, acc.: 47.46%] [G loss: 0.69972825050354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 67/86 [D loss: 0.6943165063858032, acc.: 49.02%] [G loss: 0.6978726983070374]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 68/86 [D loss: 0.6952299475669861, acc.: 47.56%] [G loss: 0.6976451873779297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 69/86 [D loss: 0.6960141360759735, acc.: 45.70%] [G loss: 0.7008146047592163]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 70/86 [D loss: 0.6942769587039948, acc.: 48.44%] [G loss: 0.7016507983207703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 71/86 [D loss: 0.695372462272644, acc.: 47.66%] [G loss: 0.702485978603363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 72/86 [D loss: 0.6944054067134857, acc.: 48.54%] [G loss: 0.7018531560897827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 73/86 [D loss: 0.6943264603614807, acc.: 48.24%] [G loss: 0.7024719715118408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 74/86 [D loss: 0.6952691376209259, acc.: 46.53%] [G loss: 0.7014147639274597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 75/86 [D loss: 0.695776492357254, acc.: 46.78%] [G loss: 0.703119158744812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 76/86 [D loss: 0.6945559680461884, acc.: 48.34%] [G loss: 0.7045106291770935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 77/86 [D loss: 0.6949592530727386, acc.: 49.12%] [G loss: 0.7028236389160156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 78/86 [D loss: 0.6950839757919312, acc.: 47.31%] [G loss: 0.7022474408149719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 79/86 [D loss: 0.6942052841186523, acc.: 48.44%] [G loss: 0.703201949596405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 80/86 [D loss: 0.694422572851181, acc.: 48.49%] [G loss: 0.7011651396751404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 81/86 [D loss: 0.6941336691379547, acc.: 48.29%] [G loss: 0.7028085589408875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 82/86 [D loss: 0.6942248642444611, acc.: 48.54%] [G loss: 0.7025317549705505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 83/86 [D loss: 0.6934083700180054, acc.: 50.68%] [G loss: 0.7025330662727356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 84/86 [D loss: 0.6942793726921082, acc.: 48.34%] [G loss: 0.7009688019752502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 28/200, Batch 85/86 [D loss: 0.69539475440979, acc.: 47.12%] [G loss: 0.7005301713943481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 28/200, Batch 86/86 [D loss: 0.6944643259048462, acc.: 48.83%] [G loss: 0.6992615461349487]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 1/86 [D loss: 0.6952280700206757, acc.: 46.53%] [G loss: 0.6990076303482056]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 2/86 [D loss: 0.6950375437736511, acc.: 48.00%] [G loss: 0.7004597783088684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 3/86 [D loss: 0.6940864324569702, acc.: 50.24%] [G loss: 0.7000700235366821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 4/86 [D loss: 0.694656103849411, acc.: 48.44%] [G loss: 0.700416088104248]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 5/86 [D loss: 0.6942644417285919, acc.: 48.39%] [G loss: 0.7002646923065186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 6/86 [D loss: 0.6951006352901459, acc.: 48.24%] [G loss: 0.7002872824668884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 7/86 [D loss: 0.694812685251236, acc.: 47.46%] [G loss: 0.6972306966781616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 8/86 [D loss: 0.6962267160415649, acc.: 46.29%] [G loss: 0.6976209282875061]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 9/86 [D loss: 0.6943129897117615, acc.: 49.66%] [G loss: 0.7015225887298584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 10/86 [D loss: 0.6944881677627563, acc.: 48.83%] [G loss: 0.6987991333007812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 11/86 [D loss: 0.6942308843135834, acc.: 48.54%] [G loss: 0.6983801126480103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 12/86 [D loss: 0.6958998143672943, acc.: 46.19%] [G loss: 0.6976607441902161]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 13/86 [D loss: 0.6944663822650909, acc.: 49.85%] [G loss: 0.6920794248580933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 14/86 [D loss: 0.6982478499412537, acc.: 45.02%] [G loss: 0.6949468851089478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 15/86 [D loss: 0.6945250630378723, acc.: 48.44%] [G loss: 0.7042933702468872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 16/86 [D loss: 0.6946657299995422, acc.: 48.00%] [G loss: 0.6953367590904236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 17/86 [D loss: 0.6971612870693207, acc.: 42.53%] [G loss: 0.6957266926765442]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 18/86 [D loss: 0.6944575309753418, acc.: 47.56%] [G loss: 0.6970873475074768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 19/86 [D loss: 0.693355917930603, acc.: 50.54%] [G loss: 0.6914657354354858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 20/86 [D loss: 0.6976804733276367, acc.: 45.80%] [G loss: 0.6852883100509644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 21/86 [D loss: 0.6994275450706482, acc.: 44.78%] [G loss: 0.7003962993621826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 22/86 [D loss: 0.6914771497249603, acc.: 54.00%] [G loss: 0.6980257034301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 23/86 [D loss: 0.6972713470458984, acc.: 44.29%] [G loss: 0.6939203143119812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 24/86 [D loss: 0.6974546611309052, acc.: 43.90%] [G loss: 0.6947694420814514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 25/86 [D loss: 0.6937887668609619, acc.: 49.85%] [G loss: 0.6943110227584839]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 26/86 [D loss: 0.6943513453006744, acc.: 49.17%] [G loss: 0.6850587725639343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 27/86 [D loss: 0.6994153559207916, acc.: 45.17%] [G loss: 0.6873201131820679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 28/86 [D loss: 0.6961512565612793, acc.: 48.39%] [G loss: 0.7014778256416321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 29/86 [D loss: 0.6928602755069733, acc.: 50.49%] [G loss: 0.697598397731781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 30/86 [D loss: 0.6971757709980011, acc.: 43.51%] [G loss: 0.6955410242080688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 31/86 [D loss: 0.6963648796081543, acc.: 46.39%] [G loss: 0.6974274516105652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 32/86 [D loss: 0.6942387819290161, acc.: 48.68%] [G loss: 0.693584680557251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 33/86 [D loss: 0.6951903700828552, acc.: 47.85%] [G loss: 0.6922827959060669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 34/86 [D loss: 0.6962064206600189, acc.: 47.71%] [G loss: 0.6944403648376465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 35/86 [D loss: 0.6951333284378052, acc.: 49.46%] [G loss: 0.6996946334838867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 36/86 [D loss: 0.6948113739490509, acc.: 48.93%] [G loss: 0.6998974084854126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 37/86 [D loss: 0.6948978900909424, acc.: 46.73%] [G loss: 0.6998974680900574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 38/86 [D loss: 0.6952100694179535, acc.: 47.75%] [G loss: 0.6992362141609192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 39/86 [D loss: 0.6941401958465576, acc.: 50.20%] [G loss: 0.6985952854156494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 40/86 [D loss: 0.6949212849140167, acc.: 47.66%] [G loss: 0.6970781087875366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 41/86 [D loss: 0.6963168978691101, acc.: 44.63%] [G loss: 0.6993973851203918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 42/86 [D loss: 0.6932951807975769, acc.: 50.20%] [G loss: 0.7042235732078552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 43/86 [D loss: 0.6950626373291016, acc.: 47.85%] [G loss: 0.7041739225387573]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 44/86 [D loss: 0.6956590712070465, acc.: 47.36%] [G loss: 0.702567994594574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 45/86 [D loss: 0.6936502158641815, acc.: 50.00%] [G loss: 0.6998215913772583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 46/86 [D loss: 0.6946333050727844, acc.: 48.24%] [G loss: 0.7014929056167603]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 47/86 [D loss: 0.6949898600578308, acc.: 49.12%] [G loss: 0.7015832662582397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 48/86 [D loss: 0.694032609462738, acc.: 48.97%] [G loss: 0.7016385793685913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 49/86 [D loss: 0.6939947307109833, acc.: 48.78%] [G loss: 0.7036636471748352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 50/86 [D loss: 0.6946468949317932, acc.: 48.58%] [G loss: 0.7026010155677795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 51/86 [D loss: 0.6940330564975739, acc.: 50.63%] [G loss: 0.700905978679657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 52/86 [D loss: 0.6944661140441895, acc.: 50.88%] [G loss: 0.6986091136932373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 53/86 [D loss: 0.6957961916923523, acc.: 45.75%] [G loss: 0.701561689376831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 54/86 [D loss: 0.6946524083614349, acc.: 49.32%] [G loss: 0.7029558420181274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 55/86 [D loss: 0.6939854621887207, acc.: 49.02%] [G loss: 0.7018017768859863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 56/86 [D loss: 0.6951037347316742, acc.: 48.29%] [G loss: 0.700488269329071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 57/86 [D loss: 0.6947152614593506, acc.: 47.71%] [G loss: 0.6999067068099976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 58/86 [D loss: 0.6950558423995972, acc.: 46.68%] [G loss: 0.6999845504760742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 59/86 [D loss: 0.6952304840087891, acc.: 47.31%] [G loss: 0.69901442527771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 60/86 [D loss: 0.69482421875, acc.: 49.46%] [G loss: 0.7001221776008606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 61/86 [D loss: 0.6949052512645721, acc.: 45.75%] [G loss: 0.7028706073760986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 62/86 [D loss: 0.6951231062412262, acc.: 48.58%] [G loss: 0.7006898522377014]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 63/86 [D loss: 0.6940928101539612, acc.: 50.15%] [G loss: 0.6996374726295471]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 64/86 [D loss: 0.6946663558483124, acc.: 47.80%] [G loss: 0.7002019882202148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 65/86 [D loss: 0.6955846846103668, acc.: 46.44%] [G loss: 0.6980069875717163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 66/86 [D loss: 0.6937333941459656, acc.: 48.97%] [G loss: 0.6973546147346497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 67/86 [D loss: 0.6949475705623627, acc.: 47.31%] [G loss: 0.698523998260498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 68/86 [D loss: 0.6943167150020599, acc.: 48.78%] [G loss: 0.7013416290283203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 69/86 [D loss: 0.6942608952522278, acc.: 49.27%] [G loss: 0.6989863514900208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 70/86 [D loss: 0.694033682346344, acc.: 49.90%] [G loss: 0.6983755826950073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 71/86 [D loss: 0.6952024698257446, acc.: 46.39%] [G loss: 0.6973965167999268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 72/86 [D loss: 0.6938303411006927, acc.: 49.95%] [G loss: 0.6949933767318726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 73/86 [D loss: 0.6942616403102875, acc.: 50.59%] [G loss: 0.69767826795578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 74/86 [D loss: 0.6942269504070282, acc.: 49.17%] [G loss: 0.6993740200996399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 75/86 [D loss: 0.6952853202819824, acc.: 46.53%] [G loss: 0.6975539922714233]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 76/86 [D loss: 0.6952822804450989, acc.: 47.17%] [G loss: 0.698214590549469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 77/86 [D loss: 0.694545567035675, acc.: 47.90%] [G loss: 0.6947969198226929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 78/86 [D loss: 0.6950766444206238, acc.: 48.24%] [G loss: 0.6954441070556641]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 79/86 [D loss: 0.6950427889823914, acc.: 48.05%] [G loss: 0.6980265974998474]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 80/86 [D loss: 0.6935775578022003, acc.: 49.32%] [G loss: 0.6985546350479126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 81/86 [D loss: 0.6944478154182434, acc.: 48.63%] [G loss: 0.6984097957611084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 82/86 [D loss: 0.6947693228721619, acc.: 46.09%] [G loss: 0.696221113204956]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 83/86 [D loss: 0.6952112019062042, acc.: 46.73%] [G loss: 0.6952183246612549]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 84/86 [D loss: 0.6949903666973114, acc.: 47.71%] [G loss: 0.6947422027587891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 29/200, Batch 85/86 [D loss: 0.6955604255199432, acc.: 46.58%] [G loss: 0.6993659138679504]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 29/200, Batch 86/86 [D loss: 0.6935863792896271, acc.: 49.76%] [G loss: 0.6981996893882751]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 1/86 [D loss: 0.6956599354743958, acc.: 45.90%] [G loss: 0.6990188956260681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 2/86 [D loss: 0.6940617263317108, acc.: 48.44%] [G loss: 0.697530210018158]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 3/86 [D loss: 0.6952333152294159, acc.: 47.66%] [G loss: 0.6979326009750366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 4/86 [D loss: 0.6943663954734802, acc.: 49.37%] [G loss: 0.6965105533599854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 5/86 [D loss: 0.696739137172699, acc.: 45.61%] [G loss: 0.6965909004211426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 6/86 [D loss: 0.6938580274581909, acc.: 49.85%] [G loss: 0.7002049088478088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 7/86 [D loss: 0.6949238181114197, acc.: 47.71%] [G loss: 0.6996431946754456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 8/86 [D loss: 0.6943500936031342, acc.: 48.14%] [G loss: 0.6983540654182434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 9/86 [D loss: 0.6938741505146027, acc.: 48.73%] [G loss: 0.6959748268127441]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 10/86 [D loss: 0.6954253017902374, acc.: 47.46%] [G loss: 0.6960282325744629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 11/86 [D loss: 0.6960688531398773, acc.: 45.51%] [G loss: 0.6974653005599976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 12/86 [D loss: 0.6941965222358704, acc.: 48.83%] [G loss: 0.701869785785675]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 13/86 [D loss: 0.6947633624076843, acc.: 48.19%] [G loss: 0.6991075873374939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 14/86 [D loss: 0.6948878765106201, acc.: 46.58%] [G loss: 0.6996748447418213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 15/86 [D loss: 0.6953220963478088, acc.: 47.36%] [G loss: 0.6983715295791626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 16/86 [D loss: 0.6946550607681274, acc.: 47.66%] [G loss: 0.6996148228645325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 17/86 [D loss: 0.6959919929504395, acc.: 45.80%] [G loss: 0.6963451504707336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 18/86 [D loss: 0.6953230798244476, acc.: 47.75%] [G loss: 0.7018357515335083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 19/86 [D loss: 0.6944906115531921, acc.: 48.97%] [G loss: 0.70119708776474]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 20/86 [D loss: 0.6948724985122681, acc.: 47.46%] [G loss: 0.699164867401123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 21/86 [D loss: 0.6945011913776398, acc.: 49.56%] [G loss: 0.6989853382110596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 22/86 [D loss: 0.6951664090156555, acc.: 47.36%] [G loss: 0.6975846290588379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 23/86 [D loss: 0.6952897906303406, acc.: 45.85%] [G loss: 0.6982481479644775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 24/86 [D loss: 0.6946362853050232, acc.: 48.63%] [G loss: 0.7003734111785889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 25/86 [D loss: 0.6948141157627106, acc.: 48.63%] [G loss: 0.6995498538017273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 26/86 [D loss: 0.6952303349971771, acc.: 47.51%] [G loss: 0.6994688510894775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 27/86 [D loss: 0.6948518753051758, acc.: 47.95%] [G loss: 0.6975346803665161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 28/86 [D loss: 0.6950793266296387, acc.: 48.14%] [G loss: 0.6946610808372498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 29/86 [D loss: 0.6965853869915009, acc.: 45.80%] [G loss: 0.6977956295013428]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 30/86 [D loss: 0.6942606568336487, acc.: 50.00%] [G loss: 0.7004319429397583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 31/86 [D loss: 0.6938583850860596, acc.: 48.83%] [G loss: 0.7004348635673523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 32/86 [D loss: 0.6953867673873901, acc.: 46.83%] [G loss: 0.6986506581306458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 33/86 [D loss: 0.6951954960823059, acc.: 46.53%] [G loss: 0.6978680491447449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 34/86 [D loss: 0.6947808265686035, acc.: 47.61%] [G loss: 0.695254921913147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 35/86 [D loss: 0.6951685547828674, acc.: 46.53%] [G loss: 0.6989983320236206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 36/86 [D loss: 0.6956725716590881, acc.: 47.51%] [G loss: 0.7009581327438354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 37/86 [D loss: 0.6952347159385681, acc.: 47.07%] [G loss: 0.7001621723175049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 38/86 [D loss: 0.695435643196106, acc.: 46.24%] [G loss: 0.6995362043380737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 39/86 [D loss: 0.6950761377811432, acc.: 46.63%] [G loss: 0.6977196335792542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 40/86 [D loss: 0.6955098211765289, acc.: 48.34%] [G loss: 0.6947944760322571]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 41/86 [D loss: 0.6961935758590698, acc.: 45.95%] [G loss: 0.698652982711792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 42/86 [D loss: 0.6951633989810944, acc.: 47.75%] [G loss: 0.7002800703048706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 43/86 [D loss: 0.6951967477798462, acc.: 47.46%] [G loss: 0.7004575133323669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 44/86 [D loss: 0.6953163743019104, acc.: 46.63%] [G loss: 0.6997960805892944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 45/86 [D loss: 0.6953939199447632, acc.: 48.24%] [G loss: 0.6973713636398315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 46/86 [D loss: 0.6957646906375885, acc.: 47.07%] [G loss: 0.6940327286720276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 47/86 [D loss: 0.6964598298072815, acc.: 45.80%] [G loss: 0.6988425254821777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 48/86 [D loss: 0.6940021514892578, acc.: 49.37%] [G loss: 0.7012097239494324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 49/86 [D loss: 0.6952949464321136, acc.: 47.12%] [G loss: 0.6990365386009216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 50/86 [D loss: 0.6955519020557404, acc.: 46.92%] [G loss: 0.6995576620101929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 51/86 [D loss: 0.6942158639431, acc.: 49.22%] [G loss: 0.6981675624847412]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 52/86 [D loss: 0.6946631968021393, acc.: 48.49%] [G loss: 0.6942917108535767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 53/86 [D loss: 0.6968729197978973, acc.: 45.85%] [G loss: 0.6996708512306213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 54/86 [D loss: 0.6934637129306793, acc.: 50.20%] [G loss: 0.7016146183013916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 55/86 [D loss: 0.6939886808395386, acc.: 48.78%] [G loss: 0.7002366185188293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 56/86 [D loss: 0.6953610777854919, acc.: 46.48%] [G loss: 0.6988521814346313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 57/86 [D loss: 0.6948117911815643, acc.: 48.00%] [G loss: 0.698660135269165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 58/86 [D loss: 0.6960494518280029, acc.: 45.51%] [G loss: 0.6939892768859863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 59/86 [D loss: 0.696345865726471, acc.: 46.44%] [G loss: 0.6952176690101624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 60/86 [D loss: 0.6945363581180573, acc.: 50.05%] [G loss: 0.7019375562667847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 61/86 [D loss: 0.6946726739406586, acc.: 47.51%] [G loss: 0.7003881931304932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 62/86 [D loss: 0.6941110491752625, acc.: 48.93%] [G loss: 0.699253261089325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 63/86 [D loss: 0.6950597167015076, acc.: 47.17%] [G loss: 0.6979526877403259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 64/86 [D loss: 0.6951667070388794, acc.: 47.22%] [G loss: 0.6961618661880493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 65/86 [D loss: 0.6960252225399017, acc.: 46.04%] [G loss: 0.6954855918884277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 66/86 [D loss: 0.6953509747982025, acc.: 46.88%] [G loss: 0.7019181251525879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 67/86 [D loss: 0.6945557296276093, acc.: 48.39%] [G loss: 0.6990877389907837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 68/86 [D loss: 0.6956524550914764, acc.: 46.34%] [G loss: 0.6980291604995728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 69/86 [D loss: 0.6945606470108032, acc.: 49.22%] [G loss: 0.6984367966651917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 70/86 [D loss: 0.6946950852870941, acc.: 48.88%] [G loss: 0.6977518200874329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 71/86 [D loss: 0.694546103477478, acc.: 48.49%] [G loss: 0.6991294026374817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 72/86 [D loss: 0.6943648755550385, acc.: 49.37%] [G loss: 0.7010919451713562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 73/86 [D loss: 0.6937119960784912, acc.: 49.61%] [G loss: 0.7006095051765442]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 74/86 [D loss: 0.6952707469463348, acc.: 46.78%] [G loss: 0.6995960474014282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 75/86 [D loss: 0.6940171718597412, acc.: 48.34%] [G loss: 0.7007718086242676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 76/86 [D loss: 0.6948806345462799, acc.: 47.41%] [G loss: 0.6983426809310913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 77/86 [D loss: 0.6946207582950592, acc.: 47.61%] [G loss: 0.6971333026885986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 78/86 [D loss: 0.6957205533981323, acc.: 46.48%] [G loss: 0.6988625526428223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 79/86 [D loss: 0.6943980157375336, acc.: 48.44%] [G loss: 0.7002785205841064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 80/86 [D loss: 0.6948385238647461, acc.: 49.02%] [G loss: 0.7008752822875977]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 81/86 [D loss: 0.6951994299888611, acc.: 47.90%] [G loss: 0.6985309720039368]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 82/86 [D loss: 0.6952057778835297, acc.: 47.12%] [G loss: 0.6981784105300903]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 83/86 [D loss: 0.6958493292331696, acc.: 46.19%] [G loss: 0.6996017694473267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 84/86 [D loss: 0.6942262649536133, acc.: 50.39%] [G loss: 0.7019938230514526]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 30/200, Batch 85/86 [D loss: 0.6943391561508179, acc.: 49.90%] [G loss: 0.7021093368530273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 30/200, Batch 86/86 [D loss: 0.6953026354312897, acc.: 47.27%] [G loss: 0.7010080218315125]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 1/86 [D loss: 0.6950294077396393, acc.: 47.61%] [G loss: 0.6991572380065918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 2/86 [D loss: 0.6950317919254303, acc.: 47.90%] [G loss: 0.6975927352905273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 3/86 [D loss: 0.6946589946746826, acc.: 48.34%] [G loss: 0.6998611092567444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 4/86 [D loss: 0.6948122382164001, acc.: 48.10%] [G loss: 0.7004013061523438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 5/86 [D loss: 0.6943734288215637, acc.: 47.71%] [G loss: 0.7009398937225342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 6/86 [D loss: 0.6949786841869354, acc.: 47.85%] [G loss: 0.7018633484840393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 7/86 [D loss: 0.6954081654548645, acc.: 47.12%] [G loss: 0.7004621028900146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 8/86 [D loss: 0.6942834258079529, acc.: 47.75%] [G loss: 0.6981809139251709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 9/86 [D loss: 0.6952789723873138, acc.: 47.02%] [G loss: 0.6972931623458862]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 10/86 [D loss: 0.6949168741703033, acc.: 47.85%] [G loss: 0.7006140947341919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 11/86 [D loss: 0.6933970153331757, acc.: 50.24%] [G loss: 0.7005038857460022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 12/86 [D loss: 0.6945256888866425, acc.: 49.46%] [G loss: 0.6995408535003662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 13/86 [D loss: 0.6948599517345428, acc.: 48.49%] [G loss: 0.6994163990020752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 14/86 [D loss: 0.6960785090923309, acc.: 46.58%] [G loss: 0.6983100175857544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 15/86 [D loss: 0.6953803598880768, acc.: 48.44%] [G loss: 0.6964271664619446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 16/86 [D loss: 0.6963768899440765, acc.: 46.48%] [G loss: 0.6991830468177795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 17/86 [D loss: 0.6935674548149109, acc.: 49.51%] [G loss: 0.6996708512306213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 18/86 [D loss: 0.6940981447696686, acc.: 48.88%] [G loss: 0.699506402015686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 19/86 [D loss: 0.6942265927791595, acc.: 48.14%] [G loss: 0.698074221611023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 20/86 [D loss: 0.6954117119312286, acc.: 47.75%] [G loss: 0.696820080280304]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 21/86 [D loss: 0.6959349811077118, acc.: 46.63%] [G loss: 0.694690465927124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 22/86 [D loss: 0.6954202055931091, acc.: 47.41%] [G loss: 0.7004794478416443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 23/86 [D loss: 0.6939087212085724, acc.: 49.22%] [G loss: 0.7000123858451843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 24/86 [D loss: 0.6946992874145508, acc.: 48.00%] [G loss: 0.7004636526107788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 25/86 [D loss: 0.6942037642002106, acc.: 48.83%] [G loss: 0.6979814171791077]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 26/86 [D loss: 0.6954863965511322, acc.: 46.48%] [G loss: 0.6965981125831604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 27/86 [D loss: 0.694875955581665, acc.: 47.36%] [G loss: 0.6967340111732483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 28/86 [D loss: 0.6954377293586731, acc.: 47.90%] [G loss: 0.7019484043121338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 29/86 [D loss: 0.6946251392364502, acc.: 47.07%] [G loss: 0.7014142870903015]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 30/86 [D loss: 0.6949213743209839, acc.: 47.51%] [G loss: 0.6991259455680847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 31/86 [D loss: 0.6938671171665192, acc.: 50.29%] [G loss: 0.6990359425544739]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 32/86 [D loss: 0.6947650015354156, acc.: 47.02%] [G loss: 0.6960905194282532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 33/86 [D loss: 0.6960658729076385, acc.: 45.26%] [G loss: 0.6965614557266235]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 34/86 [D loss: 0.6951172351837158, acc.: 48.29%] [G loss: 0.7022614479064941]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 35/86 [D loss: 0.6941689848899841, acc.: 48.54%] [G loss: 0.699589192867279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 36/86 [D loss: 0.695633590221405, acc.: 46.78%] [G loss: 0.698701024055481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 37/86 [D loss: 0.6942736506462097, acc.: 49.46%] [G loss: 0.6989215016365051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 38/86 [D loss: 0.6957480609416962, acc.: 45.17%] [G loss: 0.6963040232658386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 39/86 [D loss: 0.6946389079093933, acc.: 49.76%] [G loss: 0.6973603963851929]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 40/86 [D loss: 0.6956256031990051, acc.: 46.97%] [G loss: 0.7030220031738281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 41/86 [D loss: 0.6929730772972107, acc.: 50.88%] [G loss: 0.7015153169631958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 42/86 [D loss: 0.6950734555721283, acc.: 46.97%] [G loss: 0.7004747986793518]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 43/86 [D loss: 0.6948267817497253, acc.: 47.02%] [G loss: 0.6991772651672363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 44/86 [D loss: 0.6950743198394775, acc.: 47.07%] [G loss: 0.6955112218856812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 45/86 [D loss: 0.6964443325996399, acc.: 46.24%] [G loss: 0.6981260180473328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 46/86 [D loss: 0.6942017078399658, acc.: 49.80%] [G loss: 0.7031418085098267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 47/86 [D loss: 0.6928707659244537, acc.: 50.98%] [G loss: 0.7022078633308411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 48/86 [D loss: 0.6949518024921417, acc.: 46.88%] [G loss: 0.7003447413444519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 49/86 [D loss: 0.6933301389217377, acc.: 50.10%] [G loss: 0.7012404799461365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 50/86 [D loss: 0.6944087743759155, acc.: 48.97%] [G loss: 0.6996708512306213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 51/86 [D loss: 0.6958065330982208, acc.: 47.02%] [G loss: 0.6974725127220154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 52/86 [D loss: 0.6948214769363403, acc.: 47.95%] [G loss: 0.7033594250679016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 53/86 [D loss: 0.6939694881439209, acc.: 49.71%] [G loss: 0.7016656398773193]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 54/86 [D loss: 0.6951710283756256, acc.: 47.61%] [G loss: 0.7016870379447937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 55/86 [D loss: 0.6942285597324371, acc.: 49.22%] [G loss: 0.7010797262191772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 56/86 [D loss: 0.6945919692516327, acc.: 48.58%] [G loss: 0.6958256363868713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 57/86 [D loss: 0.6958784759044647, acc.: 46.97%] [G loss: 0.6981425881385803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 58/86 [D loss: 0.6950721144676208, acc.: 47.80%] [G loss: 0.7020326256752014]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 59/86 [D loss: 0.693295806646347, acc.: 51.86%] [G loss: 0.7013271450996399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 60/86 [D loss: 0.6957930624485016, acc.: 46.04%] [G loss: 0.6997292041778564]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 61/86 [D loss: 0.6940384209156036, acc.: 47.75%] [G loss: 0.6997659206390381]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 62/86 [D loss: 0.6941189765930176, acc.: 49.46%] [G loss: 0.696682333946228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 63/86 [D loss: 0.6966036558151245, acc.: 45.70%] [G loss: 0.6958209872245789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 64/86 [D loss: 0.6948314309120178, acc.: 47.90%] [G loss: 0.7032247185707092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 65/86 [D loss: 0.6929021775722504, acc.: 51.07%] [G loss: 0.7002168893814087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 66/86 [D loss: 0.6952279508113861, acc.: 47.66%] [G loss: 0.6987606287002563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 67/86 [D loss: 0.6949320435523987, acc.: 47.46%] [G loss: 0.6981414556503296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 68/86 [D loss: 0.6949544548988342, acc.: 46.73%] [G loss: 0.6937997937202454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 69/86 [D loss: 0.6968812346458435, acc.: 45.85%] [G loss: 0.6955276727676392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 70/86 [D loss: 0.6951196491718292, acc.: 47.07%] [G loss: 0.7008678317070007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 71/86 [D loss: 0.6935549080371857, acc.: 50.29%] [G loss: 0.6988727450370789]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 72/86 [D loss: 0.6957007050514221, acc.: 47.27%] [G loss: 0.6980605721473694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 73/86 [D loss: 0.6939393579959869, acc.: 49.95%] [G loss: 0.6988112330436707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 74/86 [D loss: 0.6937783062458038, acc.: 49.85%] [G loss: 0.6942751407623291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 75/86 [D loss: 0.6964725852012634, acc.: 46.48%] [G loss: 0.6936405301094055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 76/86 [D loss: 0.6956340968608856, acc.: 46.00%] [G loss: 0.6995182633399963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 77/86 [D loss: 0.6943808495998383, acc.: 49.46%] [G loss: 0.6992923021316528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 78/86 [D loss: 0.6953614056110382, acc.: 46.63%] [G loss: 0.6979396939277649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 79/86 [D loss: 0.6951439678668976, acc.: 47.90%] [G loss: 0.6993051767349243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 80/86 [D loss: 0.6939407885074615, acc.: 49.80%] [G loss: 0.6964843273162842]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 81/86 [D loss: 0.6943531632423401, acc.: 48.44%] [G loss: 0.6961489319801331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 82/86 [D loss: 0.6947290897369385, acc.: 48.97%] [G loss: 0.7011774778366089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 83/86 [D loss: 0.6937239170074463, acc.: 49.95%] [G loss: 0.7003353834152222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 31/200, Batch 84/86 [D loss: 0.694559633731842, acc.: 48.73%] [G loss: 0.7000492215156555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 85/86 [D loss: 0.6949372589588165, acc.: 47.90%] [G loss: 0.7004948854446411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 31/200, Batch 86/86 [D loss: 0.6944769024848938, acc.: 47.66%] [G loss: 0.7002668380737305]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 1/86 [D loss: 0.6942344903945923, acc.: 47.51%] [G loss: 0.6986940503120422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 2/86 [D loss: 0.6948782503604889, acc.: 46.83%] [G loss: 0.7017678022384644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 3/86 [D loss: 0.6938656568527222, acc.: 49.02%] [G loss: 0.7026184797286987]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 4/86 [D loss: 0.6938799917697906, acc.: 49.46%] [G loss: 0.7014942169189453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 5/86 [D loss: 0.6942822933197021, acc.: 50.10%] [G loss: 0.7002336382865906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 6/86 [D loss: 0.6949716210365295, acc.: 48.10%] [G loss: 0.697973370552063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 7/86 [D loss: 0.6949684619903564, acc.: 47.31%] [G loss: 0.6990244388580322]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 8/86 [D loss: 0.6944350004196167, acc.: 49.12%] [G loss: 0.7015562653541565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 9/86 [D loss: 0.6930466294288635, acc.: 49.76%] [G loss: 0.7010462880134583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 10/86 [D loss: 0.694343626499176, acc.: 49.76%] [G loss: 0.7015836238861084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 11/86 [D loss: 0.6946999430656433, acc.: 49.22%] [G loss: 0.7009018659591675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 12/86 [D loss: 0.6943901777267456, acc.: 48.88%] [G loss: 0.6991321444511414]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 13/86 [D loss: 0.6952478587627411, acc.: 46.78%] [G loss: 0.6992907524108887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 14/86 [D loss: 0.6941930651664734, acc.: 48.58%] [G loss: 0.7017898559570312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 15/86 [D loss: 0.6942340433597565, acc.: 48.97%] [G loss: 0.6999720931053162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 16/86 [D loss: 0.695389598608017, acc.: 46.34%] [G loss: 0.7013571262359619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 17/86 [D loss: 0.6933813095092773, acc.: 49.95%] [G loss: 0.6999514102935791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 18/86 [D loss: 0.6955761015415192, acc.: 45.31%] [G loss: 0.6963412761688232]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 19/86 [D loss: 0.6961985230445862, acc.: 46.44%] [G loss: 0.6984982490539551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 20/86 [D loss: 0.6942042708396912, acc.: 48.58%] [G loss: 0.7006134986877441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 21/86 [D loss: 0.6948955059051514, acc.: 48.34%] [G loss: 0.6978719830513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 22/86 [D loss: 0.6947927474975586, acc.: 48.63%] [G loss: 0.7000818252563477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 23/86 [D loss: 0.6942743062973022, acc.: 47.46%] [G loss: 0.6967253088951111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 24/86 [D loss: 0.69538813829422, acc.: 46.44%] [G loss: 0.6971505880355835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 25/86 [D loss: 0.6955374479293823, acc.: 47.27%] [G loss: 0.6999425888061523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 26/86 [D loss: 0.6948077380657196, acc.: 48.73%] [G loss: 0.7009931206703186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 27/86 [D loss: 0.6937806904315948, acc.: 49.46%] [G loss: 0.6996956467628479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 28/86 [D loss: 0.694715827703476, acc.: 47.75%] [G loss: 0.699954092502594]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 29/86 [D loss: 0.6942299604415894, acc.: 48.68%] [G loss: 0.698023796081543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 30/86 [D loss: 0.6950976550579071, acc.: 47.12%] [G loss: 0.6965600848197937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 31/86 [D loss: 0.6949274837970734, acc.: 47.46%] [G loss: 0.7013185024261475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 32/86 [D loss: 0.6938717365264893, acc.: 48.88%] [G loss: 0.7015641331672668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 33/86 [D loss: 0.6944788098335266, acc.: 48.39%] [G loss: 0.6991640329360962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 34/86 [D loss: 0.6952251493930817, acc.: 48.05%] [G loss: 0.7005318999290466]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 35/86 [D loss: 0.6939496695995331, acc.: 49.02%] [G loss: 0.6980335116386414]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 36/86 [D loss: 0.6950671374797821, acc.: 48.39%] [G loss: 0.6937388777732849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 37/86 [D loss: 0.6970081627368927, acc.: 45.26%] [G loss: 0.6983487606048584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 38/86 [D loss: 0.692540317773819, acc.: 51.56%] [G loss: 0.7018729448318481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 39/86 [D loss: 0.6944401264190674, acc.: 48.29%] [G loss: 0.6990212798118591]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 40/86 [D loss: 0.6958507597446442, acc.: 46.58%] [G loss: 0.6987385749816895]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 41/86 [D loss: 0.6943384408950806, acc.: 48.97%] [G loss: 0.696679413318634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 42/86 [D loss: 0.6950872242450714, acc.: 47.85%] [G loss: 0.6930080652236938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 43/86 [D loss: 0.6979749798774719, acc.: 43.51%] [G loss: 0.6983181238174438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 44/86 [D loss: 0.6935073435306549, acc.: 48.73%] [G loss: 0.701240062713623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 45/86 [D loss: 0.6947827339172363, acc.: 47.66%] [G loss: 0.6977547407150269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 46/86 [D loss: 0.6963753700256348, acc.: 45.31%] [G loss: 0.6980668306350708]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 47/86 [D loss: 0.6949760317802429, acc.: 46.58%] [G loss: 0.6965049505233765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 48/86 [D loss: 0.6932349503040314, acc.: 50.73%] [G loss: 0.693635106086731]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 49/86 [D loss: 0.6955589950084686, acc.: 47.07%] [G loss: 0.6959963440895081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 50/86 [D loss: 0.6945637166500092, acc.: 47.90%] [G loss: 0.7004542350769043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 51/86 [D loss: 0.6938747763633728, acc.: 49.66%] [G loss: 0.6982979774475098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 52/86 [D loss: 0.6948103606700897, acc.: 48.24%] [G loss: 0.6971278190612793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 53/86 [D loss: 0.6950741708278656, acc.: 48.05%] [G loss: 0.6976027488708496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 54/86 [D loss: 0.694716215133667, acc.: 47.61%] [G loss: 0.6958794593811035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 55/86 [D loss: 0.6959374845027924, acc.: 46.09%] [G loss: 0.6937945485115051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 56/86 [D loss: 0.6952175498008728, acc.: 47.56%] [G loss: 0.7015763521194458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 57/86 [D loss: 0.6936342716217041, acc.: 50.93%] [G loss: 0.6997491717338562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 58/86 [D loss: 0.6951155364513397, acc.: 46.73%] [G loss: 0.6992651224136353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 59/86 [D loss: 0.6951147317886353, acc.: 47.85%] [G loss: 0.6994430422782898]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 60/86 [D loss: 0.694437712430954, acc.: 49.12%] [G loss: 0.695527970790863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 61/86 [D loss: 0.6954493820667267, acc.: 46.29%] [G loss: 0.6968401074409485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 62/86 [D loss: 0.6955172717571259, acc.: 47.51%] [G loss: 0.7016226649284363]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 63/86 [D loss: 0.6926329731941223, acc.: 50.63%] [G loss: 0.7007564902305603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 64/86 [D loss: 0.6950300335884094, acc.: 48.05%] [G loss: 0.7003566026687622]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 65/86 [D loss: 0.6948091089725494, acc.: 47.17%] [G loss: 0.6985089182853699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 66/86 [D loss: 0.6947808563709259, acc.: 47.07%] [G loss: 0.6989921927452087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 67/86 [D loss: 0.6959321200847626, acc.: 45.65%] [G loss: 0.6969041228294373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 68/86 [D loss: 0.6947779953479767, acc.: 47.66%] [G loss: 0.7015101313591003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 69/86 [D loss: 0.6945672035217285, acc.: 48.63%] [G loss: 0.6999075412750244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 70/86 [D loss: 0.6949390172958374, acc.: 47.22%] [G loss: 0.700706422328949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 71/86 [D loss: 0.6946357488632202, acc.: 48.39%] [G loss: 0.6996530294418335]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 72/86 [D loss: 0.6945628821849823, acc.: 49.71%] [G loss: 0.6974626779556274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 73/86 [D loss: 0.6946148872375488, acc.: 48.93%] [G loss: 0.6968855261802673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 74/86 [D loss: 0.6960322558879852, acc.: 45.70%] [G loss: 0.699928879737854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 75/86 [D loss: 0.6939210891723633, acc.: 49.02%] [G loss: 0.7005919814109802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 76/86 [D loss: 0.6941957175731659, acc.: 49.61%] [G loss: 0.7006367444992065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 77/86 [D loss: 0.6944101452827454, acc.: 49.41%] [G loss: 0.6981179118156433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 78/86 [D loss: 0.6942225694656372, acc.: 49.32%] [G loss: 0.6975307464599609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 79/86 [D loss: 0.6960296332836151, acc.: 46.19%] [G loss: 0.6973050832748413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 80/86 [D loss: 0.6955525577068329, acc.: 46.88%] [G loss: 0.7001474499702454]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 81/86 [D loss: 0.6934362053871155, acc.: 50.39%] [G loss: 0.7011438012123108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 82/86 [D loss: 0.6947478652000427, acc.: 46.92%] [G loss: 0.6997820734977722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 83/86 [D loss: 0.6937247812747955, acc.: 49.80%] [G loss: 0.6985557079315186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 84/86 [D loss: 0.6941078305244446, acc.: 49.41%] [G loss: 0.6976817846298218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 32/200, Batch 85/86 [D loss: 0.694423645734787, acc.: 49.22%] [G loss: 0.6965437531471252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 32/200, Batch 86/86 [D loss: 0.6947381496429443, acc.: 48.34%] [G loss: 0.6992936134338379]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 1/86 [D loss: 0.6925784647464752, acc.: 52.10%] [G loss: 0.699409008026123]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 2/86 [D loss: 0.6940446197986603, acc.: 48.63%] [G loss: 0.7001970410346985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 3/86 [D loss: 0.6945909857749939, acc.: 47.71%] [G loss: 0.6982937455177307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 4/86 [D loss: 0.6943418383598328, acc.: 49.85%] [G loss: 0.6983378529548645]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 5/86 [D loss: 0.6948175430297852, acc.: 46.63%] [G loss: 0.6951028108596802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 6/86 [D loss: 0.6951315104961395, acc.: 47.80%] [G loss: 0.6977697014808655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 7/86 [D loss: 0.6945958733558655, acc.: 48.00%] [G loss: 0.700166642665863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 8/86 [D loss: 0.6946893334388733, acc.: 47.95%] [G loss: 0.699182391166687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 9/86 [D loss: 0.6947030425071716, acc.: 47.56%] [G loss: 0.6970556378364563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 10/86 [D loss: 0.6938931345939636, acc.: 49.71%] [G loss: 0.6960500478744507]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 11/86 [D loss: 0.6963109970092773, acc.: 46.29%] [G loss: 0.6957733631134033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 12/86 [D loss: 0.69509357213974, acc.: 47.02%] [G loss: 0.6981557607650757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 13/86 [D loss: 0.6929956078529358, acc.: 50.54%] [G loss: 0.6996825933456421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 14/86 [D loss: 0.6951605975627899, acc.: 46.78%] [G loss: 0.697746753692627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 15/86 [D loss: 0.6949253082275391, acc.: 48.19%] [G loss: 0.6971945762634277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 16/86 [D loss: 0.6942858397960663, acc.: 49.46%] [G loss: 0.694263219833374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 17/86 [D loss: 0.695794552564621, acc.: 46.29%] [G loss: 0.6942018270492554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 18/86 [D loss: 0.6952754855155945, acc.: 46.92%] [G loss: 0.7009366154670715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 19/86 [D loss: 0.6938841342926025, acc.: 49.22%] [G loss: 0.6996467113494873]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 20/86 [D loss: 0.6943097412586212, acc.: 49.56%] [G loss: 0.6988941431045532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 21/86 [D loss: 0.6950487196445465, acc.: 46.14%] [G loss: 0.6997266411781311]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 22/86 [D loss: 0.6952293217182159, acc.: 47.90%] [G loss: 0.695285439491272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 23/86 [D loss: 0.6951986253261566, acc.: 46.68%] [G loss: 0.6960709095001221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 24/86 [D loss: 0.6944913268089294, acc.: 49.17%] [G loss: 0.701418399810791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 25/86 [D loss: 0.6937333941459656, acc.: 49.76%] [G loss: 0.6995488405227661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 26/86 [D loss: 0.6947835683822632, acc.: 48.44%] [G loss: 0.6978320479393005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 27/86 [D loss: 0.6939010322093964, acc.: 49.02%] [G loss: 0.6992736458778381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 28/86 [D loss: 0.6950919032096863, acc.: 48.00%] [G loss: 0.6962355375289917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 29/86 [D loss: 0.6962611973285675, acc.: 45.51%] [G loss: 0.6947633028030396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 30/86 [D loss: 0.6946457624435425, acc.: 49.02%] [G loss: 0.7008581757545471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 31/86 [D loss: 0.6928552985191345, acc.: 51.27%] [G loss: 0.7000454664230347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 32/86 [D loss: 0.6948077380657196, acc.: 47.90%] [G loss: 0.6997055411338806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 33/86 [D loss: 0.6949965059757233, acc.: 47.61%] [G loss: 0.6989893317222595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 34/86 [D loss: 0.6949813067913055, acc.: 47.66%] [G loss: 0.6962389945983887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 35/86 [D loss: 0.6964950561523438, acc.: 45.26%] [G loss: 0.6950128078460693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 36/86 [D loss: 0.6949827373027802, acc.: 48.10%] [G loss: 0.7007052302360535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 37/86 [D loss: 0.6939710080623627, acc.: 49.61%] [G loss: 0.7008463740348816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 38/86 [D loss: 0.6955374777317047, acc.: 46.83%] [G loss: 0.7000784277915955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 39/86 [D loss: 0.6942935287952423, acc.: 48.49%] [G loss: 0.6986208558082581]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 40/86 [D loss: 0.6927875280380249, acc.: 51.22%] [G loss: 0.6969923973083496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 41/86 [D loss: 0.6947961747646332, acc.: 48.54%] [G loss: 0.6963286399841309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 42/86 [D loss: 0.696555882692337, acc.: 45.17%] [G loss: 0.7022352814674377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 43/86 [D loss: 0.6943718492984772, acc.: 47.90%] [G loss: 0.7013562917709351]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 44/86 [D loss: 0.6948556303977966, acc.: 48.10%] [G loss: 0.6998403072357178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 45/86 [D loss: 0.6944009065628052, acc.: 47.31%] [G loss: 0.6998624205589294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 46/86 [D loss: 0.6942300796508789, acc.: 48.88%] [G loss: 0.6980569362640381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 47/86 [D loss: 0.6952938139438629, acc.: 46.73%] [G loss: 0.695608377456665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 48/86 [D loss: 0.6948002576828003, acc.: 47.07%] [G loss: 0.6995993852615356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 49/86 [D loss: 0.6946964263916016, acc.: 49.12%] [G loss: 0.7015923261642456]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 50/86 [D loss: 0.693846732378006, acc.: 49.61%] [G loss: 0.6988397836685181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 51/86 [D loss: 0.6945963501930237, acc.: 46.09%] [G loss: 0.6986605525016785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 52/86 [D loss: 0.6938464343547821, acc.: 50.73%] [G loss: 0.6985285878181458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 53/86 [D loss: 0.6946933567523956, acc.: 48.19%] [G loss: 0.6941267848014832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 54/86 [D loss: 0.6968016028404236, acc.: 45.21%] [G loss: 0.6988176703453064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 55/86 [D loss: 0.6933384239673615, acc.: 52.44%] [G loss: 0.701549232006073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 56/86 [D loss: 0.6951244473457336, acc.: 47.71%] [G loss: 0.6987637877464294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 57/86 [D loss: 0.6950816214084625, acc.: 47.85%] [G loss: 0.6983790993690491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 58/86 [D loss: 0.69427490234375, acc.: 49.27%] [G loss: 0.6972051858901978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 59/86 [D loss: 0.6939252018928528, acc.: 50.05%] [G loss: 0.6947450041770935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 60/86 [D loss: 0.6966141164302826, acc.: 44.82%] [G loss: 0.6952011585235596]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 61/86 [D loss: 0.6944937109947205, acc.: 47.56%] [G loss: 0.7007321119308472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 62/86 [D loss: 0.6945878565311432, acc.: 48.19%] [G loss: 0.6995747089385986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 63/86 [D loss: 0.6955222189426422, acc.: 46.83%] [G loss: 0.699386477470398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 64/86 [D loss: 0.6938779950141907, acc.: 48.88%] [G loss: 0.6983431577682495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 65/86 [D loss: 0.6941860914230347, acc.: 49.76%] [G loss: 0.6946629285812378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 66/86 [D loss: 0.6958938539028168, acc.: 45.61%] [G loss: 0.6964365243911743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 67/86 [D loss: 0.6947003304958344, acc.: 47.71%] [G loss: 0.7013943791389465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 68/86 [D loss: 0.6933447122573853, acc.: 49.22%] [G loss: 0.6993839740753174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 69/86 [D loss: 0.6946623027324677, acc.: 47.95%] [G loss: 0.6984742283821106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 70/86 [D loss: 0.6941916346549988, acc.: 49.02%] [G loss: 0.6997276544570923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 71/86 [D loss: 0.6934541165828705, acc.: 50.49%] [G loss: 0.6959118843078613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 72/86 [D loss: 0.6956564486026764, acc.: 46.09%] [G loss: 0.6950685977935791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 73/86 [D loss: 0.6960658133029938, acc.: 47.12%] [G loss: 0.7013635039329529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 74/86 [D loss: 0.6936769783496857, acc.: 49.07%] [G loss: 0.6994171738624573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 75/86 [D loss: 0.6954862773418427, acc.: 46.97%] [G loss: 0.6991010308265686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 76/86 [D loss: 0.6945262849330902, acc.: 47.27%] [G loss: 0.6994240283966064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 77/86 [D loss: 0.6940138339996338, acc.: 50.05%] [G loss: 0.6960633993148804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 78/86 [D loss: 0.695557028055191, acc.: 46.34%] [G loss: 0.6937214136123657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 79/86 [D loss: 0.6962263882160187, acc.: 45.70%] [G loss: 0.7003552913665771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 80/86 [D loss: 0.6925779283046722, acc.: 51.03%] [G loss: 0.7003511786460876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 81/86 [D loss: 0.6948003768920898, acc.: 47.90%] [G loss: 0.6980330348014832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 82/86 [D loss: 0.6945905089378357, acc.: 48.68%] [G loss: 0.7000895738601685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 83/86 [D loss: 0.6929661631584167, acc.: 51.66%] [G loss: 0.6971790790557861]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 33/200, Batch 84/86 [D loss: 0.6939685940742493, acc.: 48.78%] [G loss: 0.6953594088554382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 85/86 [D loss: 0.6955784857273102, acc.: 47.07%] [G loss: 0.6991294026374817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 33/200, Batch 86/86 [D loss: 0.6939046084880829, acc.: 48.78%] [G loss: 0.7023575901985168]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 1/86 [D loss: 0.6935293674468994, acc.: 49.66%] [G loss: 0.6998393535614014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 2/86 [D loss: 0.6950331032276154, acc.: 47.17%] [G loss: 0.6992368102073669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 3/86 [D loss: 0.6949228048324585, acc.: 47.46%] [G loss: 0.6990923285484314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 4/86 [D loss: 0.6950559020042419, acc.: 47.02%] [G loss: 0.6969619989395142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 5/86 [D loss: 0.6951631605625153, acc.: 47.12%] [G loss: 0.6980029940605164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 6/86 [D loss: 0.6943036913871765, acc.: 47.90%] [G loss: 0.7006741166114807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 7/86 [D loss: 0.6935924887657166, acc.: 49.56%] [G loss: 0.7005842924118042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 8/86 [D loss: 0.6944112181663513, acc.: 48.78%] [G loss: 0.6999039649963379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 9/86 [D loss: 0.6944986581802368, acc.: 47.95%] [G loss: 0.6988690495491028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 10/86 [D loss: 0.694754034280777, acc.: 47.31%] [G loss: 0.6987496614456177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 11/86 [D loss: 0.695574939250946, acc.: 46.14%] [G loss: 0.6954175233840942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 12/86 [D loss: 0.695195198059082, acc.: 46.78%] [G loss: 0.7014620304107666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 13/86 [D loss: 0.6929395794868469, acc.: 53.03%] [G loss: 0.7016741037368774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 14/86 [D loss: 0.6937934458255768, acc.: 48.97%] [G loss: 0.6996131539344788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 15/86 [D loss: 0.6949218213558197, acc.: 46.78%] [G loss: 0.6987130045890808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 16/86 [D loss: 0.6935029923915863, acc.: 50.44%] [G loss: 0.6971778869628906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 17/86 [D loss: 0.6945390105247498, acc.: 47.56%] [G loss: 0.6974633932113647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 18/86 [D loss: 0.6944147348403931, acc.: 48.78%] [G loss: 0.7025659084320068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 19/86 [D loss: 0.6931199133396149, acc.: 50.10%] [G loss: 0.7009513974189758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 20/86 [D loss: 0.6936508417129517, acc.: 49.46%] [G loss: 0.6999142169952393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 21/86 [D loss: 0.6948050558567047, acc.: 46.73%] [G loss: 0.6991540789604187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 22/86 [D loss: 0.6944603621959686, acc.: 48.34%] [G loss: 0.6960895657539368]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 23/86 [D loss: 0.6949955821037292, acc.: 47.22%] [G loss: 0.6966555118560791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 24/86 [D loss: 0.6956215500831604, acc.: 46.09%] [G loss: 0.6989567279815674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 25/86 [D loss: 0.6942425966262817, acc.: 49.71%] [G loss: 0.7011732459068298]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 26/86 [D loss: 0.6941012740135193, acc.: 50.24%] [G loss: 0.6997039914131165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 27/86 [D loss: 0.6934955716133118, acc.: 50.29%] [G loss: 0.6995065212249756]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 28/86 [D loss: 0.6940532028675079, acc.: 50.24%] [G loss: 0.6961871385574341]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 29/86 [D loss: 0.6952653229236603, acc.: 48.05%] [G loss: 0.6970065832138062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 30/86 [D loss: 0.6942353248596191, acc.: 49.17%] [G loss: 0.6991991996765137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 31/86 [D loss: 0.6934249997138977, acc.: 50.05%] [G loss: 0.6987898349761963]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 32/86 [D loss: 0.6951568126678467, acc.: 46.88%] [G loss: 0.6981570720672607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 33/86 [D loss: 0.6940357387065887, acc.: 48.97%] [G loss: 0.6996796727180481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 34/86 [D loss: 0.694087564945221, acc.: 48.93%] [G loss: 0.6978877186775208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 35/86 [D loss: 0.6943715810775757, acc.: 47.75%] [G loss: 0.6984381079673767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 36/86 [D loss: 0.6942607462406158, acc.: 48.19%] [G loss: 0.7019073367118835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 37/86 [D loss: 0.6944348812103271, acc.: 48.10%] [G loss: 0.7009999752044678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 38/86 [D loss: 0.6942726075649261, acc.: 47.51%] [G loss: 0.700680136680603]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 39/86 [D loss: 0.6950001418590546, acc.: 47.80%] [G loss: 0.6992405652999878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 40/86 [D loss: 0.6938975155353546, acc.: 49.61%] [G loss: 0.6986585259437561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 41/86 [D loss: 0.6953647136688232, acc.: 45.65%] [G loss: 0.6985339522361755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 42/86 [D loss: 0.6950308978557587, acc.: 46.83%] [G loss: 0.6999944448471069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 43/86 [D loss: 0.6937803030014038, acc.: 50.10%] [G loss: 0.6997878551483154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 44/86 [D loss: 0.694545567035675, acc.: 48.39%] [G loss: 0.7009919881820679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 45/86 [D loss: 0.6937887668609619, acc.: 50.34%] [G loss: 0.6994854807853699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 46/86 [D loss: 0.6942758560180664, acc.: 49.41%] [G loss: 0.6977035999298096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 47/86 [D loss: 0.6952120959758759, acc.: 47.22%] [G loss: 0.6977334022521973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 48/86 [D loss: 0.693899929523468, acc.: 48.58%] [G loss: 0.7000300884246826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 49/86 [D loss: 0.6933186948299408, acc.: 50.73%] [G loss: 0.7011784315109253]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 50/86 [D loss: 0.6941056549549103, acc.: 48.58%] [G loss: 0.6989355087280273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 51/86 [D loss: 0.694596916437149, acc.: 48.00%] [G loss: 0.6978117227554321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 52/86 [D loss: 0.6936612129211426, acc.: 49.95%] [G loss: 0.6973049640655518]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 53/86 [D loss: 0.6946873068809509, acc.: 48.83%] [G loss: 0.6982066035270691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 54/86 [D loss: 0.6943987607955933, acc.: 48.19%] [G loss: 0.7004895210266113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 55/86 [D loss: 0.69355109333992, acc.: 49.80%] [G loss: 0.6989859342575073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 56/86 [D loss: 0.693822979927063, acc.: 49.27%] [G loss: 0.698959231376648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 57/86 [D loss: 0.6944716572761536, acc.: 49.02%] [G loss: 0.6982578635215759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 58/86 [D loss: 0.694758266210556, acc.: 48.05%] [G loss: 0.698898196220398]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 59/86 [D loss: 0.6950538158416748, acc.: 47.07%] [G loss: 0.6979118585586548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 60/86 [D loss: 0.6937032639980316, acc.: 49.02%] [G loss: 0.7006233334541321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 61/86 [D loss: 0.6933192312717438, acc.: 49.46%] [G loss: 0.7006071209907532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 62/86 [D loss: 0.6938555240631104, acc.: 49.41%] [G loss: 0.6991325616836548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 63/86 [D loss: 0.6941854655742645, acc.: 48.29%] [G loss: 0.699466347694397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 64/86 [D loss: 0.6948784589767456, acc.: 47.07%] [G loss: 0.6992144584655762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 65/86 [D loss: 0.6937210857868195, acc.: 49.07%] [G loss: 0.6972440481185913]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 66/86 [D loss: 0.695136159658432, acc.: 47.46%] [G loss: 0.69890958070755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 67/86 [D loss: 0.6929121613502502, acc.: 50.44%] [G loss: 0.7003117799758911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 68/86 [D loss: 0.6929312348365784, acc.: 51.95%] [G loss: 0.6997092962265015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 69/86 [D loss: 0.693085640668869, acc.: 51.32%] [G loss: 0.7006276845932007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 70/86 [D loss: 0.6941197514533997, acc.: 49.12%] [G loss: 0.6968340277671814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 71/86 [D loss: 0.6950802803039551, acc.: 48.00%] [G loss: 0.694770097732544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 72/86 [D loss: 0.6953303813934326, acc.: 46.73%] [G loss: 0.6995109915733337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 73/86 [D loss: 0.6940957009792328, acc.: 47.95%] [G loss: 0.700995922088623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 74/86 [D loss: 0.6947234272956848, acc.: 46.97%] [G loss: 0.6992888450622559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 75/86 [D loss: 0.6945805251598358, acc.: 47.31%] [G loss: 0.6989961862564087]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 76/86 [D loss: 0.6941336989402771, acc.: 49.66%] [G loss: 0.6965641975402832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 77/86 [D loss: 0.6952658295631409, acc.: 46.63%] [G loss: 0.6950393319129944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 78/86 [D loss: 0.695123702287674, acc.: 46.78%] [G loss: 0.7009140849113464]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 79/86 [D loss: 0.6945463120937347, acc.: 46.97%] [G loss: 0.7008161544799805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 80/86 [D loss: 0.6952282786369324, acc.: 47.51%] [G loss: 0.6995693445205688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 81/86 [D loss: 0.6950682997703552, acc.: 47.75%] [G loss: 0.6997678875923157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 82/86 [D loss: 0.694886714220047, acc.: 46.92%] [G loss: 0.6952732801437378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 83/86 [D loss: 0.6948042809963226, acc.: 47.61%] [G loss: 0.6961512565612793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 84/86 [D loss: 0.6960878372192383, acc.: 46.19%] [G loss: 0.7020507454872131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 34/200, Batch 85/86 [D loss: 0.6929237842559814, acc.: 51.07%] [G loss: 0.6998892426490784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 34/200, Batch 86/86 [D loss: 0.6953456997871399, acc.: 47.02%] [G loss: 0.6982244253158569]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 35/200, Batch 1/86 [D loss: 0.6949975788593292, acc.: 47.22%] [G loss: 0.6987850666046143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 2/86 [D loss: 0.6938016712665558, acc.: 49.76%] [G loss: 0.6965741515159607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 3/86 [D loss: 0.695330023765564, acc.: 48.34%] [G loss: 0.6946794390678406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 4/86 [D loss: 0.6969624757766724, acc.: 43.41%] [G loss: 0.7030413150787354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 5/86 [D loss: 0.6926993131637573, acc.: 51.61%] [G loss: 0.7006866931915283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 6/86 [D loss: 0.6960418820381165, acc.: 46.29%] [G loss: 0.6977738738059998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 7/86 [D loss: 0.6947380602359772, acc.: 47.56%] [G loss: 0.7005653977394104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 8/86 [D loss: 0.6931052803993225, acc.: 50.44%] [G loss: 0.6967797875404358]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 9/86 [D loss: 0.6959896087646484, acc.: 45.65%] [G loss: 0.694381058216095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 10/86 [D loss: 0.6971113681793213, acc.: 43.65%] [G loss: 0.7026453018188477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 11/86 [D loss: 0.6934348940849304, acc.: 50.00%] [G loss: 0.7012513875961304]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 12/86 [D loss: 0.6947849094867706, acc.: 47.51%] [G loss: 0.6993111968040466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 13/86 [D loss: 0.6946261823177338, acc.: 48.05%] [G loss: 0.7006611227989197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 14/86 [D loss: 0.6937370598316193, acc.: 49.80%] [G loss: 0.6978100538253784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 15/86 [D loss: 0.6944057941436768, acc.: 49.17%] [G loss: 0.6955046653747559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 16/86 [D loss: 0.6948226690292358, acc.: 48.44%] [G loss: 0.7015580534934998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 17/86 [D loss: 0.6933575868606567, acc.: 51.12%] [G loss: 0.7005956768989563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 18/86 [D loss: 0.6945896744728088, acc.: 47.90%] [G loss: 0.7009462118148804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 19/86 [D loss: 0.6939881145954132, acc.: 49.22%] [G loss: 0.6994902491569519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 20/86 [D loss: 0.6939532458782196, acc.: 49.51%] [G loss: 0.697433352470398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 21/86 [D loss: 0.6955353617668152, acc.: 46.88%] [G loss: 0.6946805715560913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 22/86 [D loss: 0.696058601140976, acc.: 45.75%] [G loss: 0.7009981870651245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 23/86 [D loss: 0.6930332183837891, acc.: 50.83%] [G loss: 0.7014007568359375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 24/86 [D loss: 0.6951866745948792, acc.: 47.22%] [G loss: 0.6989426016807556]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 25/86 [D loss: 0.6948356330394745, acc.: 47.02%] [G loss: 0.7007277011871338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 26/86 [D loss: 0.6944620907306671, acc.: 48.83%] [G loss: 0.6960747241973877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 27/86 [D loss: 0.6962515413761139, acc.: 45.70%] [G loss: 0.6948080062866211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 28/86 [D loss: 0.6960053443908691, acc.: 46.39%] [G loss: 0.702411949634552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 29/86 [D loss: 0.6926931142807007, acc.: 51.61%] [G loss: 0.7008554339408875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 30/86 [D loss: 0.6952985525131226, acc.: 47.07%] [G loss: 0.6983363032341003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 31/86 [D loss: 0.6942856907844543, acc.: 48.63%] [G loss: 0.6985009908676147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 32/86 [D loss: 0.6929424405097961, acc.: 51.90%] [G loss: 0.6954344511032104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 33/86 [D loss: 0.6956177055835724, acc.: 44.97%] [G loss: 0.6921366453170776]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 34/86 [D loss: 0.696713387966156, acc.: 45.80%] [G loss: 0.6992108821868896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 35/86 [D loss: 0.6920845806598663, acc.: 51.46%] [G loss: 0.7006366848945618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 36/86 [D loss: 0.6948013603687286, acc.: 47.75%] [G loss: 0.6974754333496094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 37/86 [D loss: 0.6951240301132202, acc.: 47.22%] [G loss: 0.698686420917511]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 38/86 [D loss: 0.6935781836509705, acc.: 50.78%] [G loss: 0.6968269348144531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 39/86 [D loss: 0.694496363401413, acc.: 48.05%] [G loss: 0.6931154727935791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 40/86 [D loss: 0.6965816915035248, acc.: 45.31%] [G loss: 0.6972383856773376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 41/86 [D loss: 0.6933587789535522, acc.: 48.88%] [G loss: 0.7019731998443604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 42/86 [D loss: 0.6943288147449493, acc.: 48.88%] [G loss: 0.699129581451416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 43/86 [D loss: 0.6952890157699585, acc.: 46.34%] [G loss: 0.6997662782669067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 44/86 [D loss: 0.694758802652359, acc.: 48.05%] [G loss: 0.6985849142074585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 45/86 [D loss: 0.6933857500553131, acc.: 49.85%] [G loss: 0.6960232853889465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 46/86 [D loss: 0.6947591006755829, acc.: 47.61%] [G loss: 0.6965944170951843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 47/86 [D loss: 0.6930137872695923, acc.: 50.88%] [G loss: 0.7017404437065125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 48/86 [D loss: 0.6939650475978851, acc.: 49.46%] [G loss: 0.700427234172821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 49/86 [D loss: 0.6947470307350159, acc.: 48.49%] [G loss: 0.6998116374015808]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 50/86 [D loss: 0.693755567073822, acc.: 49.07%] [G loss: 0.7006663680076599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 51/86 [D loss: 0.6939134895801544, acc.: 48.68%] [G loss: 0.6968631744384766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 52/86 [D loss: 0.695618063211441, acc.: 45.95%] [G loss: 0.6978492140769958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 53/86 [D loss: 0.6942331194877625, acc.: 48.73%] [G loss: 0.7015302181243896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 54/86 [D loss: 0.6933738887310028, acc.: 49.02%] [G loss: 0.7002688646316528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 55/86 [D loss: 0.6945172548294067, acc.: 47.27%] [G loss: 0.6993595361709595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 56/86 [D loss: 0.6941634118556976, acc.: 49.51%] [G loss: 0.6987249255180359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 57/86 [D loss: 0.6948210000991821, acc.: 47.02%] [G loss: 0.6967777013778687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 58/86 [D loss: 0.6945099532604218, acc.: 48.78%] [G loss: 0.6973638534545898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 59/86 [D loss: 0.6946707963943481, acc.: 47.75%] [G loss: 0.7003833055496216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 60/86 [D loss: 0.6941137313842773, acc.: 49.27%] [G loss: 0.70121830701828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 61/86 [D loss: 0.6937184929847717, acc.: 48.88%] [G loss: 0.7006272077560425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 62/86 [D loss: 0.6941450238227844, acc.: 47.80%] [G loss: 0.7009154558181763]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 63/86 [D loss: 0.6938879787921906, acc.: 50.78%] [G loss: 0.6976454257965088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 64/86 [D loss: 0.6949229538440704, acc.: 47.95%] [G loss: 0.6952265501022339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 65/86 [D loss: 0.6947534084320068, acc.: 47.12%] [G loss: 0.6999956369400024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 66/86 [D loss: 0.6925174593925476, acc.: 52.25%] [G loss: 0.700991153717041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 67/86 [D loss: 0.6939131021499634, acc.: 49.61%] [G loss: 0.7000375986099243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 68/86 [D loss: 0.6941821575164795, acc.: 48.34%] [G loss: 0.6999301910400391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 69/86 [D loss: 0.6944696009159088, acc.: 48.39%] [G loss: 0.6978244781494141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 70/86 [D loss: 0.6947423219680786, acc.: 49.07%] [G loss: 0.6975365281105042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 71/86 [D loss: 0.6938144564628601, acc.: 50.20%] [G loss: 0.7011690139770508]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 72/86 [D loss: 0.6925225853919983, acc.: 52.20%] [G loss: 0.7001137733459473]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 73/86 [D loss: 0.6939063966274261, acc.: 50.00%] [G loss: 0.6999183297157288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 74/86 [D loss: 0.6939685344696045, acc.: 50.05%] [G loss: 0.700710117816925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 75/86 [D loss: 0.6943444609642029, acc.: 48.14%] [G loss: 0.6981459856033325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 76/86 [D loss: 0.6949380040168762, acc.: 47.22%] [G loss: 0.6944665312767029]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 77/86 [D loss: 0.6949949264526367, acc.: 47.41%] [G loss: 0.701747715473175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 78/86 [D loss: 0.6936248540878296, acc.: 49.41%] [G loss: 0.7004225850105286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 79/86 [D loss: 0.6944563090801239, acc.: 48.10%] [G loss: 0.6991251707077026]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 80/86 [D loss: 0.6946257054805756, acc.: 47.95%] [G loss: 0.7008088827133179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 81/86 [D loss: 0.6934959590435028, acc.: 50.05%] [G loss: 0.6963431239128113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 82/86 [D loss: 0.6955753862857819, acc.: 46.63%] [G loss: 0.6960384845733643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 83/86 [D loss: 0.6956369280815125, acc.: 46.34%] [G loss: 0.6998326778411865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 84/86 [D loss: 0.6932089030742645, acc.: 49.27%] [G loss: 0.7023584842681885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 35/200, Batch 85/86 [D loss: 0.6943617761135101, acc.: 48.54%] [G loss: 0.698085606098175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 35/200, Batch 86/86 [D loss: 0.6939417719841003, acc.: 47.36%] [G loss: 0.7009884119033813]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 1/86 [D loss: 0.693814605474472, acc.: 49.07%] [G loss: 0.6982062458992004]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 2/86 [D loss: 0.6948202550411224, acc.: 47.46%] [G loss: 0.6955289840698242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 3/86 [D loss: 0.6959442794322968, acc.: 45.07%] [G loss: 0.6981407403945923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 4/86 [D loss: 0.6938520669937134, acc.: 49.07%] [G loss: 0.7020286321640015]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 5/86 [D loss: 0.6940732598304749, acc.: 48.44%] [G loss: 0.6988381743431091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 6/86 [D loss: 0.6947092711925507, acc.: 47.17%] [G loss: 0.7006044983863831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 7/86 [D loss: 0.6942462921142578, acc.: 48.83%] [G loss: 0.6965246200561523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 8/86 [D loss: 0.6952612102031708, acc.: 47.56%] [G loss: 0.694975733757019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 9/86 [D loss: 0.6962119936943054, acc.: 44.38%] [G loss: 0.7007285356521606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 10/86 [D loss: 0.6929522454738617, acc.: 50.98%] [G loss: 0.701151430606842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 11/86 [D loss: 0.6948506534099579, acc.: 47.27%] [G loss: 0.6985514163970947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 12/86 [D loss: 0.6948589086532593, acc.: 48.34%] [G loss: 0.6995251774787903]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 13/86 [D loss: 0.6942940056324005, acc.: 48.44%] [G loss: 0.6983647346496582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 14/86 [D loss: 0.6947979032993317, acc.: 47.41%] [G loss: 0.6938415169715881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 15/86 [D loss: 0.6960830986499786, acc.: 45.17%] [G loss: 0.6995290517807007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 16/86 [D loss: 0.692034125328064, acc.: 52.29%] [G loss: 0.7018409371376038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 17/86 [D loss: 0.694439560174942, acc.: 48.93%] [G loss: 0.6994670629501343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 18/86 [D loss: 0.6948675811290741, acc.: 47.61%] [G loss: 0.7009169459342957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 19/86 [D loss: 0.6942216157913208, acc.: 49.02%] [G loss: 0.6986175179481506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 20/86 [D loss: 0.6938432157039642, acc.: 49.76%] [G loss: 0.6968555450439453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 21/86 [D loss: 0.6959621608257294, acc.: 45.56%] [G loss: 0.6980709433555603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 22/86 [D loss: 0.6933963596820831, acc.: 51.03%] [G loss: 0.700577437877655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 23/86 [D loss: 0.6933178305625916, acc.: 49.71%] [G loss: 0.7005054354667664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 24/86 [D loss: 0.6934974193572998, acc.: 51.07%] [G loss: 0.6992990970611572]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 25/86 [D loss: 0.6940023899078369, acc.: 49.71%] [G loss: 0.6989073157310486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 26/86 [D loss: 0.6939818859100342, acc.: 48.68%] [G loss: 0.6973814964294434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 27/86 [D loss: 0.6947032809257507, acc.: 47.80%] [G loss: 0.7002761363983154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 28/86 [D loss: 0.6943197250366211, acc.: 47.61%] [G loss: 0.7017273902893066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 29/86 [D loss: 0.6934820711612701, acc.: 50.59%] [G loss: 0.7014576196670532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 30/86 [D loss: 0.6945106685161591, acc.: 47.75%] [G loss: 0.7004565000534058]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 31/86 [D loss: 0.6934866309165955, acc.: 51.61%] [G loss: 0.7001208662986755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 32/86 [D loss: 0.6935390532016754, acc.: 50.29%] [G loss: 0.6988701820373535]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 33/86 [D loss: 0.6949106454849243, acc.: 48.49%] [G loss: 0.7011075615882874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 34/86 [D loss: 0.6935063600540161, acc.: 50.59%] [G loss: 0.7019017338752747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 35/86 [D loss: 0.6938577592372894, acc.: 48.29%] [G loss: 0.6997725963592529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 36/86 [D loss: 0.6940731406211853, acc.: 48.78%] [G loss: 0.7003136873245239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 37/86 [D loss: 0.6931066513061523, acc.: 50.29%] [G loss: 0.699108362197876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 38/86 [D loss: 0.6944540441036224, acc.: 48.63%] [G loss: 0.6960868239402771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 39/86 [D loss: 0.6957883238792419, acc.: 46.58%] [G loss: 0.6996716856956482]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 40/86 [D loss: 0.6937406063079834, acc.: 49.22%] [G loss: 0.7010090351104736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 41/86 [D loss: 0.6950710415840149, acc.: 46.92%] [G loss: 0.7000494003295898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 42/86 [D loss: 0.6948455274105072, acc.: 48.39%] [G loss: 0.6994316577911377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 43/86 [D loss: 0.6938557624816895, acc.: 47.95%] [G loss: 0.6988701820373535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 44/86 [D loss: 0.6946749985218048, acc.: 48.24%] [G loss: 0.6967716813087463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 45/86 [D loss: 0.6956994533538818, acc.: 46.09%] [G loss: 0.6996588110923767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 46/86 [D loss: 0.6943398714065552, acc.: 48.44%] [G loss: 0.7028775215148926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 47/86 [D loss: 0.6940862834453583, acc.: 47.80%] [G loss: 0.70060133934021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 48/86 [D loss: 0.6946655511856079, acc.: 47.17%] [G loss: 0.6999306678771973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 49/86 [D loss: 0.6938136219978333, acc.: 49.12%] [G loss: 0.7001523971557617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 50/86 [D loss: 0.694196343421936, acc.: 48.39%] [G loss: 0.6951340436935425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 51/86 [D loss: 0.6958386301994324, acc.: 46.73%] [G loss: 0.6973221302032471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 52/86 [D loss: 0.6937291324138641, acc.: 49.95%] [G loss: 0.7023499011993408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 53/86 [D loss: 0.694071501493454, acc.: 49.90%] [G loss: 0.6991084814071655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 54/86 [D loss: 0.6938141286373138, acc.: 49.80%] [G loss: 0.6995993256568909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 55/86 [D loss: 0.6946201622486115, acc.: 47.71%] [G loss: 0.6993662118911743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 56/86 [D loss: 0.6933077275753021, acc.: 50.88%] [G loss: 0.6974784135818481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 57/86 [D loss: 0.6957060396671295, acc.: 47.66%] [G loss: 0.6977002620697021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 58/86 [D loss: 0.69523486495018, acc.: 47.27%] [G loss: 0.7005212903022766]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 59/86 [D loss: 0.6927044987678528, acc.: 50.20%] [G loss: 0.6997805237770081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 60/86 [D loss: 0.6949129104614258, acc.: 48.49%] [G loss: 0.6993277668952942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 61/86 [D loss: 0.6935031414031982, acc.: 49.85%] [G loss: 0.6996742486953735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 62/86 [D loss: 0.6935563087463379, acc.: 49.80%] [G loss: 0.697913408279419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 63/86 [D loss: 0.6949678659439087, acc.: 47.17%] [G loss: 0.6981815695762634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 64/86 [D loss: 0.695021778345108, acc.: 47.12%] [G loss: 0.700375497341156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 65/86 [D loss: 0.6933884024620056, acc.: 50.44%] [G loss: 0.7008967399597168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 66/86 [D loss: 0.6940415799617767, acc.: 49.51%] [G loss: 0.6998384594917297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 67/86 [D loss: 0.6938379406929016, acc.: 49.66%] [G loss: 0.6998724341392517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 68/86 [D loss: 0.6936614811420441, acc.: 49.80%] [G loss: 0.6980345249176025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 69/86 [D loss: 0.6940035223960876, acc.: 49.90%] [G loss: 0.6990550756454468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 70/86 [D loss: 0.694257527589798, acc.: 47.75%] [G loss: 0.7003893852233887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 71/86 [D loss: 0.693211704492569, acc.: 49.71%] [G loss: 0.7002497315406799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 72/86 [D loss: 0.6948399543762207, acc.: 47.90%] [G loss: 0.7011913657188416]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 73/86 [D loss: 0.6939410865306854, acc.: 47.90%] [G loss: 0.69983971118927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 74/86 [D loss: 0.6937891244888306, acc.: 48.39%] [G loss: 0.6973021626472473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 75/86 [D loss: 0.6958142220973969, acc.: 45.95%] [G loss: 0.6988846063613892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 76/86 [D loss: 0.6940855979919434, acc.: 48.39%] [G loss: 0.7014604806900024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 77/86 [D loss: 0.6933726370334625, acc.: 49.80%] [G loss: 0.7006794810295105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 78/86 [D loss: 0.6941560804843903, acc.: 49.22%] [G loss: 0.6993994116783142]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 79/86 [D loss: 0.6942464709281921, acc.: 47.85%] [G loss: 0.7004260420799255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 80/86 [D loss: 0.6930089890956879, acc.: 51.32%] [G loss: 0.696486234664917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 81/86 [D loss: 0.6960824728012085, acc.: 45.07%] [G loss: 0.6994192600250244]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 36/200, Batch 82/86 [D loss: 0.694165050983429, acc.: 48.10%] [G loss: 0.7031713724136353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 83/86 [D loss: 0.6934894621372223, acc.: 48.73%] [G loss: 0.7004458904266357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 84/86 [D loss: 0.694817066192627, acc.: 47.41%] [G loss: 0.700423538684845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 85/86 [D loss: 0.6934085786342621, acc.: 51.42%] [G loss: 0.700346052646637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 36/200, Batch 86/86 [D loss: 0.6938006579875946, acc.: 50.54%] [G loss: 0.6975036859512329]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 1/86 [D loss: 0.6962875127792358, acc.: 43.80%] [G loss: 0.6985499858856201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 2/86 [D loss: 0.6941010057926178, acc.: 47.22%] [G loss: 0.7041386961936951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 3/86 [D loss: 0.6930460035800934, acc.: 50.59%] [G loss: 0.7003589272499084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 4/86 [D loss: 0.6959935426712036, acc.: 44.53%] [G loss: 0.6993294954299927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 5/86 [D loss: 0.6944650113582611, acc.: 49.17%] [G loss: 0.6998060345649719]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 6/86 [D loss: 0.6939892172813416, acc.: 49.32%] [G loss: 0.6974751949310303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 7/86 [D loss: 0.6949910223484039, acc.: 48.24%] [G loss: 0.6909569501876831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 8/86 [D loss: 0.6957099437713623, acc.: 46.53%] [G loss: 0.704109787940979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 9/86 [D loss: 0.6917581856250763, acc.: 52.49%] [G loss: 0.7008233666419983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 10/86 [D loss: 0.6961683630943298, acc.: 45.70%] [G loss: 0.6973279714584351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 11/86 [D loss: 0.6955180466175079, acc.: 45.12%] [G loss: 0.6993969678878784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 12/86 [D loss: 0.6935285329818726, acc.: 51.46%] [G loss: 0.6984924674034119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 13/86 [D loss: 0.6947459280490875, acc.: 46.73%] [G loss: 0.6929695010185242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 14/86 [D loss: 0.6966589689254761, acc.: 43.75%] [G loss: 0.7002378702163696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 15/86 [D loss: 0.6921211183071136, acc.: 52.34%] [G loss: 0.702532172203064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 16/86 [D loss: 0.6954763829708099, acc.: 45.85%] [G loss: 0.6973949074745178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 17/86 [D loss: 0.6963973343372345, acc.: 44.09%] [G loss: 0.6985388994216919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 18/86 [D loss: 0.6937520802021027, acc.: 49.37%] [G loss: 0.6995338797569275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 19/86 [D loss: 0.693770706653595, acc.: 50.20%] [G loss: 0.694559633731842]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 20/86 [D loss: 0.6960818767547607, acc.: 46.63%] [G loss: 0.6968099474906921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 21/86 [D loss: 0.6942972242832184, acc.: 47.95%] [G loss: 0.7018890976905823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 22/86 [D loss: 0.6933545172214508, acc.: 49.56%] [G loss: 0.6989089250564575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 23/86 [D loss: 0.6940941214561462, acc.: 49.85%] [G loss: 0.7007176280021667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 24/86 [D loss: 0.6934657096862793, acc.: 50.10%] [G loss: 0.6990652084350586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 25/86 [D loss: 0.6942941844463348, acc.: 48.97%] [G loss: 0.698201596736908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 26/86 [D loss: 0.6933983564376831, acc.: 50.34%] [G loss: 0.6974436640739441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 27/86 [D loss: 0.6941027939319611, acc.: 48.29%] [G loss: 0.7015889883041382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 28/86 [D loss: 0.6935790181159973, acc.: 50.24%] [G loss: 0.701454758644104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 29/86 [D loss: 0.694346159696579, acc.: 48.93%] [G loss: 0.7009063959121704]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 30/86 [D loss: 0.6941225826740265, acc.: 48.83%] [G loss: 0.6996161937713623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 31/86 [D loss: 0.6942099034786224, acc.: 48.05%] [G loss: 0.6992139220237732]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 32/86 [D loss: 0.6933998763561249, acc.: 50.88%] [G loss: 0.7007665038108826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 33/86 [D loss: 0.6929114162921906, acc.: 50.34%] [G loss: 0.7020465135574341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 34/86 [D loss: 0.6926083266735077, acc.: 51.12%] [G loss: 0.7024363875389099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 35/86 [D loss: 0.6934674382209778, acc.: 50.00%] [G loss: 0.701740026473999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 36/86 [D loss: 0.6940868198871613, acc.: 47.80%] [G loss: 0.7020822763442993]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 37/86 [D loss: 0.6931709945201874, acc.: 50.39%] [G loss: 0.7015798687934875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 38/86 [D loss: 0.6934293806552887, acc.: 50.68%] [G loss: 0.6993832588195801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 39/86 [D loss: 0.6940291523933411, acc.: 48.78%] [G loss: 0.7020299434661865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 40/86 [D loss: 0.6943075954914093, acc.: 47.41%] [G loss: 0.7026798725128174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 41/86 [D loss: 0.6942735612392426, acc.: 48.63%] [G loss: 0.7015334963798523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 42/86 [D loss: 0.6940064132213593, acc.: 50.15%] [G loss: 0.7011367082595825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 43/86 [D loss: 0.6935978531837463, acc.: 49.80%] [G loss: 0.700793981552124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 44/86 [D loss: 0.6941358745098114, acc.: 47.80%] [G loss: 0.7000241279602051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 45/86 [D loss: 0.6939319372177124, acc.: 49.41%] [G loss: 0.701565682888031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 46/86 [D loss: 0.6935136020183563, acc.: 49.32%] [G loss: 0.7015241980552673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 47/86 [D loss: 0.6938448846340179, acc.: 49.37%] [G loss: 0.700360119342804]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 48/86 [D loss: 0.6941822171211243, acc.: 48.97%] [G loss: 0.7003962397575378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 49/86 [D loss: 0.6928870975971222, acc.: 51.42%] [G loss: 0.6991496682167053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 50/86 [D loss: 0.6939749121665955, acc.: 48.93%] [G loss: 0.6975095272064209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 51/86 [D loss: 0.6945585310459137, acc.: 47.66%] [G loss: 0.6997731924057007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 52/86 [D loss: 0.6933186054229736, acc.: 49.32%] [G loss: 0.7016170620918274]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 53/86 [D loss: 0.6934648156166077, acc.: 50.39%] [G loss: 0.700324296951294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 54/86 [D loss: 0.693504273891449, acc.: 50.20%] [G loss: 0.7006011009216309]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 55/86 [D loss: 0.6926379203796387, acc.: 51.27%] [G loss: 0.699723482131958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 56/86 [D loss: 0.6934693455696106, acc.: 49.41%] [G loss: 0.6972571015357971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 57/86 [D loss: 0.6945920884609222, acc.: 48.14%] [G loss: 0.7003051042556763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 58/86 [D loss: 0.6942397356033325, acc.: 48.24%] [G loss: 0.7015864253044128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 59/86 [D loss: 0.6930323243141174, acc.: 50.24%] [G loss: 0.6998785138130188]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 60/86 [D loss: 0.693503737449646, acc.: 48.54%] [G loss: 0.6991145610809326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 61/86 [D loss: 0.6936643719673157, acc.: 49.95%] [G loss: 0.6997276544570923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 62/86 [D loss: 0.695325642824173, acc.: 46.83%] [G loss: 0.6960000395774841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 63/86 [D loss: 0.6942563354969025, acc.: 49.07%] [G loss: 0.698073148727417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 64/86 [D loss: 0.6938915848731995, acc.: 48.29%] [G loss: 0.7007024884223938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 65/86 [D loss: 0.6939613223075867, acc.: 48.34%] [G loss: 0.7001588940620422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 66/86 [D loss: 0.6939820051193237, acc.: 49.02%] [G loss: 0.6982126235961914]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 67/86 [D loss: 0.6932086050510406, acc.: 50.39%] [G loss: 0.6991326212882996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 68/86 [D loss: 0.6941955983638763, acc.: 48.24%] [G loss: 0.6989284157752991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 69/86 [D loss: 0.6945130825042725, acc.: 47.56%] [G loss: 0.6995705366134644]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 70/86 [D loss: 0.693289041519165, acc.: 48.00%] [G loss: 0.7018477916717529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 71/86 [D loss: 0.6935363113880157, acc.: 49.71%] [G loss: 0.7008697986602783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 72/86 [D loss: 0.693368524312973, acc.: 49.17%] [G loss: 0.6996631026268005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 73/86 [D loss: 0.6938289403915405, acc.: 49.17%] [G loss: 0.699279248714447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 74/86 [D loss: 0.6942354440689087, acc.: 47.66%] [G loss: 0.6971738934516907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 75/86 [D loss: 0.695574164390564, acc.: 46.44%] [G loss: 0.6991279125213623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 76/86 [D loss: 0.6927920281887054, acc.: 51.17%] [G loss: 0.7013272047042847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 77/86 [D loss: 0.693644642829895, acc.: 49.51%] [G loss: 0.6989152431488037]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 78/86 [D loss: 0.6944543719291687, acc.: 48.54%] [G loss: 0.6998169422149658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 79/86 [D loss: 0.6938726603984833, acc.: 49.41%] [G loss: 0.6998389959335327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 80/86 [D loss: 0.695138692855835, acc.: 47.07%] [G loss: 0.6974252462387085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 81/86 [D loss: 0.6949900090694427, acc.: 46.04%] [G loss: 0.7003637552261353]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 37/200, Batch 82/86 [D loss: 0.6928034424781799, acc.: 52.00%] [G loss: 0.7027051448822021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 83/86 [D loss: 0.6942954361438751, acc.: 48.05%] [G loss: 0.7005849480628967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 84/86 [D loss: 0.6939398050308228, acc.: 49.41%] [G loss: 0.7000098824501038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 85/86 [D loss: 0.6939273774623871, acc.: 48.88%] [G loss: 0.6985976099967957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 37/200, Batch 86/86 [D loss: 0.6942913234233856, acc.: 47.90%] [G loss: 0.6952511668205261]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 1/86 [D loss: 0.6957031786441803, acc.: 47.02%] [G loss: 0.7010976076126099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 2/86 [D loss: 0.6935106813907623, acc.: 49.51%] [G loss: 0.7002047896385193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 3/86 [D loss: 0.6942805051803589, acc.: 48.39%] [G loss: 0.6999252438545227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 4/86 [D loss: 0.6940235197544098, acc.: 48.44%] [G loss: 0.700313150882721]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 5/86 [D loss: 0.6932289302349091, acc.: 49.90%] [G loss: 0.6987144947052002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 6/86 [D loss: 0.6948270797729492, acc.: 48.49%] [G loss: 0.6970862150192261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 7/86 [D loss: 0.6940699815750122, acc.: 48.58%] [G loss: 0.7004843950271606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 8/86 [D loss: 0.6928784549236298, acc.: 50.54%] [G loss: 0.7025554776191711]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 9/86 [D loss: 0.6942307949066162, acc.: 48.34%] [G loss: 0.6996665596961975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 10/86 [D loss: 0.6941859126091003, acc.: 47.31%] [G loss: 0.7007930874824524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 11/86 [D loss: 0.69333216547966, acc.: 49.22%] [G loss: 0.6992276310920715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 12/86 [D loss: 0.6945046484470367, acc.: 47.75%] [G loss: 0.6963878870010376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 13/86 [D loss: 0.6943525075912476, acc.: 48.05%] [G loss: 0.7020301222801208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 14/86 [D loss: 0.6932159066200256, acc.: 49.85%] [G loss: 0.7008135914802551]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 15/86 [D loss: 0.6944110691547394, acc.: 48.29%] [G loss: 0.6990411281585693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 16/86 [D loss: 0.694146990776062, acc.: 49.56%] [G loss: 0.6999304890632629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 17/86 [D loss: 0.6941304802894592, acc.: 47.71%] [G loss: 0.6968147158622742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 18/86 [D loss: 0.6956291496753693, acc.: 46.63%] [G loss: 0.6980935335159302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 19/86 [D loss: 0.6938558518886566, acc.: 48.00%] [G loss: 0.7017154097557068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 20/86 [D loss: 0.6925807297229767, acc.: 52.05%] [G loss: 0.7010595798492432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 21/86 [D loss: 0.6943337321281433, acc.: 48.00%] [G loss: 0.7002784609794617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 22/86 [D loss: 0.6948112845420837, acc.: 48.49%] [G loss: 0.6994821429252625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 23/86 [D loss: 0.6931337714195251, acc.: 49.95%] [G loss: 0.6972149610519409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 24/86 [D loss: 0.694745808839798, acc.: 46.83%] [G loss: 0.6975825428962708]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 25/86 [D loss: 0.6930857300758362, acc.: 50.44%] [G loss: 0.7014016509056091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 26/86 [D loss: 0.6934858560562134, acc.: 50.20%] [G loss: 0.7004187107086182]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 27/86 [D loss: 0.6943103671073914, acc.: 47.90%] [G loss: 0.7009553909301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 28/86 [D loss: 0.6936264634132385, acc.: 48.97%] [G loss: 0.7000184059143066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 29/86 [D loss: 0.6936336755752563, acc.: 50.29%] [G loss: 0.6946755051612854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 30/86 [D loss: 0.6973867416381836, acc.: 42.72%] [G loss: 0.6975783705711365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 31/86 [D loss: 0.6943891048431396, acc.: 47.66%] [G loss: 0.7035225629806519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 32/86 [D loss: 0.6933287680149078, acc.: 49.85%] [G loss: 0.6984684467315674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 33/86 [D loss: 0.6956265568733215, acc.: 46.68%] [G loss: 0.70058274269104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 34/86 [D loss: 0.6937403380870819, acc.: 49.66%] [G loss: 0.6993587613105774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 35/86 [D loss: 0.6934161186218262, acc.: 50.73%] [G loss: 0.6964249610900879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 36/86 [D loss: 0.6959706544876099, acc.: 45.61%] [G loss: 0.6969015598297119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 37/86 [D loss: 0.6949880421161652, acc.: 46.14%] [G loss: 0.7025559544563293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 38/86 [D loss: 0.6922095119953156, acc.: 52.54%] [G loss: 0.7002103328704834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 39/86 [D loss: 0.6946129500865936, acc.: 47.22%] [G loss: 0.698455274105072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 40/86 [D loss: 0.6943767666816711, acc.: 48.88%] [G loss: 0.7003744840621948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 41/86 [D loss: 0.6938119232654572, acc.: 48.73%] [G loss: 0.6969649195671082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 42/86 [D loss: 0.6947834193706512, acc.: 48.24%] [G loss: 0.6967788934707642]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 43/86 [D loss: 0.6945182383060455, acc.: 48.54%] [G loss: 0.7020387649536133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 44/86 [D loss: 0.6926329135894775, acc.: 50.98%] [G loss: 0.7015533447265625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 45/86 [D loss: 0.6945676505565643, acc.: 48.34%] [G loss: 0.7002602815628052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 46/86 [D loss: 0.6936840116977692, acc.: 49.51%] [G loss: 0.7002304792404175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 47/86 [D loss: 0.6937305927276611, acc.: 48.14%] [G loss: 0.6989681720733643]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 48/86 [D loss: 0.695294976234436, acc.: 47.07%] [G loss: 0.6956987380981445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 49/86 [D loss: 0.6953321993350983, acc.: 47.31%] [G loss: 0.7008407711982727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 50/86 [D loss: 0.6926749646663666, acc.: 51.46%] [G loss: 0.7011674642562866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 51/86 [D loss: 0.6941237151622772, acc.: 49.32%] [G loss: 0.6996392607688904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 52/86 [D loss: 0.6939952373504639, acc.: 49.22%] [G loss: 0.7015038728713989]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 53/86 [D loss: 0.693869948387146, acc.: 48.68%] [G loss: 0.699233889579773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 54/86 [D loss: 0.6948542594909668, acc.: 48.19%] [G loss: 0.6977308988571167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 55/86 [D loss: 0.6950890421867371, acc.: 47.36%] [G loss: 0.7013753652572632]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 56/86 [D loss: 0.6923636794090271, acc.: 52.54%] [G loss: 0.7022644281387329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 57/86 [D loss: 0.6938065588474274, acc.: 50.20%] [G loss: 0.6993277668952942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 58/86 [D loss: 0.6943149268627167, acc.: 49.32%] [G loss: 0.7005932927131653]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 59/86 [D loss: 0.6936802268028259, acc.: 49.80%] [G loss: 0.6980253458023071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 60/86 [D loss: 0.6948001682758331, acc.: 48.19%] [G loss: 0.6976490616798401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 61/86 [D loss: 0.6952775418758392, acc.: 45.36%] [G loss: 0.7006574273109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 62/86 [D loss: 0.6929410696029663, acc.: 51.32%] [G loss: 0.7026336193084717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 63/86 [D loss: 0.6933421492576599, acc.: 50.05%] [G loss: 0.699183464050293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 64/86 [D loss: 0.6947534084320068, acc.: 47.61%] [G loss: 0.7004035115242004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 65/86 [D loss: 0.6930850744247437, acc.: 50.49%] [G loss: 0.6985389590263367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 66/86 [D loss: 0.6932428777217865, acc.: 49.02%] [G loss: 0.6975688338279724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 67/86 [D loss: 0.6936256289482117, acc.: 49.51%] [G loss: 0.7005658149719238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 68/86 [D loss: 0.6935530006885529, acc.: 50.10%] [G loss: 0.7008836269378662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 69/86 [D loss: 0.6938267648220062, acc.: 47.56%] [G loss: 0.7007722854614258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 70/86 [D loss: 0.6935803592205048, acc.: 49.22%] [G loss: 0.7005969285964966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 71/86 [D loss: 0.6934303343296051, acc.: 50.78%] [G loss: 0.6996254920959473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 72/86 [D loss: 0.6945140361785889, acc.: 47.56%] [G loss: 0.6998140811920166]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 73/86 [D loss: 0.6932936906814575, acc.: 49.41%] [G loss: 0.7000555396080017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 74/86 [D loss: 0.6931919753551483, acc.: 50.39%] [G loss: 0.700518012046814]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 75/86 [D loss: 0.6944547891616821, acc.: 47.90%] [G loss: 0.6999757885932922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 76/86 [D loss: 0.6936959326267242, acc.: 48.44%] [G loss: 0.7008497714996338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 77/86 [D loss: 0.6946936249732971, acc.: 47.27%] [G loss: 0.6984236836433411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 78/86 [D loss: 0.6942839026451111, acc.: 49.07%] [G loss: 0.6995562314987183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 79/86 [D loss: 0.694306343793869, acc.: 48.10%] [G loss: 0.70039963722229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 80/86 [D loss: 0.6938871741294861, acc.: 48.34%] [G loss: 0.699126660823822]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 81/86 [D loss: 0.6943907141685486, acc.: 47.31%] [G loss: 0.7000001668930054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 82/86 [D loss: 0.6940307915210724, acc.: 49.12%] [G loss: 0.7008964419364929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 38/200, Batch 83/86 [D loss: 0.6933861374855042, acc.: 50.49%] [G loss: 0.6973345875740051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 84/86 [D loss: 0.6944136619567871, acc.: 48.29%] [G loss: 0.6985179781913757]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 85/86 [D loss: 0.6933362483978271, acc.: 49.66%] [G loss: 0.7011126279830933]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 38/200, Batch 86/86 [D loss: 0.6928869485855103, acc.: 49.80%] [G loss: 0.7001718878746033]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 1/86 [D loss: 0.6946402490139008, acc.: 48.34%] [G loss: 0.699574887752533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 2/86 [D loss: 0.6931957602500916, acc.: 50.68%] [G loss: 0.6976447105407715]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 3/86 [D loss: 0.6939257979393005, acc.: 47.56%] [G loss: 0.6955102682113647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 4/86 [D loss: 0.6960088014602661, acc.: 46.00%] [G loss: 0.7003664374351501]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 5/86 [D loss: 0.69292351603508, acc.: 50.34%] [G loss: 0.7012516856193542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 6/86 [D loss: 0.6940528452396393, acc.: 48.83%] [G loss: 0.7003867030143738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 7/86 [D loss: 0.6939162015914917, acc.: 49.41%] [G loss: 0.7007285952568054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 8/86 [D loss: 0.6937999129295349, acc.: 50.44%] [G loss: 0.6977704763412476]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 9/86 [D loss: 0.6949519217014313, acc.: 47.31%] [G loss: 0.6979416012763977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 10/86 [D loss: 0.694074809551239, acc.: 47.66%] [G loss: 0.702126145362854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 11/86 [D loss: 0.692723959684372, acc.: 51.61%] [G loss: 0.7005844116210938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 12/86 [D loss: 0.6948730945587158, acc.: 46.48%] [G loss: 0.7001029849052429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 13/86 [D loss: 0.6932371556758881, acc.: 50.73%] [G loss: 0.6987765431404114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 14/86 [D loss: 0.6941246688365936, acc.: 47.90%] [G loss: 0.6955370306968689]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 15/86 [D loss: 0.6954148709774017, acc.: 46.00%] [G loss: 0.696631908416748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 16/86 [D loss: 0.6949067115783691, acc.: 47.22%] [G loss: 0.7026000618934631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 17/86 [D loss: 0.693516343832016, acc.: 49.66%] [G loss: 0.6994377374649048]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 18/86 [D loss: 0.6949307322502136, acc.: 48.63%] [G loss: 0.6994727849960327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 19/86 [D loss: 0.6943354904651642, acc.: 47.85%] [G loss: 0.6992244124412537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 20/86 [D loss: 0.6942256689071655, acc.: 49.37%] [G loss: 0.6962407231330872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 21/86 [D loss: 0.6965906322002411, acc.: 45.41%] [G loss: 0.6987023949623108]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 22/86 [D loss: 0.6934513449668884, acc.: 50.10%] [G loss: 0.7030704021453857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 23/86 [D loss: 0.693244606256485, acc.: 50.15%] [G loss: 0.6996446847915649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 24/86 [D loss: 0.6943130493164062, acc.: 47.66%] [G loss: 0.7000799179077148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 25/86 [D loss: 0.6930089592933655, acc.: 50.49%] [G loss: 0.6993117928504944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 26/86 [D loss: 0.6936082541942596, acc.: 48.29%] [G loss: 0.6952199339866638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 27/86 [D loss: 0.6975953578948975, acc.: 43.41%] [G loss: 0.6970593333244324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 28/86 [D loss: 0.6941633224487305, acc.: 48.88%] [G loss: 0.7021856904029846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 29/86 [D loss: 0.6937114894390106, acc.: 49.71%] [G loss: 0.699556291103363]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 30/86 [D loss: 0.6952532231807709, acc.: 46.88%] [G loss: 0.7010911703109741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 31/86 [D loss: 0.6940288841724396, acc.: 48.44%] [G loss: 0.6996235847473145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 32/86 [D loss: 0.6931676268577576, acc.: 50.93%] [G loss: 0.6966376304626465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 33/86 [D loss: 0.6962213218212128, acc.: 45.36%] [G loss: 0.6992590427398682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 34/86 [D loss: 0.6934644877910614, acc.: 49.85%] [G loss: 0.7028061747550964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 35/86 [D loss: 0.6935751736164093, acc.: 50.34%] [G loss: 0.7011474370956421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 36/86 [D loss: 0.694505363702774, acc.: 47.90%] [G loss: 0.7011831998825073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 37/86 [D loss: 0.6938621699810028, acc.: 49.17%] [G loss: 0.7008446455001831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 38/86 [D loss: 0.6945223212242126, acc.: 47.75%] [G loss: 0.6985304951667786]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 39/86 [D loss: 0.6946226954460144, acc.: 47.02%] [G loss: 0.7001644968986511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 40/86 [D loss: 0.693876713514328, acc.: 49.27%] [G loss: 0.7035276889801025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 41/86 [D loss: 0.6930315494537354, acc.: 50.88%] [G loss: 0.701782763004303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 42/86 [D loss: 0.6938514411449432, acc.: 48.24%] [G loss: 0.7013410329818726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 43/86 [D loss: 0.6934655904769897, acc.: 49.37%] [G loss: 0.7009558081626892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 44/86 [D loss: 0.6950371861457825, acc.: 46.78%] [G loss: 0.698971688747406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 45/86 [D loss: 0.6940463185310364, acc.: 48.93%] [G loss: 0.7007709741592407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 46/86 [D loss: 0.6927710473537445, acc.: 51.03%] [G loss: 0.7022954821586609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 47/86 [D loss: 0.6940581202507019, acc.: 47.56%] [G loss: 0.6992815732955933]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 48/86 [D loss: 0.6939005255699158, acc.: 49.02%] [G loss: 0.7008413076400757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 49/86 [D loss: 0.6932356357574463, acc.: 50.83%] [G loss: 0.6973708868026733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 50/86 [D loss: 0.6947521567344666, acc.: 47.41%] [G loss: 0.6976264119148254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 51/86 [D loss: 0.6951158046722412, acc.: 47.02%] [G loss: 0.7021335959434509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 52/86 [D loss: 0.6925648748874664, acc.: 51.71%] [G loss: 0.6999198794364929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 53/86 [D loss: 0.6949309408664703, acc.: 46.29%] [G loss: 0.6997179985046387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 54/86 [D loss: 0.6939718127250671, acc.: 48.44%] [G loss: 0.6998511552810669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 55/86 [D loss: 0.6931935846805573, acc.: 50.63%] [G loss: 0.6970318555831909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 56/86 [D loss: 0.6961551606655121, acc.: 44.82%] [G loss: 0.697434663772583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 57/86 [D loss: 0.6936832666397095, acc.: 48.93%] [G loss: 0.701261579990387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 58/86 [D loss: 0.6943126022815704, acc.: 47.80%] [G loss: 0.7000564336776733]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 59/86 [D loss: 0.6952262222766876, acc.: 47.27%] [G loss: 0.6991807818412781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 60/86 [D loss: 0.6943152546882629, acc.: 48.63%] [G loss: 0.7003641128540039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 61/86 [D loss: 0.6932858228683472, acc.: 48.68%] [G loss: 0.6952937841415405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 62/86 [D loss: 0.6967732906341553, acc.: 45.41%] [G loss: 0.697027325630188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 63/86 [D loss: 0.6947080492973328, acc.: 46.73%] [G loss: 0.7018420100212097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 64/86 [D loss: 0.6937477290630341, acc.: 49.02%] [G loss: 0.6995513439178467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 65/86 [D loss: 0.6946625709533691, acc.: 47.51%] [G loss: 0.7005621194839478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 66/86 [D loss: 0.692962646484375, acc.: 50.63%] [G loss: 0.7012598514556885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 67/86 [D loss: 0.6933822929859161, acc.: 49.76%] [G loss: 0.695500910282135]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 68/86 [D loss: 0.6958296000957489, acc.: 46.00%] [G loss: 0.6965558528900146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 69/86 [D loss: 0.693967878818512, acc.: 48.44%] [G loss: 0.7023192644119263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 70/86 [D loss: 0.6939253509044647, acc.: 49.85%] [G loss: 0.6991856098175049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 71/86 [D loss: 0.6940974593162537, acc.: 48.10%] [G loss: 0.6995255351066589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 72/86 [D loss: 0.6938690543174744, acc.: 49.37%] [G loss: 0.7004087567329407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 73/86 [D loss: 0.6938444972038269, acc.: 49.56%] [G loss: 0.6999106407165527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 74/86 [D loss: 0.6947218775749207, acc.: 47.80%] [G loss: 0.699287474155426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 75/86 [D loss: 0.6944258511066437, acc.: 47.46%] [G loss: 0.7026959657669067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 76/86 [D loss: 0.6943382918834686, acc.: 48.05%] [G loss: 0.7001687288284302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 77/86 [D loss: 0.6946238577365875, acc.: 47.90%] [G loss: 0.7017716765403748]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 39/200, Batch 78/86 [D loss: 0.6934689879417419, acc.: 50.49%] [G loss: 0.7000346183776855]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 79/86 [D loss: 0.6941218376159668, acc.: 49.71%] [G loss: 0.6980316042900085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 80/86 [D loss: 0.6946002244949341, acc.: 47.66%] [G loss: 0.6979619860649109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 81/86 [D loss: 0.6949160993099213, acc.: 47.41%] [G loss: 0.702534019947052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 82/86 [D loss: 0.6943936944007874, acc.: 47.66%] [G loss: 0.7000652551651001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 83/86 [D loss: 0.6938109993934631, acc.: 48.97%] [G loss: 0.6997238993644714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 84/86 [D loss: 0.6937507390975952, acc.: 49.85%] [G loss: 0.699273943901062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 85/86 [D loss: 0.6940293312072754, acc.: 47.85%] [G loss: 0.6971226334571838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 39/200, Batch 86/86 [D loss: 0.6948604583740234, acc.: 46.83%] [G loss: 0.6993535757064819]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 1/86 [D loss: 0.6945880353450775, acc.: 47.27%] [G loss: 0.7024877667427063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 2/86 [D loss: 0.6930876970291138, acc.: 51.37%] [G loss: 0.7015479207038879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 3/86 [D loss: 0.6949009895324707, acc.: 48.14%] [G loss: 0.7002807855606079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 4/86 [D loss: 0.6943180561065674, acc.: 46.83%] [G loss: 0.6990014314651489]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 5/86 [D loss: 0.6931195557117462, acc.: 50.98%] [G loss: 0.697667121887207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 6/86 [D loss: 0.6944298446178436, acc.: 48.19%] [G loss: 0.698401689529419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 7/86 [D loss: 0.69443279504776, acc.: 47.46%] [G loss: 0.702077329158783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 8/86 [D loss: 0.6934382617473602, acc.: 49.17%] [G loss: 0.7001953721046448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 9/86 [D loss: 0.694783478975296, acc.: 47.17%] [G loss: 0.6997898817062378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 10/86 [D loss: 0.6944699287414551, acc.: 47.27%] [G loss: 0.7000961303710938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 11/86 [D loss: 0.6944045722484589, acc.: 48.14%] [G loss: 0.6976368427276611]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 12/86 [D loss: 0.6946784555912018, acc.: 47.61%] [G loss: 0.6992515325546265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 13/86 [D loss: 0.6941340267658234, acc.: 49.27%] [G loss: 0.7028105854988098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 14/86 [D loss: 0.6929078698158264, acc.: 51.07%] [G loss: 0.6995519995689392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 15/86 [D loss: 0.6947861909866333, acc.: 46.04%] [G loss: 0.7018927335739136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 16/86 [D loss: 0.6933350265026093, acc.: 49.07%] [G loss: 0.7000979781150818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 17/86 [D loss: 0.6931545734405518, acc.: 50.83%] [G loss: 0.6966532468795776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 18/86 [D loss: 0.6956377625465393, acc.: 46.04%] [G loss: 0.6989700794219971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 19/86 [D loss: 0.6932496428489685, acc.: 50.20%] [G loss: 0.7028506994247437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 20/86 [D loss: 0.6934152841567993, acc.: 50.29%] [G loss: 0.700679361820221]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 21/86 [D loss: 0.6944676339626312, acc.: 47.41%] [G loss: 0.7009091377258301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 22/86 [D loss: 0.6935731768608093, acc.: 49.90%] [G loss: 0.6996899843215942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 23/86 [D loss: 0.6937122642993927, acc.: 50.49%] [G loss: 0.6965498328208923]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 24/86 [D loss: 0.6954165101051331, acc.: 45.51%] [G loss: 0.7004784345626831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 25/86 [D loss: 0.6923168301582336, acc.: 51.37%] [G loss: 0.7012385725975037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 26/86 [D loss: 0.6941995620727539, acc.: 48.10%] [G loss: 0.6998389959335327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 27/86 [D loss: 0.6946702301502228, acc.: 47.56%] [G loss: 0.7007203102111816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 28/86 [D loss: 0.6929472386837006, acc.: 50.15%] [G loss: 0.6975477933883667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 29/86 [D loss: 0.6943469941616058, acc.: 48.49%] [G loss: 0.6952379941940308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 30/86 [D loss: 0.6958061158657074, acc.: 45.70%] [G loss: 0.7018623352050781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 31/86 [D loss: 0.6928707957267761, acc.: 50.00%] [G loss: 0.7014121413230896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 32/86 [D loss: 0.6945459842681885, acc.: 46.97%] [G loss: 0.6983542442321777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 33/86 [D loss: 0.6941699683666229, acc.: 47.66%] [G loss: 0.6995350122451782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 34/86 [D loss: 0.6933057606220245, acc.: 49.12%] [G loss: 0.6973230838775635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 35/86 [D loss: 0.6938615441322327, acc.: 50.20%] [G loss: 0.6952345371246338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 36/86 [D loss: 0.6963483393192291, acc.: 46.63%] [G loss: 0.6994889974594116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 37/86 [D loss: 0.6926514804363251, acc.: 50.78%] [G loss: 0.7013245820999146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 38/86 [D loss: 0.6933798789978027, acc.: 49.85%] [G loss: 0.6980712413787842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 39/86 [D loss: 0.6935150325298309, acc.: 49.71%] [G loss: 0.6996716260910034]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 40/86 [D loss: 0.6920183897018433, acc.: 52.29%] [G loss: 0.6978089213371277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 41/86 [D loss: 0.6944399476051331, acc.: 47.51%] [G loss: 0.6950466632843018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 42/86 [D loss: 0.6954973340034485, acc.: 47.27%] [G loss: 0.6993318796157837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 43/86 [D loss: 0.6930895447731018, acc.: 49.95%] [G loss: 0.6992218494415283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 44/86 [D loss: 0.6947988867759705, acc.: 47.80%] [G loss: 0.6982653737068176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 45/86 [D loss: 0.6953124105930328, acc.: 46.14%] [G loss: 0.6985453963279724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 46/86 [D loss: 0.6930394768714905, acc.: 50.34%] [G loss: 0.6973839402198792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 47/86 [D loss: 0.6952795386314392, acc.: 47.12%] [G loss: 0.6956638693809509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 48/86 [D loss: 0.6945851743221283, acc.: 48.10%] [G loss: 0.7003306150436401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 49/86 [D loss: 0.6926905810832977, acc.: 50.83%] [G loss: 0.7009656429290771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 50/86 [D loss: 0.6946952939033508, acc.: 47.27%] [G loss: 0.7001731395721436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 51/86 [D loss: 0.6936300098896027, acc.: 49.07%] [G loss: 0.698440432548523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 52/86 [D loss: 0.693628191947937, acc.: 49.51%] [G loss: 0.698546826839447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 53/86 [D loss: 0.6944979727268219, acc.: 46.68%] [G loss: 0.6973206400871277]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 54/86 [D loss: 0.6948930025100708, acc.: 46.58%] [G loss: 0.6999248266220093]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 55/86 [D loss: 0.6930083632469177, acc.: 50.63%] [G loss: 0.7021955847740173]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 56/86 [D loss: 0.6941533088684082, acc.: 48.29%] [G loss: 0.7013857364654541]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 57/86 [D loss: 0.6934714913368225, acc.: 49.51%] [G loss: 0.7004452347755432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 58/86 [D loss: 0.6942921578884125, acc.: 49.12%] [G loss: 0.7000830173492432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 59/86 [D loss: 0.6937683820724487, acc.: 50.05%] [G loss: 0.6983669400215149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 60/86 [D loss: 0.6942014992237091, acc.: 49.61%] [G loss: 0.7002567052841187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 61/86 [D loss: 0.6934425830841064, acc.: 49.51%] [G loss: 0.701242983341217]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 62/86 [D loss: 0.6938571929931641, acc.: 49.76%] [G loss: 0.7004859447479248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 63/86 [D loss: 0.6942247450351715, acc.: 47.31%] [G loss: 0.70076584815979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 64/86 [D loss: 0.6936580240726471, acc.: 49.22%] [G loss: 0.6994138956069946]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 65/86 [D loss: 0.69463911652565, acc.: 48.05%] [G loss: 0.6995331645011902]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 66/86 [D loss: 0.6935549378395081, acc.: 49.61%] [G loss: 0.7019526958465576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 67/86 [D loss: 0.6931446194648743, acc.: 51.95%] [G loss: 0.7006280422210693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 68/86 [D loss: 0.6931229829788208, acc.: 50.34%] [G loss: 0.7000678777694702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 69/86 [D loss: 0.6945131421089172, acc.: 48.49%] [G loss: 0.7008406519889832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 70/86 [D loss: 0.6937345862388611, acc.: 50.88%] [G loss: 0.6989122629165649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 71/86 [D loss: 0.6938654482364655, acc.: 49.02%] [G loss: 0.6986296772956848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 72/86 [D loss: 0.6952951550483704, acc.: 46.48%] [G loss: 0.6998234987258911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 73/86 [D loss: 0.6939119398593903, acc.: 47.90%] [G loss: 0.7013617753982544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 74/86 [D loss: 0.6935904920101166, acc.: 49.80%] [G loss: 0.7011711597442627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 75/86 [D loss: 0.693576842546463, acc.: 48.63%] [G loss: 0.7003732919692993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 76/86 [D loss: 0.6931297481060028, acc.: 50.78%] [G loss: 0.6984412670135498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 77/86 [D loss: 0.6945489645004272, acc.: 47.12%] [G loss: 0.6977499723434448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 78/86 [D loss: 0.6942669451236725, acc.: 48.63%] [G loss: 0.7010995745658875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 79/86 [D loss: 0.6924535036087036, acc.: 52.25%] [G loss: 0.7014027833938599]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 80/86 [D loss: 0.693783164024353, acc.: 48.54%] [G loss: 0.6999008655548096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 81/86 [D loss: 0.693680077791214, acc.: 49.37%] [G loss: 0.699892520904541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 82/86 [D loss: 0.6943285465240479, acc.: 46.73%] [G loss: 0.6970826387405396]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 83/86 [D loss: 0.6945748627185822, acc.: 48.44%] [G loss: 0.69816654920578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 84/86 [D loss: 0.6945578157901764, acc.: 48.39%] [G loss: 0.7012699842453003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 40/200, Batch 85/86 [D loss: 0.6925076246261597, acc.: 51.22%] [G loss: 0.6999175548553467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 40/200, Batch 86/86 [D loss: 0.6938014030456543, acc.: 49.90%] [G loss: 0.6989021301269531]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 1/86 [D loss: 0.6939831078052521, acc.: 49.76%] [G loss: 0.7004899978637695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 2/86 [D loss: 0.6928232312202454, acc.: 51.27%] [G loss: 0.6959904432296753]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 3/86 [D loss: 0.6947528421878815, acc.: 48.00%] [G loss: 0.6971234679222107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 4/86 [D loss: 0.6934642195701599, acc.: 49.76%] [G loss: 0.7021524906158447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 5/86 [D loss: 0.6927036345005035, acc.: 51.37%] [G loss: 0.6988449692726135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 6/86 [D loss: 0.694675624370575, acc.: 46.53%] [G loss: 0.6995728015899658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 7/86 [D loss: 0.6936909854412079, acc.: 48.93%] [G loss: 0.7002345323562622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 8/86 [D loss: 0.6935996115207672, acc.: 49.61%] [G loss: 0.6954526305198669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 9/86 [D loss: 0.6957466304302216, acc.: 46.00%] [G loss: 0.6967917680740356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 10/86 [D loss: 0.6945973336696625, acc.: 46.68%] [G loss: 0.7016271352767944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 11/86 [D loss: 0.6931841373443604, acc.: 49.66%] [G loss: 0.6978710293769836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 12/86 [D loss: 0.6942015290260315, acc.: 45.85%] [G loss: 0.6989811658859253]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 13/86 [D loss: 0.6930761635303497, acc.: 50.88%] [G loss: 0.6991664171218872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 14/86 [D loss: 0.6940675377845764, acc.: 49.27%] [G loss: 0.6974567770957947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 15/86 [D loss: 0.6950972676277161, acc.: 47.56%] [G loss: 0.6967934370040894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 16/86 [D loss: 0.6945346891880035, acc.: 46.58%] [G loss: 0.701949954032898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 17/86 [D loss: 0.6937979161739349, acc.: 49.66%] [G loss: 0.6997392177581787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 18/86 [D loss: 0.6947533190250397, acc.: 46.39%] [G loss: 0.6988329887390137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 19/86 [D loss: 0.6935392916202545, acc.: 49.56%] [G loss: 0.6999708414077759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 20/86 [D loss: 0.6927542686462402, acc.: 50.39%] [G loss: 0.6968954801559448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 21/86 [D loss: 0.6957333087921143, acc.: 46.24%] [G loss: 0.6971489191055298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 22/86 [D loss: 0.6940816938877106, acc.: 48.88%] [G loss: 0.702495813369751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 23/86 [D loss: 0.6932964622974396, acc.: 51.37%] [G loss: 0.6989499926567078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 24/86 [D loss: 0.6961471736431122, acc.: 44.63%] [G loss: 0.6983760595321655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 25/86 [D loss: 0.6931051909923553, acc.: 50.05%] [G loss: 0.6997178792953491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 26/86 [D loss: 0.6940064430236816, acc.: 48.34%] [G loss: 0.6959188580513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 27/86 [D loss: 0.6956667602062225, acc.: 46.88%] [G loss: 0.6968616247177124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 28/86 [D loss: 0.6939667761325836, acc.: 49.12%] [G loss: 0.701016902923584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 29/86 [D loss: 0.6933644711971283, acc.: 49.07%] [G loss: 0.6990761160850525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 30/86 [D loss: 0.6939705610275269, acc.: 48.58%] [G loss: 0.6993207931518555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 31/86 [D loss: 0.6936300992965698, acc.: 49.95%] [G loss: 0.6988609433174133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 32/86 [D loss: 0.6935807466506958, acc.: 49.07%] [G loss: 0.6986055374145508]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 33/86 [D loss: 0.6948411762714386, acc.: 46.29%] [G loss: 0.6980515718460083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 34/86 [D loss: 0.6938787996768951, acc.: 49.07%] [G loss: 0.7015618681907654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 35/86 [D loss: 0.6934868395328522, acc.: 48.49%] [G loss: 0.7001574635505676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 36/86 [D loss: 0.6943149864673615, acc.: 47.90%] [G loss: 0.7004483938217163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 37/86 [D loss: 0.6932470202445984, acc.: 49.56%] [G loss: 0.6986825466156006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 38/86 [D loss: 0.6944977641105652, acc.: 47.85%] [G loss: 0.6967326998710632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 39/86 [D loss: 0.6943051517009735, acc.: 48.29%] [G loss: 0.7011884450912476]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 40/86 [D loss: 0.6927299499511719, acc.: 51.27%] [G loss: 0.7001312375068665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 41/86 [D loss: 0.6943047940731049, acc.: 47.41%] [G loss: 0.6988353729248047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 42/86 [D loss: 0.6936752796173096, acc.: 48.14%] [G loss: 0.6994410157203674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 43/86 [D loss: 0.6937004625797272, acc.: 49.22%] [G loss: 0.6989117860794067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 44/86 [D loss: 0.6938593983650208, acc.: 48.44%] [G loss: 0.696550726890564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 45/86 [D loss: 0.6942458748817444, acc.: 48.14%] [G loss: 0.7000605463981628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 46/86 [D loss: 0.6926573812961578, acc.: 50.93%] [G loss: 0.7016595602035522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 47/86 [D loss: 0.6934790313243866, acc.: 49.02%] [G loss: 0.700797975063324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 48/86 [D loss: 0.6931446194648743, acc.: 51.07%] [G loss: 0.7000442743301392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 49/86 [D loss: 0.6930290460586548, acc.: 51.12%] [G loss: 0.699209988117218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 50/86 [D loss: 0.6948866546154022, acc.: 47.41%] [G loss: 0.6991167068481445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 51/86 [D loss: 0.6937357485294342, acc.: 49.46%] [G loss: 0.7004637122154236]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 52/86 [D loss: 0.69259974360466, acc.: 52.69%] [G loss: 0.7004094123840332]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 53/86 [D loss: 0.693833738565445, acc.: 48.83%] [G loss: 0.6994103789329529]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 54/86 [D loss: 0.6938194632530212, acc.: 48.83%] [G loss: 0.7001069784164429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 55/86 [D loss: 0.6932663321495056, acc.: 50.68%] [G loss: 0.6980484127998352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 56/86 [D loss: 0.6941751837730408, acc.: 48.39%] [G loss: 0.6981488466262817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 57/86 [D loss: 0.6941999793052673, acc.: 49.37%] [G loss: 0.7006773948669434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 58/86 [D loss: 0.6931447982788086, acc.: 50.59%] [G loss: 0.6998254060745239]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 59/86 [D loss: 0.693710595369339, acc.: 49.27%] [G loss: 0.6982815265655518]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 60/86 [D loss: 0.6940750777721405, acc.: 49.51%] [G loss: 0.6998529434204102]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 61/86 [D loss: 0.6935853362083435, acc.: 48.63%] [G loss: 0.696047306060791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 62/86 [D loss: 0.693932294845581, acc.: 48.54%] [G loss: 0.6976335048675537]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 63/86 [D loss: 0.6938095092773438, acc.: 49.90%] [G loss: 0.7014099359512329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 64/86 [D loss: 0.6925510466098785, acc.: 52.29%] [G loss: 0.6995579600334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 65/86 [D loss: 0.6949580311775208, acc.: 46.92%] [G loss: 0.6990090608596802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 66/86 [D loss: 0.6935190856456757, acc.: 49.41%] [G loss: 0.6985217332839966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 67/86 [D loss: 0.6939394772052765, acc.: 48.49%] [G loss: 0.6981064081192017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 68/86 [D loss: 0.6942468881607056, acc.: 49.71%] [G loss: 0.6985094547271729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 69/86 [D loss: 0.6930770874023438, acc.: 49.27%] [G loss: 0.7011091709136963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 70/86 [D loss: 0.6935677528381348, acc.: 48.10%] [G loss: 0.6994602084159851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 71/86 [D loss: 0.6939032971858978, acc.: 48.49%] [G loss: 0.7000369429588318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 72/86 [D loss: 0.6933408081531525, acc.: 49.27%] [G loss: 0.6982588171958923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 73/86 [D loss: 0.6935429573059082, acc.: 48.58%] [G loss: 0.6964989304542542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 74/86 [D loss: 0.6949440836906433, acc.: 47.61%] [G loss: 0.6984133720397949]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 75/86 [D loss: 0.6934071481227875, acc.: 50.63%] [G loss: 0.7002562284469604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 76/86 [D loss: 0.6930599212646484, acc.: 49.95%] [G loss: 0.7001996636390686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 77/86 [D loss: 0.6940273344516754, acc.: 48.78%] [G loss: 0.700341522693634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 78/86 [D loss: 0.6934380531311035, acc.: 48.97%] [G loss: 0.6997787952423096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 79/86 [D loss: 0.6948925256729126, acc.: 47.31%] [G loss: 0.696193277835846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 80/86 [D loss: 0.6962246596813202, acc.: 45.31%] [G loss: 0.6991942524909973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 81/86 [D loss: 0.6932685971260071, acc.: 48.97%] [G loss: 0.7019892930984497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 82/86 [D loss: 0.6932147741317749, acc.: 51.51%] [G loss: 0.6990994215011597]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 83/86 [D loss: 0.6941916942596436, acc.: 48.05%] [G loss: 0.7004774212837219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 84/86 [D loss: 0.6934337615966797, acc.: 50.59%] [G loss: 0.6998299360275269]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 41/200, Batch 85/86 [D loss: 0.6942512392997742, acc.: 48.58%] [G loss: 0.6951122879981995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 41/200, Batch 86/86 [D loss: 0.6961438059806824, acc.: 45.56%] [G loss: 0.7014111280441284]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 1/86 [D loss: 0.6918633282184601, acc.: 52.93%] [G loss: 0.70125812292099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 2/86 [D loss: 0.6942175030708313, acc.: 48.49%] [G loss: 0.6982740759849548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 3/86 [D loss: 0.6940414309501648, acc.: 49.02%] [G loss: 0.7004714012145996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 4/86 [D loss: 0.692306786775589, acc.: 51.95%] [G loss: 0.7001073360443115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 5/86 [D loss: 0.6942894458770752, acc.: 49.02%] [G loss: 0.6942895650863647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 6/86 [D loss: 0.6967139542102814, acc.: 45.56%] [G loss: 0.7011629343032837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 7/86 [D loss: 0.6920972168445587, acc.: 51.61%] [G loss: 0.7003589868545532]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 8/86 [D loss: 0.6945307552814484, acc.: 47.17%] [G loss: 0.6977171897888184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 9/86 [D loss: 0.6948341429233551, acc.: 47.66%] [G loss: 0.6999631524085999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 10/86 [D loss: 0.6929516494274139, acc.: 50.15%] [G loss: 0.6983909010887146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 11/86 [D loss: 0.6946271061897278, acc.: 48.44%] [G loss: 0.6948977708816528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 12/86 [D loss: 0.6954054832458496, acc.: 46.78%] [G loss: 0.6984173655509949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 13/86 [D loss: 0.6918637454509735, acc.: 52.44%] [G loss: 0.7009503245353699]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 14/86 [D loss: 0.6933896541595459, acc.: 49.76%] [G loss: 0.698626697063446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 15/86 [D loss: 0.694848358631134, acc.: 47.12%] [G loss: 0.6992453336715698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 16/86 [D loss: 0.6938863396644592, acc.: 50.68%] [G loss: 0.6996808648109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 17/86 [D loss: 0.6925274431705475, acc.: 52.93%] [G loss: 0.6974031925201416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 18/86 [D loss: 0.6945848762989044, acc.: 48.44%] [G loss: 0.6961644291877747]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 19/86 [D loss: 0.6933994293212891, acc.: 50.00%] [G loss: 0.7032694816589355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 20/86 [D loss: 0.6931403875350952, acc.: 50.10%] [G loss: 0.7001317739486694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 21/86 [D loss: 0.6940309405326843, acc.: 48.68%] [G loss: 0.7001980543136597]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 22/86 [D loss: 0.6933882832527161, acc.: 49.32%] [G loss: 0.7002851963043213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 23/86 [D loss: 0.6928504109382629, acc.: 51.12%] [G loss: 0.697944164276123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 24/86 [D loss: 0.6945893168449402, acc.: 47.95%] [G loss: 0.6973005533218384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 25/86 [D loss: 0.692653626203537, acc.: 52.20%] [G loss: 0.701744794845581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 26/86 [D loss: 0.6930583417415619, acc.: 49.61%] [G loss: 0.7004858255386353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 27/86 [D loss: 0.6945143938064575, acc.: 49.27%] [G loss: 0.7006953358650208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 28/86 [D loss: 0.6930617094039917, acc.: 51.17%] [G loss: 0.6989873647689819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 29/86 [D loss: 0.6936146020889282, acc.: 49.61%] [G loss: 0.6985255479812622]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 30/86 [D loss: 0.6935519576072693, acc.: 49.46%] [G loss: 0.7003925442695618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 31/86 [D loss: 0.6930830776691437, acc.: 50.83%] [G loss: 0.7006517648696899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 32/86 [D loss: 0.6926813721656799, acc.: 50.29%] [G loss: 0.699163556098938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 33/86 [D loss: 0.6940109431743622, acc.: 48.63%] [G loss: 0.700721025466919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 34/86 [D loss: 0.6928472816944122, acc.: 51.86%] [G loss: 0.6990329027175903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 35/86 [D loss: 0.6938095986843109, acc.: 48.93%] [G loss: 0.6977006793022156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 36/86 [D loss: 0.6943365335464478, acc.: 48.83%] [G loss: 0.7001539468765259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 37/86 [D loss: 0.6925519108772278, acc.: 49.90%] [G loss: 0.7006932497024536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 38/86 [D loss: 0.6933167278766632, acc.: 49.71%] [G loss: 0.7007846236228943]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 39/86 [D loss: 0.6939736604690552, acc.: 48.24%] [G loss: 0.6998428702354431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 40/86 [D loss: 0.6926978826522827, acc.: 51.61%] [G loss: 0.6983545422554016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 41/86 [D loss: 0.6942093670368195, acc.: 47.85%] [G loss: 0.6974412798881531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 42/86 [D loss: 0.6946148872375488, acc.: 47.95%] [G loss: 0.7005534768104553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 43/86 [D loss: 0.6924454867839813, acc.: 51.66%] [G loss: 0.7017132043838501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 44/86 [D loss: 0.6935098171234131, acc.: 49.51%] [G loss: 0.7006336450576782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 45/86 [D loss: 0.6932409703731537, acc.: 50.20%] [G loss: 0.7007904052734375]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 46/86 [D loss: 0.6929329633712769, acc.: 51.12%] [G loss: 0.6991451382637024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 47/86 [D loss: 0.6949054002761841, acc.: 46.53%] [G loss: 0.697548508644104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 48/86 [D loss: 0.6944288909435272, acc.: 49.85%] [G loss: 0.7014281153678894]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 49/86 [D loss: 0.6927315294742584, acc.: 51.51%] [G loss: 0.7005009055137634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 50/86 [D loss: 0.6933300793170929, acc.: 49.51%] [G loss: 0.6995458006858826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 51/86 [D loss: 0.6937532424926758, acc.: 49.61%] [G loss: 0.7005759477615356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 52/86 [D loss: 0.6929667294025421, acc.: 49.85%] [G loss: 0.6996912956237793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 53/86 [D loss: 0.6939443051815033, acc.: 48.58%] [G loss: 0.6963989734649658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 54/86 [D loss: 0.6944617629051208, acc.: 48.39%] [G loss: 0.7021273374557495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 55/86 [D loss: 0.6927689909934998, acc.: 50.39%] [G loss: 0.700236439704895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 56/86 [D loss: 0.6940067410469055, acc.: 48.19%] [G loss: 0.6994428634643555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 57/86 [D loss: 0.69258251786232, acc.: 51.03%] [G loss: 0.699988842010498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 58/86 [D loss: 0.6922232806682587, acc.: 54.05%] [G loss: 0.6962981820106506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 59/86 [D loss: 0.6951062083244324, acc.: 46.83%] [G loss: 0.695749819278717]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 60/86 [D loss: 0.6926746070384979, acc.: 50.98%] [G loss: 0.7019504308700562]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 61/86 [D loss: 0.6924781799316406, acc.: 50.98%] [G loss: 0.6995625495910645]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 62/86 [D loss: 0.6955662071704865, acc.: 43.55%] [G loss: 0.696461021900177]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 63/86 [D loss: 0.6932981908321381, acc.: 50.15%] [G loss: 0.6999669671058655]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 64/86 [D loss: 0.6922913491725922, acc.: 51.22%] [G loss: 0.6964256763458252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 65/86 [D loss: 0.6954775452613831, acc.: 46.73%] [G loss: 0.6961289048194885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 66/86 [D loss: 0.6939746141433716, acc.: 49.61%] [G loss: 0.7015680074691772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 67/86 [D loss: 0.6921435296535492, acc.: 52.98%] [G loss: 0.6999558210372925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 68/86 [D loss: 0.694685310125351, acc.: 48.19%] [G loss: 0.6994878649711609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 69/86 [D loss: 0.6933832168579102, acc.: 47.95%] [G loss: 0.7007448673248291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 70/86 [D loss: 0.6943087875843048, acc.: 48.44%] [G loss: 0.6977617740631104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 71/86 [D loss: 0.6944732069969177, acc.: 48.68%] [G loss: 0.696082592010498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 72/86 [D loss: 0.6945946216583252, acc.: 47.71%] [G loss: 0.7013388276100159]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 73/86 [D loss: 0.6923063099384308, acc.: 51.03%] [G loss: 0.7003530263900757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 74/86 [D loss: 0.6942364871501923, acc.: 48.97%] [G loss: 0.6983428597450256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 75/86 [D loss: 0.6939870417118073, acc.: 48.39%] [G loss: 0.6990849375724792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 76/86 [D loss: 0.6925045847892761, acc.: 52.00%] [G loss: 0.6990587115287781]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 77/86 [D loss: 0.6947710812091827, acc.: 46.58%] [G loss: 0.6957780718803406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 78/86 [D loss: 0.6939106583595276, acc.: 49.02%] [G loss: 0.7015661597251892]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 79/86 [D loss: 0.6927290558815002, acc.: 51.66%] [G loss: 0.7002002000808716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 80/86 [D loss: 0.6933163702487946, acc.: 50.15%] [G loss: 0.6998057961463928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 81/86 [D loss: 0.6939082145690918, acc.: 49.56%] [G loss: 0.7002089023590088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 82/86 [D loss: 0.692867785692215, acc.: 52.25%] [G loss: 0.6970922350883484]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 83/86 [D loss: 0.6938716769218445, acc.: 49.27%] [G loss: 0.6961144208908081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 84/86 [D loss: 0.6939558386802673, acc.: 49.22%] [G loss: 0.7006703615188599]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 42/200, Batch 85/86 [D loss: 0.6918719112873077, acc.: 53.03%] [G loss: 0.7006824016571045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 42/200, Batch 86/86 [D loss: 0.6945773959159851, acc.: 47.66%] [G loss: 0.6989965438842773]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 1/86 [D loss: 0.692572683095932, acc.: 50.59%] [G loss: 0.7004453539848328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 2/86 [D loss: 0.6923549175262451, acc.: 50.98%] [G loss: 0.6993038654327393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 3/86 [D loss: 0.6949166357517242, acc.: 47.95%] [G loss: 0.698974072933197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 4/86 [D loss: 0.694430410861969, acc.: 48.19%] [G loss: 0.700961709022522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 5/86 [D loss: 0.6925372183322906, acc.: 51.95%] [G loss: 0.6990006566047668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 6/86 [D loss: 0.6934220492839813, acc.: 49.41%] [G loss: 0.6999004483222961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 7/86 [D loss: 0.693210631608963, acc.: 51.66%] [G loss: 0.7005818486213684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 8/86 [D loss: 0.6936073005199432, acc.: 49.27%] [G loss: 0.700958788394928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 9/86 [D loss: 0.6933799982070923, acc.: 48.78%] [G loss: 0.69862961769104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 10/86 [D loss: 0.6941631436347961, acc.: 48.19%] [G loss: 0.7010816931724548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 11/86 [D loss: 0.6929018497467041, acc.: 50.73%] [G loss: 0.7017635107040405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 12/86 [D loss: 0.6926158964633942, acc.: 51.71%] [G loss: 0.6995854377746582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 13/86 [D loss: 0.6938576698303223, acc.: 48.44%] [G loss: 0.700080156326294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 14/86 [D loss: 0.6931058466434479, acc.: 51.07%] [G loss: 0.6991840600967407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 15/86 [D loss: 0.6937406957149506, acc.: 49.95%] [G loss: 0.697551965713501]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 16/86 [D loss: 0.6940982639789581, acc.: 48.44%] [G loss: 0.6988773345947266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 17/86 [D loss: 0.6930035352706909, acc.: 50.83%] [G loss: 0.702531099319458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 18/86 [D loss: 0.6929495334625244, acc.: 50.00%] [G loss: 0.6999465823173523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 19/86 [D loss: 0.6931213438510895, acc.: 50.24%] [G loss: 0.6998857259750366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 20/86 [D loss: 0.6932108998298645, acc.: 50.24%] [G loss: 0.6993860006332397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 21/86 [D loss: 0.6928508877754211, acc.: 51.17%] [G loss: 0.6977124810218811]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 22/86 [D loss: 0.6948306858539581, acc.: 47.27%] [G loss: 0.69951331615448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 23/86 [D loss: 0.6932033002376556, acc.: 49.95%] [G loss: 0.7002259492874146]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 24/86 [D loss: 0.6934627890586853, acc.: 50.29%] [G loss: 0.6993004083633423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 25/86 [D loss: 0.6925229430198669, acc.: 50.73%] [G loss: 0.698211133480072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 26/86 [D loss: 0.693194180727005, acc.: 50.24%] [G loss: 0.69954514503479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 27/86 [D loss: 0.6937752664089203, acc.: 50.98%] [G loss: 0.6985833644866943]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 28/86 [D loss: 0.6948967576026917, acc.: 46.34%] [G loss: 0.700258731842041]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 29/86 [D loss: 0.6938614249229431, acc.: 49.02%] [G loss: 0.7024790644645691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 30/86 [D loss: 0.693072497844696, acc.: 50.88%] [G loss: 0.700179398059845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 31/86 [D loss: 0.6934333145618439, acc.: 50.54%] [G loss: 0.7011630535125732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 32/86 [D loss: 0.6937244534492493, acc.: 48.83%] [G loss: 0.6991345286369324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 33/86 [D loss: 0.694768100976944, acc.: 47.31%] [G loss: 0.698717474937439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 34/86 [D loss: 0.6936220526695251, acc.: 49.90%] [G loss: 0.7002578377723694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 35/86 [D loss: 0.6926920711994171, acc.: 50.78%] [G loss: 0.7009980082511902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 36/86 [D loss: 0.693405270576477, acc.: 49.80%] [G loss: 0.6999902725219727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 37/86 [D loss: 0.6939120888710022, acc.: 50.10%] [G loss: 0.7004823684692383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 38/86 [D loss: 0.6928795874118805, acc.: 50.93%] [G loss: 0.6987183690071106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 39/86 [D loss: 0.6941118240356445, acc.: 48.93%] [G loss: 0.6975893378257751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 40/86 [D loss: 0.6935155689716339, acc.: 49.41%] [G loss: 0.7007755637168884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 41/86 [D loss: 0.6933972537517548, acc.: 50.59%] [G loss: 0.7013581991195679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 42/86 [D loss: 0.6937785148620605, acc.: 48.88%] [G loss: 0.7001188397407532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 43/86 [D loss: 0.6931240856647491, acc.: 49.80%] [G loss: 0.7014889717102051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 44/86 [D loss: 0.6934953927993774, acc.: 50.63%] [G loss: 0.6984213590621948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 45/86 [D loss: 0.6950041055679321, acc.: 48.68%] [G loss: 0.6984532475471497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 46/86 [D loss: 0.6934887766838074, acc.: 50.34%] [G loss: 0.7012573480606079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 47/86 [D loss: 0.6918984949588776, acc.: 52.73%] [G loss: 0.6997183561325073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 48/86 [D loss: 0.6944607496261597, acc.: 48.63%] [G loss: 0.7001574039459229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 49/86 [D loss: 0.6932675838470459, acc.: 50.54%] [G loss: 0.7007180452346802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 50/86 [D loss: 0.6939446032047272, acc.: 48.29%] [G loss: 0.6995049715042114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 51/86 [D loss: 0.6945580840110779, acc.: 47.61%] [G loss: 0.6983349919319153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 52/86 [D loss: 0.6935270726680756, acc.: 50.73%] [G loss: 0.70115065574646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 53/86 [D loss: 0.6942538321018219, acc.: 47.07%] [G loss: 0.7001460790634155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 54/86 [D loss: 0.694340705871582, acc.: 48.54%] [G loss: 0.7000094652175903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 55/86 [D loss: 0.6936188042163849, acc.: 49.37%] [G loss: 0.6996983885765076]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 56/86 [D loss: 0.6938541829586029, acc.: 49.41%] [G loss: 0.6990697979927063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 57/86 [D loss: 0.6937984526157379, acc.: 49.27%] [G loss: 0.700094997882843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 58/86 [D loss: 0.6929876506328583, acc.: 50.98%] [G loss: 0.6996082067489624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 59/86 [D loss: 0.6936309635639191, acc.: 49.46%] [G loss: 0.7001086473464966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 60/86 [D loss: 0.6942948400974274, acc.: 47.90%] [G loss: 0.7014274597167969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 61/86 [D loss: 0.693015068769455, acc.: 51.51%] [G loss: 0.6998173594474792]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 62/86 [D loss: 0.6933439075946808, acc.: 50.49%] [G loss: 0.7002631425857544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 63/86 [D loss: 0.6929089426994324, acc.: 50.59%] [G loss: 0.7018989324569702]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 64/86 [D loss: 0.6930201947689056, acc.: 50.54%] [G loss: 0.7011662721633911]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 65/86 [D loss: 0.6935253739356995, acc.: 49.32%] [G loss: 0.7008669972419739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 66/86 [D loss: 0.6934401988983154, acc.: 49.07%] [G loss: 0.6986318230628967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 67/86 [D loss: 0.6927898526191711, acc.: 51.07%] [G loss: 0.6991626620292664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 68/86 [D loss: 0.6940439939498901, acc.: 49.22%] [G loss: 0.6995442509651184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 69/86 [D loss: 0.6938057541847229, acc.: 48.39%] [G loss: 0.7011746168136597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 70/86 [D loss: 0.692315012216568, acc.: 51.81%] [G loss: 0.6992039084434509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 71/86 [D loss: 0.693583071231842, acc.: 50.39%] [G loss: 0.6991815567016602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 72/86 [D loss: 0.6938228905200958, acc.: 48.54%] [G loss: 0.7002159357070923]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 73/86 [D loss: 0.6932355165481567, acc.: 50.73%] [G loss: 0.6977193355560303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 74/86 [D loss: 0.6938602924346924, acc.: 48.34%] [G loss: 0.7000218033790588]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 75/86 [D loss: 0.6926735043525696, acc.: 52.00%] [G loss: 0.7008225917816162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 76/86 [D loss: 0.6934807002544403, acc.: 49.27%] [G loss: 0.6995761394500732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 77/86 [D loss: 0.6940988004207611, acc.: 48.63%] [G loss: 0.7004823088645935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 78/86 [D loss: 0.6932488977909088, acc.: 50.34%] [G loss: 0.6988303661346436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 79/86 [D loss: 0.6937786936759949, acc.: 49.90%] [G loss: 0.6987365484237671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 80/86 [D loss: 0.694118082523346, acc.: 49.02%] [G loss: 0.6999809741973877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 81/86 [D loss: 0.6928587853908539, acc.: 51.17%] [G loss: 0.7012308835983276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 82/86 [D loss: 0.6931217908859253, acc.: 49.95%] [G loss: 0.6998748779296875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 83/86 [D loss: 0.6933285593986511, acc.: 48.97%] [G loss: 0.699813187122345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 84/86 [D loss: 0.69336798787117, acc.: 50.59%] [G loss: 0.6991842985153198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 43/200, Batch 85/86 [D loss: 0.6934807300567627, acc.: 50.05%] [G loss: 0.6983991861343384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 43/200, Batch 86/86 [D loss: 0.6937640905380249, acc.: 49.41%] [G loss: 0.6989758610725403]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 1s 18ms/step\n",
      "Epoch 44/200, Batch 1/86 [D loss: 0.6932885944843292, acc.: 50.29%] [G loss: 0.700806200504303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 2/86 [D loss: 0.6927591860294342, acc.: 51.51%] [G loss: 0.7004295587539673]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 3/86 [D loss: 0.6928781867027283, acc.: 49.56%] [G loss: 0.6995916366577148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 4/86 [D loss: 0.6930641829967499, acc.: 51.17%] [G loss: 0.6991168260574341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 5/86 [D loss: 0.6940015554428101, acc.: 49.41%] [G loss: 0.7002413272857666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 6/86 [D loss: 0.6926739811897278, acc.: 51.51%] [G loss: 0.7016519904136658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 7/86 [D loss: 0.693034291267395, acc.: 51.71%] [G loss: 0.7006287574768066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 8/86 [D loss: 0.6928355991840363, acc.: 51.27%] [G loss: 0.7001852989196777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 9/86 [D loss: 0.6928122043609619, acc.: 50.49%] [G loss: 0.6995383501052856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 10/86 [D loss: 0.6936671435832977, acc.: 48.24%] [G loss: 0.6984758377075195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 11/86 [D loss: 0.6935745179653168, acc.: 50.29%] [G loss: 0.7008392214775085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 12/86 [D loss: 0.6929708123207092, acc.: 49.95%] [G loss: 0.7012226581573486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 13/86 [D loss: 0.6937722861766815, acc.: 49.02%] [G loss: 0.7008012533187866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 14/86 [D loss: 0.6932583749294281, acc.: 50.15%] [G loss: 0.7018329501152039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 15/86 [D loss: 0.6924259066581726, acc.: 52.15%] [G loss: 0.6996685862541199]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 16/86 [D loss: 0.69320148229599, acc.: 49.66%] [G loss: 0.7006303668022156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 17/86 [D loss: 0.6936183869838715, acc.: 49.61%] [G loss: 0.7015531063079834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 18/86 [D loss: 0.6935631930828094, acc.: 49.37%] [G loss: 0.701145350933075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 19/86 [D loss: 0.6931371092796326, acc.: 50.54%] [G loss: 0.7011069655418396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 20/86 [D loss: 0.6930796504020691, acc.: 50.93%] [G loss: 0.6997907757759094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 21/86 [D loss: 0.6937626898288727, acc.: 49.51%] [G loss: 0.698948085308075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 22/86 [D loss: 0.6939977407455444, acc.: 49.76%] [G loss: 0.6996534466743469]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 23/86 [D loss: 0.6929676532745361, acc.: 49.71%] [G loss: 0.6998041868209839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 24/86 [D loss: 0.6924405992031097, acc.: 52.10%] [G loss: 0.7006564140319824]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 25/86 [D loss: 0.6931246519088745, acc.: 49.12%] [G loss: 0.7019135355949402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 26/86 [D loss: 0.6929800808429718, acc.: 50.83%] [G loss: 0.7017273306846619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 27/86 [D loss: 0.693010538816452, acc.: 49.95%] [G loss: 0.7001562118530273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 28/86 [D loss: 0.6931928992271423, acc.: 50.78%] [G loss: 0.701349139213562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 29/86 [D loss: 0.6918657124042511, acc.: 52.49%] [G loss: 0.7014518976211548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 30/86 [D loss: 0.6931636929512024, acc.: 50.39%] [G loss: 0.7016136646270752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 31/86 [D loss: 0.6928378343582153, acc.: 50.59%] [G loss: 0.7010641098022461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 32/86 [D loss: 0.6930999755859375, acc.: 49.32%] [G loss: 0.7012402415275574]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 33/86 [D loss: 0.6939161717891693, acc.: 48.19%] [G loss: 0.7013863921165466]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 34/86 [D loss: 0.6936396658420563, acc.: 50.34%] [G loss: 0.7016427516937256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 35/86 [D loss: 0.6941538751125336, acc.: 48.39%] [G loss: 0.7015668749809265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 36/86 [D loss: 0.6934567093849182, acc.: 48.58%] [G loss: 0.7016541361808777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 37/86 [D loss: 0.6939761638641357, acc.: 48.93%] [G loss: 0.7003927826881409]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 38/86 [D loss: 0.6933075785636902, acc.: 50.20%] [G loss: 0.7000638842582703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 39/86 [D loss: 0.693992555141449, acc.: 47.41%] [G loss: 0.6986491680145264]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 40/86 [D loss: 0.6928243637084961, acc.: 50.29%] [G loss: 0.7015116214752197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 41/86 [D loss: 0.6925179958343506, acc.: 50.93%] [G loss: 0.7000903487205505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 42/86 [D loss: 0.6933475434780121, acc.: 50.39%] [G loss: 0.6995447874069214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 43/86 [D loss: 0.6931049227714539, acc.: 50.63%] [G loss: 0.7011232376098633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 44/86 [D loss: 0.6939541101455688, acc.: 49.12%] [G loss: 0.6993928551673889]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 45/86 [D loss: 0.6934719681739807, acc.: 49.80%] [G loss: 0.701185941696167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 46/86 [D loss: 0.6932756006717682, acc.: 50.29%] [G loss: 0.7013919949531555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 47/86 [D loss: 0.6930795609951019, acc.: 51.46%] [G loss: 0.7012275457382202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 48/86 [D loss: 0.6932144463062286, acc.: 49.41%] [G loss: 0.701550304889679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 49/86 [D loss: 0.6933868229389191, acc.: 49.41%] [G loss: 0.7003426551818848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 50/86 [D loss: 0.692648857831955, acc.: 51.42%] [G loss: 0.7013940811157227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 51/86 [D loss: 0.6933817863464355, acc.: 50.44%] [G loss: 0.7011074423789978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 52/86 [D loss: 0.6938866972923279, acc.: 50.20%] [G loss: 0.7008967399597168]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 53/86 [D loss: 0.6935951411724091, acc.: 49.32%] [G loss: 0.7012249231338501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 54/86 [D loss: 0.6932011246681213, acc.: 49.56%] [G loss: 0.699831485748291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 55/86 [D loss: 0.693083643913269, acc.: 50.78%] [G loss: 0.6994174122810364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 56/86 [D loss: 0.6928631365299225, acc.: 51.07%] [G loss: 0.7002502083778381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 57/86 [D loss: 0.6938642859458923, acc.: 48.39%] [G loss: 0.7010494470596313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 58/86 [D loss: 0.6937559843063354, acc.: 48.00%] [G loss: 0.7018677592277527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 59/86 [D loss: 0.6929886341094971, acc.: 49.66%] [G loss: 0.7003015875816345]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 60/86 [D loss: 0.6933617293834686, acc.: 49.66%] [G loss: 0.6999114155769348]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 61/86 [D loss: 0.6926104724407196, acc.: 52.25%] [G loss: 0.7010215520858765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 62/86 [D loss: 0.6930933892726898, acc.: 50.34%] [G loss: 0.7000486254692078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 63/86 [D loss: 0.6928446888923645, acc.: 51.90%] [G loss: 0.7006893157958984]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 64/86 [D loss: 0.6950121223926544, acc.: 46.88%] [G loss: 0.7004806995391846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 65/86 [D loss: 0.692770779132843, acc.: 52.73%] [G loss: 0.7016148567199707]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 66/86 [D loss: 0.6927166879177094, acc.: 49.61%] [G loss: 0.7012686729431152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 67/86 [D loss: 0.6924391686916351, acc.: 52.54%] [G loss: 0.7004809379577637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 68/86 [D loss: 0.6928120851516724, acc.: 51.86%] [G loss: 0.6993470191955566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 69/86 [D loss: 0.6935368478298187, acc.: 49.76%] [G loss: 0.700701892375946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 70/86 [D loss: 0.6924619972705841, acc.: 51.76%] [G loss: 0.7001723647117615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 71/86 [D loss: 0.6939891278743744, acc.: 47.61%] [G loss: 0.7004307508468628]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 72/86 [D loss: 0.6925829946994781, acc.: 51.56%] [G loss: 0.7008541822433472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 73/86 [D loss: 0.6936821341514587, acc.: 50.10%] [G loss: 0.6999427080154419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 74/86 [D loss: 0.6933812201023102, acc.: 49.32%] [G loss: 0.700592041015625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 75/86 [D loss: 0.6921472251415253, acc.: 51.22%] [G loss: 0.7005853056907654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 76/86 [D loss: 0.6929854452610016, acc.: 51.71%] [G loss: 0.7010980844497681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 77/86 [D loss: 0.6923228800296783, acc.: 51.46%] [G loss: 0.7017823457717896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 78/86 [D loss: 0.6934115886688232, acc.: 48.19%] [G loss: 0.7001120448112488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 79/86 [D loss: 0.6935803890228271, acc.: 49.61%] [G loss: 0.7002590894699097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 80/86 [D loss: 0.6931368410587311, acc.: 49.56%] [G loss: 0.7005729079246521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 81/86 [D loss: 0.6934270560741425, acc.: 48.88%] [G loss: 0.7015753984451294]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 82/86 [D loss: 0.6926822066307068, acc.: 50.59%] [G loss: 0.7020623683929443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 83/86 [D loss: 0.6927199959754944, acc.: 51.37%] [G loss: 0.7019317746162415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 84/86 [D loss: 0.6921487152576447, acc.: 52.10%] [G loss: 0.702204704284668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 44/200, Batch 85/86 [D loss: 0.6930650770664215, acc.: 51.17%] [G loss: 0.7012318968772888]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 44/200, Batch 86/86 [D loss: 0.6934637725353241, acc.: 50.73%] [G loss: 0.6998810768127441]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 1/86 [D loss: 0.692067414522171, acc.: 52.05%] [G loss: 0.7011322975158691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 2/86 [D loss: 0.6929269731044769, acc.: 50.88%] [G loss: 0.7015372514724731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 3/86 [D loss: 0.693457841873169, acc.: 50.73%] [G loss: 0.7010176181793213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 4/86 [D loss: 0.6925654709339142, acc.: 52.25%] [G loss: 0.7004021406173706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 5/86 [D loss: 0.6935323178768158, acc.: 49.12%] [G loss: 0.700390100479126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 6/86 [D loss: 0.693502813577652, acc.: 50.54%] [G loss: 0.7006959915161133]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 7/86 [D loss: 0.6925651133060455, acc.: 51.07%] [G loss: 0.699799656867981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 8/86 [D loss: 0.6929572820663452, acc.: 50.78%] [G loss: 0.7013302445411682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 9/86 [D loss: 0.6925182342529297, acc.: 50.93%] [G loss: 0.7006683349609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 10/86 [D loss: 0.6929061412811279, acc.: 50.88%] [G loss: 0.701583981513977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 11/86 [D loss: 0.6938271522521973, acc.: 49.07%] [G loss: 0.7008888125419617]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 12/86 [D loss: 0.6921256482601166, acc.: 53.08%] [G loss: 0.7017755508422852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 13/86 [D loss: 0.6926854848861694, acc.: 51.66%] [G loss: 0.701287567615509]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 14/86 [D loss: 0.6922870874404907, acc.: 52.73%] [G loss: 0.7001062035560608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 15/86 [D loss: 0.6934729218482971, acc.: 50.24%] [G loss: 0.6991531252861023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 16/86 [D loss: 0.6930570602416992, acc.: 51.17%] [G loss: 0.7013128995895386]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 17/86 [D loss: 0.6929270923137665, acc.: 51.27%] [G loss: 0.7020330429077148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 18/86 [D loss: 0.6937671899795532, acc.: 49.51%] [G loss: 0.7012722492218018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 19/86 [D loss: 0.6929918527603149, acc.: 50.73%] [G loss: 0.7005425095558167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 20/86 [D loss: 0.6929164528846741, acc.: 50.78%] [G loss: 0.7014076709747314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 21/86 [D loss: 0.692037969827652, acc.: 52.83%] [G loss: 0.7009475231170654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 22/86 [D loss: 0.6932621598243713, acc.: 49.56%] [G loss: 0.7003949880599976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 23/86 [D loss: 0.6925207078456879, acc.: 50.54%] [G loss: 0.7012288570404053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 24/86 [D loss: 0.6930999457836151, acc.: 50.24%] [G loss: 0.7017695903778076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 25/86 [D loss: 0.693336695432663, acc.: 50.10%] [G loss: 0.701008677482605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 26/86 [D loss: 0.6929002702236176, acc.: 51.46%] [G loss: 0.701739490032196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 27/86 [D loss: 0.6929735541343689, acc.: 51.95%] [G loss: 0.7010870575904846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 28/86 [D loss: 0.6918958127498627, acc.: 53.27%] [G loss: 0.7021315097808838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 29/86 [D loss: 0.6930740773677826, acc.: 51.32%] [G loss: 0.7018332481384277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 30/86 [D loss: 0.6935370564460754, acc.: 50.05%] [G loss: 0.701223611831665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 31/86 [D loss: 0.6930312514305115, acc.: 50.73%] [G loss: 0.7014905214309692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 32/86 [D loss: 0.6929529309272766, acc.: 50.83%] [G loss: 0.7012113928794861]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 33/86 [D loss: 0.6933520436286926, acc.: 49.90%] [G loss: 0.7005082964897156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 34/86 [D loss: 0.6934005618095398, acc.: 50.20%] [G loss: 0.7010849118232727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 35/86 [D loss: 0.6926409006118774, acc.: 51.42%] [G loss: 0.701219379901886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 36/86 [D loss: 0.692802369594574, acc.: 51.37%] [G loss: 0.7004047632217407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 37/86 [D loss: 0.6937576532363892, acc.: 48.78%] [G loss: 0.7003253102302551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 38/86 [D loss: 0.6934754252433777, acc.: 49.32%] [G loss: 0.700223982334137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 39/86 [D loss: 0.6936059892177582, acc.: 48.78%] [G loss: 0.6992948055267334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 40/86 [D loss: 0.6935263574123383, acc.: 49.80%] [G loss: 0.7012631893157959]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 41/86 [D loss: 0.6930682063102722, acc.: 51.03%] [G loss: 0.7012455463409424]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 42/86 [D loss: 0.6932457089424133, acc.: 50.00%] [G loss: 0.6997655630111694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 43/86 [D loss: 0.693547785282135, acc.: 48.78%] [G loss: 0.6995592713356018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 44/86 [D loss: 0.6936221122741699, acc.: 49.12%] [G loss: 0.7018654942512512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 45/86 [D loss: 0.6928826570510864, acc.: 51.03%] [G loss: 0.7012995481491089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 46/86 [D loss: 0.6929181218147278, acc.: 50.83%] [G loss: 0.7013380527496338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 47/86 [D loss: 0.6926537752151489, acc.: 50.20%] [G loss: 0.7009222507476807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 48/86 [D loss: 0.6936722695827484, acc.: 49.66%] [G loss: 0.7003162503242493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 49/86 [D loss: 0.693837970495224, acc.: 49.41%] [G loss: 0.6999040842056274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 50/86 [D loss: 0.6931926906108856, acc.: 50.93%] [G loss: 0.7015594840049744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 51/86 [D loss: 0.6929613947868347, acc.: 50.34%] [G loss: 0.7023259997367859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 52/86 [D loss: 0.6924709677696228, acc.: 50.98%] [G loss: 0.7005273103713989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 53/86 [D loss: 0.6931193768978119, acc.: 49.66%] [G loss: 0.70135098695755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 54/86 [D loss: 0.6942022144794464, acc.: 48.10%] [G loss: 0.7007384300231934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 55/86 [D loss: 0.6931184828281403, acc.: 49.07%] [G loss: 0.7001661062240601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 56/86 [D loss: 0.6929247081279755, acc.: 50.15%] [G loss: 0.7015324831008911]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 57/86 [D loss: 0.6930265724658966, acc.: 51.17%] [G loss: 0.7022873759269714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 58/86 [D loss: 0.6923505365848541, acc.: 52.15%] [G loss: 0.7001137733459473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 59/86 [D loss: 0.6928491294384003, acc.: 51.66%] [G loss: 0.6995475888252258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 60/86 [D loss: 0.6924859583377838, acc.: 50.73%] [G loss: 0.7026952505111694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 61/86 [D loss: 0.6922257840633392, acc.: 52.39%] [G loss: 0.7008157968521118]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 62/86 [D loss: 0.6928395330905914, acc.: 50.49%] [G loss: 0.7014384269714355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 63/86 [D loss: 0.6928003430366516, acc.: 51.51%] [G loss: 0.7011350989341736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 64/86 [D loss: 0.6925082504749298, acc.: 52.05%] [G loss: 0.6996840238571167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 65/86 [D loss: 0.692975252866745, acc.: 51.42%] [G loss: 0.6999849081039429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 66/86 [D loss: 0.6930857598781586, acc.: 49.90%] [G loss: 0.6997990608215332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 67/86 [D loss: 0.6927708387374878, acc.: 51.71%] [G loss: 0.7007644176483154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 68/86 [D loss: 0.6929101347923279, acc.: 51.03%] [G loss: 0.7013086080551147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 69/86 [D loss: 0.6931528747081757, acc.: 49.76%] [G loss: 0.700903594493866]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 70/86 [D loss: 0.6932799518108368, acc.: 49.71%] [G loss: 0.7013760209083557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 71/86 [D loss: 0.6938065886497498, acc.: 49.12%] [G loss: 0.7008296251296997]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 72/86 [D loss: 0.6922711133956909, acc.: 51.95%] [G loss: 0.7015809416770935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 73/86 [D loss: 0.6927444040775299, acc.: 51.66%] [G loss: 0.7018275260925293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 74/86 [D loss: 0.6924908757209778, acc.: 51.86%] [G loss: 0.7005407214164734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 75/86 [D loss: 0.6927123963832855, acc.: 51.90%] [G loss: 0.7018899321556091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 76/86 [D loss: 0.6929022371768951, acc.: 50.83%] [G loss: 0.7019943594932556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 77/86 [D loss: 0.692367672920227, acc.: 51.61%] [G loss: 0.7028183937072754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 78/86 [D loss: 0.6926105320453644, acc.: 51.56%] [G loss: 0.7004638910293579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 79/86 [D loss: 0.6936571598052979, acc.: 48.88%] [G loss: 0.7021583318710327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 80/86 [D loss: 0.692606121301651, acc.: 52.05%] [G loss: 0.7015189528465271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 81/86 [D loss: 0.6939773559570312, acc.: 48.44%] [G loss: 0.7001051902770996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 45/200, Batch 82/86 [D loss: 0.6933577358722687, acc.: 49.66%] [G loss: 0.7004556059837341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 83/86 [D loss: 0.6932390034198761, acc.: 50.39%] [G loss: 0.7007504105567932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 84/86 [D loss: 0.691995233297348, acc.: 52.73%] [G loss: 0.7014604210853577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 85/86 [D loss: 0.692575603723526, acc.: 52.59%] [G loss: 0.7011030316352844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 45/200, Batch 86/86 [D loss: 0.6931126415729523, acc.: 49.71%] [G loss: 0.7006763815879822]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 1/86 [D loss: 0.6932684183120728, acc.: 50.54%] [G loss: 0.700961172580719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 2/86 [D loss: 0.6932277381420135, acc.: 49.41%] [G loss: 0.7017143964767456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 3/86 [D loss: 0.6930252909660339, acc.: 50.93%] [G loss: 0.7025426030158997]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 4/86 [D loss: 0.6929860413074493, acc.: 49.17%] [G loss: 0.7000629901885986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 5/86 [D loss: 0.692750483751297, acc.: 50.15%] [G loss: 0.7009716629981995]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 6/86 [D loss: 0.6933575868606567, acc.: 49.76%] [G loss: 0.7029328346252441]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 7/86 [D loss: 0.693710446357727, acc.: 48.78%] [G loss: 0.7013280391693115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 8/86 [D loss: 0.6925421059131622, acc.: 51.27%] [G loss: 0.7001055479049683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 9/86 [D loss: 0.6926154792308807, acc.: 52.29%] [G loss: 0.7012068033218384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 10/86 [D loss: 0.6932789385318756, acc.: 49.90%] [G loss: 0.7009768486022949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 11/86 [D loss: 0.6931430399417877, acc.: 50.10%] [G loss: 0.7000373601913452]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 12/86 [D loss: 0.6930819749832153, acc.: 50.29%] [G loss: 0.7000592947006226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 13/86 [D loss: 0.6925474405288696, acc.: 51.27%] [G loss: 0.7014838457107544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 14/86 [D loss: 0.6925911605358124, acc.: 52.59%] [G loss: 0.7015300989151001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 15/86 [D loss: 0.6934354603290558, acc.: 49.46%] [G loss: 0.7003506422042847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 16/86 [D loss: 0.6925867795944214, acc.: 50.24%] [G loss: 0.7014281749725342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 17/86 [D loss: 0.6924393475055695, acc.: 51.22%] [G loss: 0.7001622319221497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 18/86 [D loss: 0.6927512288093567, acc.: 51.07%] [G loss: 0.6996235847473145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 19/86 [D loss: 0.6926222741603851, acc.: 50.34%] [G loss: 0.7009027600288391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 20/86 [D loss: 0.6933328211307526, acc.: 49.80%] [G loss: 0.7015401721000671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 21/86 [D loss: 0.692559152841568, acc.: 51.46%] [G loss: 0.7007949352264404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 22/86 [D loss: 0.6933764815330505, acc.: 49.46%] [G loss: 0.7014451026916504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 23/86 [D loss: 0.6928808093070984, acc.: 49.61%] [G loss: 0.7009295225143433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 24/86 [D loss: 0.692785233259201, acc.: 51.12%] [G loss: 0.7008836269378662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 25/86 [D loss: 0.6920934617519379, acc.: 51.61%] [G loss: 0.7001241445541382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 26/86 [D loss: 0.6933327317237854, acc.: 49.12%] [G loss: 0.7006803750991821]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 27/86 [D loss: 0.6924678087234497, acc.: 50.39%] [G loss: 0.7006368041038513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 28/86 [D loss: 0.693848192691803, acc.: 48.78%] [G loss: 0.7017526030540466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 29/86 [D loss: 0.69259312748909, acc.: 50.93%] [G loss: 0.7029866576194763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 30/86 [D loss: 0.693374902009964, acc.: 49.71%] [G loss: 0.7016854882240295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 31/86 [D loss: 0.6922094225883484, acc.: 52.25%] [G loss: 0.700843095779419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 32/86 [D loss: 0.6931069493293762, acc.: 49.80%] [G loss: 0.7007611989974976]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 33/86 [D loss: 0.6925190687179565, acc.: 52.44%] [G loss: 0.7019065618515015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 34/86 [D loss: 0.6927869319915771, acc.: 52.00%] [G loss: 0.7008098363876343]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 35/86 [D loss: 0.6927400231361389, acc.: 50.63%] [G loss: 0.7013569474220276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 36/86 [D loss: 0.6933691203594208, acc.: 49.56%] [G loss: 0.6994772553443909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 37/86 [D loss: 0.6951159238815308, acc.: 46.63%] [G loss: 0.7007489204406738]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 38/86 [D loss: 0.6922166347503662, acc.: 52.20%] [G loss: 0.7004384994506836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 39/86 [D loss: 0.6932865977287292, acc.: 50.34%] [G loss: 0.7014427781105042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 40/86 [D loss: 0.6934686303138733, acc.: 51.12%] [G loss: 0.7001046538352966]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 41/86 [D loss: 0.6938095986843109, acc.: 47.61%] [G loss: 0.699364185333252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 42/86 [D loss: 0.693004846572876, acc.: 49.41%] [G loss: 0.7006999254226685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 43/86 [D loss: 0.6926355361938477, acc.: 51.86%] [G loss: 0.7015400528907776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 44/86 [D loss: 0.6924577355384827, acc.: 50.15%] [G loss: 0.7015221118927002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 45/86 [D loss: 0.6923017501831055, acc.: 52.83%] [G loss: 0.6996564865112305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 46/86 [D loss: 0.6939065754413605, acc.: 48.14%] [G loss: 0.7003179788589478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 47/86 [D loss: 0.6933567225933075, acc.: 50.68%] [G loss: 0.7009698152542114]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 48/86 [D loss: 0.6923053860664368, acc.: 51.51%] [G loss: 0.7016400694847107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 49/86 [D loss: 0.6927684247493744, acc.: 51.56%] [G loss: 0.7007614374160767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 50/86 [D loss: 0.6941038072109222, acc.: 49.17%] [G loss: 0.7005043625831604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 51/86 [D loss: 0.692868173122406, acc.: 51.07%] [G loss: 0.6990796327590942]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 52/86 [D loss: 0.6931782960891724, acc.: 50.73%] [G loss: 0.6999324560165405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 53/86 [D loss: 0.6917555630207062, acc.: 52.20%] [G loss: 0.7002017498016357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 54/86 [D loss: 0.6931666731834412, acc.: 50.59%] [G loss: 0.7003410458564758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 55/86 [D loss: 0.6920107305049896, acc.: 52.83%] [G loss: 0.700575053691864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 56/86 [D loss: 0.6938325762748718, acc.: 49.27%] [G loss: 0.7006686925888062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 57/86 [D loss: 0.6931338310241699, acc.: 50.20%] [G loss: 0.7009315490722656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 58/86 [D loss: 0.6923589408397675, acc.: 50.83%] [G loss: 0.7012544870376587]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 59/86 [D loss: 0.6930084228515625, acc.: 50.73%] [G loss: 0.7005563378334045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 60/86 [D loss: 0.692432165145874, acc.: 51.86%] [G loss: 0.7004303336143494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 61/86 [D loss: 0.6928498148918152, acc.: 50.44%] [G loss: 0.7012248635292053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 62/86 [D loss: 0.6930518746376038, acc.: 50.93%] [G loss: 0.7019450664520264]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 63/86 [D loss: 0.6926709711551666, acc.: 50.68%] [G loss: 0.7017014622688293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 64/86 [D loss: 0.6922572255134583, acc.: 51.27%] [G loss: 0.7002516388893127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 65/86 [D loss: 0.6928406655788422, acc.: 50.93%] [G loss: 0.7002198100090027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 66/86 [D loss: 0.6931585669517517, acc.: 51.12%] [G loss: 0.7006885409355164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 67/86 [D loss: 0.693062961101532, acc.: 51.07%] [G loss: 0.7005802989006042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 68/86 [D loss: 0.6931271255016327, acc.: 50.34%] [G loss: 0.7012280225753784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 69/86 [D loss: 0.6928240060806274, acc.: 51.51%] [G loss: 0.7002130746841431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 70/86 [D loss: 0.6924051642417908, acc.: 51.37%] [G loss: 0.7005679607391357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 71/86 [D loss: 0.6923632323741913, acc.: 52.00%] [G loss: 0.6998022198677063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 72/86 [D loss: 0.6924267411231995, acc.: 52.10%] [G loss: 0.7002162933349609]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 73/86 [D loss: 0.6924527883529663, acc.: 51.22%] [G loss: 0.7016260623931885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 74/86 [D loss: 0.6935260593891144, acc.: 49.95%] [G loss: 0.7013500928878784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 75/86 [D loss: 0.6922931671142578, acc.: 52.49%] [G loss: 0.7002972364425659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 76/86 [D loss: 0.6925459504127502, acc.: 51.86%] [G loss: 0.6997997760772705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 77/86 [D loss: 0.6929206550121307, acc.: 50.20%] [G loss: 0.7003482580184937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 78/86 [D loss: 0.6930396854877472, acc.: 51.37%] [G loss: 0.7011610865592957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 79/86 [D loss: 0.6931892037391663, acc.: 50.10%] [G loss: 0.7014121413230896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 80/86 [D loss: 0.6931964159011841, acc.: 49.66%] [G loss: 0.7012187242507935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 81/86 [D loss: 0.693461149930954, acc.: 50.00%] [G loss: 0.6998546719551086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 82/86 [D loss: 0.6921610236167908, acc.: 52.69%] [G loss: 0.7005624771118164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 83/86 [D loss: 0.6927078664302826, acc.: 50.54%] [G loss: 0.7013981342315674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 84/86 [D loss: 0.6921779811382294, acc.: 52.83%] [G loss: 0.7011862993240356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 46/200, Batch 85/86 [D loss: 0.6934025585651398, acc.: 50.10%] [G loss: 0.7012857794761658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 46/200, Batch 86/86 [D loss: 0.6930955052375793, acc.: 50.54%] [G loss: 0.7006574869155884]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 1/86 [D loss: 0.6926400661468506, acc.: 50.54%] [G loss: 0.7001978158950806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 2/86 [D loss: 0.6934740245342255, acc.: 49.95%] [G loss: 0.7008942365646362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 3/86 [D loss: 0.693475067615509, acc.: 50.05%] [G loss: 0.7000608444213867]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 4/86 [D loss: 0.6938076019287109, acc.: 48.49%] [G loss: 0.7008272409439087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 5/86 [D loss: 0.6927725970745087, acc.: 51.07%] [G loss: 0.6994967460632324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 6/86 [D loss: 0.6920760273933411, acc.: 51.71%] [G loss: 0.700950026512146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 7/86 [D loss: 0.6929966509342194, acc.: 51.22%] [G loss: 0.6995771527290344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 8/86 [D loss: 0.6923204660415649, acc.: 51.46%] [G loss: 0.7000159025192261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 9/86 [D loss: 0.6943753659725189, acc.: 48.58%] [G loss: 0.7008604407310486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 10/86 [D loss: 0.6932848691940308, acc.: 49.71%] [G loss: 0.700914204120636]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 11/86 [D loss: 0.6930547952651978, acc.: 50.88%] [G loss: 0.6999818682670593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 12/86 [D loss: 0.692654013633728, acc.: 51.12%] [G loss: 0.7011280059814453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 13/86 [D loss: 0.693025529384613, acc.: 50.83%] [G loss: 0.6992126703262329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 14/86 [D loss: 0.693176656961441, acc.: 50.20%] [G loss: 0.7000691890716553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 15/86 [D loss: 0.6938782632350922, acc.: 47.31%] [G loss: 0.7017062902450562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 16/86 [D loss: 0.6936788558959961, acc.: 49.90%] [G loss: 0.7014015316963196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 17/86 [D loss: 0.6922988593578339, acc.: 51.56%] [G loss: 0.701606035232544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 18/86 [D loss: 0.6940375566482544, acc.: 48.00%] [G loss: 0.7013184428215027]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 19/86 [D loss: 0.6943993866443634, acc.: 48.14%] [G loss: 0.7007066011428833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 20/86 [D loss: 0.6926174461841583, acc.: 51.07%] [G loss: 0.7016860842704773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 21/86 [D loss: 0.6922295987606049, acc.: 52.44%] [G loss: 0.7005001306533813]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 22/86 [D loss: 0.6927033364772797, acc.: 50.63%] [G loss: 0.7011517882347107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 23/86 [D loss: 0.6929596662521362, acc.: 49.76%] [G loss: 0.6995506882667542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 24/86 [D loss: 0.6942543089389801, acc.: 48.93%] [G loss: 0.7006494998931885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 25/86 [D loss: 0.6923740208148956, acc.: 52.00%] [G loss: 0.70059734582901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 26/86 [D loss: 0.6921215951442719, acc.: 52.25%] [G loss: 0.7007498145103455]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 27/86 [D loss: 0.692448079586029, acc.: 51.17%] [G loss: 0.7016170620918274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 28/86 [D loss: 0.6928762793540955, acc.: 49.95%] [G loss: 0.6980642676353455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 29/86 [D loss: 0.6939652264118195, acc.: 49.32%] [G loss: 0.6997394561767578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 30/86 [D loss: 0.692660003900528, acc.: 51.27%] [G loss: 0.7006085515022278]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 31/86 [D loss: 0.6931340396404266, acc.: 49.85%] [G loss: 0.7014545202255249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 32/86 [D loss: 0.6928463578224182, acc.: 51.37%] [G loss: 0.7008786201477051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 33/86 [D loss: 0.6936270296573639, acc.: 48.63%] [G loss: 0.6995802521705627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 34/86 [D loss: 0.6926925778388977, acc.: 52.93%] [G loss: 0.7001739144325256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 35/86 [D loss: 0.6929257214069366, acc.: 50.63%] [G loss: 0.7005428671836853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 36/86 [D loss: 0.6927048861980438, acc.: 51.46%] [G loss: 0.6995552778244019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 37/86 [D loss: 0.6932012736797333, acc.: 49.56%] [G loss: 0.699042558670044]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 38/86 [D loss: 0.6935780942440033, acc.: 48.83%] [G loss: 0.6995397806167603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 39/86 [D loss: 0.6925942003726959, acc.: 52.64%] [G loss: 0.7005227208137512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 40/86 [D loss: 0.6929700374603271, acc.: 50.39%] [G loss: 0.7013893127441406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 41/86 [D loss: 0.692141979932785, acc.: 51.07%] [G loss: 0.6998302936553955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 42/86 [D loss: 0.693105936050415, acc.: 48.68%] [G loss: 0.7009192705154419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 43/86 [D loss: 0.6925123929977417, acc.: 51.42%] [G loss: 0.7007337212562561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 44/86 [D loss: 0.6918377578258514, acc.: 53.47%] [G loss: 0.7008645534515381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 45/86 [D loss: 0.6924046277999878, acc.: 51.81%] [G loss: 0.6998186111450195]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 47/200, Batch 46/86 [D loss: 0.6926396787166595, acc.: 51.32%] [G loss: 0.7009227275848389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 47/86 [D loss: 0.6934468150138855, acc.: 49.07%] [G loss: 0.6998792290687561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 48/86 [D loss: 0.6925539672374725, acc.: 52.00%] [G loss: 0.7008086442947388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 49/86 [D loss: 0.6927632689476013, acc.: 49.95%] [G loss: 0.699675440788269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 50/86 [D loss: 0.6935029625892639, acc.: 49.32%] [G loss: 0.7002466917037964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 51/86 [D loss: 0.6924205124378204, acc.: 51.07%] [G loss: 0.7006142735481262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 52/86 [D loss: 0.6928753554821014, acc.: 51.66%] [G loss: 0.7012448310852051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 53/86 [D loss: 0.6929469704627991, acc.: 50.00%] [G loss: 0.7001540660858154]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 54/86 [D loss: 0.6929931938648224, acc.: 50.54%] [G loss: 0.6986228227615356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 55/86 [D loss: 0.6931922435760498, acc.: 50.83%] [G loss: 0.6999450922012329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 56/86 [D loss: 0.6923627853393555, acc.: 50.98%] [G loss: 0.7004233598709106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 57/86 [D loss: 0.6932684183120728, acc.: 48.54%] [G loss: 0.701985239982605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 58/86 [D loss: 0.6927740275859833, acc.: 50.39%] [G loss: 0.7007548809051514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 59/86 [D loss: 0.693335771560669, acc.: 49.66%] [G loss: 0.7006728649139404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 60/86 [D loss: 0.6927594542503357, acc.: 51.27%] [G loss: 0.7002363801002502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 61/86 [D loss: 0.6929743587970734, acc.: 51.51%] [G loss: 0.7006431818008423]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 62/86 [D loss: 0.6931897401809692, acc.: 50.15%] [G loss: 0.7004716396331787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 63/86 [D loss: 0.6927123665809631, acc.: 50.59%] [G loss: 0.6999000310897827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 64/86 [D loss: 0.6927380561828613, acc.: 51.86%] [G loss: 0.7013806104660034]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 65/86 [D loss: 0.6929187476634979, acc.: 51.61%] [G loss: 0.7011528611183167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 66/86 [D loss: 0.692943662405014, acc.: 50.93%] [G loss: 0.7005873918533325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 67/86 [D loss: 0.6924494206905365, acc.: 51.95%] [G loss: 0.7014079093933105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 68/86 [D loss: 0.6934698820114136, acc.: 48.97%] [G loss: 0.6997695565223694]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 69/86 [D loss: 0.6918407380580902, acc.: 53.22%] [G loss: 0.7008075714111328]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 70/86 [D loss: 0.6940672099590302, acc.: 49.76%] [G loss: 0.7005188465118408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 71/86 [D loss: 0.6937685310840607, acc.: 49.17%] [G loss: 0.7008535861968994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 72/86 [D loss: 0.6928934454917908, acc.: 51.46%] [G loss: 0.7005842924118042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 73/86 [D loss: 0.6933839619159698, acc.: 49.61%] [G loss: 0.7013252973556519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 74/86 [D loss: 0.693080723285675, acc.: 50.63%] [G loss: 0.6997172236442566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 75/86 [D loss: 0.6931026577949524, acc.: 49.85%] [G loss: 0.7005234360694885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 76/86 [D loss: 0.6936567425727844, acc.: 50.05%] [G loss: 0.7017120122909546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 77/86 [D loss: 0.6917544603347778, acc.: 54.15%] [G loss: 0.7003644704818726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 78/86 [D loss: 0.6932834982872009, acc.: 49.17%] [G loss: 0.7006529569625854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 79/86 [D loss: 0.6917562186717987, acc.: 53.37%] [G loss: 0.7004193067550659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 80/86 [D loss: 0.6920953691005707, acc.: 52.10%] [G loss: 0.7011282444000244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 81/86 [D loss: 0.692352682352066, acc.: 51.71%] [G loss: 0.7016807794570923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 82/86 [D loss: 0.6925143301486969, acc.: 51.27%] [G loss: 0.700745701789856]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 83/86 [D loss: 0.6932300627231598, acc.: 49.76%] [G loss: 0.7004963159561157]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 47/200, Batch 84/86 [D loss: 0.6930336356163025, acc.: 51.27%] [G loss: 0.7002829313278198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 85/86 [D loss: 0.6931354701519012, acc.: 49.41%] [G loss: 0.7010799646377563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 47/200, Batch 86/86 [D loss: 0.6928291320800781, acc.: 50.34%] [G loss: 0.6998215317726135]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 1/86 [D loss: 0.6930414140224457, acc.: 50.63%] [G loss: 0.700537919998169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 2/86 [D loss: 0.6931747794151306, acc.: 51.12%] [G loss: 0.7000104188919067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 3/86 [D loss: 0.6936355233192444, acc.: 49.02%] [G loss: 0.6998094320297241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 4/86 [D loss: 0.6925697922706604, acc.: 51.90%] [G loss: 0.7006255984306335]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 5/86 [D loss: 0.6917842030525208, acc.: 53.12%] [G loss: 0.69941645860672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 6/86 [D loss: 0.6927409768104553, acc.: 50.49%] [G loss: 0.6989398002624512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 7/86 [D loss: 0.6933155059814453, acc.: 50.29%] [G loss: 0.7007185220718384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 8/86 [D loss: 0.6917016506195068, acc.: 53.37%] [G loss: 0.7006657719612122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 9/86 [D loss: 0.693377286195755, acc.: 49.90%] [G loss: 0.6992884278297424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 10/86 [D loss: 0.6936192512512207, acc.: 50.00%] [G loss: 0.697935163974762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 11/86 [D loss: 0.6929222941398621, acc.: 50.54%] [G loss: 0.7011967897415161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 12/86 [D loss: 0.6918106377124786, acc.: 52.34%] [G loss: 0.7011873722076416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 13/86 [D loss: 0.6927988529205322, acc.: 52.29%] [G loss: 0.7006858587265015]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 14/86 [D loss: 0.6930614113807678, acc.: 51.03%] [G loss: 0.6992565989494324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 15/86 [D loss: 0.6941823959350586, acc.: 47.66%] [G loss: 0.6991702318191528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 16/86 [D loss: 0.692175030708313, acc.: 53.03%] [G loss: 0.699504017829895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 17/86 [D loss: 0.692552387714386, acc.: 52.34%] [G loss: 0.6996051073074341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 18/86 [D loss: 0.6928233504295349, acc.: 49.22%] [G loss: 0.700428307056427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 19/86 [D loss: 0.6934929192066193, acc.: 50.49%] [G loss: 0.6989070177078247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 20/86 [D loss: 0.6934851109981537, acc.: 49.51%] [G loss: 0.7004615664482117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 21/86 [D loss: 0.6930215954780579, acc.: 49.66%] [G loss: 0.699738085269928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 22/86 [D loss: 0.6933419704437256, acc.: 49.80%] [G loss: 0.7008417844772339]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 23/86 [D loss: 0.6930716037750244, acc.: 48.83%] [G loss: 0.6988587975502014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 24/86 [D loss: 0.6933740377426147, acc.: 49.51%] [G loss: 0.6991565823554993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 25/86 [D loss: 0.6940256059169769, acc.: 48.68%] [G loss: 0.7003973722457886]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 26/86 [D loss: 0.6929349899291992, acc.: 50.93%] [G loss: 0.7003206610679626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 27/86 [D loss: 0.6929912865161896, acc.: 50.20%] [G loss: 0.7003137469291687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 28/86 [D loss: 0.6926477551460266, acc.: 52.29%] [G loss: 0.6994691491127014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 29/86 [D loss: 0.6936880350112915, acc.: 48.88%] [G loss: 0.6999347805976868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 30/86 [D loss: 0.6928934454917908, acc.: 50.34%] [G loss: 0.6999796628952026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 31/86 [D loss: 0.6925750076770782, acc.: 52.44%] [G loss: 0.7010706663131714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 32/86 [D loss: 0.6932676136493683, acc.: 49.51%] [G loss: 0.7010818719863892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 33/86 [D loss: 0.693442165851593, acc.: 48.83%] [G loss: 0.700777530670166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 34/86 [D loss: 0.6927254498004913, acc.: 51.07%] [G loss: 0.7006253004074097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 35/86 [D loss: 0.6929244995117188, acc.: 51.27%] [G loss: 0.6997121572494507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 36/86 [D loss: 0.6927862465381622, acc.: 51.07%] [G loss: 0.7000913023948669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 37/86 [D loss: 0.6928625404834747, acc.: 50.88%] [G loss: 0.700033962726593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 38/86 [D loss: 0.6929474174976349, acc.: 51.42%] [G loss: 0.6996811628341675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 39/86 [D loss: 0.6938027739524841, acc.: 48.83%] [G loss: 0.6995241641998291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 40/86 [D loss: 0.6930955946445465, acc.: 51.37%] [G loss: 0.7015447020530701]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 41/86 [D loss: 0.6928088366985321, acc.: 50.73%] [G loss: 0.699820339679718]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 42/86 [D loss: 0.6929543614387512, acc.: 50.34%] [G loss: 0.700025737285614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 43/86 [D loss: 0.6928505897521973, acc.: 50.78%] [G loss: 0.7008067965507507]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 44/86 [D loss: 0.6934144198894501, acc.: 50.78%] [G loss: 0.7013726830482483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 45/86 [D loss: 0.6929775774478912, acc.: 49.71%] [G loss: 0.6987585425376892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 46/86 [D loss: 0.6938278079032898, acc.: 47.95%] [G loss: 0.7020437121391296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 47/86 [D loss: 0.6924967765808105, acc.: 51.42%] [G loss: 0.7014449834823608]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 48/86 [D loss: 0.6917375922203064, acc.: 52.34%] [G loss: 0.7012917399406433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 49/86 [D loss: 0.6925328075885773, acc.: 50.59%] [G loss: 0.7006864547729492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 50/86 [D loss: 0.6933921277523041, acc.: 51.12%] [G loss: 0.7000017166137695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 51/86 [D loss: 0.6929797530174255, acc.: 50.15%] [G loss: 0.7004692554473877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 52/86 [D loss: 0.693202018737793, acc.: 49.32%] [G loss: 0.7012295722961426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 53/86 [D loss: 0.6931834518909454, acc.: 50.20%] [G loss: 0.6999611854553223]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 54/86 [D loss: 0.6929866671562195, acc.: 50.05%] [G loss: 0.699042797088623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 55/86 [D loss: 0.6922333836555481, acc.: 52.44%] [G loss: 0.7001706957817078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 56/86 [D loss: 0.6920855641365051, acc.: 52.20%] [G loss: 0.6982454657554626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 57/86 [D loss: 0.6923261284828186, acc.: 50.93%] [G loss: 0.6995387673377991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 58/86 [D loss: 0.6934066712856293, acc.: 49.61%] [G loss: 0.7004745006561279]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 59/86 [D loss: 0.6930038630962372, acc.: 50.63%] [G loss: 0.7003562450408936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 60/86 [D loss: 0.6927375793457031, acc.: 51.07%] [G loss: 0.7014672756195068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 61/86 [D loss: 0.6926765143871307, acc.: 51.07%] [G loss: 0.7001224756240845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 62/86 [D loss: 0.6936038732528687, acc.: 49.22%] [G loss: 0.7003053426742554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 63/86 [D loss: 0.6928752660751343, acc.: 50.54%] [G loss: 0.701348602771759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 64/86 [D loss: 0.6932229697704315, acc.: 50.78%] [G loss: 0.7023403644561768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 65/86 [D loss: 0.6939887702465057, acc.: 49.07%] [G loss: 0.701042890548706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 66/86 [D loss: 0.6922735571861267, acc.: 52.44%] [G loss: 0.7002029418945312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 67/86 [D loss: 0.6930335462093353, acc.: 50.49%] [G loss: 0.6991356611251831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 68/86 [D loss: 0.6938100159168243, acc.: 49.51%] [G loss: 0.7009514570236206]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 69/86 [D loss: 0.6922503709793091, acc.: 51.22%] [G loss: 0.6999871730804443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 70/86 [D loss: 0.6933372318744659, acc.: 50.20%] [G loss: 0.6996997594833374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 71/86 [D loss: 0.692187637090683, acc.: 52.00%] [G loss: 0.6996351480484009]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 72/86 [D loss: 0.6933103501796722, acc.: 50.29%] [G loss: 0.6994165778160095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 73/86 [D loss: 0.6923507153987885, acc.: 51.81%] [G loss: 0.6999406218528748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 74/86 [D loss: 0.6932275593280792, acc.: 50.15%] [G loss: 0.7000109553337097]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 75/86 [D loss: 0.693111002445221, acc.: 50.78%] [G loss: 0.7005730867385864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 76/86 [D loss: 0.6927435994148254, acc.: 51.37%] [G loss: 0.700276255607605]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 77/86 [D loss: 0.6925199031829834, acc.: 51.61%] [G loss: 0.7005468010902405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 78/86 [D loss: 0.692611426115036, acc.: 50.88%] [G loss: 0.7003010511398315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 79/86 [D loss: 0.6920329928398132, acc.: 53.61%] [G loss: 0.6990069150924683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 80/86 [D loss: 0.6924584805965424, acc.: 51.03%] [G loss: 0.70025235414505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 81/86 [D loss: 0.6929043829441071, acc.: 50.88%] [G loss: 0.7001268267631531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 82/86 [D loss: 0.6916943192481995, acc.: 52.44%] [G loss: 0.7008159756660461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 83/86 [D loss: 0.6929115653038025, acc.: 50.15%] [G loss: 0.7009540796279907]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 48/200, Batch 84/86 [D loss: 0.6935084462165833, acc.: 49.46%] [G loss: 0.7019895911216736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 85/86 [D loss: 0.6916854679584503, acc.: 52.64%] [G loss: 0.7014203071594238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 48/200, Batch 86/86 [D loss: 0.6932846903800964, acc.: 49.46%] [G loss: 0.7013679146766663]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 1/86 [D loss: 0.6936623454093933, acc.: 49.90%] [G loss: 0.7009443044662476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 2/86 [D loss: 0.6924978792667389, acc.: 51.37%] [G loss: 0.7007644176483154]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 3/86 [D loss: 0.6931275427341461, acc.: 50.39%] [G loss: 0.6996359825134277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 4/86 [D loss: 0.693706750869751, acc.: 49.85%] [G loss: 0.6996932625770569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 5/86 [D loss: 0.6922125220298767, acc.: 51.95%] [G loss: 0.7013112306594849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 6/86 [D loss: 0.6928005516529083, acc.: 51.27%] [G loss: 0.7021238803863525]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 7/86 [D loss: 0.6919719874858856, acc.: 53.22%] [G loss: 0.6994714140892029]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 8/86 [D loss: 0.6942501366138458, acc.: 47.90%] [G loss: 0.6995947360992432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 9/86 [D loss: 0.6930542290210724, acc.: 50.98%] [G loss: 0.6998483538627625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 10/86 [D loss: 0.6932650804519653, acc.: 49.37%] [G loss: 0.6993104219436646]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 11/86 [D loss: 0.6938073337078094, acc.: 48.68%] [G loss: 0.6985917091369629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 12/86 [D loss: 0.6934710144996643, acc.: 49.66%] [G loss: 0.6988639831542969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 13/86 [D loss: 0.6938660442829132, acc.: 48.83%] [G loss: 0.7003672122955322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 14/86 [D loss: 0.6930713653564453, acc.: 50.49%] [G loss: 0.7005888223648071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 15/86 [D loss: 0.6929529011249542, acc.: 51.12%] [G loss: 0.700617790222168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 16/86 [D loss: 0.6923806965351105, acc.: 52.05%] [G loss: 0.6984291672706604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 17/86 [D loss: 0.6940832734107971, acc.: 49.46%] [G loss: 0.7008558511734009]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 18/86 [D loss: 0.69240802526474, acc.: 53.12%] [G loss: 0.7007207870483398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 19/86 [D loss: 0.6920059323310852, acc.: 52.54%] [G loss: 0.6986231803894043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 20/86 [D loss: 0.6923233270645142, acc.: 52.00%] [G loss: 0.6990228295326233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 21/86 [D loss: 0.6934274137020111, acc.: 50.10%] [G loss: 0.6984522342681885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 22/86 [D loss: 0.6930496990680695, acc.: 52.05%] [G loss: 0.6994739174842834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 23/86 [D loss: 0.6930972635746002, acc.: 49.56%] [G loss: 0.6988500952720642]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 24/86 [D loss: 0.6926793456077576, acc.: 50.68%] [G loss: 0.7005452513694763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 25/86 [D loss: 0.6925943493843079, acc.: 51.76%] [G loss: 0.6984919905662537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 26/86 [D loss: 0.6946832835674286, acc.: 47.85%] [G loss: 0.7010483145713806]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 27/86 [D loss: 0.6927486062049866, acc.: 52.10%] [G loss: 0.6986405253410339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 28/86 [D loss: 0.6926157772541046, acc.: 51.66%] [G loss: 0.7006694674491882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 29/86 [D loss: 0.6922660768032074, acc.: 51.17%] [G loss: 0.6976525783538818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 30/86 [D loss: 0.6950893402099609, acc.: 47.22%] [G loss: 0.6990346908569336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 31/86 [D loss: 0.6922254264354706, acc.: 53.12%] [G loss: 0.6981980204582214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 32/86 [D loss: 0.6946562230587006, acc.: 47.31%] [G loss: 0.6993247270584106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 33/86 [D loss: 0.6914431750774384, acc.: 53.66%] [G loss: 0.6983428001403809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 34/86 [D loss: 0.693545013666153, acc.: 48.78%] [G loss: 0.698432445526123]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 35/86 [D loss: 0.6923443377017975, acc.: 50.68%] [G loss: 0.6987345218658447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 36/86 [D loss: 0.6927786767482758, acc.: 50.93%] [G loss: 0.698185920715332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 37/86 [D loss: 0.6921828389167786, acc.: 52.05%] [G loss: 0.6982181668281555]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 38/86 [D loss: 0.6926040947437286, acc.: 51.37%] [G loss: 0.6982553601264954]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 39/86 [D loss: 0.6931626796722412, acc.: 50.29%] [G loss: 0.6987253427505493]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 40/86 [D loss: 0.6920126378536224, acc.: 52.83%] [G loss: 0.6998567581176758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 41/86 [D loss: 0.6928696036338806, acc.: 50.59%] [G loss: 0.6988554000854492]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 42/86 [D loss: 0.69341841340065, acc.: 49.95%] [G loss: 0.6985813975334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 43/86 [D loss: 0.6918854415416718, acc.: 52.34%] [G loss: 0.7010130286216736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 44/86 [D loss: 0.692449301481247, acc.: 51.12%] [G loss: 0.6992089152336121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 45/86 [D loss: 0.6927871108055115, acc.: 50.59%] [G loss: 0.6998535394668579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 46/86 [D loss: 0.6920813918113708, acc.: 51.95%] [G loss: 0.6990583539009094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 47/86 [D loss: 0.6916983425617218, acc.: 52.00%] [G loss: 0.698706865310669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 48/86 [D loss: 0.6929894089698792, acc.: 50.00%] [G loss: 0.7000248432159424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 49/86 [D loss: 0.6921818554401398, acc.: 52.73%] [G loss: 0.6990736722946167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 50/86 [D loss: 0.6927578151226044, acc.: 50.39%] [G loss: 0.6985225081443787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 51/86 [D loss: 0.6924135982990265, acc.: 51.17%] [G loss: 0.6998680233955383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 52/86 [D loss: 0.6926106810569763, acc.: 50.88%] [G loss: 0.6999527812004089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 53/86 [D loss: 0.6921747624874115, acc.: 51.61%] [G loss: 0.7004947662353516]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 49/200, Batch 54/86 [D loss: 0.6923756003379822, acc.: 52.00%] [G loss: 0.7000706195831299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 55/86 [D loss: 0.6922680139541626, acc.: 51.51%] [G loss: 0.700324296951294]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 56/86 [D loss: 0.692484587430954, acc.: 51.86%] [G loss: 0.7001025080680847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 57/86 [D loss: 0.6922231912612915, acc.: 52.10%] [G loss: 0.7001219987869263]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 58/86 [D loss: 0.6915014088153839, acc.: 54.35%] [G loss: 0.6983129978179932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 59/86 [D loss: 0.6944700181484222, acc.: 47.46%] [G loss: 0.7004583477973938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 60/86 [D loss: 0.6920212507247925, acc.: 52.69%] [G loss: 0.699720561504364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 61/86 [D loss: 0.6927483379840851, acc.: 50.78%] [G loss: 0.6991546750068665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 62/86 [D loss: 0.6925637722015381, acc.: 51.12%] [G loss: 0.6990975141525269]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 63/86 [D loss: 0.6932356953620911, acc.: 50.34%] [G loss: 0.6991096138954163]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 64/86 [D loss: 0.6916687488555908, acc.: 52.98%] [G loss: 0.7011314630508423]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 65/86 [D loss: 0.6919236183166504, acc.: 51.86%] [G loss: 0.6998588442802429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 66/86 [D loss: 0.6927087903022766, acc.: 51.32%] [G loss: 0.6995460987091064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 67/86 [D loss: 0.6928384602069855, acc.: 51.27%] [G loss: 0.6998642086982727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 68/86 [D loss: 0.6935686469078064, acc.: 49.95%] [G loss: 0.6991804242134094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 69/86 [D loss: 0.6927940249443054, acc.: 51.42%] [G loss: 0.6992959380149841]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 70/86 [D loss: 0.6925618648529053, acc.: 51.66%] [G loss: 0.700279951095581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 71/86 [D loss: 0.6913181245326996, acc.: 54.83%] [G loss: 0.6994136571884155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 72/86 [D loss: 0.6933943033218384, acc.: 48.88%] [G loss: 0.7006256580352783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 73/86 [D loss: 0.6934551298618317, acc.: 50.83%] [G loss: 0.6999974846839905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 74/86 [D loss: 0.6927179992198944, acc.: 50.63%] [G loss: 0.6997122764587402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 75/86 [D loss: 0.6929236054420471, acc.: 49.41%] [G loss: 0.6990312933921814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 76/86 [D loss: 0.6928649544715881, acc.: 50.98%] [G loss: 0.6976733803749084]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 77/86 [D loss: 0.6927427351474762, acc.: 51.56%] [G loss: 0.6999558806419373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 78/86 [D loss: 0.6930654644966125, acc.: 50.59%] [G loss: 0.697982668876648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 79/86 [D loss: 0.6923503875732422, acc.: 52.44%] [G loss: 0.6996672749519348]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 49/200, Batch 80/86 [D loss: 0.6932195723056793, acc.: 50.49%] [G loss: 0.697373628616333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 81/86 [D loss: 0.6927507519721985, acc.: 49.85%] [G loss: 0.700279951095581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 82/86 [D loss: 0.6919115483760834, acc.: 52.64%] [G loss: 0.6995766162872314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 83/86 [D loss: 0.692287027835846, acc.: 51.76%] [G loss: 0.699489414691925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 84/86 [D loss: 0.691437155008316, acc.: 54.93%] [G loss: 0.6983214020729065]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 85/86 [D loss: 0.6933064758777618, acc.: 50.39%] [G loss: 0.6982898116111755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 49/200, Batch 86/86 [D loss: 0.6921687722206116, acc.: 52.88%] [G loss: 0.6981871128082275]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 1/86 [D loss: 0.6922928392887115, acc.: 51.66%] [G loss: 0.6976416110992432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 2/86 [D loss: 0.6920266151428223, acc.: 53.12%] [G loss: 0.6987546682357788]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 3/86 [D loss: 0.6926791667938232, acc.: 50.44%] [G loss: 0.6982428431510925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 4/86 [D loss: 0.6923578083515167, acc.: 51.27%] [G loss: 0.7006113529205322]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 5/86 [D loss: 0.6927831470966339, acc.: 50.29%] [G loss: 0.69864821434021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 6/86 [D loss: 0.692469596862793, acc.: 52.05%] [G loss: 0.6988381147384644]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 7/86 [D loss: 0.6923618018627167, acc.: 51.61%] [G loss: 0.6977053284645081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 8/86 [D loss: 0.6935644149780273, acc.: 50.49%] [G loss: 0.699120283126831]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 9/86 [D loss: 0.6920467615127563, acc.: 51.76%] [G loss: 0.6987380981445312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 10/86 [D loss: 0.6918491423130035, acc.: 51.42%] [G loss: 0.6998112201690674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 11/86 [D loss: 0.6918492317199707, acc.: 53.22%] [G loss: 0.698415219783783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 12/86 [D loss: 0.6929706931114197, acc.: 51.27%] [G loss: 0.7008944749832153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 13/86 [D loss: 0.6925052106380463, acc.: 51.27%] [G loss: 0.7008731365203857]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 14/86 [D loss: 0.6932143867015839, acc.: 48.88%] [G loss: 0.6989349722862244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 15/86 [D loss: 0.6929475963115692, acc.: 50.88%] [G loss: 0.69923996925354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 16/86 [D loss: 0.6929997801780701, acc.: 51.27%] [G loss: 0.6991389989852905]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 17/86 [D loss: 0.6920700669288635, acc.: 52.00%] [G loss: 0.6994086503982544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 18/86 [D loss: 0.6918420791625977, acc.: 52.10%] [G loss: 0.6989474892616272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 19/86 [D loss: 0.6918196678161621, acc.: 52.49%] [G loss: 0.6994884014129639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 20/86 [D loss: 0.6928756535053253, acc.: 50.05%] [G loss: 0.6992145776748657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 21/86 [D loss: 0.6923083961009979, acc.: 51.42%] [G loss: 0.7001025676727295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 22/86 [D loss: 0.6927797496318817, acc.: 52.15%] [G loss: 0.6981630325317383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 23/86 [D loss: 0.6925361454486847, acc.: 50.54%] [G loss: 0.6993162035942078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 24/86 [D loss: 0.6920480728149414, acc.: 52.00%] [G loss: 0.698890745639801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 25/86 [D loss: 0.6924170851707458, acc.: 49.51%] [G loss: 0.6999484300613403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 26/86 [D loss: 0.6921999454498291, acc.: 51.81%] [G loss: 0.7004346251487732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 27/86 [D loss: 0.6921201646327972, acc.: 52.44%] [G loss: 0.6989734172821045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 28/86 [D loss: 0.6918013095855713, acc.: 52.69%] [G loss: 0.7003121972084045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 29/86 [D loss: 0.6917347311973572, acc.: 52.64%] [G loss: 0.6977787017822266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 30/86 [D loss: 0.6918773353099823, acc.: 52.10%] [G loss: 0.6995789408683777]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 31/86 [D loss: 0.6919289827346802, acc.: 53.32%] [G loss: 0.698334813117981]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 32/86 [D loss: 0.6913380026817322, acc.: 54.15%] [G loss: 0.7002962827682495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 33/86 [D loss: 0.6923891603946686, acc.: 51.81%] [G loss: 0.698837161064148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 34/86 [D loss: 0.691697746515274, acc.: 52.34%] [G loss: 0.6981990933418274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 35/86 [D loss: 0.6921693980693817, acc.: 51.27%] [G loss: 0.6977294087409973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 36/86 [D loss: 0.69181689620018, acc.: 51.27%] [G loss: 0.698720395565033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 37/86 [D loss: 0.6922799944877625, acc.: 52.29%] [G loss: 0.6990482211112976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 38/86 [D loss: 0.6926194429397583, acc.: 52.05%] [G loss: 0.698600172996521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 39/86 [D loss: 0.693049430847168, acc.: 50.54%] [G loss: 0.7001739740371704]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 40/86 [D loss: 0.691765546798706, acc.: 53.61%] [G loss: 0.698328971862793]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 41/86 [D loss: 0.6911870539188385, acc.: 54.44%] [G loss: 0.6974273920059204]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 42/86 [D loss: 0.6913505792617798, acc.: 53.47%] [G loss: 0.6978405117988586]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 43/86 [D loss: 0.6932405233383179, acc.: 50.05%] [G loss: 0.6989853978157043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 44/86 [D loss: 0.6918774247169495, acc.: 53.37%] [G loss: 0.6986376643180847]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 45/86 [D loss: 0.6917305886745453, acc.: 52.69%] [G loss: 0.6995410919189453]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 46/86 [D loss: 0.6927923858165741, acc.: 50.34%] [G loss: 0.6986990571022034]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 47/86 [D loss: 0.6923094093799591, acc.: 51.22%] [G loss: 0.6985890865325928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 48/86 [D loss: 0.6925333738327026, acc.: 51.12%] [G loss: 0.6993890404701233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 49/86 [D loss: 0.6912560164928436, acc.: 53.03%] [G loss: 0.6998463869094849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 50/86 [D loss: 0.6914330422878265, acc.: 50.73%] [G loss: 0.6972768902778625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 51/86 [D loss: 0.692450761795044, acc.: 51.81%] [G loss: 0.6991572976112366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 52/86 [D loss: 0.6921823620796204, acc.: 52.15%] [G loss: 0.6967836618423462]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 53/86 [D loss: 0.6930809617042542, acc.: 50.20%] [G loss: 0.6990782618522644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 54/86 [D loss: 0.6920655071735382, acc.: 51.51%] [G loss: 0.6985587477684021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 55/86 [D loss: 0.6927917897701263, acc.: 51.76%] [G loss: 0.6976503133773804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 56/86 [D loss: 0.6923954784870148, acc.: 50.93%] [G loss: 0.6997512578964233]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 57/86 [D loss: 0.6928896009922028, acc.: 50.49%] [G loss: 0.6974777579307556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 58/86 [D loss: 0.6923732459545135, acc.: 51.27%] [G loss: 0.6969117522239685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 59/86 [D loss: 0.6939236223697662, acc.: 49.76%] [G loss: 0.6972601413726807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 60/86 [D loss: 0.6914830505847931, acc.: 52.88%] [G loss: 0.6991725564002991]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 61/86 [D loss: 0.6926683783531189, acc.: 51.71%] [G loss: 0.6992194056510925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 62/86 [D loss: 0.6925292015075684, acc.: 50.20%] [G loss: 0.6974310278892517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 63/86 [D loss: 0.6930352449417114, acc.: 51.42%] [G loss: 0.6972055435180664]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 64/86 [D loss: 0.6924875676631927, acc.: 50.39%] [G loss: 0.6985281705856323]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 65/86 [D loss: 0.692591518163681, acc.: 51.27%] [G loss: 0.6996789574623108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 66/86 [D loss: 0.6920152902603149, acc.: 51.32%] [G loss: 0.6988657116889954]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 67/86 [D loss: 0.6932722926139832, acc.: 49.95%] [G loss: 0.698151707649231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 68/86 [D loss: 0.6937121748924255, acc.: 49.12%] [G loss: 0.6998523473739624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 69/86 [D loss: 0.6921277642250061, acc.: 52.20%] [G loss: 0.697727382183075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 70/86 [D loss: 0.6913806796073914, acc.: 52.25%] [G loss: 0.6993551254272461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 71/86 [D loss: 0.6926851570606232, acc.: 52.05%] [G loss: 0.6968376040458679]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 72/86 [D loss: 0.6934592723846436, acc.: 50.29%] [G loss: 0.6997831463813782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 73/86 [D loss: 0.6919463574886322, acc.: 52.00%] [G loss: 0.6978157162666321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 74/86 [D loss: 0.6936032474040985, acc.: 50.00%] [G loss: 0.7001256346702576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 75/86 [D loss: 0.690724641084671, acc.: 55.13%] [G loss: 0.7002580165863037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 76/86 [D loss: 0.6935367584228516, acc.: 49.17%] [G loss: 0.6990647912025452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 77/86 [D loss: 0.6910288035869598, acc.: 54.00%] [G loss: 0.6995831727981567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 78/86 [D loss: 0.6922935843467712, acc.: 51.22%] [G loss: 0.6993378400802612]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 79/86 [D loss: 0.6925651431083679, acc.: 50.98%] [G loss: 0.6986565589904785]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 80/86 [D loss: 0.6926607191562653, acc.: 50.05%] [G loss: 0.6984193921089172]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 81/86 [D loss: 0.6928614377975464, acc.: 50.83%] [G loss: 0.6998869776725769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 82/86 [D loss: 0.6925807595252991, acc.: 50.59%] [G loss: 0.6995415091514587]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 83/86 [D loss: 0.6927196383476257, acc.: 52.15%] [G loss: 0.698448896408081]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 84/86 [D loss: 0.6918337047100067, acc.: 52.73%] [G loss: 0.6981383562088013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 50/200, Batch 85/86 [D loss: 0.6924519836902618, acc.: 51.32%] [G loss: 0.6990920305252075]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 50/200, Batch 86/86 [D loss: 0.6918478310108185, acc.: 53.52%] [G loss: 0.7000569701194763]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 1/86 [D loss: 0.6925837695598602, acc.: 51.71%] [G loss: 0.699824333190918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 2/86 [D loss: 0.693169355392456, acc.: 50.59%] [G loss: 0.6988158226013184]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 3/86 [D loss: 0.6921646595001221, acc.: 51.90%] [G loss: 0.6997889280319214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 4/86 [D loss: 0.6919941604137421, acc.: 50.88%] [G loss: 0.6992238759994507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 5/86 [D loss: 0.6927235126495361, acc.: 50.88%] [G loss: 0.6997112035751343]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 6/86 [D loss: 0.69267338514328, acc.: 51.66%] [G loss: 0.7000659108161926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 7/86 [D loss: 0.6925966143608093, acc.: 50.68%] [G loss: 0.6994691491127014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 8/86 [D loss: 0.6920638084411621, acc.: 52.54%] [G loss: 0.7000228762626648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 9/86 [D loss: 0.6922708749771118, acc.: 50.73%] [G loss: 0.6995869874954224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 10/86 [D loss: 0.6918789744377136, acc.: 52.20%] [G loss: 0.6988822817802429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 11/86 [D loss: 0.6921059787273407, acc.: 53.08%] [G loss: 0.6985018849372864]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 12/86 [D loss: 0.6918568015098572, acc.: 52.49%] [G loss: 0.6984891295433044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 13/86 [D loss: 0.6926793158054352, acc.: 51.56%] [G loss: 0.6992782354354858]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 14/86 [D loss: 0.6919649839401245, acc.: 53.37%] [G loss: 0.6992515921592712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 15/86 [D loss: 0.6922040581703186, acc.: 50.98%] [G loss: 0.698779284954071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 16/86 [D loss: 0.6921176910400391, acc.: 52.78%] [G loss: 0.7000125646591187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 17/86 [D loss: 0.6922504603862762, acc.: 51.22%] [G loss: 0.6997424364089966]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 18/86 [D loss: 0.6923863887786865, acc.: 51.56%] [G loss: 0.7005940675735474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 19/86 [D loss: 0.6919573247432709, acc.: 52.10%] [G loss: 0.6995749473571777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 20/86 [D loss: 0.693056583404541, acc.: 50.73%] [G loss: 0.7002468705177307]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 21/86 [D loss: 0.6919284164905548, acc.: 52.10%] [G loss: 0.6986126899719238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 22/86 [D loss: 0.6916281282901764, acc.: 52.54%] [G loss: 0.6997801661491394]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 23/86 [D loss: 0.6925246119499207, acc.: 50.93%] [G loss: 0.7000622749328613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 24/86 [D loss: 0.6928361058235168, acc.: 50.93%] [G loss: 0.6991545557975769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 25/86 [D loss: 0.6921877264976501, acc.: 52.00%] [G loss: 0.7000382542610168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 26/86 [D loss: 0.6928734481334686, acc.: 49.90%] [G loss: 0.6989330649375916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 27/86 [D loss: 0.6924415826797485, acc.: 52.44%] [G loss: 0.6988189816474915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 28/86 [D loss: 0.6926842927932739, acc.: 50.54%] [G loss: 0.6986024975776672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 29/86 [D loss: 0.6924338340759277, acc.: 50.59%] [G loss: 0.7010536193847656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 30/86 [D loss: 0.6925764083862305, acc.: 51.32%] [G loss: 0.6995973587036133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 31/86 [D loss: 0.691604733467102, acc.: 52.39%] [G loss: 0.6995140910148621]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 32/86 [D loss: 0.6925636827945709, acc.: 52.34%] [G loss: 0.6989434957504272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 33/86 [D loss: 0.6919447183609009, acc.: 52.15%] [G loss: 0.7001758813858032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 34/86 [D loss: 0.6922213137149811, acc.: 51.90%] [G loss: 0.698617160320282]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 35/86 [D loss: 0.692949116230011, acc.: 50.20%] [G loss: 0.6992778182029724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 36/86 [D loss: 0.6924435198307037, acc.: 53.22%] [G loss: 0.7004567980766296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 37/86 [D loss: 0.691796600818634, acc.: 52.49%] [G loss: 0.6996244788169861]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 38/86 [D loss: 0.6930306553840637, acc.: 50.05%] [G loss: 0.6993398666381836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 39/86 [D loss: 0.6921308040618896, acc.: 50.93%] [G loss: 0.6994789242744446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 40/86 [D loss: 0.6928179562091827, acc.: 51.12%] [G loss: 0.6973808407783508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 41/86 [D loss: 0.6916515231132507, acc.: 51.71%] [G loss: 0.6987553834915161]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 42/86 [D loss: 0.6918525695800781, acc.: 51.76%] [G loss: 0.6996817588806152]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 43/86 [D loss: 0.6923891603946686, acc.: 52.05%] [G loss: 0.6997112035751343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 44/86 [D loss: 0.6928724646568298, acc.: 50.59%] [G loss: 0.6977210640907288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 45/86 [D loss: 0.6933117210865021, acc.: 50.44%] [G loss: 0.6998085975646973]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 46/86 [D loss: 0.6925894618034363, acc.: 50.98%] [G loss: 0.6997134685516357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 47/86 [D loss: 0.6923500299453735, acc.: 51.51%] [G loss: 0.6971092820167542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 48/86 [D loss: 0.6925097107887268, acc.: 52.00%] [G loss: 0.6989939212799072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 49/86 [D loss: 0.6930795013904572, acc.: 50.05%] [G loss: 0.6984698176383972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 50/86 [D loss: 0.6921618580818176, acc.: 51.95%] [G loss: 0.6984097957611084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 51/86 [D loss: 0.6918809115886688, acc.: 53.52%] [G loss: 0.6987770199775696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 52/86 [D loss: 0.69227334856987, acc.: 51.07%] [G loss: 0.6979689598083496]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 53/86 [D loss: 0.6926975846290588, acc.: 50.78%] [G loss: 0.6996303796768188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 54/86 [D loss: 0.6921041309833527, acc.: 50.98%] [G loss: 0.6987000107765198]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 55/86 [D loss: 0.6920012831687927, acc.: 52.34%] [G loss: 0.6985264420509338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 56/86 [D loss: 0.6926077306270599, acc.: 50.83%] [G loss: 0.698063850402832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 57/86 [D loss: 0.6920400857925415, acc.: 52.64%] [G loss: 0.6982138752937317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 58/86 [D loss: 0.6926301717758179, acc.: 50.98%] [G loss: 0.699213445186615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 59/86 [D loss: 0.6919599175453186, acc.: 51.32%] [G loss: 0.6971392631530762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 60/86 [D loss: 0.6923955082893372, acc.: 51.71%] [G loss: 0.6993385553359985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 61/86 [D loss: 0.6923824548721313, acc.: 51.90%] [G loss: 0.6984844207763672]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 62/86 [D loss: 0.6927784085273743, acc.: 50.59%] [G loss: 0.6974360942840576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 63/86 [D loss: 0.6922717988491058, acc.: 50.83%] [G loss: 0.6983017325401306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 64/86 [D loss: 0.6922509372234344, acc.: 51.37%] [G loss: 0.6988219022750854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 65/86 [D loss: 0.6921475529670715, acc.: 53.56%] [G loss: 0.6991288065910339]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 66/86 [D loss: 0.6913510262966156, acc.: 53.96%] [G loss: 0.6986919641494751]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 67/86 [D loss: 0.6920206546783447, acc.: 52.64%] [G loss: 0.6985820531845093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 68/86 [D loss: 0.6927560269832611, acc.: 50.68%] [G loss: 0.6976677179336548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 69/86 [D loss: 0.692882239818573, acc.: 50.54%] [G loss: 0.6977270245552063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 70/86 [D loss: 0.6925191581249237, acc.: 51.46%] [G loss: 0.6982882618904114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 71/86 [D loss: 0.6920549869537354, acc.: 51.32%] [G loss: 0.699444591999054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 72/86 [D loss: 0.6923239827156067, acc.: 52.34%] [G loss: 0.6987650990486145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 73/86 [D loss: 0.6932661831378937, acc.: 50.15%] [G loss: 0.6984654068946838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 74/86 [D loss: 0.691782683134079, acc.: 52.29%] [G loss: 0.6989546418190002]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 75/86 [D loss: 0.6931648254394531, acc.: 50.63%] [G loss: 0.6989690661430359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 76/86 [D loss: 0.6920237839221954, acc.: 53.27%] [G loss: 0.6988323330879211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 77/86 [D loss: 0.6922761499881744, acc.: 51.61%] [G loss: 0.6984202861785889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 78/86 [D loss: 0.691391110420227, acc.: 53.52%] [G loss: 0.6989254951477051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 79/86 [D loss: 0.6929077208042145, acc.: 50.05%] [G loss: 0.6986881494522095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 80/86 [D loss: 0.6921696364879608, acc.: 52.69%] [G loss: 0.6993629932403564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 81/86 [D loss: 0.6924129128456116, acc.: 50.00%] [G loss: 0.6978773474693298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 82/86 [D loss: 0.6924936473369598, acc.: 51.86%] [G loss: 0.7004443407058716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 83/86 [D loss: 0.6921464502811432, acc.: 51.46%] [G loss: 0.6997727155685425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 84/86 [D loss: 0.6923057734966278, acc.: 51.42%] [G loss: 0.6997678875923157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 51/200, Batch 85/86 [D loss: 0.6918357312679291, acc.: 52.29%] [G loss: 0.6984630823135376]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 51/200, Batch 86/86 [D loss: 0.6926498115062714, acc.: 52.34%] [G loss: 0.7005060315132141]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 1/86 [D loss: 0.6925047636032104, acc.: 51.17%] [G loss: 0.6975874304771423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 2/86 [D loss: 0.6927452385425568, acc.: 50.05%] [G loss: 0.7000336050987244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 3/86 [D loss: 0.6934276223182678, acc.: 48.00%] [G loss: 0.7017672061920166]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 4/86 [D loss: 0.692257434129715, acc.: 52.10%] [G loss: 0.6989095211029053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 5/86 [D loss: 0.6929911375045776, acc.: 50.49%] [G loss: 0.6992453336715698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 6/86 [D loss: 0.6928427517414093, acc.: 49.85%] [G loss: 0.6995851993560791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 7/86 [D loss: 0.6924209296703339, acc.: 51.66%] [G loss: 0.699643611907959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 8/86 [D loss: 0.692330539226532, acc.: 52.10%] [G loss: 0.6988138556480408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 9/86 [D loss: 0.6923864781856537, acc.: 51.71%] [G loss: 0.6974598169326782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 10/86 [D loss: 0.6935025751590729, acc.: 48.88%] [G loss: 0.6985189914703369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 11/86 [D loss: 0.6922161877155304, acc.: 52.83%] [G loss: 0.6987785696983337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 12/86 [D loss: 0.6926684081554413, acc.: 50.49%] [G loss: 0.6996177434921265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 13/86 [D loss: 0.6925571858882904, acc.: 50.98%] [G loss: 0.6990784406661987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 14/86 [D loss: 0.6923565566539764, acc.: 51.86%] [G loss: 0.6994596123695374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 15/86 [D loss: 0.6925965845584869, acc.: 50.54%] [G loss: 0.6987800002098083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 16/86 [D loss: 0.6916996240615845, acc.: 52.54%] [G loss: 0.6982117295265198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 17/86 [D loss: 0.6935487687587738, acc.: 50.34%] [G loss: 0.7002419233322144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 18/86 [D loss: 0.6919973194599152, acc.: 53.42%] [G loss: 0.6998393535614014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 19/86 [D loss: 0.6925820708274841, acc.: 51.90%] [G loss: 0.6999590396881104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 20/86 [D loss: 0.6913948059082031, acc.: 53.96%] [G loss: 0.6986951231956482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 21/86 [D loss: 0.6927962005138397, acc.: 52.05%] [G loss: 0.698158860206604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 22/86 [D loss: 0.6920891106128693, acc.: 53.17%] [G loss: 0.6990304589271545]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 23/86 [D loss: 0.6927167773246765, acc.: 50.49%] [G loss: 0.6983462572097778]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 24/86 [D loss: 0.691980630159378, acc.: 52.39%] [G loss: 0.6993889808654785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 25/86 [D loss: 0.6931090354919434, acc.: 51.42%] [G loss: 0.6974326968193054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 26/86 [D loss: 0.6935204863548279, acc.: 49.46%] [G loss: 0.6987828016281128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 27/86 [D loss: 0.6926179528236389, acc.: 52.10%] [G loss: 0.6983937621116638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 28/86 [D loss: 0.6931541264057159, acc.: 50.39%] [G loss: 0.6990638971328735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 29/86 [D loss: 0.6932215988636017, acc.: 51.07%] [G loss: 0.6979552507400513]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 30/86 [D loss: 0.6940528154373169, acc.: 47.56%] [G loss: 0.7010757923126221]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 31/86 [D loss: 0.6926866769790649, acc.: 51.37%] [G loss: 0.699408233165741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 32/86 [D loss: 0.6918880343437195, acc.: 53.56%] [G loss: 0.7001866698265076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 33/86 [D loss: 0.6934662759304047, acc.: 49.56%] [G loss: 0.6982903480529785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 34/86 [D loss: 0.6927367448806763, acc.: 49.51%] [G loss: 0.6989639401435852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 35/86 [D loss: 0.6924371123313904, acc.: 51.86%] [G loss: 0.6991080045700073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 36/86 [D loss: 0.6928220391273499, acc.: 51.42%] [G loss: 0.7009127140045166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 37/86 [D loss: 0.6924086809158325, acc.: 51.90%] [G loss: 0.7000486254692078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 38/86 [D loss: 0.6916028261184692, acc.: 52.54%] [G loss: 0.7000668048858643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 39/86 [D loss: 0.6932552456855774, acc.: 51.03%] [G loss: 0.6996529698371887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 40/86 [D loss: 0.6929292380809784, acc.: 51.07%] [G loss: 0.6993808746337891]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 41/86 [D loss: 0.692278653383255, acc.: 52.54%] [G loss: 0.6989269852638245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 42/86 [D loss: 0.6931929886341095, acc.: 49.90%] [G loss: 0.7000707387924194]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 43/86 [D loss: 0.6925172209739685, acc.: 50.88%] [G loss: 0.7010206580162048]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 44/86 [D loss: 0.6918898820877075, acc.: 51.86%] [G loss: 0.6993522644042969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 45/86 [D loss: 0.6937838494777679, acc.: 48.83%] [G loss: 0.6996451020240784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 46/86 [D loss: 0.692531943321228, acc.: 51.81%] [G loss: 0.7002274394035339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 47/86 [D loss: 0.6919056475162506, acc.: 52.78%] [G loss: 0.7002261281013489]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 48/86 [D loss: 0.6922385096549988, acc.: 52.44%] [G loss: 0.7001044750213623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 49/86 [D loss: 0.6927176415920258, acc.: 51.03%] [G loss: 0.6992099285125732]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 50/86 [D loss: 0.6927165687084198, acc.: 50.98%] [G loss: 0.6997785568237305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 51/86 [D loss: 0.6921525299549103, acc.: 52.29%] [G loss: 0.699856162071228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 52/86 [D loss: 0.6925802528858185, acc.: 51.86%] [G loss: 0.7017935514450073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 53/86 [D loss: 0.6932090818881989, acc.: 50.73%] [G loss: 0.6986632347106934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 54/86 [D loss: 0.6923370063304901, acc.: 52.15%] [G loss: 0.7005492448806763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 55/86 [D loss: 0.6922386884689331, acc.: 51.81%] [G loss: 0.6997619867324829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 56/86 [D loss: 0.6926749348640442, acc.: 51.56%] [G loss: 0.7011712193489075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 57/86 [D loss: 0.6921302974224091, acc.: 53.08%] [G loss: 0.7000207901000977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 58/86 [D loss: 0.6932057738304138, acc.: 49.71%] [G loss: 0.701492965221405]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 59/86 [D loss: 0.6925856173038483, acc.: 51.32%] [G loss: 0.6995137929916382]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 60/86 [D loss: 0.6930184960365295, acc.: 49.66%] [G loss: 0.7002584338188171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 61/86 [D loss: 0.6927303969860077, acc.: 51.32%] [G loss: 0.698898434638977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 62/86 [D loss: 0.6934897005558014, acc.: 49.12%] [G loss: 0.699967622756958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 63/86 [D loss: 0.6923122704029083, acc.: 51.81%] [G loss: 0.6992865204811096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 64/86 [D loss: 0.6928563416004181, acc.: 51.22%] [G loss: 0.7000658512115479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 65/86 [D loss: 0.6922218799591064, acc.: 51.76%] [G loss: 0.6993294358253479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 66/86 [D loss: 0.6921028196811676, acc.: 52.00%] [G loss: 0.699763298034668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 67/86 [D loss: 0.6929557919502258, acc.: 50.68%] [G loss: 0.6999136209487915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 68/86 [D loss: 0.6920354664325714, acc.: 52.64%] [G loss: 0.6997974514961243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 69/86 [D loss: 0.693324863910675, acc.: 50.44%] [G loss: 0.6990022659301758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 70/86 [D loss: 0.6927899420261383, acc.: 50.34%] [G loss: 0.6974726319313049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 71/86 [D loss: 0.6929438710212708, acc.: 51.22%] [G loss: 0.7004082798957825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 72/86 [D loss: 0.6941621601581573, acc.: 47.75%] [G loss: 0.6988232731819153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 73/86 [D loss: 0.6920726895332336, acc.: 52.59%] [G loss: 0.6990126371383667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 74/86 [D loss: 0.6922597587108612, acc.: 52.64%] [G loss: 0.697243869304657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 75/86 [D loss: 0.6932031512260437, acc.: 51.17%] [G loss: 0.7011502981185913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 76/86 [D loss: 0.6924102902412415, acc.: 51.81%] [G loss: 0.6974822878837585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 77/86 [D loss: 0.6933388411998749, acc.: 50.05%] [G loss: 0.6997562050819397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 78/86 [D loss: 0.6934017539024353, acc.: 50.78%] [G loss: 0.697871744632721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 79/86 [D loss: 0.6931853592395782, acc.: 50.10%] [G loss: 0.6992751955986023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 80/86 [D loss: 0.6929938793182373, acc.: 50.15%] [G loss: 0.6981035470962524]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 52/200, Batch 81/86 [D loss: 0.693479597568512, acc.: 49.37%] [G loss: 0.6992779970169067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 82/86 [D loss: 0.6926619410514832, acc.: 50.49%] [G loss: 0.6992237567901611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 83/86 [D loss: 0.6937776803970337, acc.: 50.00%] [G loss: 0.6992501020431519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 84/86 [D loss: 0.6927165687084198, acc.: 52.64%] [G loss: 0.7004624009132385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 85/86 [D loss: 0.692415714263916, acc.: 51.71%] [G loss: 0.7006077766418457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 52/200, Batch 86/86 [D loss: 0.6924125552177429, acc.: 50.83%] [G loss: 0.700391948223114]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 1/86 [D loss: 0.6935329437255859, acc.: 49.22%] [G loss: 0.7006811499595642]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 2/86 [D loss: 0.692501574754715, acc.: 51.66%] [G loss: 0.7008798122406006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 3/86 [D loss: 0.6926737427711487, acc.: 51.12%] [G loss: 0.7010855078697205]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 4/86 [D loss: 0.6919934153556824, acc.: 52.25%] [G loss: 0.700645923614502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 5/86 [D loss: 0.6933112442493439, acc.: 50.24%] [G loss: 0.7003424167633057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 6/86 [D loss: 0.6928142011165619, acc.: 52.05%] [G loss: 0.7004991769790649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 7/86 [D loss: 0.6936337649822235, acc.: 49.37%] [G loss: 0.7004847526550293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 8/86 [D loss: 0.6927892565727234, acc.: 51.17%] [G loss: 0.7008920311927795]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 9/86 [D loss: 0.6931424140930176, acc.: 50.10%] [G loss: 0.7001551985740662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 10/86 [D loss: 0.6930932700634003, acc.: 51.17%] [G loss: 0.7005316019058228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 11/86 [D loss: 0.6932789981365204, acc.: 49.90%] [G loss: 0.7012656927108765]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 12/86 [D loss: 0.6935224235057831, acc.: 49.41%] [G loss: 0.7001534104347229]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 13/86 [D loss: 0.6923553645610809, acc.: 51.56%] [G loss: 0.7004879117012024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 14/86 [D loss: 0.6935624182224274, acc.: 49.80%] [G loss: 0.6992937922477722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 15/86 [D loss: 0.6932086944580078, acc.: 50.24%] [G loss: 0.7013550400733948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 16/86 [D loss: 0.6928068697452545, acc.: 51.07%] [G loss: 0.7015705704689026]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 17/86 [D loss: 0.6931293308734894, acc.: 50.59%] [G loss: 0.6992209553718567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 18/86 [D loss: 0.6930555999279022, acc.: 50.44%] [G loss: 0.7010194659233093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 19/86 [D loss: 0.6927500367164612, acc.: 50.44%] [G loss: 0.7002438902854919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 20/86 [D loss: 0.6922547221183777, acc.: 52.69%] [G loss: 0.7013305425643921]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 21/86 [D loss: 0.6933891773223877, acc.: 49.22%] [G loss: 0.6989818215370178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 22/86 [D loss: 0.6935169100761414, acc.: 48.58%] [G loss: 0.7009662985801697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 23/86 [D loss: 0.6927918195724487, acc.: 51.07%] [G loss: 0.7001306414604187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 24/86 [D loss: 0.6925445199012756, acc.: 50.20%] [G loss: 0.7009224891662598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 25/86 [D loss: 0.6923706531524658, acc.: 51.66%] [G loss: 0.697828471660614]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 26/86 [D loss: 0.694097101688385, acc.: 47.95%] [G loss: 0.6995527148246765]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 27/86 [D loss: 0.6921322047710419, acc.: 52.20%] [G loss: 0.7000271081924438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 28/86 [D loss: 0.6932758390903473, acc.: 48.97%] [G loss: 0.7002312541007996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 29/86 [D loss: 0.6922461092472076, acc.: 51.71%] [G loss: 0.7003124356269836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 30/86 [D loss: 0.6936589479446411, acc.: 50.44%] [G loss: 0.700356662273407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 31/86 [D loss: 0.6921730041503906, acc.: 52.73%] [G loss: 0.6987724304199219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 32/86 [D loss: 0.6933445036411285, acc.: 49.07%] [G loss: 0.7004321813583374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 33/86 [D loss: 0.6925094425678253, acc.: 51.71%] [G loss: 0.6994234919548035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 34/86 [D loss: 0.693803071975708, acc.: 51.07%] [G loss: 0.6996184587478638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 35/86 [D loss: 0.6931990087032318, acc.: 49.85%] [G loss: 0.6990280151367188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 36/86 [D loss: 0.6920494735240936, acc.: 52.39%] [G loss: 0.6994168758392334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 37/86 [D loss: 0.6930745542049408, acc.: 51.17%] [G loss: 0.6997534036636353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 38/86 [D loss: 0.6926852464675903, acc.: 52.69%] [G loss: 0.6998307108879089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 39/86 [D loss: 0.6936482191085815, acc.: 49.95%] [G loss: 0.7002660036087036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 40/86 [D loss: 0.6919145882129669, acc.: 51.66%] [G loss: 0.6985338926315308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 41/86 [D loss: 0.6934153735637665, acc.: 50.39%] [G loss: 0.698870837688446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 42/86 [D loss: 0.6932027637958527, acc.: 49.80%] [G loss: 0.6988515853881836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 43/86 [D loss: 0.692942351102829, acc.: 50.49%] [G loss: 0.7012064456939697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 44/86 [D loss: 0.6930108368396759, acc.: 50.29%] [G loss: 0.6981351375579834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 45/86 [D loss: 0.6925703287124634, acc.: 52.00%] [G loss: 0.6987560391426086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 46/86 [D loss: 0.6922187805175781, acc.: 52.29%] [G loss: 0.697571873664856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 47/86 [D loss: 0.6923380494117737, acc.: 51.51%] [G loss: 0.6990852952003479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 48/86 [D loss: 0.6931311786174774, acc.: 50.34%] [G loss: 0.6990219354629517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 49/86 [D loss: 0.692740797996521, acc.: 51.42%] [G loss: 0.6990664005279541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 50/86 [D loss: 0.6927943825721741, acc.: 50.24%] [G loss: 0.6964412927627563]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 51/86 [D loss: 0.6936879456043243, acc.: 49.90%] [G loss: 0.7002683877944946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 52/86 [D loss: 0.6934912204742432, acc.: 49.66%] [G loss: 0.6990938782691956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 53/86 [D loss: 0.6926432549953461, acc.: 50.54%] [G loss: 0.7008243203163147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 54/86 [D loss: 0.6928805410861969, acc.: 52.15%] [G loss: 0.6984135508537292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 55/86 [D loss: 0.6936217546463013, acc.: 50.34%] [G loss: 0.7001120448112488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 56/86 [D loss: 0.6920818388462067, acc.: 51.90%] [G loss: 0.6988712549209595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 57/86 [D loss: 0.6930589973926544, acc.: 49.71%] [G loss: 0.6994369029998779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 58/86 [D loss: 0.6931347250938416, acc.: 51.71%] [G loss: 0.6991700530052185]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 59/86 [D loss: 0.693618655204773, acc.: 48.24%] [G loss: 0.7003081440925598]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 60/86 [D loss: 0.6921077370643616, acc.: 52.88%] [G loss: 0.6996484994888306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 61/86 [D loss: 0.6936922371387482, acc.: 48.63%] [G loss: 0.7011075019836426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 62/86 [D loss: 0.6928815543651581, acc.: 51.61%] [G loss: 0.6989604830741882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 63/86 [D loss: 0.6936885714530945, acc.: 49.22%] [G loss: 0.7018682956695557]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 64/86 [D loss: 0.6925318539142609, acc.: 51.90%] [G loss: 0.6999431848526001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 65/86 [D loss: 0.6932068467140198, acc.: 50.10%] [G loss: 0.7003108263015747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 66/86 [D loss: 0.6922954320907593, acc.: 51.51%] [G loss: 0.6995024681091309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 67/86 [D loss: 0.694182276725769, acc.: 47.80%] [G loss: 0.699676513671875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 68/86 [D loss: 0.6913129389286041, acc.: 54.69%] [G loss: 0.6990637183189392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 69/86 [D loss: 0.6936158239841461, acc.: 49.41%] [G loss: 0.6995615363121033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 70/86 [D loss: 0.6925112307071686, acc.: 52.15%] [G loss: 0.6987416744232178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 71/86 [D loss: 0.6937284171581268, acc.: 49.37%] [G loss: 0.6972178816795349]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 72/86 [D loss: 0.6925093531608582, acc.: 51.56%] [G loss: 0.7003262042999268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 53/200, Batch 73/86 [D loss: 0.6934053003787994, acc.: 50.00%] [G loss: 0.6981464624404907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 74/86 [D loss: 0.6924299597740173, acc.: 51.17%] [G loss: 0.6993123888969421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 75/86 [D loss: 0.6943680644035339, acc.: 48.88%] [G loss: 0.6966513395309448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 76/86 [D loss: 0.6928597986698151, acc.: 52.15%] [G loss: 0.6984474062919617]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 77/86 [D loss: 0.6934376657009125, acc.: 49.56%] [G loss: 0.6980230212211609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 78/86 [D loss: 0.6940799653530121, acc.: 48.19%] [G loss: 0.6993392705917358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 79/86 [D loss: 0.6933759152889252, acc.: 49.02%] [G loss: 0.6985833048820496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 80/86 [D loss: 0.6937054991722107, acc.: 50.29%] [G loss: 0.699424684047699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 81/86 [D loss: 0.6919145584106445, acc.: 53.12%] [G loss: 0.6998895406723022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 82/86 [D loss: 0.6929024457931519, acc.: 50.15%] [G loss: 0.6996256113052368]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 83/86 [D loss: 0.6928267776966095, acc.: 51.61%] [G loss: 0.6978046894073486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 84/86 [D loss: 0.6940879225730896, acc.: 48.97%] [G loss: 0.7002971768379211]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 85/86 [D loss: 0.6920051872730255, acc.: 52.39%] [G loss: 0.6986958980560303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 53/200, Batch 86/86 [D loss: 0.6933656632900238, acc.: 49.46%] [G loss: 0.6998815536499023]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 1/86 [D loss: 0.6929166913032532, acc.: 50.78%] [G loss: 0.6989587545394897]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 2/86 [D loss: 0.6932729780673981, acc.: 50.05%] [G loss: 0.700525164604187]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 3/86 [D loss: 0.6930585205554962, acc.: 49.37%] [G loss: 0.7011367678642273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 4/86 [D loss: 0.691843718290329, acc.: 53.61%] [G loss: 0.7008634209632874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 5/86 [D loss: 0.6921502649784088, acc.: 51.90%] [G loss: 0.6994463801383972]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 6/86 [D loss: 0.6934620440006256, acc.: 50.54%] [G loss: 0.7010546922683716]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 7/86 [D loss: 0.692084014415741, acc.: 52.49%] [G loss: 0.7001655101776123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 8/86 [D loss: 0.6919479370117188, acc.: 53.17%] [G loss: 0.7006127834320068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 9/86 [D loss: 0.692936897277832, acc.: 51.90%] [G loss: 0.7008972764015198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 10/86 [D loss: 0.6925385892391205, acc.: 51.12%] [G loss: 0.7002168297767639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 11/86 [D loss: 0.6922780275344849, acc.: 52.78%] [G loss: 0.700232207775116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 12/86 [D loss: 0.6924380660057068, acc.: 51.86%] [G loss: 0.7005410194396973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 13/86 [D loss: 0.6925686299800873, acc.: 51.32%] [G loss: 0.7010313868522644]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 14/86 [D loss: 0.6928043365478516, acc.: 51.61%] [G loss: 0.7008242607116699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 15/86 [D loss: 0.6929133236408234, acc.: 50.59%] [G loss: 0.701134979724884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 16/86 [D loss: 0.6919367015361786, acc.: 53.08%] [G loss: 0.7004978656768799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 17/86 [D loss: 0.6915186047554016, acc.: 53.71%] [G loss: 0.7014979720115662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 18/86 [D loss: 0.6930822432041168, acc.: 48.68%] [G loss: 0.7002647519111633]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 19/86 [D loss: 0.693554699420929, acc.: 49.80%] [G loss: 0.7005383968353271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 20/86 [D loss: 0.6916109025478363, acc.: 52.59%] [G loss: 0.7009686231613159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 21/86 [D loss: 0.6928987205028534, acc.: 50.24%] [G loss: 0.7003529667854309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 22/86 [D loss: 0.6924600303173065, acc.: 51.76%] [G loss: 0.7000802159309387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 23/86 [D loss: 0.6934871673583984, acc.: 49.71%] [G loss: 0.7004899978637695]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 24/86 [D loss: 0.6927695572376251, acc.: 51.07%] [G loss: 0.7017480731010437]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 25/86 [D loss: 0.6926904022693634, acc.: 51.56%] [G loss: 0.6997173428535461]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 26/86 [D loss: 0.6924349367618561, acc.: 51.42%] [G loss: 0.6985575556755066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 27/86 [D loss: 0.6930901408195496, acc.: 50.10%] [G loss: 0.6994072794914246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 28/86 [D loss: 0.6926034390926361, acc.: 50.59%] [G loss: 0.7002221941947937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 29/86 [D loss: 0.6930536925792694, acc.: 52.00%] [G loss: 0.6991026401519775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 30/86 [D loss: 0.6927002966403961, acc.: 50.98%] [G loss: 0.7014551162719727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 31/86 [D loss: 0.6925611197948456, acc.: 51.37%] [G loss: 0.6979103088378906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 32/86 [D loss: 0.6932713389396667, acc.: 51.32%] [G loss: 0.7002320289611816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 33/86 [D loss: 0.693175196647644, acc.: 50.29%] [G loss: 0.7009447813034058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 34/86 [D loss: 0.6930859684944153, acc.: 50.39%] [G loss: 0.7011134624481201]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 35/86 [D loss: 0.693163275718689, acc.: 50.10%] [G loss: 0.6997489333152771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 36/86 [D loss: 0.6933734118938446, acc.: 51.46%] [G loss: 0.701688289642334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 37/86 [D loss: 0.6917600929737091, acc.: 53.08%] [G loss: 0.6995074152946472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 38/86 [D loss: 0.6932626962661743, acc.: 50.98%] [G loss: 0.6996774077415466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 39/86 [D loss: 0.6933168768882751, acc.: 50.00%] [G loss: 0.6993798017501831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 40/86 [D loss: 0.6928104758262634, acc.: 50.59%] [G loss: 0.7009680271148682]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 41/86 [D loss: 0.6919627487659454, acc.: 52.88%] [G loss: 0.7003413438796997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 42/86 [D loss: 0.693089097738266, acc.: 48.63%] [G loss: 0.700616717338562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 43/86 [D loss: 0.6921132206916809, acc.: 52.64%] [G loss: 0.6996475458145142]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 44/86 [D loss: 0.693066418170929, acc.: 50.49%] [G loss: 0.7008437514305115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 45/86 [D loss: 0.692251056432724, acc.: 52.05%] [G loss: 0.7013390064239502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 46/86 [D loss: 0.6924327313899994, acc.: 51.42%] [G loss: 0.700332760810852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 47/86 [D loss: 0.6921131014823914, acc.: 51.86%] [G loss: 0.700136125087738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 48/86 [D loss: 0.6928500235080719, acc.: 50.78%] [G loss: 0.7008898854255676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 49/86 [D loss: 0.6925010681152344, acc.: 52.10%] [G loss: 0.6994779706001282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 50/86 [D loss: 0.6918641328811646, acc.: 52.93%] [G loss: 0.6997732520103455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 51/86 [D loss: 0.6920978426933289, acc.: 52.10%] [G loss: 0.7009379267692566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 52/86 [D loss: 0.6934297382831573, acc.: 49.27%] [G loss: 0.7016201019287109]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 53/86 [D loss: 0.6923035383224487, acc.: 52.83%] [G loss: 0.7010191679000854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 54/86 [D loss: 0.6925793886184692, acc.: 50.39%] [G loss: 0.7006429433822632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 55/86 [D loss: 0.693309873342514, acc.: 49.51%] [G loss: 0.7005605697631836]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 56/86 [D loss: 0.6927648186683655, acc.: 51.03%] [G loss: 0.7004968523979187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 57/86 [D loss: 0.6927685141563416, acc.: 50.20%] [G loss: 0.7015584707260132]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 58/86 [D loss: 0.6927656531333923, acc.: 50.59%] [G loss: 0.7017171382904053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 59/86 [D loss: 0.6919872164726257, acc.: 52.39%] [G loss: 0.70063716173172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 60/86 [D loss: 0.6918831467628479, acc.: 53.17%] [G loss: 0.7002602219581604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 61/86 [D loss: 0.6925236284732819, acc.: 52.25%] [G loss: 0.7011584043502808]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 62/86 [D loss: 0.6920079290866852, acc.: 52.44%] [G loss: 0.7018227577209473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 63/86 [D loss: 0.6924267411231995, acc.: 51.51%] [G loss: 0.7004679441452026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 64/86 [D loss: 0.6925704777240753, acc.: 51.32%] [G loss: 0.7009658217430115]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 65/86 [D loss: 0.6927060186862946, acc.: 51.27%] [G loss: 0.7013742923736572]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 66/86 [D loss: 0.6913374960422516, acc.: 54.20%] [G loss: 0.7004848122596741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 67/86 [D loss: 0.6927679479122162, acc.: 51.86%] [G loss: 0.7026365399360657]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 68/86 [D loss: 0.6922658383846283, acc.: 50.34%] [G loss: 0.7008616328239441]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 69/86 [D loss: 0.6940194368362427, acc.: 49.71%] [G loss: 0.7012182474136353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 70/86 [D loss: 0.691727340221405, acc.: 54.15%] [G loss: 0.7021042108535767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 71/86 [D loss: 0.6924665868282318, acc.: 52.05%] [G loss: 0.7000454068183899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 72/86 [D loss: 0.6920031011104584, acc.: 53.32%] [G loss: 0.7016128301620483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 73/86 [D loss: 0.6930577158927917, acc.: 50.24%] [G loss: 0.7014400362968445]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 74/86 [D loss: 0.69213005900383, acc.: 52.93%] [G loss: 0.6998123526573181]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 75/86 [D loss: 0.6921269595623016, acc.: 52.05%] [G loss: 0.700124979019165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 76/86 [D loss: 0.6917265057563782, acc.: 53.12%] [G loss: 0.702021598815918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 77/86 [D loss: 0.6928680837154388, acc.: 51.03%] [G loss: 0.7014158964157104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 78/86 [D loss: 0.6921467781066895, acc.: 51.46%] [G loss: 0.7015726566314697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 79/86 [D loss: 0.6923923790454865, acc.: 51.95%] [G loss: 0.7017148733139038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 80/86 [D loss: 0.6929031014442444, acc.: 49.85%] [G loss: 0.7015518546104431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 81/86 [D loss: 0.6923378705978394, acc.: 52.39%] [G loss: 0.7016924619674683]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 82/86 [D loss: 0.6924735009670258, acc.: 51.22%] [G loss: 0.7005617022514343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 83/86 [D loss: 0.6920993030071259, acc.: 52.10%] [G loss: 0.7007440328598022]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 54/200, Batch 84/86 [D loss: 0.691402792930603, acc.: 54.20%] [G loss: 0.7009616494178772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 85/86 [D loss: 0.691524475812912, acc.: 54.05%] [G loss: 0.7008150219917297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 54/200, Batch 86/86 [D loss: 0.6913870871067047, acc.: 53.47%] [G loss: 0.7012637853622437]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 16ms/step\n",
      "Epoch 55/200, Batch 1/86 [D loss: 0.6924899220466614, acc.: 52.15%] [G loss: 0.7012740969657898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 2/86 [D loss: 0.6930674016475677, acc.: 51.22%] [G loss: 0.7006059288978577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 3/86 [D loss: 0.6915597915649414, acc.: 52.54%] [G loss: 0.7012208700180054]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 4/86 [D loss: 0.6920791864395142, acc.: 51.81%] [G loss: 0.70075523853302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 5/86 [D loss: 0.6923688352108002, acc.: 50.10%] [G loss: 0.7020691633224487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 6/86 [D loss: 0.6928090751171112, acc.: 50.63%] [G loss: 0.7011038661003113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 7/86 [D loss: 0.6917701959609985, acc.: 52.15%] [G loss: 0.7010602355003357]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 8/86 [D loss: 0.69268798828125, acc.: 49.66%] [G loss: 0.7016560435295105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 9/86 [D loss: 0.6929068565368652, acc.: 50.93%] [G loss: 0.7021782398223877]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 10/86 [D loss: 0.6925938129425049, acc.: 50.44%] [G loss: 0.7014814615249634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 11/86 [D loss: 0.6921758353710175, acc.: 52.20%] [G loss: 0.7017509937286377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 12/86 [D loss: 0.6922253966331482, acc.: 52.49%] [G loss: 0.7024573683738708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 13/86 [D loss: 0.6923860311508179, acc.: 51.03%] [G loss: 0.7013857364654541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 14/86 [D loss: 0.6925541758537292, acc.: 51.07%] [G loss: 0.7020494937896729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 15/86 [D loss: 0.6928462982177734, acc.: 50.98%] [G loss: 0.7017670273780823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 16/86 [D loss: 0.6922658383846283, acc.: 51.12%] [G loss: 0.7014434337615967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 17/86 [D loss: 0.6922910213470459, acc.: 52.49%] [G loss: 0.7011785507202148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 18/86 [D loss: 0.6918813586235046, acc.: 53.61%] [G loss: 0.7023782730102539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 19/86 [D loss: 0.6922766864299774, acc.: 52.25%] [G loss: 0.700335681438446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 20/86 [D loss: 0.6923230588436127, acc.: 51.61%] [G loss: 0.7019665837287903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 21/86 [D loss: 0.6914554834365845, acc.: 53.56%] [G loss: 0.7019797563552856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 22/86 [D loss: 0.6919669806957245, acc.: 52.25%] [G loss: 0.7018722295761108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 23/86 [D loss: 0.6927694380283356, acc.: 51.66%] [G loss: 0.6994277834892273]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 24/86 [D loss: 0.693405032157898, acc.: 49.61%] [G loss: 0.7015348672866821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 25/86 [D loss: 0.6918267011642456, acc.: 52.39%] [G loss: 0.7036750316619873]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 26/86 [D loss: 0.6928697526454926, acc.: 50.68%] [G loss: 0.7015664577484131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 27/86 [D loss: 0.6933034360408783, acc.: 49.85%] [G loss: 0.7009454965591431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 28/86 [D loss: 0.6927517950534821, acc.: 50.88%] [G loss: 0.702655553817749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 29/86 [D loss: 0.6928742229938507, acc.: 50.98%] [G loss: 0.7018311023712158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 30/86 [D loss: 0.6922557353973389, acc.: 52.39%] [G loss: 0.7022773027420044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 31/86 [D loss: 0.6926806271076202, acc.: 50.88%] [G loss: 0.7022599577903748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 32/86 [D loss: 0.6929180324077606, acc.: 50.59%] [G loss: 0.7025970816612244]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 33/86 [D loss: 0.6916530132293701, acc.: 54.79%] [G loss: 0.7007582187652588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 34/86 [D loss: 0.6920002102851868, acc.: 52.44%] [G loss: 0.7016984224319458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 35/86 [D loss: 0.6936874091625214, acc.: 48.88%] [G loss: 0.7025801539421082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 36/86 [D loss: 0.6913225650787354, acc.: 54.64%] [G loss: 0.701452374458313]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 37/86 [D loss: 0.6930990815162659, acc.: 50.39%] [G loss: 0.7002067565917969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 38/86 [D loss: 0.6923608183860779, acc.: 51.37%] [G loss: 0.7012661695480347]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 39/86 [D loss: 0.6916942596435547, acc.: 52.39%] [G loss: 0.7010669708251953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 40/86 [D loss: 0.6916672885417938, acc.: 53.52%] [G loss: 0.7010042667388916]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 41/86 [D loss: 0.6917594373226166, acc.: 51.95%] [G loss: 0.7019127011299133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 42/86 [D loss: 0.6920627355575562, acc.: 52.88%] [G loss: 0.7026206254959106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 43/86 [D loss: 0.6924282014369965, acc.: 50.49%] [G loss: 0.7013112306594849]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 44/86 [D loss: 0.6931750178337097, acc.: 49.90%] [G loss: 0.7019222378730774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 45/86 [D loss: 0.6921794712543488, acc.: 52.64%] [G loss: 0.7000565528869629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 46/86 [D loss: 0.691785991191864, acc.: 52.93%] [G loss: 0.7020708918571472]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 47/86 [D loss: 0.6928527057170868, acc.: 50.34%] [G loss: 0.6996399760246277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 48/86 [D loss: 0.6929464042186737, acc.: 50.00%] [G loss: 0.7033425569534302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 49/86 [D loss: 0.6922286152839661, acc.: 52.15%] [G loss: 0.70085608959198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 50/86 [D loss: 0.6926196217536926, acc.: 50.63%] [G loss: 0.7025560736656189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 51/86 [D loss: 0.6926891207695007, acc.: 52.64%] [G loss: 0.6994872093200684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 52/86 [D loss: 0.693385899066925, acc.: 49.51%] [G loss: 0.7027502059936523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 53/86 [D loss: 0.6913169026374817, acc.: 53.66%] [G loss: 0.7008013725280762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 54/86 [D loss: 0.6923088729381561, acc.: 52.49%] [G loss: 0.7019928097724915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 55/86 [D loss: 0.6915764510631561, acc.: 52.10%] [G loss: 0.6993189454078674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 56/86 [D loss: 0.6928874850273132, acc.: 50.78%] [G loss: 0.7015359401702881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 57/86 [D loss: 0.6915676891803741, acc.: 52.73%] [G loss: 0.7000445127487183]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 58/86 [D loss: 0.6918050348758698, acc.: 53.08%] [G loss: 0.7014410495758057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 59/86 [D loss: 0.6922764182090759, acc.: 51.07%] [G loss: 0.6995542049407959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 60/86 [D loss: 0.6928896009922028, acc.: 49.46%] [G loss: 0.7011272311210632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 61/86 [D loss: 0.6920467019081116, acc.: 51.66%] [G loss: 0.7007406949996948]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 62/86 [D loss: 0.6916880011558533, acc.: 52.39%] [G loss: 0.7014989852905273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 63/86 [D loss: 0.6921243369579315, acc.: 52.54%] [G loss: 0.6996052861213684]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 64/86 [D loss: 0.6933071911334991, acc.: 50.68%] [G loss: 0.7029445171356201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 65/86 [D loss: 0.6925840973854065, acc.: 51.61%] [G loss: 0.6998836994171143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 66/86 [D loss: 0.6914339065551758, acc.: 52.25%] [G loss: 0.7028100490570068]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 67/86 [D loss: 0.6927973031997681, acc.: 50.68%] [G loss: 0.7011760473251343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 68/86 [D loss: 0.6932208836078644, acc.: 50.24%] [G loss: 0.7020270228385925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 69/86 [D loss: 0.6921848058700562, acc.: 51.56%] [G loss: 0.7021166086196899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 70/86 [D loss: 0.6914345622062683, acc.: 53.91%] [G loss: 0.7006935477256775]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 71/86 [D loss: 0.6926310658454895, acc.: 51.51%] [G loss: 0.701724648475647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 72/86 [D loss: 0.6925039887428284, acc.: 51.22%] [G loss: 0.7013982534408569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 73/86 [D loss: 0.6922876834869385, acc.: 52.49%] [G loss: 0.7015402913093567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 74/86 [D loss: 0.6918005645275116, acc.: 52.83%] [G loss: 0.7023260593414307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 75/86 [D loss: 0.6932622492313385, acc.: 51.27%] [G loss: 0.7005985975265503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 76/86 [D loss: 0.6922855079174042, acc.: 51.66%] [G loss: 0.7018519639968872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 77/86 [D loss: 0.6918257176876068, acc.: 52.44%] [G loss: 0.7014110088348389]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 78/86 [D loss: 0.69194296002388, acc.: 52.83%] [G loss: 0.7015737891197205]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 79/86 [D loss: 0.6924146413803101, acc.: 51.03%] [G loss: 0.6999743580818176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 80/86 [D loss: 0.6926109492778778, acc.: 50.73%] [G loss: 0.7022637724876404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 81/86 [D loss: 0.6926760673522949, acc.: 51.12%] [G loss: 0.7024548053741455]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 82/86 [D loss: 0.6908472776412964, acc.: 54.10%] [G loss: 0.702759325504303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 83/86 [D loss: 0.6925399303436279, acc.: 51.22%] [G loss: 0.6994779109954834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 84/86 [D loss: 0.6926348805427551, acc.: 50.63%] [G loss: 0.7033591270446777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 55/200, Batch 85/86 [D loss: 0.691972404718399, acc.: 53.86%] [G loss: 0.7012844085693359]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 55/200, Batch 86/86 [D loss: 0.691622793674469, acc.: 52.93%] [G loss: 0.7015700936317444]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 1/86 [D loss: 0.6920478641986847, acc.: 52.29%] [G loss: 0.7015909552574158]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 2/86 [D loss: 0.692851185798645, acc.: 50.49%] [G loss: 0.702703595161438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 3/86 [D loss: 0.6917734146118164, acc.: 53.12%] [G loss: 0.7022951245307922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 4/86 [D loss: 0.6918034255504608, acc.: 51.56%] [G loss: 0.7012295126914978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 5/86 [D loss: 0.6920652091503143, acc.: 52.64%] [G loss: 0.702014148235321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 6/86 [D loss: 0.6923338174819946, acc.: 51.66%] [G loss: 0.7025420665740967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 7/86 [D loss: 0.6917739808559418, acc.: 54.00%] [G loss: 0.7018802762031555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 8/86 [D loss: 0.6916446089744568, acc.: 53.42%] [G loss: 0.7025892734527588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 9/86 [D loss: 0.6922513544559479, acc.: 50.98%] [G loss: 0.702406644821167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 10/86 [D loss: 0.6918942630290985, acc.: 52.83%] [G loss: 0.7014582753181458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 11/86 [D loss: 0.6910403072834015, acc.: 53.86%] [G loss: 0.7030915021896362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 12/86 [D loss: 0.6909382343292236, acc.: 53.66%] [G loss: 0.7028315663337708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 13/86 [D loss: 0.6920212507247925, acc.: 52.15%] [G loss: 0.7035303115844727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 14/86 [D loss: 0.6923598051071167, acc.: 50.83%] [G loss: 0.7035738229751587]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 15/86 [D loss: 0.6924328207969666, acc.: 51.07%] [G loss: 0.7024332284927368]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 16/86 [D loss: 0.6917342245578766, acc.: 51.76%] [G loss: 0.7022959589958191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 17/86 [D loss: 0.6919946074485779, acc.: 52.73%] [G loss: 0.7034652829170227]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 18/86 [D loss: 0.6924579739570618, acc.: 52.69%] [G loss: 0.7017467021942139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 19/86 [D loss: 0.6912282705307007, acc.: 53.22%] [G loss: 0.702889621257782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 20/86 [D loss: 0.6921842396259308, acc.: 52.54%] [G loss: 0.7001644968986511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 21/86 [D loss: 0.6932656764984131, acc.: 50.68%] [G loss: 0.7020447850227356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 22/86 [D loss: 0.6917526721954346, acc.: 53.56%] [G loss: 0.7018148303031921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 23/86 [D loss: 0.6923461258411407, acc.: 50.49%] [G loss: 0.702589213848114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 24/86 [D loss: 0.691493034362793, acc.: 53.61%] [G loss: 0.7012843489646912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 25/86 [D loss: 0.6932639181613922, acc.: 49.66%] [G loss: 0.7036682367324829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 26/86 [D loss: 0.6911201775074005, acc.: 54.25%] [G loss: 0.7016770839691162]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 27/86 [D loss: 0.6925614774227142, acc.: 50.44%] [G loss: 0.7023645043373108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 28/86 [D loss: 0.6915571987628937, acc.: 53.76%] [G loss: 0.7018229365348816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 29/86 [D loss: 0.6935282349586487, acc.: 49.80%] [G loss: 0.7016485929489136]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 30/86 [D loss: 0.691224604845047, acc.: 52.59%] [G loss: 0.7009615898132324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 31/86 [D loss: 0.6923637986183167, acc.: 51.32%] [G loss: 0.7009571194648743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 32/86 [D loss: 0.6911556124687195, acc.: 53.96%] [G loss: 0.7016199827194214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 33/86 [D loss: 0.6923959851264954, acc.: 51.76%] [G loss: 0.701526403427124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 34/86 [D loss: 0.6919264495372772, acc.: 51.42%] [G loss: 0.7023137211799622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 35/86 [D loss: 0.6918444931507111, acc.: 52.69%] [G loss: 0.7000991702079773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 36/86 [D loss: 0.6916382610797882, acc.: 53.08%] [G loss: 0.7022386789321899]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 37/86 [D loss: 0.6928148865699768, acc.: 51.22%] [G loss: 0.6997122168540955]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 38/86 [D loss: 0.6933518648147583, acc.: 50.93%] [G loss: 0.701797366142273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 39/86 [D loss: 0.6925302743911743, acc.: 52.10%] [G loss: 0.6995569467544556]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 40/86 [D loss: 0.6913433969020844, acc.: 52.78%] [G loss: 0.7021758556365967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 41/86 [D loss: 0.6928693950176239, acc.: 52.00%] [G loss: 0.6968371868133545]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 42/86 [D loss: 0.6955572068691254, acc.: 46.63%] [G loss: 0.700993537902832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 43/86 [D loss: 0.6918877065181732, acc.: 53.17%] [G loss: 0.699821412563324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 44/86 [D loss: 0.6924290955066681, acc.: 51.12%] [G loss: 0.7014046907424927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 45/86 [D loss: 0.6919805407524109, acc.: 50.78%] [G loss: 0.6963315606117249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 46/86 [D loss: 0.6950235366821289, acc.: 46.29%] [G loss: 0.7020322680473328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 47/86 [D loss: 0.6924231946468353, acc.: 50.24%] [G loss: 0.7001465559005737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 48/86 [D loss: 0.6926647424697876, acc.: 50.78%] [G loss: 0.7019434571266174]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 49/86 [D loss: 0.6914357244968414, acc.: 53.42%] [G loss: 0.6994973421096802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 50/86 [D loss: 0.6943026781082153, acc.: 47.41%] [G loss: 0.7018726468086243]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 51/86 [D loss: 0.6916733682155609, acc.: 53.42%] [G loss: 0.7009965777397156]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 52/86 [D loss: 0.6914932429790497, acc.: 52.98%] [G loss: 0.7003797292709351]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 53/86 [D loss: 0.6918938755989075, acc.: 50.78%] [G loss: 0.7011196613311768]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 54/86 [D loss: 0.6926731765270233, acc.: 50.93%] [G loss: 0.7033873796463013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 55/86 [D loss: 0.6929938793182373, acc.: 49.95%] [G loss: 0.701970100402832]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 56/86 [D loss: 0.6918885409832001, acc.: 53.22%] [G loss: 0.7004939317703247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 57/86 [D loss: 0.6913753151893616, acc.: 54.10%] [G loss: 0.7010612487792969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 58/86 [D loss: 0.6930457949638367, acc.: 51.95%] [G loss: 0.7027053833007812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 59/86 [D loss: 0.6916977167129517, acc.: 52.25%] [G loss: 0.7020375728607178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 60/86 [D loss: 0.6914023756980896, acc.: 52.44%] [G loss: 0.7021350860595703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 61/86 [D loss: 0.6914424896240234, acc.: 51.90%] [G loss: 0.7026960253715515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 62/86 [D loss: 0.6924887597560883, acc.: 50.83%] [G loss: 0.7014967203140259]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 63/86 [D loss: 0.6918557584285736, acc.: 52.83%] [G loss: 0.7030168771743774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 64/86 [D loss: 0.6913780868053436, acc.: 53.03%] [G loss: 0.701625645160675]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 65/86 [D loss: 0.69193434715271, acc.: 53.91%] [G loss: 0.7013821005821228]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 66/86 [D loss: 0.691862553358078, acc.: 51.71%] [G loss: 0.7022936940193176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 67/86 [D loss: 0.6913987696170807, acc.: 53.08%] [G loss: 0.7019089460372925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 68/86 [D loss: 0.6923448145389557, acc.: 51.32%] [G loss: 0.7011536955833435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 69/86 [D loss: 0.691723108291626, acc.: 52.59%] [G loss: 0.7022091150283813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 70/86 [D loss: 0.6912969946861267, acc.: 54.20%] [G loss: 0.7032404541969299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 71/86 [D loss: 0.6921333074569702, acc.: 52.49%] [G loss: 0.7011083364486694]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 72/86 [D loss: 0.6924176216125488, acc.: 52.00%] [G loss: 0.7023542523384094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 56/200, Batch 73/86 [D loss: 0.6906060576438904, acc.: 53.52%] [G loss: 0.7014736533164978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 74/86 [D loss: 0.6909647881984711, acc.: 54.00%] [G loss: 0.7029699683189392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 75/86 [D loss: 0.6917279362678528, acc.: 52.83%] [G loss: 0.7017322778701782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 76/86 [D loss: 0.6917241215705872, acc.: 52.98%] [G loss: 0.7008785009384155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 77/86 [D loss: 0.6904865503311157, acc.: 53.71%] [G loss: 0.7017255425453186]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 78/86 [D loss: 0.691482663154602, acc.: 52.49%] [G loss: 0.702786386013031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 79/86 [D loss: 0.6924777030944824, acc.: 51.37%] [G loss: 0.7015295028686523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 80/86 [D loss: 0.6911988258361816, acc.: 53.42%] [G loss: 0.7017015814781189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 81/86 [D loss: 0.6910098493099213, acc.: 54.05%] [G loss: 0.7022839188575745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 82/86 [D loss: 0.6923443675041199, acc.: 51.27%] [G loss: 0.7018102407455444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 83/86 [D loss: 0.6915820837020874, acc.: 52.83%] [G loss: 0.7025504112243652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 84/86 [D loss: 0.6916829943656921, acc.: 52.15%] [G loss: 0.7014182209968567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 85/86 [D loss: 0.6914963126182556, acc.: 54.44%] [G loss: 0.7020004391670227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 56/200, Batch 86/86 [D loss: 0.691487580537796, acc.: 52.44%] [G loss: 0.7021770477294922]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 1/86 [D loss: 0.6925373077392578, acc.: 51.12%] [G loss: 0.7025672793388367]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 2/86 [D loss: 0.6910285353660583, acc.: 53.76%] [G loss: 0.7018938064575195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 3/86 [D loss: 0.6906002461910248, acc.: 55.57%] [G loss: 0.7018328905105591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 4/86 [D loss: 0.6913354098796844, acc.: 55.27%] [G loss: 0.7023959159851074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 5/86 [D loss: 0.6922899186611176, acc.: 51.46%] [G loss: 0.7006328701972961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 6/86 [D loss: 0.6913646161556244, acc.: 52.64%] [G loss: 0.701305091381073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 7/86 [D loss: 0.6918549835681915, acc.: 52.59%] [G loss: 0.7022742629051208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 8/86 [D loss: 0.6897147595882416, acc.: 55.81%] [G loss: 0.7006782293319702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 9/86 [D loss: 0.6924026608467102, acc.: 50.44%] [G loss: 0.7006842494010925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 10/86 [D loss: 0.690457284450531, acc.: 54.44%] [G loss: 0.701063871383667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 11/86 [D loss: 0.6919888257980347, acc.: 53.56%] [G loss: 0.7020391821861267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 12/86 [D loss: 0.6916649043560028, acc.: 53.22%] [G loss: 0.7000912427902222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 13/86 [D loss: 0.6933202743530273, acc.: 50.83%] [G loss: 0.7020956873893738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 14/86 [D loss: 0.6911379992961884, acc.: 53.61%] [G loss: 0.7014765739440918]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 15/86 [D loss: 0.6922726631164551, acc.: 52.54%] [G loss: 0.7005754113197327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 16/86 [D loss: 0.6920479834079742, acc.: 51.66%] [G loss: 0.7024918794631958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 17/86 [D loss: 0.6924141049385071, acc.: 52.25%] [G loss: 0.6990821957588196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 18/86 [D loss: 0.6917224228382111, acc.: 53.08%] [G loss: 0.7025138735771179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 19/86 [D loss: 0.6927365064620972, acc.: 49.80%] [G loss: 0.7009639739990234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 20/86 [D loss: 0.6920947134494781, acc.: 51.46%] [G loss: 0.7017250657081604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 21/86 [D loss: 0.6922069191932678, acc.: 52.34%] [G loss: 0.6992166042327881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 22/86 [D loss: 0.6920645833015442, acc.: 51.66%] [G loss: 0.7009131908416748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 23/86 [D loss: 0.6931021511554718, acc.: 50.44%] [G loss: 0.7002130150794983]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 24/86 [D loss: 0.6913906633853912, acc.: 53.12%] [G loss: 0.7008887529373169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 25/86 [D loss: 0.6921157538890839, acc.: 50.63%] [G loss: 0.6988215446472168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 26/86 [D loss: 0.6914527416229248, acc.: 53.52%] [G loss: 0.7012031674385071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 27/86 [D loss: 0.6926089227199554, acc.: 51.86%] [G loss: 0.6997197866439819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 28/86 [D loss: 0.6919983327388763, acc.: 52.05%] [G loss: 0.7010630369186401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 29/86 [D loss: 0.6919218897819519, acc.: 51.95%] [G loss: 0.6982479691505432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 30/86 [D loss: 0.6928842961788177, acc.: 49.41%] [G loss: 0.7019307017326355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 31/86 [D loss: 0.6924814581871033, acc.: 50.98%] [G loss: 0.6993201971054077]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 32/86 [D loss: 0.6911492347717285, acc.: 53.56%] [G loss: 0.7005395293235779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 33/86 [D loss: 0.6912490427494049, acc.: 54.10%] [G loss: 0.6978611350059509]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 34/86 [D loss: 0.6938043534755707, acc.: 49.17%] [G loss: 0.7024018168449402]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 35/86 [D loss: 0.6917417049407959, acc.: 53.66%] [G loss: 0.7001458406448364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 36/86 [D loss: 0.6930289268493652, acc.: 49.37%] [G loss: 0.7032961845397949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 37/86 [D loss: 0.6908257007598877, acc.: 54.93%] [G loss: 0.7000192999839783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 38/86 [D loss: 0.6926270127296448, acc.: 50.44%] [G loss: 0.7000257968902588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 39/86 [D loss: 0.6919556558132172, acc.: 52.64%] [G loss: 0.7011391520500183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 40/86 [D loss: 0.6929813027381897, acc.: 50.63%] [G loss: 0.7017906904220581]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 41/86 [D loss: 0.6926737129688263, acc.: 51.42%] [G loss: 0.701251208782196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 42/86 [D loss: 0.691769152879715, acc.: 52.20%] [G loss: 0.699028491973877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 43/86 [D loss: 0.692796528339386, acc.: 50.68%] [G loss: 0.7021013498306274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 44/86 [D loss: 0.6912522614002228, acc.: 53.47%] [G loss: 0.7011108994483948]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 45/86 [D loss: 0.6918160319328308, acc.: 52.05%] [G loss: 0.7016383409500122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 46/86 [D loss: 0.6920890212059021, acc.: 53.12%] [G loss: 0.7005577087402344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 47/86 [D loss: 0.6923587918281555, acc.: 51.32%] [G loss: 0.7023492455482483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 48/86 [D loss: 0.6907985508441925, acc.: 54.74%] [G loss: 0.7011094093322754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 49/86 [D loss: 0.6911048889160156, acc.: 53.56%] [G loss: 0.7029390931129456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 50/86 [D loss: 0.6911066770553589, acc.: 54.00%] [G loss: 0.7015771865844727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 51/86 [D loss: 0.6925235986709595, acc.: 51.37%] [G loss: 0.7014589309692383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 52/86 [D loss: 0.6910772621631622, acc.: 55.03%] [G loss: 0.701292872428894]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 53/86 [D loss: 0.691880077123642, acc.: 52.29%] [G loss: 0.7001631855964661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 54/86 [D loss: 0.690923422574997, acc.: 53.86%] [G loss: 0.701275110244751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 55/86 [D loss: 0.6920427680015564, acc.: 52.25%] [G loss: 0.7008423805236816]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 56/86 [D loss: 0.6914437413215637, acc.: 53.91%] [G loss: 0.7016568779945374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 57/86 [D loss: 0.691165417432785, acc.: 53.76%] [G loss: 0.7000876665115356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 58/86 [D loss: 0.6924385726451874, acc.: 51.71%] [G loss: 0.701932966709137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 59/86 [D loss: 0.6914272010326385, acc.: 52.88%] [G loss: 0.6989635229110718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 60/86 [D loss: 0.6934956312179565, acc.: 49.12%] [G loss: 0.7027100324630737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 61/86 [D loss: 0.6910755336284637, acc.: 54.15%] [G loss: 0.7007988095283508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 62/86 [D loss: 0.6924599707126617, acc.: 51.51%] [G loss: 0.702124834060669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 63/86 [D loss: 0.6911549866199493, acc.: 53.17%] [G loss: 0.7010226249694824]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 64/86 [D loss: 0.6927501857280731, acc.: 51.42%] [G loss: 0.7009495496749878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 65/86 [D loss: 0.6911595165729523, acc.: 53.37%] [G loss: 0.7024139761924744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 66/86 [D loss: 0.6923049986362457, acc.: 51.37%] [G loss: 0.7001305818557739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 67/86 [D loss: 0.6921561658382416, acc.: 53.37%] [G loss: 0.7016775608062744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 68/86 [D loss: 0.691465824842453, acc.: 53.56%] [G loss: 0.6990820169448853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 69/86 [D loss: 0.6936875283718109, acc.: 49.02%] [G loss: 0.7021897435188293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 70/86 [D loss: 0.6917346119880676, acc.: 53.08%] [G loss: 0.6987475752830505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 71/86 [D loss: 0.6912843585014343, acc.: 52.98%] [G loss: 0.7017021179199219]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 72/86 [D loss: 0.6913062930107117, acc.: 54.10%] [G loss: 0.6992020606994629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 73/86 [D loss: 0.6934021413326263, acc.: 49.51%] [G loss: 0.6997779607772827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 74/86 [D loss: 0.6915824711322784, acc.: 53.08%] [G loss: 0.7002545595169067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 75/86 [D loss: 0.6915024816989899, acc.: 53.27%] [G loss: 0.7014833092689514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 76/86 [D loss: 0.6914008855819702, acc.: 52.34%] [G loss: 0.7008799314498901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 77/86 [D loss: 0.6916907727718353, acc.: 51.12%] [G loss: 0.7003626823425293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 78/86 [D loss: 0.6916142404079437, acc.: 52.49%] [G loss: 0.7021335363388062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 79/86 [D loss: 0.6920490860939026, acc.: 52.49%] [G loss: 0.7006624937057495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 80/86 [D loss: 0.6919355094432831, acc.: 50.78%] [G loss: 0.7009825706481934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 81/86 [D loss: 0.6915227770805359, acc.: 52.93%] [G loss: 0.7002221345901489]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 82/86 [D loss: 0.6907936036586761, acc.: 53.37%] [G loss: 0.7017382383346558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 83/86 [D loss: 0.6911800503730774, acc.: 53.96%] [G loss: 0.7002701759338379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 57/200, Batch 84/86 [D loss: 0.6918919682502747, acc.: 51.27%] [G loss: 0.7001863718032837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 85/86 [D loss: 0.6918277144432068, acc.: 52.54%] [G loss: 0.7006440758705139]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 57/200, Batch 86/86 [D loss: 0.6924080848693848, acc.: 52.10%] [G loss: 0.7004615068435669]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 1/86 [D loss: 0.6907938420772552, acc.: 54.00%] [G loss: 0.7009673714637756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 2/86 [D loss: 0.6916673481464386, acc.: 52.78%] [G loss: 0.7006925344467163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 3/86 [D loss: 0.6917460560798645, acc.: 52.59%] [G loss: 0.6991907358169556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 4/86 [D loss: 0.6917428076267242, acc.: 52.20%] [G loss: 0.7007782459259033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 5/86 [D loss: 0.6919376850128174, acc.: 52.54%] [G loss: 0.7020748853683472]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 6/86 [D loss: 0.69176384806633, acc.: 52.88%] [G loss: 0.7008236646652222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 7/86 [D loss: 0.6918143630027771, acc.: 52.54%] [G loss: 0.7022572159767151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 8/86 [D loss: 0.6914942860603333, acc.: 52.49%] [G loss: 0.7006354928016663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 9/86 [D loss: 0.6935530006885529, acc.: 48.49%] [G loss: 0.7007006406784058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 10/86 [D loss: 0.6909559369087219, acc.: 53.66%] [G loss: 0.7003841996192932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 11/86 [D loss: 0.6921002268791199, acc.: 51.95%] [G loss: 0.7014603614807129]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 12/86 [D loss: 0.690649539232254, acc.: 53.71%] [G loss: 0.700208842754364]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 13/86 [D loss: 0.6935096681118011, acc.: 50.83%] [G loss: 0.7011474370956421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 14/86 [D loss: 0.6921994984149933, acc.: 52.39%] [G loss: 0.7005784511566162]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 15/86 [D loss: 0.6919979751110077, acc.: 52.00%] [G loss: 0.6995976567268372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 16/86 [D loss: 0.6907210052013397, acc.: 54.54%] [G loss: 0.7001782655715942]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 17/86 [D loss: 0.69182088971138, acc.: 53.03%] [G loss: 0.6997572779655457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 18/86 [D loss: 0.6918711364269257, acc.: 51.81%] [G loss: 0.701886773109436]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 19/86 [D loss: 0.6928133368492126, acc.: 52.15%] [G loss: 0.701216459274292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 20/86 [D loss: 0.6917587220668793, acc.: 53.22%] [G loss: 0.7016845941543579]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 21/86 [D loss: 0.692464292049408, acc.: 50.83%] [G loss: 0.7000003457069397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 22/86 [D loss: 0.693303644657135, acc.: 48.97%] [G loss: 0.7011458873748779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 23/86 [D loss: 0.6909742653369904, acc.: 54.00%] [G loss: 0.7009345889091492]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 24/86 [D loss: 0.6920524537563324, acc.: 52.44%] [G loss: 0.7021238207817078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 25/86 [D loss: 0.6914851665496826, acc.: 52.49%] [G loss: 0.7005073428153992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 26/86 [D loss: 0.6921481490135193, acc.: 51.51%] [G loss: 0.7006826996803284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 27/86 [D loss: 0.6900799870491028, acc.: 55.27%] [G loss: 0.6998524069786072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 28/86 [D loss: 0.6910771727561951, acc.: 53.42%] [G loss: 0.7012538313865662]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 29/86 [D loss: 0.6915030181407928, acc.: 52.44%] [G loss: 0.7006609439849854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 30/86 [D loss: 0.6922057569026947, acc.: 50.98%] [G loss: 0.6976500749588013]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 31/86 [D loss: 0.6920336186885834, acc.: 52.15%] [G loss: 0.7022005319595337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 32/86 [D loss: 0.6922791302204132, acc.: 50.29%] [G loss: 0.6987093687057495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 33/86 [D loss: 0.6924895644187927, acc.: 50.93%] [G loss: 0.7002721428871155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 34/86 [D loss: 0.6917158365249634, acc.: 51.90%] [G loss: 0.697886049747467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 35/86 [D loss: 0.6938469707965851, acc.: 48.10%] [G loss: 0.7002829909324646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 36/86 [D loss: 0.6897204518318176, acc.: 54.98%] [G loss: 0.7001283168792725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 37/86 [D loss: 0.6923541128635406, acc.: 51.03%] [G loss: 0.7003704309463501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 38/86 [D loss: 0.6914395987987518, acc.: 53.42%] [G loss: 0.7005115747451782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 39/86 [D loss: 0.6931959688663483, acc.: 50.05%] [G loss: 0.6991596221923828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 40/86 [D loss: 0.6914782822132111, acc.: 54.59%] [G loss: 0.7016105055809021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 41/86 [D loss: 0.691726803779602, acc.: 52.29%] [G loss: 0.6992619037628174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 42/86 [D loss: 0.6910321116447449, acc.: 53.37%] [G loss: 0.6996572613716125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 43/86 [D loss: 0.6925847828388214, acc.: 50.10%] [G loss: 0.6970429420471191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 44/86 [D loss: 0.693186342716217, acc.: 50.78%] [G loss: 0.7014349102973938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 45/86 [D loss: 0.6920930445194244, acc.: 51.66%] [G loss: 0.698379635810852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 46/86 [D loss: 0.6927779316902161, acc.: 50.54%] [G loss: 0.7005631923675537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 47/86 [D loss: 0.6909486651420593, acc.: 55.42%] [G loss: 0.6965100765228271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 48/86 [D loss: 0.6928346157073975, acc.: 51.81%] [G loss: 0.6993048191070557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 49/86 [D loss: 0.6916682124137878, acc.: 51.86%] [G loss: 0.6998857259750366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 50/86 [D loss: 0.6916168630123138, acc.: 53.56%] [G loss: 0.7009290456771851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 51/86 [D loss: 0.6908068060874939, acc.: 54.59%] [G loss: 0.6994283199310303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 52/86 [D loss: 0.6935287714004517, acc.: 50.44%] [G loss: 0.6989206671714783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 53/86 [D loss: 0.6913168132305145, acc.: 52.64%] [G loss: 0.7008053064346313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 54/86 [D loss: 0.6929211616516113, acc.: 49.51%] [G loss: 0.6985983848571777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 55/86 [D loss: 0.6919132769107819, acc.: 52.20%] [G loss: 0.6995022296905518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 56/86 [D loss: 0.6927422285079956, acc.: 49.71%] [G loss: 0.698273777961731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 57/86 [D loss: 0.6929768323898315, acc.: 50.44%] [G loss: 0.7024370431900024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 58/86 [D loss: 0.6904122233390808, acc.: 54.64%] [G loss: 0.7012168169021606]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 59/86 [D loss: 0.6921307444572449, acc.: 52.10%] [G loss: 0.6998461484909058]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 60/86 [D loss: 0.6919293701648712, acc.: 54.00%] [G loss: 0.698923647403717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 61/86 [D loss: 0.6922942996025085, acc.: 51.46%] [G loss: 0.6997520923614502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 62/86 [D loss: 0.6907301545143127, acc.: 53.22%] [G loss: 0.700899064540863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 63/86 [D loss: 0.6926977932453156, acc.: 50.88%] [G loss: 0.7013260126113892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 64/86 [D loss: 0.6927708983421326, acc.: 49.95%] [G loss: 0.7012331485748291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 65/86 [D loss: 0.6933498382568359, acc.: 50.78%] [G loss: 0.6995043754577637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 66/86 [D loss: 0.6920510828495026, acc.: 51.95%] [G loss: 0.7006211876869202]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 67/86 [D loss: 0.6913941204547882, acc.: 52.83%] [G loss: 0.6998946070671082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 68/86 [D loss: 0.6921676099300385, acc.: 52.00%] [G loss: 0.7018110752105713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 69/86 [D loss: 0.6901240646839142, acc.: 55.57%] [G loss: 0.6976180076599121]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 70/86 [D loss: 0.6950162947177887, acc.: 47.85%] [G loss: 0.7009671330451965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 71/86 [D loss: 0.6912684738636017, acc.: 53.52%] [G loss: 0.7005212306976318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 72/86 [D loss: 0.6938019692897797, acc.: 48.73%] [G loss: 0.699447751045227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 73/86 [D loss: 0.6924648582935333, acc.: 51.03%] [G loss: 0.7021064758300781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 74/86 [D loss: 0.6917431056499481, acc.: 52.10%] [G loss: 0.6954810619354248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 75/86 [D loss: 0.6962086260318756, acc.: 45.70%] [G loss: 0.7023561000823975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 76/86 [D loss: 0.6903335452079773, acc.: 53.71%] [G loss: 0.6967751979827881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 77/86 [D loss: 0.6956096887588501, acc.: 47.31%] [G loss: 0.6988485455513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 78/86 [D loss: 0.6913988590240479, acc.: 53.56%] [G loss: 0.7012142539024353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 79/86 [D loss: 0.6920196413993835, acc.: 52.83%] [G loss: 0.6928287148475647]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 80/86 [D loss: 0.6962957680225372, acc.: 46.53%] [G loss: 0.7019979357719421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 58/200, Batch 81/86 [D loss: 0.6905674636363983, acc.: 54.25%] [G loss: 0.697537899017334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 82/86 [D loss: 0.6940395534038544, acc.: 48.54%] [G loss: 0.6983608603477478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 83/86 [D loss: 0.6908266544342041, acc.: 53.27%] [G loss: 0.7000324130058289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 84/86 [D loss: 0.6920885145664215, acc.: 51.37%] [G loss: 0.6955867409706116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 85/86 [D loss: 0.6935043632984161, acc.: 50.00%] [G loss: 0.699815571308136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 58/200, Batch 86/86 [D loss: 0.691107988357544, acc.: 53.27%] [G loss: 0.7000853419303894]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 1/86 [D loss: 0.6927929222583771, acc.: 49.95%] [G loss: 0.6992015242576599]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 2/86 [D loss: 0.6912829279899597, acc.: 52.59%] [G loss: 0.7007468938827515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 3/86 [D loss: 0.6931628584861755, acc.: 49.32%] [G loss: 0.6985403299331665]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 4/86 [D loss: 0.693198025226593, acc.: 50.10%] [G loss: 0.7018523812294006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 5/86 [D loss: 0.6920468509197235, acc.: 52.25%] [G loss: 0.7010560035705566]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 6/86 [D loss: 0.6925674974918365, acc.: 51.86%] [G loss: 0.7011971473693848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 7/86 [D loss: 0.6913812756538391, acc.: 52.88%] [G loss: 0.701962947845459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 8/86 [D loss: 0.6923801004886627, acc.: 51.32%] [G loss: 0.6999975442886353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 9/86 [D loss: 0.6924032866954803, acc.: 52.83%] [G loss: 0.7029970288276672]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 10/86 [D loss: 0.6919792890548706, acc.: 52.73%] [G loss: 0.7018967270851135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 11/86 [D loss: 0.6923652291297913, acc.: 52.10%] [G loss: 0.7009295225143433]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 12/86 [D loss: 0.6920109689235687, acc.: 52.88%] [G loss: 0.7017381191253662]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 13/86 [D loss: 0.6917981803417206, acc.: 52.59%] [G loss: 0.7015358209609985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 14/86 [D loss: 0.6924050152301788, acc.: 50.78%] [G loss: 0.7010648250579834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 15/86 [D loss: 0.6914332211017609, acc.: 52.69%] [G loss: 0.701080322265625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 16/86 [D loss: 0.6913780272006989, acc.: 53.32%] [G loss: 0.7017567157745361]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 17/86 [D loss: 0.6905447244644165, acc.: 55.52%] [G loss: 0.701487123966217]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 18/86 [D loss: 0.6921147704124451, acc.: 51.76%] [G loss: 0.7018875479698181]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 19/86 [D loss: 0.6913675963878632, acc.: 54.00%] [G loss: 0.7018348574638367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 20/86 [D loss: 0.6920306384563446, acc.: 51.71%] [G loss: 0.7002818584442139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 21/86 [D loss: 0.6920741498470306, acc.: 51.95%] [G loss: 0.7019041776657104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 22/86 [D loss: 0.6929917335510254, acc.: 49.12%] [G loss: 0.7005403637886047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 23/86 [D loss: 0.6915452182292938, acc.: 53.56%] [G loss: 0.7021000385284424]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 24/86 [D loss: 0.691798210144043, acc.: 51.46%] [G loss: 0.7009326815605164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 25/86 [D loss: 0.6905017495155334, acc.: 56.74%] [G loss: 0.7009381055831909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 26/86 [D loss: 0.6918922364711761, acc.: 52.69%] [G loss: 0.7014355063438416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 27/86 [D loss: 0.6906673014163971, acc.: 54.44%] [G loss: 0.700736403465271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 28/86 [D loss: 0.6919840574264526, acc.: 51.46%] [G loss: 0.7005850672721863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 29/86 [D loss: 0.6914555728435516, acc.: 52.93%] [G loss: 0.7009603381156921]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 30/86 [D loss: 0.691301167011261, acc.: 54.00%] [G loss: 0.7000783681869507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 31/86 [D loss: 0.6919935941696167, acc.: 52.25%] [G loss: 0.7008228898048401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 32/86 [D loss: 0.6921206414699554, acc.: 51.46%] [G loss: 0.7005636096000671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 33/86 [D loss: 0.6914290487766266, acc.: 53.81%] [G loss: 0.6998932361602783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 34/86 [D loss: 0.6918632686138153, acc.: 51.66%] [G loss: 0.7010279297828674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 35/86 [D loss: 0.6910730004310608, acc.: 55.08%] [G loss: 0.7009401321411133]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 36/86 [D loss: 0.692112922668457, acc.: 52.69%] [G loss: 0.7003883123397827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 37/86 [D loss: 0.6918549835681915, acc.: 51.12%] [G loss: 0.7003730535507202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 38/86 [D loss: 0.6922094523906708, acc.: 50.49%] [G loss: 0.7009773850440979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 39/86 [D loss: 0.6918116509914398, acc.: 52.93%] [G loss: 0.701041042804718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 40/86 [D loss: 0.6920410692691803, acc.: 51.71%] [G loss: 0.7009183168411255]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 41/86 [D loss: 0.6914712488651276, acc.: 53.22%] [G loss: 0.7018668055534363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 42/86 [D loss: 0.6908657848834991, acc.: 53.76%] [G loss: 0.7001137137413025]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 43/86 [D loss: 0.6925598084926605, acc.: 50.49%] [G loss: 0.7018597722053528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 44/86 [D loss: 0.691991925239563, acc.: 52.83%] [G loss: 0.7009274959564209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 45/86 [D loss: 0.6918081641197205, acc.: 53.37%] [G loss: 0.7000270485877991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 46/86 [D loss: 0.691055029630661, acc.: 53.08%] [G loss: 0.7020940184593201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 47/86 [D loss: 0.6924620270729065, acc.: 51.17%] [G loss: 0.7016569375991821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 48/86 [D loss: 0.6916387379169464, acc.: 52.83%] [G loss: 0.7002638578414917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 49/86 [D loss: 0.6927037835121155, acc.: 50.29%] [G loss: 0.7009290456771851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 50/86 [D loss: 0.6917631030082703, acc.: 51.61%] [G loss: 0.7004677653312683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 51/86 [D loss: 0.6918969750404358, acc.: 52.25%] [G loss: 0.7011194825172424]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 52/86 [D loss: 0.6919532120227814, acc.: 52.54%] [G loss: 0.700499415397644]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 53/86 [D loss: 0.6906558275222778, acc.: 53.81%] [G loss: 0.7019152641296387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 54/86 [D loss: 0.6922744512557983, acc.: 50.59%] [G loss: 0.7014061808586121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 55/86 [D loss: 0.6915164589881897, acc.: 53.12%] [G loss: 0.7012037634849548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 56/86 [D loss: 0.6925613284111023, acc.: 50.73%] [G loss: 0.7014890909194946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 57/86 [D loss: 0.6918662190437317, acc.: 52.78%] [G loss: 0.7005380988121033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 58/86 [D loss: 0.6924692094326019, acc.: 50.93%] [G loss: 0.7011150121688843]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 59/86 [D loss: 0.6919685304164886, acc.: 52.05%] [G loss: 0.701175332069397]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 60/86 [D loss: 0.6920782923698425, acc.: 52.49%] [G loss: 0.7012339234352112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 61/86 [D loss: 0.6911061704158783, acc.: 54.25%] [G loss: 0.7003382444381714]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 62/86 [D loss: 0.6909555196762085, acc.: 54.15%] [G loss: 0.7011773586273193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 63/86 [D loss: 0.6911978125572205, acc.: 53.86%] [G loss: 0.701457679271698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 64/86 [D loss: 0.691849410533905, acc.: 52.00%] [G loss: 0.7016058564186096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 65/86 [D loss: 0.6915201544761658, acc.: 53.37%] [G loss: 0.6999884843826294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 66/86 [D loss: 0.6911408603191376, acc.: 53.32%] [G loss: 0.7015504240989685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 67/86 [D loss: 0.6915484964847565, acc.: 53.03%] [G loss: 0.7026847004890442]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 68/86 [D loss: 0.6914675235748291, acc.: 53.42%] [G loss: 0.7013376951217651]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 69/86 [D loss: 0.6926413476467133, acc.: 50.78%] [G loss: 0.6999326944351196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 70/86 [D loss: 0.6927329897880554, acc.: 51.03%] [G loss: 0.7015717029571533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 71/86 [D loss: 0.6912297904491425, acc.: 52.29%] [G loss: 0.7004154920578003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 72/86 [D loss: 0.6918872892856598, acc.: 53.03%] [G loss: 0.7015116214752197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 73/86 [D loss: 0.6914284825325012, acc.: 52.93%] [G loss: 0.7004795670509338]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 74/86 [D loss: 0.6927314698696136, acc.: 50.15%] [G loss: 0.7007414698600769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 75/86 [D loss: 0.6920954883098602, acc.: 52.49%] [G loss: 0.7013632655143738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 76/86 [D loss: 0.6918728351593018, acc.: 51.86%] [G loss: 0.7008020281791687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 77/86 [D loss: 0.6913387179374695, acc.: 54.44%] [G loss: 0.7009809613227844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 78/86 [D loss: 0.6929041147232056, acc.: 50.73%] [G loss: 0.7002599239349365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 79/86 [D loss: 0.691476434469223, acc.: 53.08%] [G loss: 0.7003786563873291]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 80/86 [D loss: 0.6919814944267273, acc.: 51.12%] [G loss: 0.698300838470459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 81/86 [D loss: 0.6919345855712891, acc.: 52.69%] [G loss: 0.7003501057624817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 82/86 [D loss: 0.691277027130127, acc.: 54.10%] [G loss: 0.7005559802055359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 83/86 [D loss: 0.6939907371997833, acc.: 48.39%] [G loss: 0.698767364025116]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 59/200, Batch 84/86 [D loss: 0.6924081444740295, acc.: 51.27%] [G loss: 0.7016307711601257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 85/86 [D loss: 0.6922549605369568, acc.: 51.37%] [G loss: 0.6983723640441895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 59/200, Batch 86/86 [D loss: 0.6919631361961365, acc.: 52.05%] [G loss: 0.7017742991447449]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 1/86 [D loss: 0.6908872127532959, acc.: 54.44%] [G loss: 0.6965906620025635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 2/86 [D loss: 0.6965278685092926, acc.: 44.87%] [G loss: 0.7008976936340332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 3/86 [D loss: 0.6901655197143555, acc.: 56.10%] [G loss: 0.6985031366348267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 4/86 [D loss: 0.6944200098514557, acc.: 47.85%] [G loss: 0.6985769271850586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 5/86 [D loss: 0.6915479004383087, acc.: 53.37%] [G loss: 0.7010915875434875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 6/86 [D loss: 0.6921513080596924, acc.: 52.88%] [G loss: 0.6935303807258606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 7/86 [D loss: 0.6958163976669312, acc.: 45.70%] [G loss: 0.7021052837371826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 8/86 [D loss: 0.69106724858284, acc.: 54.10%] [G loss: 0.6984018683433533]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 9/86 [D loss: 0.6949985325336456, acc.: 47.22%] [G loss: 0.6978617310523987]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 60/200, Batch 10/86 [D loss: 0.6917197406291962, acc.: 52.29%] [G loss: 0.7013647556304932]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 11/86 [D loss: 0.6929931342601776, acc.: 50.59%] [G loss: 0.6924596428871155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 12/86 [D loss: 0.6957148313522339, acc.: 46.44%] [G loss: 0.7021019458770752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 13/86 [D loss: 0.6911415457725525, acc.: 52.98%] [G loss: 0.6975278854370117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 14/86 [D loss: 0.6941227912902832, acc.: 47.36%] [G loss: 0.7005690932273865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 15/86 [D loss: 0.6921461522579193, acc.: 52.25%] [G loss: 0.701304018497467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 16/86 [D loss: 0.6923093497753143, acc.: 51.42%] [G loss: 0.6961224675178528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 17/86 [D loss: 0.6947159767150879, acc.: 48.05%] [G loss: 0.7013974189758301]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 18/86 [D loss: 0.690687745809555, acc.: 54.74%] [G loss: 0.7006350755691528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 19/86 [D loss: 0.6923514008522034, acc.: 51.76%] [G loss: 0.6977506875991821]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 20/86 [D loss: 0.6917163133621216, acc.: 52.83%] [G loss: 0.7001600861549377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 21/86 [D loss: 0.6929402947425842, acc.: 50.29%] [G loss: 0.6985691785812378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 22/86 [D loss: 0.6926374435424805, acc.: 51.66%] [G loss: 0.7019414901733398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 23/86 [D loss: 0.6914278864860535, acc.: 53.86%] [G loss: 0.6996265053749084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 24/86 [D loss: 0.6918002367019653, acc.: 52.88%] [G loss: 0.7004773020744324]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 25/86 [D loss: 0.6917303502559662, acc.: 52.69%] [G loss: 0.6999608278274536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 26/86 [D loss: 0.6917652189731598, acc.: 52.34%] [G loss: 0.6986702680587769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 27/86 [D loss: 0.6939230263233185, acc.: 49.85%] [G loss: 0.7010862827301025]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 28/86 [D loss: 0.6911624372005463, acc.: 52.83%] [G loss: 0.7005690336227417]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 29/86 [D loss: 0.6925466060638428, acc.: 51.81%] [G loss: 0.7013790011405945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 30/86 [D loss: 0.6930931210517883, acc.: 50.39%] [G loss: 0.7015190124511719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 31/86 [D loss: 0.6924332678318024, acc.: 51.12%] [G loss: 0.7000926733016968]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 32/86 [D loss: 0.6925186216831207, acc.: 52.10%] [G loss: 0.7022719979286194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 33/86 [D loss: 0.6921285688877106, acc.: 51.76%] [G loss: 0.7000330686569214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 34/86 [D loss: 0.6912592351436615, acc.: 53.22%] [G loss: 0.701410174369812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 35/86 [D loss: 0.6916332840919495, acc.: 52.39%] [G loss: 0.7004413604736328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 36/86 [D loss: 0.6923348903656006, acc.: 51.42%] [G loss: 0.7001423239707947]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 37/86 [D loss: 0.6919450759887695, acc.: 52.69%] [G loss: 0.7000604271888733]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 38/86 [D loss: 0.6920391321182251, acc.: 52.44%] [G loss: 0.7003742456436157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 39/86 [D loss: 0.6911729574203491, acc.: 53.96%] [G loss: 0.7016236782073975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 40/86 [D loss: 0.6913900375366211, acc.: 54.39%] [G loss: 0.7008877396583557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 41/86 [D loss: 0.6918224096298218, acc.: 52.98%] [G loss: 0.7012472152709961]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 42/86 [D loss: 0.6920575499534607, acc.: 51.61%] [G loss: 0.7011587619781494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 43/86 [D loss: 0.6907697916030884, acc.: 54.54%] [G loss: 0.7029268145561218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 44/86 [D loss: 0.690956175327301, acc.: 53.66%] [G loss: 0.7005255222320557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 45/86 [D loss: 0.6918438673019409, acc.: 53.27%] [G loss: 0.7013911604881287]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 46/86 [D loss: 0.6919642388820648, acc.: 52.83%] [G loss: 0.699860692024231]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 47/86 [D loss: 0.6910735964775085, acc.: 54.15%] [G loss: 0.7008179426193237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 48/86 [D loss: 0.690700501203537, acc.: 53.81%] [G loss: 0.7002319693565369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 49/86 [D loss: 0.6919994652271271, acc.: 50.93%] [G loss: 0.6998738646507263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 50/86 [D loss: 0.691368818283081, acc.: 52.34%] [G loss: 0.7003337144851685]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 51/86 [D loss: 0.6908420026302338, acc.: 54.35%] [G loss: 0.7005726099014282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 52/86 [D loss: 0.6907392144203186, acc.: 53.56%] [G loss: 0.7016632556915283]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 53/86 [D loss: 0.6917992234230042, acc.: 52.49%] [G loss: 0.7010313272476196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 54/86 [D loss: 0.6912068128585815, acc.: 53.76%] [G loss: 0.7015477418899536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 55/86 [D loss: 0.6922417879104614, acc.: 52.05%] [G loss: 0.6994656920433044]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 56/86 [D loss: 0.69124835729599, acc.: 52.54%] [G loss: 0.7012715935707092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 57/86 [D loss: 0.6917657852172852, acc.: 51.81%] [G loss: 0.7016102075576782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 58/86 [D loss: 0.692516565322876, acc.: 51.32%] [G loss: 0.7013168931007385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 59/86 [D loss: 0.6921040117740631, acc.: 52.15%] [G loss: 0.7021793127059937]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 60/86 [D loss: 0.691826343536377, acc.: 52.29%] [G loss: 0.7005001902580261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 61/86 [D loss: 0.6923411786556244, acc.: 52.54%] [G loss: 0.7004685997962952]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 62/86 [D loss: 0.69161057472229, acc.: 53.52%] [G loss: 0.7014455199241638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 63/86 [D loss: 0.6914813816547394, acc.: 54.05%] [G loss: 0.7016169428825378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 64/86 [D loss: 0.6926827132701874, acc.: 52.15%] [G loss: 0.7001290917396545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 65/86 [D loss: 0.6910706758499146, acc.: 54.10%] [G loss: 0.7003226280212402]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 66/86 [D loss: 0.6911025047302246, acc.: 53.12%] [G loss: 0.7017804384231567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 67/86 [D loss: 0.6907893717288971, acc.: 54.64%] [G loss: 0.700335681438446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 68/86 [D loss: 0.6912487745285034, acc.: 55.18%] [G loss: 0.7001991868019104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 69/86 [D loss: 0.690821498632431, acc.: 52.78%] [G loss: 0.7012172341346741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 70/86 [D loss: 0.6915940046310425, acc.: 53.42%] [G loss: 0.7012474536895752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 71/86 [D loss: 0.6917082667350769, acc.: 53.32%] [G loss: 0.7012911438941956]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 72/86 [D loss: 0.6916100382804871, acc.: 53.52%] [G loss: 0.7013727426528931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 73/86 [D loss: 0.6911076307296753, acc.: 54.05%] [G loss: 0.7001323103904724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 74/86 [D loss: 0.6913966834545135, acc.: 53.22%] [G loss: 0.7006504535675049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 75/86 [D loss: 0.691443920135498, acc.: 52.88%] [G loss: 0.7005000114440918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 76/86 [D loss: 0.6903608739376068, acc.: 54.25%] [G loss: 0.7010184526443481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 77/86 [D loss: 0.6916394233703613, acc.: 52.69%] [G loss: 0.7025257349014282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 78/86 [D loss: 0.6907828152179718, acc.: 53.52%] [G loss: 0.7005884647369385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 79/86 [D loss: 0.6921049356460571, acc.: 52.49%] [G loss: 0.7004129886627197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 80/86 [D loss: 0.6906928420066833, acc.: 54.25%] [G loss: 0.7016739845275879]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 81/86 [D loss: 0.6917119920253754, acc.: 52.44%] [G loss: 0.7011966705322266]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 82/86 [D loss: 0.6912966966629028, acc.: 53.27%] [G loss: 0.7011101245880127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 83/86 [D loss: 0.6919227838516235, acc.: 52.34%] [G loss: 0.7002145051956177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 84/86 [D loss: 0.6921219825744629, acc.: 50.59%] [G loss: 0.7022825479507446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 60/200, Batch 85/86 [D loss: 0.6913251578807831, acc.: 53.12%] [G loss: 0.7011651992797852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 60/200, Batch 86/86 [D loss: 0.6916038691997528, acc.: 52.83%] [G loss: 0.7010033130645752]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 1/86 [D loss: 0.691035270690918, acc.: 53.27%] [G loss: 0.6997450590133667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 2/86 [D loss: 0.6924830377101898, acc.: 50.63%] [G loss: 0.7011709809303284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 3/86 [D loss: 0.6908096075057983, acc.: 54.05%] [G loss: 0.7019748091697693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 4/86 [D loss: 0.691591203212738, acc.: 52.25%] [G loss: 0.6993255615234375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 5/86 [D loss: 0.6914571821689606, acc.: 52.88%] [G loss: 0.699993371963501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 6/86 [D loss: 0.6922202110290527, acc.: 51.51%] [G loss: 0.6998446583747864]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 7/86 [D loss: 0.6913758218288422, acc.: 51.61%] [G loss: 0.7016832232475281]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 8/86 [D loss: 0.6910342276096344, acc.: 53.42%] [G loss: 0.7004922032356262]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 9/86 [D loss: 0.6919639706611633, acc.: 52.54%] [G loss: 0.7020274996757507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 10/86 [D loss: 0.6902344226837158, acc.: 54.93%] [G loss: 0.7001286745071411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 11/86 [D loss: 0.6920630931854248, acc.: 51.12%] [G loss: 0.7015630006790161]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 12/86 [D loss: 0.6909686028957367, acc.: 53.91%] [G loss: 0.700619101524353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 13/86 [D loss: 0.6925390958786011, acc.: 50.34%] [G loss: 0.6996179223060608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 14/86 [D loss: 0.6902226507663727, acc.: 54.49%] [G loss: 0.7019054889678955]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 15/86 [D loss: 0.6916448473930359, acc.: 52.05%] [G loss: 0.6991879343986511]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 16/86 [D loss: 0.6921921074390411, acc.: 50.78%] [G loss: 0.7012987732887268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 17/86 [D loss: 0.6903179883956909, acc.: 55.86%] [G loss: 0.7017276883125305]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 18/86 [D loss: 0.6914845705032349, acc.: 52.88%] [G loss: 0.7028000354766846]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 19/86 [D loss: 0.6909806430339813, acc.: 53.76%] [G loss: 0.7008020281791687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 20/86 [D loss: 0.6918061077594757, acc.: 52.73%] [G loss: 0.7014942169189453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 21/86 [D loss: 0.6918010711669922, acc.: 52.73%] [G loss: 0.7021567821502686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 22/86 [D loss: 0.6920308768749237, acc.: 52.78%] [G loss: 0.7019448280334473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 23/86 [D loss: 0.6910114586353302, acc.: 53.86%] [G loss: 0.7032215595245361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 24/86 [D loss: 0.6906012296676636, acc.: 54.30%] [G loss: 0.7016681432723999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 25/86 [D loss: 0.6915653049945831, acc.: 53.47%] [G loss: 0.7017489075660706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 26/86 [D loss: 0.6917248964309692, acc.: 52.15%] [G loss: 0.701469898223877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 27/86 [D loss: 0.6924427449703217, acc.: 51.17%] [G loss: 0.7005847096443176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 28/86 [D loss: 0.6906901299953461, acc.: 54.88%] [G loss: 0.7007768750190735]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 29/86 [D loss: 0.6915358304977417, acc.: 52.15%] [G loss: 0.7007769346237183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 30/86 [D loss: 0.6920756995677948, acc.: 51.61%] [G loss: 0.7014901638031006]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 31/86 [D loss: 0.6907521486282349, acc.: 54.35%] [G loss: 0.701320469379425]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 32/86 [D loss: 0.6917257308959961, acc.: 54.39%] [G loss: 0.7023464441299438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 33/86 [D loss: 0.6910870969295502, acc.: 53.96%] [G loss: 0.701431393623352]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 34/86 [D loss: 0.6919010579586029, acc.: 53.86%] [G loss: 0.7014690041542053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 35/86 [D loss: 0.6906579434871674, acc.: 54.35%] [G loss: 0.7016512751579285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 36/86 [D loss: 0.6919057369232178, acc.: 52.29%] [G loss: 0.7007281184196472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 37/86 [D loss: 0.6915266215801239, acc.: 51.66%] [G loss: 0.7025482654571533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 38/86 [D loss: 0.6921176016330719, acc.: 52.10%] [G loss: 0.7016103267669678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 39/86 [D loss: 0.6925058960914612, acc.: 50.98%] [G loss: 0.703388512134552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 40/86 [D loss: 0.6913654208183289, acc.: 52.25%] [G loss: 0.7010074853897095]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 41/86 [D loss: 0.6921220421791077, acc.: 52.64%] [G loss: 0.7027412056922913]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 42/86 [D loss: 0.6912926137447357, acc.: 54.10%] [G loss: 0.7030553221702576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 43/86 [D loss: 0.692749410867691, acc.: 51.12%] [G loss: 0.7016737461090088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 44/86 [D loss: 0.6908420324325562, acc.: 54.15%] [G loss: 0.7023705840110779]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 45/86 [D loss: 0.6915276646614075, acc.: 53.12%] [G loss: 0.7034484148025513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 46/86 [D loss: 0.690872848033905, acc.: 53.52%] [G loss: 0.7015298008918762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 47/86 [D loss: 0.6916382014751434, acc.: 53.76%] [G loss: 0.701998233795166]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 61/200, Batch 48/86 [D loss: 0.6916838586330414, acc.: 52.64%] [G loss: 0.7026823163032532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 49/86 [D loss: 0.6913059651851654, acc.: 51.71%] [G loss: 0.7013958692550659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 50/86 [D loss: 0.6925221979618073, acc.: 52.15%] [G loss: 0.7025654315948486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 51/86 [D loss: 0.6910509467124939, acc.: 54.64%] [G loss: 0.6997848749160767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 52/86 [D loss: 0.6928637027740479, acc.: 50.10%] [G loss: 0.7022733092308044]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 53/86 [D loss: 0.6907812654972076, acc.: 54.54%] [G loss: 0.7025794386863708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 54/86 [D loss: 0.6929216086864471, acc.: 48.63%] [G loss: 0.701153576374054]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 55/86 [D loss: 0.6907452046871185, acc.: 53.91%] [G loss: 0.7031928896903992]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 56/86 [D loss: 0.6916824281215668, acc.: 52.15%] [G loss: 0.699661374092102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 57/86 [D loss: 0.6925283968448639, acc.: 51.71%] [G loss: 0.7030267119407654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 58/86 [D loss: 0.6899509429931641, acc.: 54.98%] [G loss: 0.7024760842323303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 59/86 [D loss: 0.6914559602737427, acc.: 54.25%] [G loss: 0.702935516834259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 60/86 [D loss: 0.6919686496257782, acc.: 52.93%] [G loss: 0.7020419836044312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 61/86 [D loss: 0.6925686597824097, acc.: 51.95%] [G loss: 0.7001828551292419]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 62/86 [D loss: 0.6929742097854614, acc.: 50.73%] [G loss: 0.702572226524353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 63/86 [D loss: 0.6911708116531372, acc.: 52.93%] [G loss: 0.7000213861465454]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 64/86 [D loss: 0.6907130777835846, acc.: 53.42%] [G loss: 0.7022466063499451]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 65/86 [D loss: 0.6911202371120453, acc.: 53.08%] [G loss: 0.7008187174797058]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 66/86 [D loss: 0.6916249990463257, acc.: 52.83%] [G loss: 0.7031878232955933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 67/86 [D loss: 0.6908125281333923, acc.: 55.08%] [G loss: 0.7024039626121521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 68/86 [D loss: 0.6919987201690674, acc.: 52.34%] [G loss: 0.7018200159072876]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 69/86 [D loss: 0.6909960806369781, acc.: 53.91%] [G loss: 0.7020570039749146]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 70/86 [D loss: 0.6902738809585571, acc.: 55.47%] [G loss: 0.6999176144599915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 71/86 [D loss: 0.6921623647212982, acc.: 52.49%] [G loss: 0.7015583515167236]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 72/86 [D loss: 0.6904972493648529, acc.: 53.03%] [G loss: 0.7032620310783386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 73/86 [D loss: 0.6901695132255554, acc.: 54.74%] [G loss: 0.7027515769004822]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 74/86 [D loss: 0.6906754374504089, acc.: 54.00%] [G loss: 0.7011932134628296]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 75/86 [D loss: 0.6915245056152344, acc.: 54.49%] [G loss: 0.7009560465812683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 76/86 [D loss: 0.6904561817646027, acc.: 54.49%] [G loss: 0.7028318047523499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 77/86 [D loss: 0.6902782618999481, acc.: 54.98%] [G loss: 0.7020000219345093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 78/86 [D loss: 0.6903685629367828, acc.: 54.59%] [G loss: 0.7017738819122314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 79/86 [D loss: 0.6931208372116089, acc.: 48.29%] [G loss: 0.7006776928901672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 80/86 [D loss: 0.6909549236297607, acc.: 55.08%] [G loss: 0.7033174633979797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 81/86 [D loss: 0.6906294822692871, acc.: 54.83%] [G loss: 0.702311635017395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 82/86 [D loss: 0.6916100084781647, acc.: 53.37%] [G loss: 0.7024216651916504]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 83/86 [D loss: 0.6902881264686584, acc.: 56.69%] [G loss: 0.6998945474624634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 84/86 [D loss: 0.6920528709888458, acc.: 51.61%] [G loss: 0.7016520500183105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 61/200, Batch 85/86 [D loss: 0.6897028982639313, acc.: 55.47%] [G loss: 0.7009363770484924]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 61/200, Batch 86/86 [D loss: 0.6928845047950745, acc.: 50.29%] [G loss: 0.7009934186935425]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 1/86 [D loss: 0.6912132501602173, acc.: 52.88%] [G loss: 0.7024239301681519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 2/86 [D loss: 0.6920547485351562, acc.: 53.17%] [G loss: 0.6987787485122681]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 3/86 [D loss: 0.6913330256938934, acc.: 54.25%] [G loss: 0.7022199630737305]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 4/86 [D loss: 0.691762238740921, acc.: 52.20%] [G loss: 0.7011079788208008]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 5/86 [D loss: 0.6916368007659912, acc.: 52.78%] [G loss: 0.7030500173568726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 6/86 [D loss: 0.6911097168922424, acc.: 53.71%] [G loss: 0.7001709938049316]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 7/86 [D loss: 0.6919925510883331, acc.: 52.00%] [G loss: 0.7014930844306946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 8/86 [D loss: 0.6900854110717773, acc.: 54.79%] [G loss: 0.7025642395019531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 9/86 [D loss: 0.6910629272460938, acc.: 53.37%] [G loss: 0.6999716758728027]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 10/86 [D loss: 0.6912862360477448, acc.: 53.71%] [G loss: 0.702669620513916]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 11/86 [D loss: 0.6915354132652283, acc.: 53.32%] [G loss: 0.6979621052742004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 12/86 [D loss: 0.6939098536968231, acc.: 48.88%] [G loss: 0.7032787799835205]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 13/86 [D loss: 0.6901432573795319, acc.: 54.30%] [G loss: 0.7005355954170227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 14/86 [D loss: 0.6920775175094604, acc.: 50.88%] [G loss: 0.7029844522476196]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 15/86 [D loss: 0.6903049945831299, acc.: 55.13%] [G loss: 0.7020950317382812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 16/86 [D loss: 0.6920397281646729, acc.: 53.17%] [G loss: 0.7007675170898438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 17/86 [D loss: 0.6923741698265076, acc.: 51.03%] [G loss: 0.7026662826538086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 18/86 [D loss: 0.6903786063194275, acc.: 54.44%] [G loss: 0.7007538676261902]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 19/86 [D loss: 0.6925466656684875, acc.: 50.68%] [G loss: 0.7014754414558411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 20/86 [D loss: 0.6903774440288544, acc.: 54.54%] [G loss: 0.6998050212860107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 21/86 [D loss: 0.69302898645401, acc.: 50.20%] [G loss: 0.7010558843612671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 22/86 [D loss: 0.6910740435123444, acc.: 54.20%] [G loss: 0.7005370259284973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 23/86 [D loss: 0.6914868950843811, acc.: 53.27%] [G loss: 0.700589120388031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 24/86 [D loss: 0.6903611719608307, acc.: 54.20%] [G loss: 0.7017804384231567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 25/86 [D loss: 0.6918553113937378, acc.: 52.59%] [G loss: 0.6994630694389343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 26/86 [D loss: 0.6914520263671875, acc.: 53.08%] [G loss: 0.703079879283905]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 27/86 [D loss: 0.6908799111843109, acc.: 52.98%] [G loss: 0.7003146409988403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 28/86 [D loss: 0.6922559440135956, acc.: 52.54%] [G loss: 0.7017648220062256]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 29/86 [D loss: 0.6900331377983093, acc.: 54.93%] [G loss: 0.7027369737625122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 30/86 [D loss: 0.6925102472305298, acc.: 51.17%] [G loss: 0.7005541324615479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 31/86 [D loss: 0.6910626590251923, acc.: 52.98%] [G loss: 0.7029427289962769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 32/86 [D loss: 0.6919242739677429, acc.: 50.98%] [G loss: 0.7001579999923706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 33/86 [D loss: 0.6909256875514984, acc.: 53.81%] [G loss: 0.7040024399757385]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 34/86 [D loss: 0.6916559934616089, acc.: 53.37%] [G loss: 0.698562502861023]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 35/86 [D loss: 0.6930062174797058, acc.: 49.17%] [G loss: 0.7031416296958923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 36/86 [D loss: 0.6905115842819214, acc.: 54.30%] [G loss: 0.7023470997810364]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 37/86 [D loss: 0.6921003758907318, acc.: 52.10%] [G loss: 0.7018171548843384]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 38/86 [D loss: 0.6907556354999542, acc.: 53.86%] [G loss: 0.7020233273506165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 39/86 [D loss: 0.6922956109046936, acc.: 51.22%] [G loss: 0.6987136602401733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 40/86 [D loss: 0.6917255520820618, acc.: 52.29%] [G loss: 0.7029092907905579]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 41/86 [D loss: 0.6919744312763214, acc.: 50.49%] [G loss: 0.7006993889808655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 42/86 [D loss: 0.6910735070705414, acc.: 52.54%] [G loss: 0.7028483152389526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 43/86 [D loss: 0.6896034777164459, acc.: 55.03%] [G loss: 0.7001233100891113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 44/86 [D loss: 0.6946160197257996, acc.: 48.00%] [G loss: 0.7009433507919312]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 45/86 [D loss: 0.6904411315917969, acc.: 54.00%] [G loss: 0.7027595043182373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 46/86 [D loss: 0.6906404793262482, acc.: 53.37%] [G loss: 0.7008522152900696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 47/86 [D loss: 0.6904803514480591, acc.: 54.69%] [G loss: 0.7044460773468018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 48/86 [D loss: 0.6907431483268738, acc.: 54.64%] [G loss: 0.6994760036468506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 49/86 [D loss: 0.6934385895729065, acc.: 49.71%] [G loss: 0.702321469783783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 50/86 [D loss: 0.6900371313095093, acc.: 55.52%] [G loss: 0.7015014290809631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 51/86 [D loss: 0.6912566125392914, acc.: 51.76%] [G loss: 0.7013673782348633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 52/86 [D loss: 0.6908504664897919, acc.: 53.76%] [G loss: 0.7020546793937683]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 53/86 [D loss: 0.6910638213157654, acc.: 51.76%] [G loss: 0.7024651169776917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 54/86 [D loss: 0.6913895308971405, acc.: 52.69%] [G loss: 0.703093409538269]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 55/86 [D loss: 0.6897200644016266, acc.: 56.79%] [G loss: 0.702949583530426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 56/86 [D loss: 0.6905918419361115, acc.: 55.13%] [G loss: 0.7011644840240479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 57/86 [D loss: 0.6904284358024597, acc.: 54.10%] [G loss: 0.7007162570953369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 58/86 [D loss: 0.6913520693778992, acc.: 52.69%] [G loss: 0.7026866674423218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 59/86 [D loss: 0.6901472210884094, acc.: 55.62%] [G loss: 0.7027578353881836]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 60/86 [D loss: 0.6915391087532043, acc.: 52.29%] [G loss: 0.7031629681587219]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 61/86 [D loss: 0.6911546885967255, acc.: 52.73%] [G loss: 0.7033874988555908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 62/86 [D loss: 0.6903047263622284, acc.: 53.32%] [G loss: 0.7008350491523743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 63/86 [D loss: 0.6918901801109314, acc.: 51.86%] [G loss: 0.7040846347808838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 64/86 [D loss: 0.6906362473964691, acc.: 54.69%] [G loss: 0.7028328776359558]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 65/86 [D loss: 0.6904776990413666, acc.: 54.44%] [G loss: 0.7030820250511169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 66/86 [D loss: 0.6908056139945984, acc.: 53.56%] [G loss: 0.7023575901985168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 67/86 [D loss: 0.6911255121231079, acc.: 52.73%] [G loss: 0.702026903629303]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 68/86 [D loss: 0.6914196908473969, acc.: 52.59%] [G loss: 0.7037636041641235]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 69/86 [D loss: 0.6904963850975037, acc.: 54.35%] [G loss: 0.7013487815856934]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 70/86 [D loss: 0.6912983655929565, acc.: 53.17%] [G loss: 0.702235221862793]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 71/86 [D loss: 0.6903831958770752, acc.: 53.96%] [G loss: 0.7029768228530884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 72/86 [D loss: 0.6917727589607239, acc.: 52.39%] [G loss: 0.7029245495796204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 73/86 [D loss: 0.691547155380249, acc.: 52.73%] [G loss: 0.7010260224342346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 74/86 [D loss: 0.691586047410965, acc.: 52.64%] [G loss: 0.7014380693435669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 75/86 [D loss: 0.6907158493995667, acc.: 53.66%] [G loss: 0.702212393283844]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 76/86 [D loss: 0.6912446916103363, acc.: 52.88%] [G loss: 0.7033846378326416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 77/86 [D loss: 0.6916723847389221, acc.: 53.47%] [G loss: 0.7047812342643738]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 78/86 [D loss: 0.6911381185054779, acc.: 52.69%] [G loss: 0.7037780284881592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 79/86 [D loss: 0.6906281113624573, acc.: 55.08%] [G loss: 0.7038979530334473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 80/86 [D loss: 0.6908402442932129, acc.: 54.20%] [G loss: 0.701977550983429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 81/86 [D loss: 0.6906238496303558, acc.: 54.05%] [G loss: 0.7020829916000366]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 82/86 [D loss: 0.689816415309906, acc.: 54.30%] [G loss: 0.702987551689148]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 83/86 [D loss: 0.6908550262451172, acc.: 53.56%] [G loss: 0.7033171653747559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 84/86 [D loss: 0.6906421780586243, acc.: 53.81%] [G loss: 0.7037380337715149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 62/200, Batch 85/86 [D loss: 0.6901068985462189, acc.: 55.08%] [G loss: 0.7020900249481201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 62/200, Batch 86/86 [D loss: 0.6915982961654663, acc.: 52.54%] [G loss: 0.7018447518348694]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 1/86 [D loss: 0.6902818381786346, acc.: 56.30%] [G loss: 0.7023820877075195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 2/86 [D loss: 0.6914492547512054, acc.: 51.81%] [G loss: 0.702363133430481]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 3/86 [D loss: 0.6896733641624451, acc.: 55.71%] [G loss: 0.7010219097137451]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 4/86 [D loss: 0.6911121308803558, acc.: 55.13%] [G loss: 0.7011383175849915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 5/86 [D loss: 0.6899325251579285, acc.: 54.98%] [G loss: 0.7030094265937805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 6/86 [D loss: 0.6909945607185364, acc.: 54.44%] [G loss: 0.700377345085144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 7/86 [D loss: 0.6904652416706085, acc.: 53.91%] [G loss: 0.7019644975662231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 8/86 [D loss: 0.6907105445861816, acc.: 53.56%] [G loss: 0.7031722664833069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 9/86 [D loss: 0.6893320083618164, acc.: 56.40%] [G loss: 0.7017567157745361]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 10/86 [D loss: 0.6904000043869019, acc.: 54.25%] [G loss: 0.7031300067901611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 11/86 [D loss: 0.6910623610019684, acc.: 53.08%] [G loss: 0.7032173275947571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 12/86 [D loss: 0.6899591982364655, acc.: 54.83%] [G loss: 0.7028021216392517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 13/86 [D loss: 0.6901134848594666, acc.: 54.15%] [G loss: 0.7002272605895996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 14/86 [D loss: 0.6920533180236816, acc.: 51.03%] [G loss: 0.7036845088005066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 15/86 [D loss: 0.6905356347560883, acc.: 54.59%] [G loss: 0.7010929584503174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 16/86 [D loss: 0.6921526193618774, acc.: 51.12%] [G loss: 0.7020261287689209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 17/86 [D loss: 0.6909946203231812, acc.: 54.20%] [G loss: 0.6996886730194092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 18/86 [D loss: 0.6930881142616272, acc.: 50.05%] [G loss: 0.7020685076713562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 19/86 [D loss: 0.6912005841732025, acc.: 54.20%] [G loss: 0.701370358467102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 20/86 [D loss: 0.691706508398056, acc.: 54.25%] [G loss: 0.7010684013366699]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 21/86 [D loss: 0.6911763250827789, acc.: 52.49%] [G loss: 0.704415500164032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 22/86 [D loss: 0.690278947353363, acc.: 54.35%] [G loss: 0.6976653337478638]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 23/86 [D loss: 0.6938828825950623, acc.: 49.12%] [G loss: 0.7027973532676697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 24/86 [D loss: 0.6896713376045227, acc.: 54.44%] [G loss: 0.6993805170059204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 25/86 [D loss: 0.693914383649826, acc.: 48.73%] [G loss: 0.700394868850708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 26/86 [D loss: 0.6902368664741516, acc.: 53.91%] [G loss: 0.7030859589576721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 27/86 [D loss: 0.6925369799137115, acc.: 50.73%] [G loss: 0.6957552433013916]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 28/86 [D loss: 0.6935511827468872, acc.: 48.14%] [G loss: 0.7043879628181458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 29/86 [D loss: 0.691089391708374, acc.: 53.52%] [G loss: 0.7001644372940063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 30/86 [D loss: 0.6932976245880127, acc.: 48.88%] [G loss: 0.7011115550994873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 31/86 [D loss: 0.6900475323200226, acc.: 54.59%] [G loss: 0.7019956111907959]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 32/86 [D loss: 0.6920261681079865, acc.: 52.29%] [G loss: 0.6977929472923279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 33/86 [D loss: 0.6923687756061554, acc.: 51.12%] [G loss: 0.7034906148910522]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 34/86 [D loss: 0.68924680352211, acc.: 56.84%] [G loss: 0.7002571225166321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 35/86 [D loss: 0.6921006739139557, acc.: 52.34%] [G loss: 0.7019814848899841]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 36/86 [D loss: 0.6904098689556122, acc.: 54.98%] [G loss: 0.7021803855895996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 37/86 [D loss: 0.6908621490001678, acc.: 54.98%] [G loss: 0.7012089490890503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 38/86 [D loss: 0.6910282075405121, acc.: 53.08%] [G loss: 0.7030455470085144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 39/86 [D loss: 0.6910800337791443, acc.: 52.83%] [G loss: 0.7004615664482117]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 40/86 [D loss: 0.690824031829834, acc.: 53.17%] [G loss: 0.703322172164917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 41/86 [D loss: 0.6893658638000488, acc.: 56.05%] [G loss: 0.701524019241333]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 42/86 [D loss: 0.6905859708786011, acc.: 54.00%] [G loss: 0.701424241065979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 43/86 [D loss: 0.6890514492988586, acc.: 55.32%] [G loss: 0.7035208344459534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 44/86 [D loss: 0.6915569305419922, acc.: 53.03%] [G loss: 0.7005279064178467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 45/86 [D loss: 0.6915760338306427, acc.: 52.10%] [G loss: 0.7034210562705994]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 46/86 [D loss: 0.6900082230567932, acc.: 54.35%] [G loss: 0.7012673020362854]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 47/86 [D loss: 0.6930851638317108, acc.: 49.85%] [G loss: 0.7031610012054443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 48/86 [D loss: 0.6889406144618988, acc.: 56.79%] [G loss: 0.7025344371795654]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 49/86 [D loss: 0.6917368769645691, acc.: 52.78%] [G loss: 0.7020785212516785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 50/86 [D loss: 0.6904899179935455, acc.: 54.05%] [G loss: 0.70335853099823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 51/86 [D loss: 0.6913686990737915, acc.: 54.59%] [G loss: 0.7013905644416809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 52/86 [D loss: 0.6928216516971588, acc.: 51.56%] [G loss: 0.7030823230743408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 53/86 [D loss: 0.6897165477275848, acc.: 55.91%] [G loss: 0.7023012638092041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 54/86 [D loss: 0.690424919128418, acc.: 54.98%] [G loss: 0.7024358510971069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 55/86 [D loss: 0.6892827749252319, acc.: 54.79%] [G loss: 0.7032803297042847]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 56/86 [D loss: 0.6910524964332581, acc.: 54.00%] [G loss: 0.7019768357276917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 57/86 [D loss: 0.6914622783660889, acc.: 52.83%] [G loss: 0.7044522762298584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 58/86 [D loss: 0.6902240812778473, acc.: 54.44%] [G loss: 0.7029048204421997]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 59/86 [D loss: 0.6907013058662415, acc.: 53.71%] [G loss: 0.7035309672355652]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 60/86 [D loss: 0.6892602145671844, acc.: 57.13%] [G loss: 0.7023058533668518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 61/86 [D loss: 0.6925375759601593, acc.: 51.90%] [G loss: 0.7027970552444458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 62/86 [D loss: 0.6909164190292358, acc.: 54.74%] [G loss: 0.7040098309516907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 63/86 [D loss: 0.6901001334190369, acc.: 54.54%] [G loss: 0.7024137377738953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 64/86 [D loss: 0.6912785172462463, acc.: 53.37%] [G loss: 0.7037616968154907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 65/86 [D loss: 0.6915613412857056, acc.: 53.86%] [G loss: 0.7007454037666321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 66/86 [D loss: 0.6908924281597137, acc.: 54.74%] [G loss: 0.7041083574295044]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 67/86 [D loss: 0.6908900737762451, acc.: 54.39%] [G loss: 0.7011457681655884]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 68/86 [D loss: 0.6915289163589478, acc.: 53.17%] [G loss: 0.7027813196182251]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 69/86 [D loss: 0.6904104351997375, acc.: 54.25%] [G loss: 0.7028253078460693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 70/86 [D loss: 0.6905816495418549, acc.: 53.76%] [G loss: 0.7032201290130615]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 71/86 [D loss: 0.6913994252681732, acc.: 52.20%] [G loss: 0.7045589685440063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 72/86 [D loss: 0.6904097497463226, acc.: 53.86%] [G loss: 0.7041620016098022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 73/86 [D loss: 0.690739631652832, acc.: 53.42%] [G loss: 0.7028122544288635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 74/86 [D loss: 0.6906458139419556, acc.: 54.05%] [G loss: 0.7019262313842773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 75/86 [D loss: 0.6908901631832123, acc.: 54.49%] [G loss: 0.7018164396286011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 76/86 [D loss: 0.6899729669094086, acc.: 54.59%] [G loss: 0.7019509673118591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 77/86 [D loss: 0.6915725469589233, acc.: 53.22%] [G loss: 0.7030978202819824]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 78/86 [D loss: 0.6906533241271973, acc.: 53.27%] [G loss: 0.7029986381530762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 79/86 [D loss: 0.6910312473773956, acc.: 52.69%] [G loss: 0.7014975547790527]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 80/86 [D loss: 0.6912619173526764, acc.: 53.22%] [G loss: 0.7034431099891663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 81/86 [D loss: 0.689927875995636, acc.: 54.00%] [G loss: 0.7022904753684998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 82/86 [D loss: 0.6931358873844147, acc.: 49.61%] [G loss: 0.7030596137046814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 83/86 [D loss: 0.68979412317276, acc.: 54.44%] [G loss: 0.7022265195846558]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 84/86 [D loss: 0.6920983195304871, acc.: 51.12%] [G loss: 0.7003492712974548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 63/200, Batch 85/86 [D loss: 0.6908731758594513, acc.: 54.00%] [G loss: 0.7024421691894531]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 63/200, Batch 86/86 [D loss: 0.6914777159690857, acc.: 53.03%] [G loss: 0.7007806897163391]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 1/86 [D loss: 0.6912548840045929, acc.: 53.32%] [G loss: 0.7031345963478088]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 2/86 [D loss: 0.6916759312152863, acc.: 53.03%] [G loss: 0.7004832625389099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 3/86 [D loss: 0.6901910305023193, acc.: 54.69%] [G loss: 0.7016447186470032]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 4/86 [D loss: 0.6910216212272644, acc.: 52.05%] [G loss: 0.7024515867233276]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 5/86 [D loss: 0.6916002929210663, acc.: 52.00%] [G loss: 0.7020928263664246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 6/86 [D loss: 0.6898097395896912, acc.: 54.35%] [G loss: 0.7027893662452698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 7/86 [D loss: 0.6916109621524811, acc.: 52.69%] [G loss: 0.7025360465049744]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 8/86 [D loss: 0.6911728382110596, acc.: 52.49%] [G loss: 0.7047130465507507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 9/86 [D loss: 0.691369354724884, acc.: 52.73%] [G loss: 0.7026783227920532]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 10/86 [D loss: 0.6910068392753601, acc.: 52.83%] [G loss: 0.703381359577179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 11/86 [D loss: 0.6895908713340759, acc.: 55.81%] [G loss: 0.7027324438095093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 12/86 [D loss: 0.6921135187149048, acc.: 52.29%] [G loss: 0.7034724354743958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 13/86 [D loss: 0.6905032098293304, acc.: 54.30%] [G loss: 0.701211154460907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 14/86 [D loss: 0.6913548111915588, acc.: 53.81%] [G loss: 0.7017639875411987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 15/86 [D loss: 0.6905357539653778, acc.: 52.88%] [G loss: 0.702697217464447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 16/86 [D loss: 0.6904623508453369, acc.: 54.00%] [G loss: 0.7001189589500427]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 17/86 [D loss: 0.691894918680191, acc.: 51.95%] [G loss: 0.7033376693725586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 18/86 [D loss: 0.6906826496124268, acc.: 53.42%] [G loss: 0.7041105031967163]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 19/86 [D loss: 0.6926611363887787, acc.: 49.80%] [G loss: 0.7029435634613037]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 20/86 [D loss: 0.690488874912262, acc.: 54.98%] [G loss: 0.7032115459442139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 21/86 [D loss: 0.6900152564048767, acc.: 54.98%] [G loss: 0.7001050710678101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 22/86 [D loss: 0.6931900084018707, acc.: 50.34%] [G loss: 0.704504132270813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 23/86 [D loss: 0.6888924837112427, acc.: 56.88%] [G loss: 0.702296257019043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 24/86 [D loss: 0.6911949217319489, acc.: 50.78%] [G loss: 0.6998474597930908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 25/86 [D loss: 0.6892931163311005, acc.: 55.32%] [G loss: 0.701675534248352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 26/86 [D loss: 0.6916267275810242, acc.: 52.10%] [G loss: 0.7005856037139893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 27/86 [D loss: 0.6920267343521118, acc.: 52.34%] [G loss: 0.7025269269943237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 28/86 [D loss: 0.6921653151512146, acc.: 52.44%] [G loss: 0.6987542510032654]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 29/86 [D loss: 0.6914490461349487, acc.: 53.22%] [G loss: 0.7036619782447815]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 30/86 [D loss: 0.6904315948486328, acc.: 53.71%] [G loss: 0.6995428204536438]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 31/86 [D loss: 0.6918433308601379, acc.: 51.51%] [G loss: 0.7003850340843201]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 32/86 [D loss: 0.6898564696311951, acc.: 54.64%] [G loss: 0.7018964290618896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 33/86 [D loss: 0.6909808218479156, acc.: 53.17%] [G loss: 0.6996797919273376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 34/86 [D loss: 0.6909147799015045, acc.: 52.15%] [G loss: 0.7022022008895874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 35/86 [D loss: 0.6907863914966583, acc.: 53.42%] [G loss: 0.7016098499298096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 36/86 [D loss: 0.6933749616146088, acc.: 49.17%] [G loss: 0.7015208005905151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 37/86 [D loss: 0.6902258098125458, acc.: 55.96%] [G loss: 0.7010095119476318]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 38/86 [D loss: 0.6925833225250244, acc.: 50.44%] [G loss: 0.6989583969116211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 39/86 [D loss: 0.6915597021579742, acc.: 52.83%] [G loss: 0.70380699634552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 40/86 [D loss: 0.6899979710578918, acc.: 55.52%] [G loss: 0.6978957056999207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 41/86 [D loss: 0.6939413249492645, acc.: 49.07%] [G loss: 0.7037515640258789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 42/86 [D loss: 0.6899072527885437, acc.: 54.30%] [G loss: 0.701927900314331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 43/86 [D loss: 0.6927241384983063, acc.: 51.71%] [G loss: 0.7009909152984619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 44/86 [D loss: 0.6913301050662994, acc.: 51.12%] [G loss: 0.7027223706245422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 45/86 [D loss: 0.691235363483429, acc.: 54.10%] [G loss: 0.7004073262214661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 46/86 [D loss: 0.6929974853992462, acc.: 49.07%] [G loss: 0.7021584510803223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 47/86 [D loss: 0.6908012628555298, acc.: 54.88%] [G loss: 0.7031134963035583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 48/86 [D loss: 0.6908825635910034, acc.: 53.61%] [G loss: 0.6999477744102478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 49/86 [D loss: 0.6905501782894135, acc.: 54.83%] [G loss: 0.7030072212219238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 50/86 [D loss: 0.6903237402439117, acc.: 54.44%] [G loss: 0.6981713771820068]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 51/86 [D loss: 0.6930366158485413, acc.: 50.05%] [G loss: 0.7031074166297913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 52/86 [D loss: 0.6895497739315033, acc.: 56.05%] [G loss: 0.7013013362884521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 53/86 [D loss: 0.6914963126182556, acc.: 51.95%] [G loss: 0.7009369134902954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 54/86 [D loss: 0.6909558475017548, acc.: 54.35%] [G loss: 0.7042964100837708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 55/86 [D loss: 0.6906830370426178, acc.: 54.30%] [G loss: 0.6984268426895142]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 56/86 [D loss: 0.6934797167778015, acc.: 48.68%] [G loss: 0.7033518552780151]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 57/86 [D loss: 0.689555823802948, acc.: 55.57%] [G loss: 0.7021040320396423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 58/86 [D loss: 0.6915283501148224, acc.: 52.05%] [G loss: 0.7022047638893127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 59/86 [D loss: 0.6903248429298401, acc.: 54.00%] [G loss: 0.7035283446311951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 60/86 [D loss: 0.6905122101306915, acc.: 54.64%] [G loss: 0.7000104188919067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 61/86 [D loss: 0.6935847699642181, acc.: 50.20%] [G loss: 0.7030664086341858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 62/86 [D loss: 0.6901748180389404, acc.: 55.37%] [G loss: 0.7013643980026245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 63/86 [D loss: 0.6911986470222473, acc.: 51.76%] [G loss: 0.6994609236717224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 64/86 [D loss: 0.6916102170944214, acc.: 51.76%] [G loss: 0.7027891874313354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 65/86 [D loss: 0.690485954284668, acc.: 54.83%] [G loss: 0.7005111575126648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 66/86 [D loss: 0.6929861009120941, acc.: 50.68%] [G loss: 0.7024509906768799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 67/86 [D loss: 0.6900346577167511, acc.: 54.98%] [G loss: 0.703323245048523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 68/86 [D loss: 0.6926696002483368, acc.: 49.71%] [G loss: 0.7020322680473328]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 69/86 [D loss: 0.6908693313598633, acc.: 53.76%] [G loss: 0.7041756510734558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 70/86 [D loss: 0.6906836032867432, acc.: 53.56%] [G loss: 0.7018190622329712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 71/86 [D loss: 0.6926918923854828, acc.: 50.59%] [G loss: 0.7024245262145996]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 72/86 [D loss: 0.6892388463020325, acc.: 56.01%] [G loss: 0.702693521976471]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 73/86 [D loss: 0.6918165683746338, acc.: 52.73%] [G loss: 0.7018487453460693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 74/86 [D loss: 0.6904414594173431, acc.: 53.91%] [G loss: 0.7050811052322388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 75/86 [D loss: 0.6904888451099396, acc.: 55.03%] [G loss: 0.7005341053009033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 76/86 [D loss: 0.691432535648346, acc.: 53.61%] [G loss: 0.702243447303772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 77/86 [D loss: 0.6903993785381317, acc.: 55.62%] [G loss: 0.7021746635437012]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 78/86 [D loss: 0.6904816925525665, acc.: 52.93%] [G loss: 0.7007825374603271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 79/86 [D loss: 0.6904085576534271, acc.: 54.44%] [G loss: 0.7021651864051819]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 80/86 [D loss: 0.6905089318752289, acc.: 53.76%] [G loss: 0.7006865739822388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 81/86 [D loss: 0.6921390295028687, acc.: 52.59%] [G loss: 0.7029218673706055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 82/86 [D loss: 0.6893050074577332, acc.: 55.86%] [G loss: 0.7028465270996094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 64/200, Batch 83/86 [D loss: 0.6917000114917755, acc.: 52.69%] [G loss: 0.7023395299911499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 84/86 [D loss: 0.6908072233200073, acc.: 52.93%] [G loss: 0.7045203447341919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 85/86 [D loss: 0.6909907460212708, acc.: 53.17%] [G loss: 0.7009958624839783]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 64/200, Batch 86/86 [D loss: 0.6914319396018982, acc.: 52.44%] [G loss: 0.7041462063789368]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 1/86 [D loss: 0.6899511516094208, acc.: 56.01%] [G loss: 0.703278124332428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 2/86 [D loss: 0.6911723613739014, acc.: 53.52%] [G loss: 0.7019320130348206]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 3/86 [D loss: 0.6910627484321594, acc.: 51.76%] [G loss: 0.7024059891700745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 4/86 [D loss: 0.6914842128753662, acc.: 53.22%] [G loss: 0.70171719789505]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 5/86 [D loss: 0.6908561885356903, acc.: 53.96%] [G loss: 0.7027177810668945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 6/86 [D loss: 0.690405011177063, acc.: 54.49%] [G loss: 0.7034841775894165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 7/86 [D loss: 0.6905451118946075, acc.: 54.83%] [G loss: 0.7015324831008911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 8/86 [D loss: 0.6900893151760101, acc.: 54.10%] [G loss: 0.7003961205482483]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 9/86 [D loss: 0.6900409162044525, acc.: 55.76%] [G loss: 0.702630877494812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 10/86 [D loss: 0.6899188160896301, acc.: 54.49%] [G loss: 0.7021473050117493]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 11/86 [D loss: 0.691199004650116, acc.: 53.17%] [G loss: 0.7018903493881226]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 12/86 [D loss: 0.6899917721748352, acc.: 55.66%] [G loss: 0.7043505311012268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 13/86 [D loss: 0.690966784954071, acc.: 53.66%] [G loss: 0.7013399004936218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 14/86 [D loss: 0.692070871591568, acc.: 51.86%] [G loss: 0.7043678760528564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 15/86 [D loss: 0.6899605989456177, acc.: 54.88%] [G loss: 0.7033089995384216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 16/86 [D loss: 0.6908164322376251, acc.: 52.69%] [G loss: 0.7043384313583374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 17/86 [D loss: 0.6892062723636627, acc.: 55.76%] [G loss: 0.7037654519081116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 18/86 [D loss: 0.6920190155506134, acc.: 52.83%] [G loss: 0.702499270439148]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 19/86 [D loss: 0.6897779703140259, acc.: 56.01%] [G loss: 0.7039909362792969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 20/86 [D loss: 0.6908360123634338, acc.: 52.29%] [G loss: 0.702724277973175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 21/86 [D loss: 0.690291166305542, acc.: 54.10%] [G loss: 0.7051628828048706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 22/86 [D loss: 0.6908069252967834, acc.: 53.96%] [G loss: 0.7015467286109924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 23/86 [D loss: 0.692142128944397, acc.: 51.32%] [G loss: 0.7021851539611816]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 24/86 [D loss: 0.6900230646133423, acc.: 54.64%] [G loss: 0.7029451727867126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 25/86 [D loss: 0.6905223429203033, acc.: 53.81%] [G loss: 0.7032101154327393]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 26/86 [D loss: 0.6901713907718658, acc.: 54.64%] [G loss: 0.7031911015510559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 27/86 [D loss: 0.69015172123909, acc.: 54.20%] [G loss: 0.7018235325813293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 28/86 [D loss: 0.6907857656478882, acc.: 54.15%] [G loss: 0.7037129998207092]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 29/86 [D loss: 0.6897219717502594, acc.: 54.83%] [G loss: 0.7038959264755249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 30/86 [D loss: 0.6903464794158936, acc.: 54.39%] [G loss: 0.7041046619415283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 31/86 [D loss: 0.6914927959442139, acc.: 53.12%] [G loss: 0.7032490968704224]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 32/86 [D loss: 0.6922263205051422, acc.: 51.42%] [G loss: 0.7026950120925903]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 33/86 [D loss: 0.691564679145813, acc.: 53.32%] [G loss: 0.703533411026001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 34/86 [D loss: 0.6905433833599091, acc.: 54.35%] [G loss: 0.7011465430259705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 35/86 [D loss: 0.6919166743755341, acc.: 52.20%] [G loss: 0.7042589783668518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 36/86 [D loss: 0.689520537853241, acc.: 56.01%] [G loss: 0.7029026746749878]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 37/86 [D loss: 0.6926555037498474, acc.: 50.05%] [G loss: 0.7008471488952637]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 38/86 [D loss: 0.6897926330566406, acc.: 55.22%] [G loss: 0.7036649584770203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 39/86 [D loss: 0.6919410526752472, acc.: 51.95%] [G loss: 0.7034939527511597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 40/86 [D loss: 0.6909255683422089, acc.: 52.25%] [G loss: 0.703407883644104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 41/86 [D loss: 0.6901271343231201, acc.: 54.79%] [G loss: 0.7006403207778931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 42/86 [D loss: 0.6921185255050659, acc.: 50.68%] [G loss: 0.7025504112243652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 43/86 [D loss: 0.6894406676292419, acc.: 55.47%] [G loss: 0.7037575244903564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 44/86 [D loss: 0.6913388669490814, acc.: 52.34%] [G loss: 0.7030526995658875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 45/86 [D loss: 0.6896997690200806, acc.: 54.69%] [G loss: 0.7038331627845764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 46/86 [D loss: 0.6908935308456421, acc.: 53.86%] [G loss: 0.7007908225059509]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 47/86 [D loss: 0.6917787790298462, acc.: 50.93%] [G loss: 0.7044442296028137]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 48/86 [D loss: 0.6902231574058533, acc.: 53.91%] [G loss: 0.7013627290725708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 49/86 [D loss: 0.6912418603897095, acc.: 53.47%] [G loss: 0.7018651962280273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 50/86 [D loss: 0.6896335780620575, acc.: 55.71%] [G loss: 0.7031868696212769]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 51/86 [D loss: 0.6906041204929352, acc.: 53.91%] [G loss: 0.7031169533729553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 52/86 [D loss: 0.6915750205516815, acc.: 53.47%] [G loss: 0.7048431038856506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 53/86 [D loss: 0.6904065310955048, acc.: 54.39%] [G loss: 0.7031445503234863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 54/86 [D loss: 0.6912731230258942, acc.: 52.88%] [G loss: 0.70343017578125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 55/86 [D loss: 0.6903459429740906, acc.: 54.74%] [G loss: 0.7036486268043518]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 56/86 [D loss: 0.6911118924617767, acc.: 53.76%] [G loss: 0.7029850482940674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 57/86 [D loss: 0.690384566783905, acc.: 54.15%] [G loss: 0.704744815826416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 58/86 [D loss: 0.6907682716846466, acc.: 53.52%] [G loss: 0.7021008729934692]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 59/86 [D loss: 0.6903569102287292, acc.: 54.30%] [G loss: 0.70355224609375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 60/86 [D loss: 0.6904434859752655, acc.: 54.10%] [G loss: 0.7026064991950989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 61/86 [D loss: 0.6929180026054382, acc.: 50.73%] [G loss: 0.7033404111862183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 62/86 [D loss: 0.6902791261672974, acc.: 54.15%] [G loss: 0.7047024965286255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 63/86 [D loss: 0.6914968192577362, acc.: 51.66%] [G loss: 0.7046886086463928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 64/86 [D loss: 0.690408855676651, acc.: 53.91%] [G loss: 0.7043333053588867]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 65/86 [D loss: 0.6900770664215088, acc.: 54.39%] [G loss: 0.7041743397712708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 66/86 [D loss: 0.6909035444259644, acc.: 53.12%] [G loss: 0.7035143375396729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 67/86 [D loss: 0.6899059414863586, acc.: 54.98%] [G loss: 0.7044302225112915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 68/86 [D loss: 0.6898735165596008, acc.: 55.13%] [G loss: 0.7037917375564575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 69/86 [D loss: 0.6911023557186127, acc.: 53.47%] [G loss: 0.7058836221694946]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 70/86 [D loss: 0.6898253560066223, acc.: 54.83%] [G loss: 0.702713131904602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 71/86 [D loss: 0.69071164727211, acc.: 54.00%] [G loss: 0.7030094861984253]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 72/86 [D loss: 0.6903394758701324, acc.: 54.59%] [G loss: 0.70182865858078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 73/86 [D loss: 0.6906783878803253, acc.: 54.10%] [G loss: 0.7038378715515137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 74/86 [D loss: 0.6901347637176514, acc.: 55.32%] [G loss: 0.7042253017425537]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 75/86 [D loss: 0.6914551556110382, acc.: 52.64%] [G loss: 0.7028855085372925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 76/86 [D loss: 0.6917724013328552, acc.: 52.34%] [G loss: 0.7051548957824707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 77/86 [D loss: 0.6903228163719177, acc.: 54.69%] [G loss: 0.7031416893005371]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 78/86 [D loss: 0.6905034184455872, acc.: 53.08%] [G loss: 0.7042190432548523]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 79/86 [D loss: 0.690113365650177, acc.: 55.57%] [G loss: 0.705338716506958]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 80/86 [D loss: 0.6915305554866791, acc.: 52.64%] [G loss: 0.7026168704032898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 81/86 [D loss: 0.6912451088428497, acc.: 53.61%] [G loss: 0.7051528692245483]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 65/200, Batch 82/86 [D loss: 0.6904764473438263, acc.: 53.81%] [G loss: 0.7035059928894043]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 83/86 [D loss: 0.6913867890834808, acc.: 52.20%] [G loss: 0.7036817073822021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 84/86 [D loss: 0.6907276213169098, acc.: 54.10%] [G loss: 0.7018134593963623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 85/86 [D loss: 0.6922682225704193, acc.: 51.22%] [G loss: 0.7023743987083435]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 65/200, Batch 86/86 [D loss: 0.6906386613845825, acc.: 54.64%] [G loss: 0.7031584978103638]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 1/86 [D loss: 0.6915499567985535, acc.: 50.68%] [G loss: 0.7019748687744141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 2/86 [D loss: 0.6900267899036407, acc.: 54.10%] [G loss: 0.7047483325004578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 3/86 [D loss: 0.6908357739448547, acc.: 53.17%] [G loss: 0.7030429840087891]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 4/86 [D loss: 0.6902554631233215, acc.: 53.91%] [G loss: 0.7040165662765503]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 5/86 [D loss: 0.6894226670265198, acc.: 56.01%] [G loss: 0.7045249342918396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 6/86 [D loss: 0.690619021654129, acc.: 54.15%] [G loss: 0.7031155228614807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 7/86 [D loss: 0.690497487783432, acc.: 54.15%] [G loss: 0.7040690779685974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 8/86 [D loss: 0.691042959690094, acc.: 53.12%] [G loss: 0.7040181159973145]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 9/86 [D loss: 0.6911376416683197, acc.: 54.05%] [G loss: 0.7044156789779663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 10/86 [D loss: 0.688899427652359, acc.: 56.84%] [G loss: 0.7039836049079895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 11/86 [D loss: 0.6915184557437897, acc.: 52.64%] [G loss: 0.7035853266716003]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 12/86 [D loss: 0.6894556879997253, acc.: 55.08%] [G loss: 0.7028248906135559]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 13/86 [D loss: 0.6903063952922821, acc.: 53.96%] [G loss: 0.7040160298347473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 14/86 [D loss: 0.6907985210418701, acc.: 53.17%] [G loss: 0.7054084539413452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 15/86 [D loss: 0.6904937624931335, acc.: 53.37%] [G loss: 0.7041022777557373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 16/86 [D loss: 0.6913636922836304, acc.: 53.22%] [G loss: 0.7045261859893799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 17/86 [D loss: 0.6891833543777466, acc.: 55.86%] [G loss: 0.7036356925964355]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 18/86 [D loss: 0.6910299062728882, acc.: 53.27%] [G loss: 0.7052630186080933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 19/86 [D loss: 0.6901515126228333, acc.: 55.08%] [G loss: 0.7079365849494934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 20/86 [D loss: 0.6910930275917053, acc.: 54.00%] [G loss: 0.7029520273208618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 21/86 [D loss: 0.6905732452869415, acc.: 54.44%] [G loss: 0.7053618431091309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 22/86 [D loss: 0.6896772086620331, acc.: 55.96%] [G loss: 0.7032767534255981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 23/86 [D loss: 0.6905236542224884, acc.: 52.44%] [G loss: 0.7038028240203857]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 24/86 [D loss: 0.6898460984230042, acc.: 54.98%] [G loss: 0.7015705108642578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 25/86 [D loss: 0.6914263069629669, acc.: 53.47%] [G loss: 0.7041826844215393]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 26/86 [D loss: 0.6905713379383087, acc.: 53.66%] [G loss: 0.7031285762786865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 27/86 [D loss: 0.6912097632884979, acc.: 52.10%] [G loss: 0.7018921375274658]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 28/86 [D loss: 0.6909239888191223, acc.: 53.47%] [G loss: 0.7058181166648865]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 29/86 [D loss: 0.6906814873218536, acc.: 53.03%] [G loss: 0.7037690281867981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 30/86 [D loss: 0.6908845901489258, acc.: 53.52%] [G loss: 0.7038343548774719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 31/86 [D loss: 0.6899322271347046, acc.: 54.35%] [G loss: 0.7041179537773132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 32/86 [D loss: 0.6917442679405212, acc.: 52.25%] [G loss: 0.7020941972732544]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 33/86 [D loss: 0.691563218832016, acc.: 52.39%] [G loss: 0.7049887776374817]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 34/86 [D loss: 0.6902790069580078, acc.: 53.22%] [G loss: 0.7030782699584961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 35/86 [D loss: 0.6907455325126648, acc.: 53.03%] [G loss: 0.7068686485290527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 36/86 [D loss: 0.6897779703140259, acc.: 55.42%] [G loss: 0.701286256313324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 37/86 [D loss: 0.6921334266662598, acc.: 51.51%] [G loss: 0.7063614726066589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 38/86 [D loss: 0.6883968710899353, acc.: 57.62%] [G loss: 0.7041191458702087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 39/86 [D loss: 0.6912677884101868, acc.: 53.37%] [G loss: 0.7017589807510376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 40/86 [D loss: 0.6899703741073608, acc.: 53.91%] [G loss: 0.7054829597473145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 41/86 [D loss: 0.6913384199142456, acc.: 52.39%] [G loss: 0.7000884413719177]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 42/86 [D loss: 0.6923357248306274, acc.: 51.22%] [G loss: 0.7048484683036804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 43/86 [D loss: 0.6892423331737518, acc.: 56.05%] [G loss: 0.7026122808456421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 44/86 [D loss: 0.6917390823364258, acc.: 52.93%] [G loss: 0.7036230564117432]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 45/86 [D loss: 0.6896395981311798, acc.: 56.05%] [G loss: 0.7032492160797119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 46/86 [D loss: 0.6916198134422302, acc.: 52.59%] [G loss: 0.7000953555107117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 47/86 [D loss: 0.6917981505393982, acc.: 51.95%] [G loss: 0.7059370875358582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 48/86 [D loss: 0.6901057660579681, acc.: 53.81%] [G loss: 0.7006101012229919]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 66/200, Batch 49/86 [D loss: 0.6910123527050018, acc.: 52.15%] [G loss: 0.703726053237915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 50/86 [D loss: 0.6893203556537628, acc.: 56.49%] [G loss: 0.7012645602226257]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 51/86 [D loss: 0.6919190585613251, acc.: 52.59%] [G loss: 0.7041196823120117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 52/86 [D loss: 0.688299834728241, acc.: 56.88%] [G loss: 0.7054786086082458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 53/86 [D loss: 0.6926759481430054, acc.: 50.59%] [G loss: 0.7030923962593079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 54/86 [D loss: 0.6907080411911011, acc.: 53.86%] [G loss: 0.7048248648643494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 55/86 [D loss: 0.6912092566490173, acc.: 52.78%] [G loss: 0.7003443837165833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 56/86 [D loss: 0.6928767263889313, acc.: 50.68%] [G loss: 0.7072064876556396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 57/86 [D loss: 0.6870509386062622, acc.: 58.79%] [G loss: 0.7034487724304199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 58/86 [D loss: 0.6927106082439423, acc.: 51.17%] [G loss: 0.7031103372573853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 59/86 [D loss: 0.689146101474762, acc.: 56.59%] [G loss: 0.7069973349571228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 60/86 [D loss: 0.6906657516956329, acc.: 53.66%] [G loss: 0.6989831924438477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 61/86 [D loss: 0.6935482025146484, acc.: 49.90%] [G loss: 0.705253005027771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 62/86 [D loss: 0.6896718144416809, acc.: 54.54%] [G loss: 0.7024765610694885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 63/86 [D loss: 0.6926748156547546, acc.: 50.78%] [G loss: 0.7036836743354797]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 64/86 [D loss: 0.6893075704574585, acc.: 56.35%] [G loss: 0.7051125764846802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 65/86 [D loss: 0.6913094520568848, acc.: 53.42%] [G loss: 0.7030353546142578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 66/86 [D loss: 0.6919977962970734, acc.: 52.93%] [G loss: 0.7049513459205627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 67/86 [D loss: 0.6904956698417664, acc.: 54.83%] [G loss: 0.701015830039978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 68/86 [D loss: 0.6913370192050934, acc.: 53.81%] [G loss: 0.7046824097633362]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 69/86 [D loss: 0.6893876492977142, acc.: 56.15%] [G loss: 0.705442488193512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 70/86 [D loss: 0.6918065547943115, acc.: 51.27%] [G loss: 0.7034508585929871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 71/86 [D loss: 0.6897322833538055, acc.: 55.76%] [G loss: 0.7030550837516785]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 72/86 [D loss: 0.6899770498275757, acc.: 54.74%] [G loss: 0.7039721012115479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 73/86 [D loss: 0.690289169549942, acc.: 54.30%] [G loss: 0.7058956027030945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 74/86 [D loss: 0.6901530623435974, acc.: 54.39%] [G loss: 0.7040855884552002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 75/86 [D loss: 0.6919536590576172, acc.: 51.56%] [G loss: 0.7041205167770386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 76/86 [D loss: 0.6897028088569641, acc.: 55.62%] [G loss: 0.7058252096176147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 77/86 [D loss: 0.6899338662624359, acc.: 54.88%] [G loss: 0.7045879364013672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 78/86 [D loss: 0.6904009282588959, acc.: 54.20%] [G loss: 0.7049196362495422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 79/86 [D loss: 0.6894142031669617, acc.: 55.47%] [G loss: 0.7047818899154663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 80/86 [D loss: 0.6895004212856293, acc.: 55.91%] [G loss: 0.704340398311615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 81/86 [D loss: 0.6901736557483673, acc.: 53.71%] [G loss: 0.7051186561584473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 82/86 [D loss: 0.6904464066028595, acc.: 52.54%] [G loss: 0.7039068341255188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 83/86 [D loss: 0.690718948841095, acc.: 53.52%] [G loss: 0.7057064175605774]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 84/86 [D loss: 0.6906483769416809, acc.: 54.35%] [G loss: 0.7047041058540344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 85/86 [D loss: 0.6909842491149902, acc.: 52.59%] [G loss: 0.7046715021133423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 66/200, Batch 86/86 [D loss: 0.6898184418678284, acc.: 54.69%] [G loss: 0.7059906721115112]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 1/86 [D loss: 0.6899740397930145, acc.: 55.47%] [G loss: 0.7049874067306519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 2/86 [D loss: 0.6895580291748047, acc.: 55.37%] [G loss: 0.7061648368835449]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 3/86 [D loss: 0.6908955574035645, acc.: 52.25%] [G loss: 0.7052434682846069]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 4/86 [D loss: 0.6913406252861023, acc.: 51.51%] [G loss: 0.7074400782585144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 5/86 [D loss: 0.6895690560340881, acc.: 55.71%] [G loss: 0.7074815630912781]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 6/86 [D loss: 0.6912314891815186, acc.: 53.22%] [G loss: 0.7059667706489563]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 7/86 [D loss: 0.6905946731567383, acc.: 53.42%] [G loss: 0.7076390981674194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 8/86 [D loss: 0.6911442875862122, acc.: 53.66%] [G loss: 0.7052960395812988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 9/86 [D loss: 0.6897540092468262, acc.: 55.32%] [G loss: 0.7068646550178528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 10/86 [D loss: 0.6913072168827057, acc.: 52.93%] [G loss: 0.7033233642578125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 11/86 [D loss: 0.6910403072834015, acc.: 53.66%] [G loss: 0.7063387036323547]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 12/86 [D loss: 0.6894259452819824, acc.: 55.42%] [G loss: 0.7032487392425537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 13/86 [D loss: 0.6930621862411499, acc.: 50.68%] [G loss: 0.706132709980011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 14/86 [D loss: 0.6893641948699951, acc.: 56.01%] [G loss: 0.7036606073379517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 15/86 [D loss: 0.6920971870422363, acc.: 50.73%] [G loss: 0.7048478722572327]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 16/86 [D loss: 0.6905505061149597, acc.: 54.93%] [G loss: 0.7069297432899475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 17/86 [D loss: 0.6895258724689484, acc.: 54.79%] [G loss: 0.7005230188369751]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 18/86 [D loss: 0.6924218535423279, acc.: 52.73%] [G loss: 0.7081148028373718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 19/86 [D loss: 0.687847375869751, acc.: 56.98%] [G loss: 0.7043876051902771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 20/86 [D loss: 0.6922273933887482, acc.: 51.90%] [G loss: 0.7039767503738403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 21/86 [D loss: 0.6899914741516113, acc.: 54.64%] [G loss: 0.7072352170944214]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 22/86 [D loss: 0.691405326128006, acc.: 52.54%] [G loss: 0.6991344690322876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 23/86 [D loss: 0.6934636235237122, acc.: 49.61%] [G loss: 0.7068427801132202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 24/86 [D loss: 0.6893787384033203, acc.: 55.18%] [G loss: 0.7019233107566833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 25/86 [D loss: 0.6926936209201813, acc.: 51.22%] [G loss: 0.7030899524688721]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 26/86 [D loss: 0.6896893680095673, acc.: 55.08%] [G loss: 0.7061350345611572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 27/86 [D loss: 0.6912626624107361, acc.: 53.47%] [G loss: 0.701330840587616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 28/86 [D loss: 0.6915013194084167, acc.: 52.88%] [G loss: 0.7086915969848633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 29/86 [D loss: 0.6893511116504669, acc.: 55.71%] [G loss: 0.7022989988327026]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 30/86 [D loss: 0.6921117007732391, acc.: 51.22%] [G loss: 0.7050554156303406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 31/86 [D loss: 0.6897304952144623, acc.: 55.76%] [G loss: 0.7058781385421753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 32/86 [D loss: 0.6915848851203918, acc.: 51.27%] [G loss: 0.700695812702179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 33/86 [D loss: 0.691725492477417, acc.: 52.15%] [G loss: 0.7088879942893982]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 34/86 [D loss: 0.6897720098495483, acc.: 54.79%] [G loss: 0.702411413192749]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 35/86 [D loss: 0.6922103762626648, acc.: 51.81%] [G loss: 0.7048917412757874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 36/86 [D loss: 0.6894486546516418, acc.: 55.27%] [G loss: 0.705864667892456]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 37/86 [D loss: 0.6898543834686279, acc.: 55.27%] [G loss: 0.6987021565437317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 38/86 [D loss: 0.692586213350296, acc.: 50.59%] [G loss: 0.7069894671440125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 39/86 [D loss: 0.6896846890449524, acc.: 54.20%] [G loss: 0.7018212080001831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 40/86 [D loss: 0.6930056214332581, acc.: 49.85%] [G loss: 0.702489972114563]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 41/86 [D loss: 0.6899934113025665, acc.: 54.39%] [G loss: 0.7075033783912659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 42/86 [D loss: 0.6909059286117554, acc.: 52.20%] [G loss: 0.7016475200653076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 43/86 [D loss: 0.6930713653564453, acc.: 49.41%] [G loss: 0.7051148414611816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 44/86 [D loss: 0.6895061135292053, acc.: 56.35%] [G loss: 0.7044832706451416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 45/86 [D loss: 0.6921335458755493, acc.: 50.83%] [G loss: 0.7018339037895203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 46/86 [D loss: 0.6915966272354126, acc.: 52.73%] [G loss: 0.7071678638458252]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 47/86 [D loss: 0.6889852583408356, acc.: 56.25%] [G loss: 0.7009921669960022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 48/86 [D loss: 0.6930738091468811, acc.: 50.15%] [G loss: 0.7040877342224121]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 49/86 [D loss: 0.6877227127552032, acc.: 57.91%] [G loss: 0.7040461897850037]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 50/86 [D loss: 0.6931591033935547, acc.: 49.71%] [G loss: 0.7034333944320679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 51/86 [D loss: 0.6906141638755798, acc.: 54.20%] [G loss: 0.7063128352165222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 52/86 [D loss: 0.690354734659195, acc.: 53.22%] [G loss: 0.7010065913200378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 53/86 [D loss: 0.6923076510429382, acc.: 51.90%] [G loss: 0.7032294869422913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 54/86 [D loss: 0.6890699565410614, acc.: 57.47%] [G loss: 0.703474760055542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 55/86 [D loss: 0.6925062239170074, acc.: 51.27%] [G loss: 0.7012020945549011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 56/86 [D loss: 0.6910082399845123, acc.: 53.66%] [G loss: 0.7086089253425598]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 57/86 [D loss: 0.6890755295753479, acc.: 56.64%] [G loss: 0.7021750211715698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 58/86 [D loss: 0.6913525760173798, acc.: 50.83%] [G loss: 0.7013669013977051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 59/86 [D loss: 0.689785361289978, acc.: 55.52%] [G loss: 0.7054494023323059]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 60/86 [D loss: 0.6901532113552094, acc.: 53.08%] [G loss: 0.703730583190918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 61/86 [D loss: 0.69267737865448, acc.: 49.95%] [G loss: 0.7045757174491882]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 62/86 [D loss: 0.687765896320343, acc.: 58.54%] [G loss: 0.7029351592063904]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 63/86 [D loss: 0.6926928758621216, acc.: 52.10%] [G loss: 0.7020973563194275]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 64/86 [D loss: 0.6908852756023407, acc.: 53.08%] [G loss: 0.7065900564193726]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 65/86 [D loss: 0.6906521618366241, acc.: 53.71%] [G loss: 0.7039270401000977]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 66/86 [D loss: 0.6907736957073212, acc.: 53.81%] [G loss: 0.7067146301269531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 67/86 [D loss: 0.6899297535419464, acc.: 54.54%] [G loss: 0.7042452096939087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 68/86 [D loss: 0.6913130283355713, acc.: 53.76%] [G loss: 0.7040009498596191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 69/86 [D loss: 0.69090735912323, acc.: 53.22%] [G loss: 0.7069219946861267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 70/86 [D loss: 0.6892851889133453, acc.: 54.20%] [G loss: 0.7038165330886841]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 71/86 [D loss: 0.690136581659317, acc.: 53.56%] [G loss: 0.706093430519104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 72/86 [D loss: 0.6883170008659363, acc.: 57.28%] [G loss: 0.7039775252342224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 73/86 [D loss: 0.6908456981182098, acc.: 54.35%] [G loss: 0.705565869808197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 74/86 [D loss: 0.6888549625873566, acc.: 55.71%] [G loss: 0.7050976157188416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 75/86 [D loss: 0.6896764636039734, acc.: 55.32%] [G loss: 0.70595782995224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 76/86 [D loss: 0.69106325507164, acc.: 52.44%] [G loss: 0.7067431211471558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 77/86 [D loss: 0.6875747442245483, acc.: 57.96%] [G loss: 0.7063242197036743]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 78/86 [D loss: 0.6902718245983124, acc.: 53.66%] [G loss: 0.7063686847686768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 79/86 [D loss: 0.6899231672286987, acc.: 55.03%] [G loss: 0.7073987722396851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 80/86 [D loss: 0.6892913579940796, acc.: 55.03%] [G loss: 0.7061420679092407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 81/86 [D loss: 0.6896555423736572, acc.: 54.49%] [G loss: 0.707969069480896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 82/86 [D loss: 0.6898222863674164, acc.: 54.69%] [G loss: 0.7057642936706543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 83/86 [D loss: 0.6917095184326172, acc.: 52.29%] [G loss: 0.7055650949478149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 67/200, Batch 84/86 [D loss: 0.6890433430671692, acc.: 56.20%] [G loss: 0.7041376829147339]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 85/86 [D loss: 0.6893649697303772, acc.: 55.27%] [G loss: 0.7042420506477356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 67/200, Batch 86/86 [D loss: 0.6898186206817627, acc.: 54.54%] [G loss: 0.7069711685180664]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 1s 17ms/step\n",
      "Epoch 68/200, Batch 1/86 [D loss: 0.6899361312389374, acc.: 54.83%] [G loss: 0.7054963707923889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 2/86 [D loss: 0.6902572512626648, acc.: 54.59%] [G loss: 0.7067368030548096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 3/86 [D loss: 0.6897633671760559, acc.: 53.86%] [G loss: 0.7063934803009033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 4/86 [D loss: 0.6896320581436157, acc.: 54.54%] [G loss: 0.7048106789588928]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 5/86 [D loss: 0.689408004283905, acc.: 54.64%] [G loss: 0.7057121396064758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 6/86 [D loss: 0.6907365918159485, acc.: 54.20%] [G loss: 0.7050259113311768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 7/86 [D loss: 0.6903527081012726, acc.: 53.12%] [G loss: 0.705706000328064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 8/86 [D loss: 0.6883639693260193, acc.: 56.98%] [G loss: 0.7039275765419006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 9/86 [D loss: 0.6901189982891083, acc.: 54.54%] [G loss: 0.7047647833824158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 10/86 [D loss: 0.6884685754776001, acc.: 57.62%] [G loss: 0.7048012018203735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 11/86 [D loss: 0.6900779902935028, acc.: 54.15%] [G loss: 0.702176034450531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 12/86 [D loss: 0.6907809972763062, acc.: 52.93%] [G loss: 0.7040454149246216]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 13/86 [D loss: 0.6893186271190643, acc.: 56.45%] [G loss: 0.705292284488678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 14/86 [D loss: 0.6903052031993866, acc.: 52.93%] [G loss: 0.7035675644874573]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 15/86 [D loss: 0.6888227462768555, acc.: 55.22%] [G loss: 0.7060934901237488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 16/86 [D loss: 0.6908629834651947, acc.: 53.27%] [G loss: 0.7018517255783081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 17/86 [D loss: 0.6926723718643188, acc.: 51.12%] [G loss: 0.7077302932739258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 18/86 [D loss: 0.6883593201637268, acc.: 56.30%] [G loss: 0.7039109468460083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 19/86 [D loss: 0.6919865012168884, acc.: 50.68%] [G loss: 0.7037845253944397]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 20/86 [D loss: 0.6894688606262207, acc.: 53.22%] [G loss: 0.7088351845741272]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 21/86 [D loss: 0.6910218000411987, acc.: 53.03%] [G loss: 0.7012343406677246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 22/86 [D loss: 0.6938378810882568, acc.: 48.88%] [G loss: 0.7066648006439209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 23/86 [D loss: 0.6891184151172638, acc.: 56.20%] [G loss: 0.7010656595230103]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 24/86 [D loss: 0.6924704909324646, acc.: 50.88%] [G loss: 0.7007652521133423]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 25/86 [D loss: 0.6884861886501312, acc.: 55.66%] [G loss: 0.7066222429275513]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 26/86 [D loss: 0.6901220381259918, acc.: 54.74%] [G loss: 0.6990134716033936]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 27/86 [D loss: 0.6944102942943573, acc.: 47.61%] [G loss: 0.7056008577346802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 28/86 [D loss: 0.6872814297676086, acc.: 58.50%] [G loss: 0.7019068002700806]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 29/86 [D loss: 0.6955127120018005, acc.: 47.31%] [G loss: 0.7004374265670776]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 30/86 [D loss: 0.6903233826160431, acc.: 53.91%] [G loss: 0.7062719464302063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 31/86 [D loss: 0.6886177062988281, acc.: 56.98%] [G loss: 0.6974765062332153]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 32/86 [D loss: 0.695040225982666, acc.: 47.80%] [G loss: 0.7011042833328247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 33/86 [D loss: 0.6879658401012421, acc.: 57.42%] [G loss: 0.7042359113693237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 34/86 [D loss: 0.693943589925766, acc.: 49.07%] [G loss: 0.6986026167869568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 35/86 [D loss: 0.6929285228252411, acc.: 49.51%] [G loss: 0.7064501643180847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 36/86 [D loss: 0.6879197061061859, acc.: 56.98%] [G loss: 0.697578489780426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 37/86 [D loss: 0.6945702135562897, acc.: 48.68%] [G loss: 0.6973239183425903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 38/86 [D loss: 0.6896927952766418, acc.: 56.49%] [G loss: 0.706257700920105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 39/86 [D loss: 0.6911174654960632, acc.: 52.83%] [G loss: 0.6986494660377502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 40/86 [D loss: 0.6931940913200378, acc.: 50.29%] [G loss: 0.7033277153968811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 41/86 [D loss: 0.6896270513534546, acc.: 53.71%] [G loss: 0.704149603843689]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 42/86 [D loss: 0.6921091079711914, acc.: 50.63%] [G loss: 0.6991013288497925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 43/86 [D loss: 0.692716658115387, acc.: 49.41%] [G loss: 0.706308126449585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 44/86 [D loss: 0.6894436478614807, acc.: 55.27%] [G loss: 0.7022953033447266]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 45/86 [D loss: 0.6931656897068024, acc.: 49.12%] [G loss: 0.7020936608314514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 46/86 [D loss: 0.6900274753570557, acc.: 54.20%] [G loss: 0.706145703792572]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 47/86 [D loss: 0.689515620470047, acc.: 55.71%] [G loss: 0.7006116509437561]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 48/86 [D loss: 0.6923001110553741, acc.: 50.59%] [G loss: 0.703479528427124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 49/86 [D loss: 0.6875530183315277, acc.: 58.35%] [G loss: 0.7049344182014465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 50/86 [D loss: 0.690779834985733, acc.: 51.71%] [G loss: 0.7032355666160583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 51/86 [D loss: 0.6891287565231323, acc.: 56.10%] [G loss: 0.7061861753463745]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 52/86 [D loss: 0.6897887289524078, acc.: 55.08%] [G loss: 0.7034363150596619]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 53/86 [D loss: 0.692494809627533, acc.: 51.76%] [G loss: 0.7050337791442871]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 54/86 [D loss: 0.6900255084037781, acc.: 53.76%] [G loss: 0.7073694467544556]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 55/86 [D loss: 0.6897300779819489, acc.: 54.74%] [G loss: 0.7044428586959839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 56/86 [D loss: 0.6910195350646973, acc.: 52.49%] [G loss: 0.7065726518630981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 57/86 [D loss: 0.6897144019603729, acc.: 55.18%] [G loss: 0.7053876519203186]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 58/86 [D loss: 0.6908345520496368, acc.: 52.73%] [G loss: 0.7026230692863464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 59/86 [D loss: 0.6910184919834137, acc.: 53.52%] [G loss: 0.7067009806632996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 60/86 [D loss: 0.6890393495559692, acc.: 56.15%] [G loss: 0.7043042778968811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 61/86 [D loss: 0.6911217570304871, acc.: 52.88%] [G loss: 0.7068370580673218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 62/86 [D loss: 0.6893103122711182, acc.: 54.83%] [G loss: 0.7061840891838074]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 63/86 [D loss: 0.6910140216350555, acc.: 54.44%] [G loss: 0.7031694650650024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 64/86 [D loss: 0.6907866299152374, acc.: 52.44%] [G loss: 0.7058416604995728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 65/86 [D loss: 0.6896464228630066, acc.: 55.47%] [G loss: 0.7055120468139648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 66/86 [D loss: 0.6906462013721466, acc.: 54.00%] [G loss: 0.7037709951400757]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 67/86 [D loss: 0.6888343393802643, acc.: 56.20%] [G loss: 0.7073097229003906]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 68/86 [D loss: 0.6881206929683685, acc.: 57.42%] [G loss: 0.7035791873931885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 69/86 [D loss: 0.6919335126876831, acc.: 51.81%] [G loss: 0.7054104804992676]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 70/86 [D loss: 0.6882363855838776, acc.: 58.06%] [G loss: 0.7064505219459534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 71/86 [D loss: 0.6911564469337463, acc.: 53.27%] [G loss: 0.7047414183616638]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 72/86 [D loss: 0.69102543592453, acc.: 53.22%] [G loss: 0.7069010734558105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 73/86 [D loss: 0.6890380084514618, acc.: 55.27%] [G loss: 0.7063528895378113]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 74/86 [D loss: 0.6919618546962738, acc.: 52.39%] [G loss: 0.7048869729042053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 75/86 [D loss: 0.6907867789268494, acc.: 54.05%] [G loss: 0.7083235383033752]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 76/86 [D loss: 0.689991295337677, acc.: 54.15%] [G loss: 0.7029690146446228]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 77/86 [D loss: 0.6916361153125763, acc.: 51.56%] [G loss: 0.7075963020324707]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 78/86 [D loss: 0.69004225730896, acc.: 54.35%] [G loss: 0.7060578465461731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 79/86 [D loss: 0.6916548609733582, acc.: 51.56%] [G loss: 0.7031888961791992]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 80/86 [D loss: 0.6911004185676575, acc.: 52.93%] [G loss: 0.7070584297180176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 81/86 [D loss: 0.6897643804550171, acc.: 56.45%] [G loss: 0.7039228677749634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 82/86 [D loss: 0.6907568275928497, acc.: 53.37%] [G loss: 0.7044140100479126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 83/86 [D loss: 0.6890963912010193, acc.: 54.64%] [G loss: 0.7079063057899475]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 84/86 [D loss: 0.690091997385025, acc.: 54.35%] [G loss: 0.7041649222373962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 68/200, Batch 85/86 [D loss: 0.6924248337745667, acc.: 51.66%] [G loss: 0.7069368362426758]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 68/200, Batch 86/86 [D loss: 0.6882338225841522, acc.: 56.01%] [G loss: 0.705320417881012]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 1/86 [D loss: 0.691701740026474, acc.: 51.27%] [G loss: 0.7045717835426331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 2/86 [D loss: 0.6903232336044312, acc.: 54.64%] [G loss: 0.7076427340507507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 3/86 [D loss: 0.6883968710899353, acc.: 56.49%] [G loss: 0.7013369202613831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 4/86 [D loss: 0.6936673820018768, acc.: 48.44%] [G loss: 0.7046573162078857]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 5/86 [D loss: 0.6888795793056488, acc.: 55.47%] [G loss: 0.7051899433135986]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 6/86 [D loss: 0.6917275190353394, acc.: 51.86%] [G loss: 0.6988327503204346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 7/86 [D loss: 0.6920883655548096, acc.: 52.25%] [G loss: 0.7056282162666321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 8/86 [D loss: 0.6869548261165619, acc.: 59.33%] [G loss: 0.7046685218811035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 9/86 [D loss: 0.6910412311553955, acc.: 53.27%] [G loss: 0.6977224349975586]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 10/86 [D loss: 0.6926168203353882, acc.: 50.63%] [G loss: 0.7087875604629517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 11/86 [D loss: 0.6886215209960938, acc.: 56.10%] [G loss: 0.7013048529624939]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 12/86 [D loss: 0.6925836801528931, acc.: 50.49%] [G loss: 0.7031988501548767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 13/86 [D loss: 0.6898048222064972, acc.: 55.42%] [G loss: 0.705764651298523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 14/86 [D loss: 0.6907928586006165, acc.: 54.05%] [G loss: 0.6992948055267334]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 15/86 [D loss: 0.6928667426109314, acc.: 50.73%] [G loss: 0.704332709312439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 16/86 [D loss: 0.6894335746765137, acc.: 55.37%] [G loss: 0.7036982178688049]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 17/86 [D loss: 0.6928785145282745, acc.: 50.34%] [G loss: 0.7032549381256104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 18/86 [D loss: 0.6910466253757477, acc.: 53.08%] [G loss: 0.70511394739151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 19/86 [D loss: 0.6877543330192566, acc.: 56.25%] [G loss: 0.7021690011024475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 20/86 [D loss: 0.6928839385509491, acc.: 49.46%] [G loss: 0.7013301253318787]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 21/86 [D loss: 0.6912127435207367, acc.: 53.08%] [G loss: 0.7048863172531128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 22/86 [D loss: 0.6901607513427734, acc.: 54.15%] [G loss: 0.7030137181282043]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 23/86 [D loss: 0.6912219822406769, acc.: 52.73%] [G loss: 0.7058825492858887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 24/86 [D loss: 0.6880694627761841, acc.: 56.74%] [G loss: 0.7040746808052063]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 25/86 [D loss: 0.6893656253814697, acc.: 53.76%] [G loss: 0.7009668946266174]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 26/86 [D loss: 0.6924133002758026, acc.: 51.86%] [G loss: 0.7058449983596802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 27/86 [D loss: 0.6889565587043762, acc.: 55.71%] [G loss: 0.7036609649658203]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 28/86 [D loss: 0.6911908388137817, acc.: 53.37%] [G loss: 0.7029681205749512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 29/86 [D loss: 0.6897215247154236, acc.: 55.18%] [G loss: 0.7056818604469299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 30/86 [D loss: 0.6886269450187683, acc.: 56.84%] [G loss: 0.7030609250068665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 31/86 [D loss: 0.6916927397251129, acc.: 52.64%] [G loss: 0.7051790952682495]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 32/86 [D loss: 0.6906789243221283, acc.: 53.03%] [G loss: 0.7050162553787231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 33/86 [D loss: 0.6899335980415344, acc.: 54.00%] [G loss: 0.7048888802528381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 34/86 [D loss: 0.6892780661582947, acc.: 54.83%] [G loss: 0.7057335376739502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 35/86 [D loss: 0.6894743144512177, acc.: 54.39%] [G loss: 0.7045274972915649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 36/86 [D loss: 0.6895227432250977, acc.: 55.71%] [G loss: 0.7035402655601501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 37/86 [D loss: 0.6903634667396545, acc.: 54.64%] [G loss: 0.7080607414245605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 38/86 [D loss: 0.6896872818470001, acc.: 54.88%] [G loss: 0.7044947743415833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 39/86 [D loss: 0.6904228925704956, acc.: 53.47%] [G loss: 0.7046943306922913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 40/86 [D loss: 0.6896042227745056, acc.: 54.69%] [G loss: 0.7041851878166199]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 41/86 [D loss: 0.6901053190231323, acc.: 54.59%] [G loss: 0.7027699947357178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 42/86 [D loss: 0.6914204359054565, acc.: 52.64%] [G loss: 0.7051335573196411]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 43/86 [D loss: 0.6888555288314819, acc.: 56.20%] [G loss: 0.7046359777450562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 44/86 [D loss: 0.690993458032608, acc.: 51.66%] [G loss: 0.7020452618598938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 45/86 [D loss: 0.6912477314472198, acc.: 52.78%] [G loss: 0.7065925002098083]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 46/86 [D loss: 0.6879934072494507, acc.: 58.06%] [G loss: 0.7034995555877686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 47/86 [D loss: 0.6919474303722382, acc.: 51.95%] [G loss: 0.7033696174621582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 48/86 [D loss: 0.6900073289871216, acc.: 54.00%] [G loss: 0.7072132229804993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 49/86 [D loss: 0.6913549900054932, acc.: 53.32%] [G loss: 0.7009208798408508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 50/86 [D loss: 0.690804660320282, acc.: 53.56%] [G loss: 0.7051450610160828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 51/86 [D loss: 0.6892494857311249, acc.: 54.59%] [G loss: 0.7044025659561157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 52/86 [D loss: 0.6903857886791229, acc.: 53.37%] [G loss: 0.7039082050323486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 53/86 [D loss: 0.6903969049453735, acc.: 54.30%] [G loss: 0.7063826322555542]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 54/86 [D loss: 0.6894161999225616, acc.: 55.52%] [G loss: 0.7034095525741577]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 55/86 [D loss: 0.6909251809120178, acc.: 53.61%] [G loss: 0.7058128714561462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 56/86 [D loss: 0.689783900976181, acc.: 54.98%] [G loss: 0.7043891549110413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 57/86 [D loss: 0.6906554400920868, acc.: 53.47%] [G loss: 0.7030468583106995]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 58/86 [D loss: 0.6906742453575134, acc.: 51.61%] [G loss: 0.7054181098937988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 59/86 [D loss: 0.6880213022232056, acc.: 55.86%] [G loss: 0.7057617902755737]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 60/86 [D loss: 0.6916772425174713, acc.: 52.25%] [G loss: 0.7059043049812317]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 61/86 [D loss: 0.6891501545906067, acc.: 55.18%] [G loss: 0.704308271408081]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 62/86 [D loss: 0.6884182989597321, acc.: 57.62%] [G loss: 0.7035015821456909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 63/86 [D loss: 0.6920659840106964, acc.: 50.34%] [G loss: 0.7054247856140137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 64/86 [D loss: 0.6877749562263489, acc.: 56.93%] [G loss: 0.7053651213645935]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 65/86 [D loss: 0.6911269128322601, acc.: 53.12%] [G loss: 0.7013792991638184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 66/86 [D loss: 0.6898277103900909, acc.: 54.10%] [G loss: 0.7071139812469482]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 67/86 [D loss: 0.6892836689949036, acc.: 56.35%] [G loss: 0.7025721073150635]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 68/86 [D loss: 0.6918283998966217, acc.: 51.56%] [G loss: 0.7032749652862549]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 69/86 [D loss: 0.6876759231090546, acc.: 56.74%] [G loss: 0.7076295614242554]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 70/86 [D loss: 0.6916971504688263, acc.: 51.81%] [G loss: 0.7015632390975952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 71/86 [D loss: 0.6913230121135712, acc.: 53.08%] [G loss: 0.7081273198127747]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 72/86 [D loss: 0.6873175501823425, acc.: 58.79%] [G loss: 0.7034045457839966]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 73/86 [D loss: 0.6921992301940918, acc.: 51.32%] [G loss: 0.6992107033729553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 74/86 [D loss: 0.6900534927845001, acc.: 54.39%] [G loss: 0.7088757157325745]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 75/86 [D loss: 0.6887127459049225, acc.: 56.59%] [G loss: 0.7022886872291565]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 76/86 [D loss: 0.6919941306114197, acc.: 51.71%] [G loss: 0.7051033973693848]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 77/86 [D loss: 0.6890931725502014, acc.: 55.76%] [G loss: 0.7068158984184265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 78/86 [D loss: 0.6901620626449585, acc.: 54.39%] [G loss: 0.7009962201118469]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 79/86 [D loss: 0.6940473020076752, acc.: 49.56%] [G loss: 0.7051986455917358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 80/86 [D loss: 0.6877407729625702, acc.: 58.25%] [G loss: 0.7062526345252991]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 81/86 [D loss: 0.6912198960781097, acc.: 52.93%] [G loss: 0.7005637884140015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 82/86 [D loss: 0.6902233958244324, acc.: 53.17%] [G loss: 0.706732451915741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 83/86 [D loss: 0.6874077916145325, acc.: 57.37%] [G loss: 0.7029846906661987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 84/86 [D loss: 0.6931088268756866, acc.: 51.17%] [G loss: 0.69889897108078]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 69/200, Batch 85/86 [D loss: 0.6911121606826782, acc.: 52.88%] [G loss: 0.7075943350791931]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 69/200, Batch 86/86 [D loss: 0.6888781189918518, acc.: 56.25%] [G loss: 0.702892005443573]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 1/86 [D loss: 0.6911526918411255, acc.: 53.66%] [G loss: 0.7042601704597473]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 2/86 [D loss: 0.6896206140518188, acc.: 54.30%] [G loss: 0.706691563129425]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 3/86 [D loss: 0.6904071271419525, acc.: 53.32%] [G loss: 0.7006759643554688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 4/86 [D loss: 0.6916509568691254, acc.: 51.03%] [G loss: 0.7049048542976379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 5/86 [D loss: 0.6883919835090637, acc.: 55.52%] [G loss: 0.7055245637893677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 6/86 [D loss: 0.690517783164978, acc.: 52.98%] [G loss: 0.7040351629257202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 7/86 [D loss: 0.690446525812149, acc.: 53.03%] [G loss: 0.7077516913414001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 8/86 [D loss: 0.6887397468090057, acc.: 56.88%] [G loss: 0.7028467655181885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 9/86 [D loss: 0.6919713914394379, acc.: 51.42%] [G loss: 0.7027710676193237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 10/86 [D loss: 0.689339280128479, acc.: 56.01%] [G loss: 0.7071061134338379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 11/86 [D loss: 0.6892260313034058, acc.: 55.62%] [G loss: 0.7019889950752258]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 12/86 [D loss: 0.6931571364402771, acc.: 50.29%] [G loss: 0.7069332599639893]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 13/86 [D loss: 0.6896955966949463, acc.: 55.13%] [G loss: 0.7068030834197998]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 14/86 [D loss: 0.690470427274704, acc.: 53.81%] [G loss: 0.6988738775253296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 15/86 [D loss: 0.6925291419029236, acc.: 50.83%] [G loss: 0.7064088582992554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 16/86 [D loss: 0.6885740160942078, acc.: 56.15%] [G loss: 0.7018674612045288]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 17/86 [D loss: 0.6924866437911987, acc.: 50.34%] [G loss: 0.7027544975280762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 18/86 [D loss: 0.6901558637619019, acc.: 54.39%] [G loss: 0.707551121711731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 19/86 [D loss: 0.6887969076633453, acc.: 57.71%] [G loss: 0.7037441730499268]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 20/86 [D loss: 0.6932261884212494, acc.: 50.93%] [G loss: 0.7022032141685486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 21/86 [D loss: 0.6909829676151276, acc.: 53.12%] [G loss: 0.7066878080368042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 22/86 [D loss: 0.6894649565219879, acc.: 55.13%] [G loss: 0.7011631727218628]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 23/86 [D loss: 0.6906185448169708, acc.: 53.71%] [G loss: 0.7023900747299194]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 24/86 [D loss: 0.6892828345298767, acc.: 55.96%] [G loss: 0.7079300880432129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 25/86 [D loss: 0.6908632218837738, acc.: 53.76%] [G loss: 0.7001239061355591]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 26/86 [D loss: 0.6946019530296326, acc.: 49.85%] [G loss: 0.7050836086273193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 27/86 [D loss: 0.6883769929409027, acc.: 57.81%] [G loss: 0.7052465677261353]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 28/86 [D loss: 0.6908221244812012, acc.: 53.17%] [G loss: 0.700365424156189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 29/86 [D loss: 0.6901702284812927, acc.: 54.98%] [G loss: 0.7055116295814514]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 30/86 [D loss: 0.6876200735569, acc.: 57.23%] [G loss: 0.7062362432479858]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 31/86 [D loss: 0.6909770369529724, acc.: 53.42%] [G loss: 0.7011725902557373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 32/86 [D loss: 0.691865086555481, acc.: 52.20%] [G loss: 0.7074323892593384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 33/86 [D loss: 0.6878255605697632, acc.: 57.52%] [G loss: 0.7034955024719238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 34/86 [D loss: 0.6915188133716583, acc.: 50.98%] [G loss: 0.704075813293457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 35/86 [D loss: 0.6892559230327606, acc.: 55.81%] [G loss: 0.7068299651145935]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 36/86 [D loss: 0.6890206038951874, acc.: 54.83%] [G loss: 0.7030797004699707]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 37/86 [D loss: 0.691580206155777, acc.: 51.42%] [G loss: 0.7037979960441589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 38/86 [D loss: 0.6892350912094116, acc.: 55.47%] [G loss: 0.7050536870956421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 39/86 [D loss: 0.6885640919208527, acc.: 56.45%] [G loss: 0.7026186585426331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 40/86 [D loss: 0.6898695826530457, acc.: 54.69%] [G loss: 0.704925000667572]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 41/86 [D loss: 0.689525306224823, acc.: 55.52%] [G loss: 0.7054810523986816]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 42/86 [D loss: 0.6905108690261841, acc.: 53.71%] [G loss: 0.704189121723175]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 43/86 [D loss: 0.6894997954368591, acc.: 54.54%] [G loss: 0.7057210206985474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 44/86 [D loss: 0.6889530420303345, acc.: 55.57%] [G loss: 0.7062578201293945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 45/86 [D loss: 0.690604954957962, acc.: 53.27%] [G loss: 0.7034849524497986]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 46/86 [D loss: 0.6907878816127777, acc.: 54.05%] [G loss: 0.7068305611610413]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 47/86 [D loss: 0.6898338794708252, acc.: 54.15%] [G loss: 0.7069380283355713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 48/86 [D loss: 0.6898525357246399, acc.: 54.59%] [G loss: 0.7041003108024597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 49/86 [D loss: 0.688909113407135, acc.: 54.05%] [G loss: 0.7061759233474731]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 50/86 [D loss: 0.6894513070583344, acc.: 54.69%] [G loss: 0.7062639594078064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 51/86 [D loss: 0.688424289226532, acc.: 56.59%] [G loss: 0.7069545388221741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 52/86 [D loss: 0.6887882947921753, acc.: 56.30%] [G loss: 0.7087898850440979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 53/86 [D loss: 0.6894545257091522, acc.: 56.10%] [G loss: 0.7052680253982544]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 54/86 [D loss: 0.6892282962799072, acc.: 55.81%] [G loss: 0.7066998481750488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 55/86 [D loss: 0.6896756291389465, acc.: 55.13%] [G loss: 0.7056340575218201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 56/86 [D loss: 0.69073286652565, acc.: 53.56%] [G loss: 0.7063495516777039]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 57/86 [D loss: 0.6896456182003021, acc.: 54.59%] [G loss: 0.7073682546615601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 58/86 [D loss: 0.6897109150886536, acc.: 53.22%] [G loss: 0.7056383490562439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 59/86 [D loss: 0.6907310783863068, acc.: 53.08%] [G loss: 0.7072737812995911]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 60/86 [D loss: 0.6891307830810547, acc.: 55.66%] [G loss: 0.7072432041168213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 61/86 [D loss: 0.6890181601047516, acc.: 55.22%] [G loss: 0.7058509588241577]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 62/86 [D loss: 0.6873279213905334, acc.: 58.98%] [G loss: 0.7069756388664246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 63/86 [D loss: 0.6888217329978943, acc.: 55.91%] [G loss: 0.7079050540924072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 64/86 [D loss: 0.6897989511489868, acc.: 54.05%] [G loss: 0.7060440182685852]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 65/86 [D loss: 0.6887049973011017, acc.: 55.81%] [G loss: 0.7070062756538391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 66/86 [D loss: 0.6886913180351257, acc.: 55.42%] [G loss: 0.7048511505126953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 67/86 [D loss: 0.6902804672718048, acc.: 53.12%] [G loss: 0.7051819562911987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 68/86 [D loss: 0.6891999542713165, acc.: 54.44%] [G loss: 0.705923318862915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 69/86 [D loss: 0.6894253492355347, acc.: 54.88%] [G loss: 0.7061397433280945]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 70/86 [D loss: 0.6887407898902893, acc.: 55.96%] [G loss: 0.7059035897254944]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 71/86 [D loss: 0.6886405050754547, acc.: 56.20%] [G loss: 0.7072864770889282]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 72/86 [D loss: 0.6893962919712067, acc.: 54.05%] [G loss: 0.7061042189598083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 73/86 [D loss: 0.6888780295848846, acc.: 56.64%] [G loss: 0.7086045742034912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 74/86 [D loss: 0.6888632476329803, acc.: 54.39%] [G loss: 0.7052142024040222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 75/86 [D loss: 0.6883096098899841, acc.: 55.66%] [G loss: 0.7054110169410706]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 76/86 [D loss: 0.6886277198791504, acc.: 55.13%] [G loss: 0.7065441608428955]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 70/200, Batch 77/86 [D loss: 0.6896209716796875, acc.: 53.91%] [G loss: 0.7062250971794128]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 78/86 [D loss: 0.6901200413703918, acc.: 54.15%] [G loss: 0.7074465751647949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 79/86 [D loss: 0.6897337138652802, acc.: 54.74%] [G loss: 0.7059424519538879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 80/86 [D loss: 0.6896994709968567, acc.: 54.54%] [G loss: 0.7071367502212524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 81/86 [D loss: 0.6901214420795441, acc.: 53.61%] [G loss: 0.706621527671814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 82/86 [D loss: 0.688913106918335, acc.: 53.81%] [G loss: 0.7051932215690613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 83/86 [D loss: 0.6900905668735504, acc.: 53.71%] [G loss: 0.708450436592102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 84/86 [D loss: 0.6893320083618164, acc.: 55.03%] [G loss: 0.7057818174362183]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 85/86 [D loss: 0.690421849489212, acc.: 55.66%] [G loss: 0.7052197456359863]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 70/200, Batch 86/86 [D loss: 0.6888919770717621, acc.: 56.35%] [G loss: 0.705340564250946]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 1/86 [D loss: 0.6905208826065063, acc.: 53.76%] [G loss: 0.7047368884086609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 2/86 [D loss: 0.6904962360858917, acc.: 53.08%] [G loss: 0.7055274248123169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 3/86 [D loss: 0.6891126930713654, acc.: 55.86%] [G loss: 0.7054343819618225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 4/86 [D loss: 0.6904863119125366, acc.: 54.88%] [G loss: 0.7061563730239868]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 5/86 [D loss: 0.6887214183807373, acc.: 56.10%] [G loss: 0.7066168189048767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 6/86 [D loss: 0.6878657937049866, acc.: 57.67%] [G loss: 0.7035024166107178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 7/86 [D loss: 0.6895262002944946, acc.: 54.49%] [G loss: 0.7062892317771912]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 8/86 [D loss: 0.6883189082145691, acc.: 56.05%] [G loss: 0.7046874165534973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 9/86 [D loss: 0.6907615661621094, acc.: 52.73%] [G loss: 0.7045837640762329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 10/86 [D loss: 0.6884885430335999, acc.: 55.76%] [G loss: 0.7066109776496887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 11/86 [D loss: 0.6905996203422546, acc.: 53.08%] [G loss: 0.7055383324623108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 12/86 [D loss: 0.6890272498130798, acc.: 53.71%] [G loss: 0.7061693072319031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 13/86 [D loss: 0.6883822083473206, acc.: 56.45%] [G loss: 0.7039166688919067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 14/86 [D loss: 0.6914009749889374, acc.: 53.17%] [G loss: 0.705233097076416]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 15/86 [D loss: 0.6889001429080963, acc.: 55.66%] [G loss: 0.7060602903366089]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 16/86 [D loss: 0.6899612247943878, acc.: 52.78%] [G loss: 0.7008457779884338]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 17/86 [D loss: 0.6907986104488373, acc.: 53.12%] [G loss: 0.7093726396560669]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 18/86 [D loss: 0.6877705454826355, acc.: 56.98%] [G loss: 0.7019224166870117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 19/86 [D loss: 0.6939573287963867, acc.: 49.56%] [G loss: 0.7026112079620361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 20/86 [D loss: 0.68769970536232, acc.: 56.93%] [G loss: 0.7069069147109985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 21/86 [D loss: 0.6928799152374268, acc.: 49.07%] [G loss: 0.6989012956619263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 22/86 [D loss: 0.6901949346065521, acc.: 52.69%] [G loss: 0.7075282335281372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 23/86 [D loss: 0.6872866451740265, acc.: 57.23%] [G loss: 0.7022064924240112]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 24/86 [D loss: 0.6928886771202087, acc.: 51.32%] [G loss: 0.6991498470306396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 25/86 [D loss: 0.6905567348003387, acc.: 52.69%] [G loss: 0.708163321018219]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 26/86 [D loss: 0.688959002494812, acc.: 54.83%] [G loss: 0.7012195587158203]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 27/86 [D loss: 0.6911932826042175, acc.: 50.05%] [G loss: 0.7042120099067688]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 28/86 [D loss: 0.6879888474941254, acc.: 56.15%] [G loss: 0.7079343795776367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 29/86 [D loss: 0.6893516778945923, acc.: 54.79%] [G loss: 0.7002662420272827]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 30/86 [D loss: 0.6913464367389679, acc.: 52.83%] [G loss: 0.7069747447967529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 31/86 [D loss: 0.688263326883316, acc.: 57.47%] [G loss: 0.7015337944030762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 32/86 [D loss: 0.6909327805042267, acc.: 53.37%] [G loss: 0.7032063007354736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 33/86 [D loss: 0.6912056803703308, acc.: 52.98%] [G loss: 0.7070577144622803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 34/86 [D loss: 0.6885337829589844, acc.: 56.69%] [G loss: 0.7021187543869019]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 35/86 [D loss: 0.692091703414917, acc.: 52.10%] [G loss: 0.7026006579399109]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 36/86 [D loss: 0.6872694492340088, acc.: 58.35%] [G loss: 0.7068716287612915]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 37/86 [D loss: 0.690680056810379, acc.: 52.83%] [G loss: 0.7025043368339539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 38/86 [D loss: 0.6904726922512054, acc.: 52.78%] [G loss: 0.7060185670852661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 39/86 [D loss: 0.6885285973548889, acc.: 54.44%] [G loss: 0.7062731981277466]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 40/86 [D loss: 0.6915552914142609, acc.: 52.34%] [G loss: 0.7028562426567078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 41/86 [D loss: 0.6900037527084351, acc.: 54.88%] [G loss: 0.7085222601890564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 42/86 [D loss: 0.6888969838619232, acc.: 54.93%] [G loss: 0.7045798897743225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 43/86 [D loss: 0.6901495158672333, acc.: 54.64%] [G loss: 0.7062167525291443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 44/86 [D loss: 0.6877197325229645, acc.: 58.06%] [G loss: 0.7075084447860718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 45/86 [D loss: 0.6896656155586243, acc.: 55.32%] [G loss: 0.7042772769927979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 46/86 [D loss: 0.6923747360706329, acc.: 51.27%] [G loss: 0.7064117193222046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 47/86 [D loss: 0.6876984238624573, acc.: 57.47%] [G loss: 0.7063231468200684]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 48/86 [D loss: 0.6907365620136261, acc.: 53.32%] [G loss: 0.7061097621917725]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 49/86 [D loss: 0.6896907389163971, acc.: 54.59%] [G loss: 0.7079795598983765]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 50/86 [D loss: 0.6890051960945129, acc.: 54.83%] [G loss: 0.7045484781265259]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 51/86 [D loss: 0.6905378699302673, acc.: 53.47%] [G loss: 0.7059330940246582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 52/86 [D loss: 0.6891433894634247, acc.: 54.79%] [G loss: 0.70637047290802]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 53/86 [D loss: 0.6891328692436218, acc.: 55.42%] [G loss: 0.705922544002533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 54/86 [D loss: 0.6895385980606079, acc.: 55.37%] [G loss: 0.7078476548194885]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 55/86 [D loss: 0.6874657869338989, acc.: 57.03%] [G loss: 0.7057231068611145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 56/86 [D loss: 0.6915540993213654, acc.: 52.44%] [G loss: 0.704743504524231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 57/86 [D loss: 0.6874026656150818, acc.: 56.10%] [G loss: 0.7073590755462646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 58/86 [D loss: 0.6880160868167877, acc.: 56.15%] [G loss: 0.7054910063743591]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 59/86 [D loss: 0.6899329125881195, acc.: 53.37%] [G loss: 0.7078012228012085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 60/86 [D loss: 0.6894207894802094, acc.: 53.91%] [G loss: 0.7040742635726929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 61/86 [D loss: 0.6899600028991699, acc.: 53.47%] [G loss: 0.7049856781959534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 62/86 [D loss: 0.689767450094223, acc.: 55.13%] [G loss: 0.7078286409378052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 63/86 [D loss: 0.689111202955246, acc.: 55.47%] [G loss: 0.706771731376648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 64/86 [D loss: 0.689661830663681, acc.: 54.30%] [G loss: 0.7076035737991333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 65/86 [D loss: 0.6888567209243774, acc.: 55.57%] [G loss: 0.7064697742462158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 66/86 [D loss: 0.6889293491840363, acc.: 55.81%] [G loss: 0.7053797841072083]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 67/86 [D loss: 0.6905112862586975, acc.: 53.08%] [G loss: 0.7041804194450378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 68/86 [D loss: 0.6888096630573273, acc.: 54.20%] [G loss: 0.70584636926651]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 69/86 [D loss: 0.6902749836444855, acc.: 53.71%] [G loss: 0.7045989632606506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 70/86 [D loss: 0.6902722418308258, acc.: 54.20%] [G loss: 0.7093424797058105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 71/86 [D loss: 0.6896184384822845, acc.: 54.10%] [G loss: 0.7063764333724976]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 72/86 [D loss: 0.6882665455341339, acc.: 56.05%] [G loss: 0.7073399424552917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 73/86 [D loss: 0.6886672079563141, acc.: 54.30%] [G loss: 0.7069377899169922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 74/86 [D loss: 0.6901313662528992, acc.: 54.54%] [G loss: 0.7087286710739136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 75/86 [D loss: 0.6890529096126556, acc.: 54.10%] [G loss: 0.7071834206581116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 76/86 [D loss: 0.6892503499984741, acc.: 54.79%] [G loss: 0.7062952518463135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 77/86 [D loss: 0.6891913414001465, acc.: 54.54%] [G loss: 0.7073366641998291]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 71/200, Batch 78/86 [D loss: 0.6877550780773163, acc.: 56.59%] [G loss: 0.7082772850990295]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 79/86 [D loss: 0.6884020864963531, acc.: 56.01%] [G loss: 0.7088627219200134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 80/86 [D loss: 0.6890124380588531, acc.: 54.93%] [G loss: 0.7083810567855835]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 81/86 [D loss: 0.6885080337524414, acc.: 55.22%] [G loss: 0.7086865901947021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 82/86 [D loss: 0.6884655058383942, acc.: 55.37%] [G loss: 0.7070286273956299]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 83/86 [D loss: 0.6883071660995483, acc.: 56.45%] [G loss: 0.7072668671607971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 84/86 [D loss: 0.6885707974433899, acc.: 55.37%] [G loss: 0.707353413105011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 85/86 [D loss: 0.6888010799884796, acc.: 55.81%] [G loss: 0.7061475515365601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 71/200, Batch 86/86 [D loss: 0.6887712776660919, acc.: 54.44%] [G loss: 0.7083436250686646]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 1/86 [D loss: 0.688631534576416, acc.: 55.18%] [G loss: 0.7055874466896057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 2/86 [D loss: 0.6902214288711548, acc.: 53.27%] [G loss: 0.7051981091499329]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 3/86 [D loss: 0.6891845464706421, acc.: 56.10%] [G loss: 0.7058378458023071]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 4/86 [D loss: 0.6880480349063873, acc.: 56.20%] [G loss: 0.7074195146560669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 5/86 [D loss: 0.6900455057621002, acc.: 53.03%] [G loss: 0.7082310318946838]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 6/86 [D loss: 0.6895864307880402, acc.: 54.35%] [G loss: 0.7077158689498901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 7/86 [D loss: 0.6884406208992004, acc.: 55.57%] [G loss: 0.7072131037712097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 8/86 [D loss: 0.6888065934181213, acc.: 56.49%] [G loss: 0.7061939835548401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 9/86 [D loss: 0.6889995038509369, acc.: 54.59%] [G loss: 0.7073752284049988]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 10/86 [D loss: 0.6888827979564667, acc.: 55.91%] [G loss: 0.7072899341583252]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 11/86 [D loss: 0.6889385282993317, acc.: 55.18%] [G loss: 0.7079846858978271]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 12/86 [D loss: 0.6891022622585297, acc.: 55.91%] [G loss: 0.7066911458969116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 13/86 [D loss: 0.6891957521438599, acc.: 54.98%] [G loss: 0.706964373588562]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 14/86 [D loss: 0.6893454790115356, acc.: 55.37%] [G loss: 0.7081683278083801]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 15/86 [D loss: 0.6878759860992432, acc.: 57.13%] [G loss: 0.7080602645874023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 16/86 [D loss: 0.6893333196640015, acc.: 54.39%] [G loss: 0.7063193917274475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 17/86 [D loss: 0.6890665292739868, acc.: 53.76%] [G loss: 0.7074064612388611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 18/86 [D loss: 0.6875192821025848, acc.: 56.93%] [G loss: 0.7069123983383179]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 19/86 [D loss: 0.6891576051712036, acc.: 55.13%] [G loss: 0.7085074186325073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 20/86 [D loss: 0.6879690885543823, acc.: 56.10%] [G loss: 0.7081005573272705]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 21/86 [D loss: 0.6888027489185333, acc.: 54.69%] [G loss: 0.7067863941192627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 22/86 [D loss: 0.689497709274292, acc.: 54.35%] [G loss: 0.7053141593933105]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 23/86 [D loss: 0.689416378736496, acc.: 54.69%] [G loss: 0.7063116431236267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 24/86 [D loss: 0.6901419162750244, acc.: 54.39%] [G loss: 0.7060778141021729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 25/86 [D loss: 0.6878646910190582, acc.: 56.05%] [G loss: 0.7089025378227234]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 26/86 [D loss: 0.6898245811462402, acc.: 52.69%] [G loss: 0.705191969871521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 27/86 [D loss: 0.6889165639877319, acc.: 55.71%] [G loss: 0.7064580917358398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 28/86 [D loss: 0.6884649991989136, acc.: 55.62%] [G loss: 0.7073414325714111]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 29/86 [D loss: 0.6889228522777557, acc.: 54.10%] [G loss: 0.7059723138809204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 30/86 [D loss: 0.6877961456775665, acc.: 55.91%] [G loss: 0.7074723243713379]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 31/86 [D loss: 0.6896201372146606, acc.: 53.71%] [G loss: 0.7043159604072571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 32/86 [D loss: 0.690038800239563, acc.: 53.96%] [G loss: 0.7078278064727783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 33/86 [D loss: 0.6884098052978516, acc.: 55.32%] [G loss: 0.7036567330360413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 34/86 [D loss: 0.6898244619369507, acc.: 53.27%] [G loss: 0.7060881853103638]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 35/86 [D loss: 0.687256783246994, acc.: 56.01%] [G loss: 0.7075100541114807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 36/86 [D loss: 0.6903657019138336, acc.: 54.00%] [G loss: 0.7042427659034729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 37/86 [D loss: 0.6888217329978943, acc.: 55.22%] [G loss: 0.7081894278526306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 38/86 [D loss: 0.6885789632797241, acc.: 54.35%] [G loss: 0.7030838131904602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 39/86 [D loss: 0.6916393339633942, acc.: 50.05%] [G loss: 0.7075954675674438]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 40/86 [D loss: 0.6871404945850372, acc.: 57.52%] [G loss: 0.7093884944915771]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 41/86 [D loss: 0.6914150714874268, acc.: 51.90%] [G loss: 0.7028715014457703]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 42/86 [D loss: 0.6905599534511566, acc.: 52.39%] [G loss: 0.7081998586654663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 43/86 [D loss: 0.6900043487548828, acc.: 53.56%] [G loss: 0.7018265128135681]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 44/86 [D loss: 0.6913602948188782, acc.: 51.95%] [G loss: 0.7054990530014038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 45/86 [D loss: 0.686034083366394, acc.: 58.06%] [G loss: 0.7027338743209839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 46/86 [D loss: 0.6945070326328278, acc.: 49.46%] [G loss: 0.7022466063499451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 47/86 [D loss: 0.6855629980564117, acc.: 60.25%] [G loss: 0.7053343057632446]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 48/86 [D loss: 0.6911178529262543, acc.: 52.64%] [G loss: 0.7016902565956116]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 49/86 [D loss: 0.690286785364151, acc.: 52.88%] [G loss: 0.7065845131874084]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 50/86 [D loss: 0.6862732768058777, acc.: 58.15%] [G loss: 0.7018188238143921]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 51/86 [D loss: 0.6945381760597229, acc.: 48.29%] [G loss: 0.7032508850097656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 52/86 [D loss: 0.6867800354957581, acc.: 58.15%] [G loss: 0.7077955603599548]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 53/86 [D loss: 0.6916510164737701, acc.: 50.63%] [G loss: 0.6994773149490356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 54/86 [D loss: 0.6907338798046112, acc.: 53.17%] [G loss: 0.7067387700080872]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 55/86 [D loss: 0.687755286693573, acc.: 54.74%] [G loss: 0.7024601697921753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 56/86 [D loss: 0.6922364234924316, acc.: 51.95%] [G loss: 0.7011910080909729]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 57/86 [D loss: 0.69062939286232, acc.: 52.83%] [G loss: 0.709572970867157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 58/86 [D loss: 0.688044011592865, acc.: 55.96%] [G loss: 0.7018739581108093]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 59/86 [D loss: 0.691333144903183, acc.: 51.95%] [G loss: 0.7042794227600098]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 60/86 [D loss: 0.6883790791034698, acc.: 55.27%] [G loss: 0.7068708539009094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 61/86 [D loss: 0.6888786852359772, acc.: 54.30%] [G loss: 0.7026705145835876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 62/86 [D loss: 0.6931868195533752, acc.: 49.12%] [G loss: 0.7070038318634033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 63/86 [D loss: 0.6875267326831818, acc.: 56.79%] [G loss: 0.7067850232124329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 64/86 [D loss: 0.6898652911186218, acc.: 53.66%] [G loss: 0.7061065435409546]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 65/86 [D loss: 0.689421683549881, acc.: 54.44%] [G loss: 0.7083369493484497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 66/86 [D loss: 0.689315676689148, acc.: 54.88%] [G loss: 0.7042514085769653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 67/86 [D loss: 0.6916947364807129, acc.: 53.71%] [G loss: 0.7033867239952087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 68/86 [D loss: 0.6887731552124023, acc.: 56.25%] [G loss: 0.7070159912109375]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 69/86 [D loss: 0.6878972947597504, acc.: 56.93%] [G loss: 0.705075204372406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 70/86 [D loss: 0.6904513835906982, acc.: 53.76%] [G loss: 0.707658052444458]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 71/86 [D loss: 0.6891435086727142, acc.: 55.91%] [G loss: 0.708583652973175]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 72/86 [D loss: 0.6904778778553009, acc.: 52.59%] [G loss: 0.704247772693634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 73/86 [D loss: 0.6905405819416046, acc.: 53.76%] [G loss: 0.7092620730400085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 74/86 [D loss: 0.6869164109230042, acc.: 57.81%] [G loss: 0.7077268958091736]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 75/86 [D loss: 0.6905708909034729, acc.: 52.64%] [G loss: 0.7071018218994141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 76/86 [D loss: 0.6883029043674469, acc.: 55.66%] [G loss: 0.708231508731842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 77/86 [D loss: 0.6884517073631287, acc.: 56.88%] [G loss: 0.7061033248901367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 78/86 [D loss: 0.6919365227222443, acc.: 52.98%] [G loss: 0.7068282961845398]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 79/86 [D loss: 0.6877229809761047, acc.: 56.74%] [G loss: 0.7084711194038391]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 80/86 [D loss: 0.6886135637760162, acc.: 55.52%] [G loss: 0.7062993049621582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 81/86 [D loss: 0.6888169348239899, acc.: 55.22%] [G loss: 0.708326518535614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 82/86 [D loss: 0.6881431937217712, acc.: 55.91%] [G loss: 0.7052986025810242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 83/86 [D loss: 0.6904306411743164, acc.: 53.22%] [G loss: 0.7074474096298218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 84/86 [D loss: 0.6874878406524658, acc.: 56.64%] [G loss: 0.7104706764221191]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 72/200, Batch 85/86 [D loss: 0.6886038482189178, acc.: 55.96%] [G loss: 0.7076960802078247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 72/200, Batch 86/86 [D loss: 0.6893297135829926, acc.: 54.59%] [G loss: 0.7066398859024048]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 1/86 [D loss: 0.6881765723228455, acc.: 55.27%] [G loss: 0.7072103023529053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 2/86 [D loss: 0.6895541250705719, acc.: 54.93%] [G loss: 0.7050325274467468]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 3/86 [D loss: 0.6891533732414246, acc.: 54.20%] [G loss: 0.7075484991073608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 4/86 [D loss: 0.6894452571868896, acc.: 54.79%] [G loss: 0.7058795690536499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 5/86 [D loss: 0.6896512806415558, acc.: 53.47%] [G loss: 0.7093443870544434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 6/86 [D loss: 0.6886666417121887, acc.: 55.76%] [G loss: 0.7064090371131897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 7/86 [D loss: 0.6895515024662018, acc.: 54.49%] [G loss: 0.7070459127426147]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 8/86 [D loss: 0.6879639327526093, acc.: 56.64%] [G loss: 0.7063548564910889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 9/86 [D loss: 0.688880980014801, acc.: 54.93%] [G loss: 0.7069951295852661]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 10/86 [D loss: 0.68802809715271, acc.: 56.01%] [G loss: 0.7055992484092712]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 11/86 [D loss: 0.6904285848140717, acc.: 52.10%] [G loss: 0.7079577445983887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 12/86 [D loss: 0.6885586678981781, acc.: 56.30%] [G loss: 0.7074035406112671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 13/86 [D loss: 0.6912292540073395, acc.: 52.15%] [G loss: 0.7054771184921265]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 14/86 [D loss: 0.6876660883426666, acc.: 57.32%] [G loss: 0.7089186310768127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 15/86 [D loss: 0.690263032913208, acc.: 53.32%] [G loss: 0.7073709964752197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 16/86 [D loss: 0.6891810595989227, acc.: 54.74%] [G loss: 0.7104219198226929]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 17/86 [D loss: 0.6891417503356934, acc.: 54.15%] [G loss: 0.7083224058151245]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 18/86 [D loss: 0.6911837458610535, acc.: 52.05%] [G loss: 0.7082209587097168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 19/86 [D loss: 0.6892495453357697, acc.: 54.49%] [G loss: 0.7073315382003784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 20/86 [D loss: 0.6884272992610931, acc.: 56.10%] [G loss: 0.7072944641113281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 21/86 [D loss: 0.6879048645496368, acc.: 56.54%] [G loss: 0.7087686061859131]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 22/86 [D loss: 0.6891556680202484, acc.: 54.25%] [G loss: 0.7046123147010803]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 23/86 [D loss: 0.6883982419967651, acc.: 56.25%] [G loss: 0.7071137428283691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 24/86 [D loss: 0.6883262395858765, acc.: 57.13%] [G loss: 0.7049465775489807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 25/86 [D loss: 0.6899019777774811, acc.: 53.08%] [G loss: 0.705409586429596]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 26/86 [D loss: 0.6882733404636383, acc.: 55.52%] [G loss: 0.7083451747894287]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 27/86 [D loss: 0.6889021992683411, acc.: 55.37%] [G loss: 0.7075328826904297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 28/86 [D loss: 0.6903706192970276, acc.: 54.44%] [G loss: 0.7089749574661255]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 29/86 [D loss: 0.6895341277122498, acc.: 54.79%] [G loss: 0.7066134214401245]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 30/86 [D loss: 0.6886423826217651, acc.: 54.74%] [G loss: 0.707507848739624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 31/86 [D loss: 0.6888976097106934, acc.: 54.15%] [G loss: 0.7090436816215515]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 32/86 [D loss: 0.6902705729007721, acc.: 52.69%] [G loss: 0.7072944045066833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 33/86 [D loss: 0.6898705065250397, acc.: 53.71%] [G loss: 0.7078808546066284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 34/86 [D loss: 0.688702791929245, acc.: 54.74%] [G loss: 0.7079139351844788]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 35/86 [D loss: 0.6882030069828033, acc.: 54.69%] [G loss: 0.7081544399261475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 36/86 [D loss: 0.6881823837757111, acc.: 56.49%] [G loss: 0.7104611396789551]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 37/86 [D loss: 0.6879000961780548, acc.: 56.25%] [G loss: 0.7088003158569336]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 38/86 [D loss: 0.6891461312770844, acc.: 53.32%] [G loss: 0.7070564031600952]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 39/86 [D loss: 0.6884652972221375, acc.: 55.71%] [G loss: 0.7081700563430786]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 40/86 [D loss: 0.688752144575119, acc.: 55.57%] [G loss: 0.7068624496459961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 41/86 [D loss: 0.6889947354793549, acc.: 55.52%] [G loss: 0.7078241109848022]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 42/86 [D loss: 0.687904953956604, acc.: 56.25%] [G loss: 0.7064465284347534]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 43/86 [D loss: 0.6901189982891083, acc.: 53.96%] [G loss: 0.7053696513175964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 44/86 [D loss: 0.6897223591804504, acc.: 53.71%] [G loss: 0.7077465057373047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 45/86 [D loss: 0.689661979675293, acc.: 54.25%] [G loss: 0.7069369554519653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 46/86 [D loss: 0.6896403431892395, acc.: 54.69%] [G loss: 0.708694577217102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 47/86 [D loss: 0.6901257932186127, acc.: 55.08%] [G loss: 0.7054226994514465]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 48/86 [D loss: 0.6903500258922577, acc.: 53.91%] [G loss: 0.7074673771858215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 49/86 [D loss: 0.6884713470935822, acc.: 55.13%] [G loss: 0.7080132365226746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 50/86 [D loss: 0.6900292634963989, acc.: 52.88%] [G loss: 0.707197368144989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 51/86 [D loss: 0.6884033977985382, acc.: 56.15%] [G loss: 0.7085357308387756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 52/86 [D loss: 0.6885711848735809, acc.: 55.57%] [G loss: 0.7048050165176392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 53/86 [D loss: 0.6904712617397308, acc.: 52.69%] [G loss: 0.7084336280822754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 54/86 [D loss: 0.6873882114887238, acc.: 56.54%] [G loss: 0.7040168046951294]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 55/86 [D loss: 0.6903701424598694, acc.: 52.83%] [G loss: 0.7056007981300354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 56/86 [D loss: 0.6894274353981018, acc.: 54.74%] [G loss: 0.7093422412872314]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 57/86 [D loss: 0.6882180869579315, acc.: 56.20%] [G loss: 0.7048510909080505]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 58/86 [D loss: 0.691039651632309, acc.: 53.32%] [G loss: 0.7074128985404968]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 59/86 [D loss: 0.686163455247879, acc.: 58.59%] [G loss: 0.7086300849914551]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 60/86 [D loss: 0.6909627616405487, acc.: 52.54%] [G loss: 0.7024779319763184]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 61/86 [D loss: 0.6908024251461029, acc.: 52.29%] [G loss: 0.708561897277832]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 62/86 [D loss: 0.6882551312446594, acc.: 55.52%] [G loss: 0.7041361331939697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 63/86 [D loss: 0.6928858458995819, acc.: 50.98%] [G loss: 0.706349790096283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 64/86 [D loss: 0.6867481470108032, acc.: 58.40%] [G loss: 0.7077045440673828]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 65/86 [D loss: 0.6915861964225769, acc.: 52.59%] [G loss: 0.7013962268829346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 66/86 [D loss: 0.6906361877918243, acc.: 53.03%] [G loss: 0.7072449922561646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 67/86 [D loss: 0.6874295175075531, acc.: 57.52%] [G loss: 0.7026159763336182]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 68/86 [D loss: 0.6957090198993683, acc.: 47.36%] [G loss: 0.7038059830665588]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 69/86 [D loss: 0.6874427795410156, acc.: 57.13%] [G loss: 0.707262396812439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 70/86 [D loss: 0.6933844685554504, acc.: 48.49%] [G loss: 0.7006883025169373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 71/86 [D loss: 0.6916859149932861, acc.: 50.88%] [G loss: 0.7084617614746094]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 72/86 [D loss: 0.6858585476875305, acc.: 58.74%] [G loss: 0.7033478021621704]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 73/86 [D loss: 0.6942190825939178, acc.: 48.68%] [G loss: 0.7004990577697754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 74/86 [D loss: 0.6880475282669067, acc.: 56.05%] [G loss: 0.7091138362884521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 75/86 [D loss: 0.6904073655605316, acc.: 52.73%] [G loss: 0.701390266418457]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 76/86 [D loss: 0.6915240585803986, acc.: 52.29%] [G loss: 0.7061401605606079]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 77/86 [D loss: 0.6878696084022522, acc.: 55.57%] [G loss: 0.7058861255645752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 78/86 [D loss: 0.6901299357414246, acc.: 53.96%] [G loss: 0.6997029781341553]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 79/86 [D loss: 0.6914149522781372, acc.: 51.95%] [G loss: 0.7090511918067932]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 80/86 [D loss: 0.6883427500724792, acc.: 56.05%] [G loss: 0.7057108879089355]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 81/86 [D loss: 0.6898375153541565, acc.: 53.91%] [G loss: 0.7053956985473633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 82/86 [D loss: 0.6875425279140472, acc.: 57.03%] [G loss: 0.7090001106262207]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 83/86 [D loss: 0.6893294155597687, acc.: 56.20%] [G loss: 0.7052637338638306]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 84/86 [D loss: 0.6921137571334839, acc.: 50.39%] [G loss: 0.7062664031982422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 73/200, Batch 85/86 [D loss: 0.6873091757297516, acc.: 57.42%] [G loss: 0.7078669667243958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 73/200, Batch 86/86 [D loss: 0.6891849637031555, acc.: 55.18%] [G loss: 0.7069684863090515]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 1/86 [D loss: 0.688117504119873, acc.: 56.35%] [G loss: 0.7099000215530396]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 2/86 [D loss: 0.6887061297893524, acc.: 55.91%] [G loss: 0.7084313631057739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 3/86 [D loss: 0.6885727643966675, acc.: 54.00%] [G loss: 0.7070228457450867]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 4/86 [D loss: 0.6893820464611053, acc.: 54.59%] [G loss: 0.7098588943481445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 5/86 [D loss: 0.6871819496154785, acc.: 56.74%] [G loss: 0.7084303498268127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 6/86 [D loss: 0.6887699067592621, acc.: 55.52%] [G loss: 0.7059913277626038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 7/86 [D loss: 0.6874728202819824, acc.: 56.20%] [G loss: 0.707697331905365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 8/86 [D loss: 0.6891968250274658, acc.: 54.88%] [G loss: 0.7060719132423401]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 9/86 [D loss: 0.6882903277873993, acc.: 55.96%] [G loss: 0.7079458832740784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 10/86 [D loss: 0.6887246072292328, acc.: 53.71%] [G loss: 0.7077741622924805]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 11/86 [D loss: 0.6882103085517883, acc.: 54.74%] [G loss: 0.7087465524673462]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 12/86 [D loss: 0.6883275508880615, acc.: 56.88%] [G loss: 0.7106501460075378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 13/86 [D loss: 0.687995046377182, acc.: 55.08%] [G loss: 0.7080302834510803]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 14/86 [D loss: 0.6900363266468048, acc.: 54.88%] [G loss: 0.7092010378837585]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 15/86 [D loss: 0.6876622438430786, acc.: 55.18%] [G loss: 0.7079833149909973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 16/86 [D loss: 0.6881843209266663, acc.: 54.49%] [G loss: 0.7070250511169434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 17/86 [D loss: 0.6885931193828583, acc.: 54.79%] [G loss: 0.7101314663887024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 18/86 [D loss: 0.6884666979312897, acc.: 55.66%] [G loss: 0.7074031233787537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 19/86 [D loss: 0.6902223229408264, acc.: 53.22%] [G loss: 0.7074989080429077]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 20/86 [D loss: 0.6868425607681274, acc.: 57.76%] [G loss: 0.7082928419113159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 21/86 [D loss: 0.6888367235660553, acc.: 54.20%] [G loss: 0.706291139125824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 22/86 [D loss: 0.6880696415901184, acc.: 55.47%] [G loss: 0.7079684734344482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 23/86 [D loss: 0.6892502009868622, acc.: 54.83%] [G loss: 0.7077409625053406]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 24/86 [D loss: 0.6898023784160614, acc.: 53.61%] [G loss: 0.7071256041526794]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 25/86 [D loss: 0.6890427768230438, acc.: 54.35%] [G loss: 0.7087101936340332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 26/86 [D loss: 0.6877085864543915, acc.: 56.93%] [G loss: 0.7068963050842285]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 27/86 [D loss: 0.6876466572284698, acc.: 57.13%] [G loss: 0.7091214060783386]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 28/86 [D loss: 0.6899965703487396, acc.: 54.10%] [G loss: 0.706018328666687]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 29/86 [D loss: 0.6884596049785614, acc.: 56.05%] [G loss: 0.7080190181732178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 30/86 [D loss: 0.6881102323532104, acc.: 54.93%] [G loss: 0.7057713866233826]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 31/86 [D loss: 0.6878517270088196, acc.: 57.13%] [G loss: 0.7040219306945801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 32/86 [D loss: 0.6890737116336823, acc.: 54.54%] [G loss: 0.7071576714515686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 33/86 [D loss: 0.6879494786262512, acc.: 55.66%] [G loss: 0.7076235413551331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 34/86 [D loss: 0.6901364028453827, acc.: 53.91%] [G loss: 0.7081433534622192]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 35/86 [D loss: 0.6898268163204193, acc.: 53.32%] [G loss: 0.7089714407920837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 36/86 [D loss: 0.6890614926815033, acc.: 54.64%] [G loss: 0.7087714672088623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 37/86 [D loss: 0.6878065168857574, acc.: 55.18%] [G loss: 0.7091413736343384]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 38/86 [D loss: 0.6893333494663239, acc.: 55.08%] [G loss: 0.7076630592346191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 39/86 [D loss: 0.6876765489578247, acc.: 56.98%] [G loss: 0.7071928977966309]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 40/86 [D loss: 0.688275009393692, acc.: 56.59%] [G loss: 0.7061942219734192]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 41/86 [D loss: 0.6887962818145752, acc.: 55.81%] [G loss: 0.7051138877868652]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 42/86 [D loss: 0.6878011226654053, acc.: 58.06%] [G loss: 0.7107561230659485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 43/86 [D loss: 0.6889304518699646, acc.: 54.64%] [G loss: 0.7077040672302246]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 44/86 [D loss: 0.6879639029502869, acc.: 55.76%] [G loss: 0.7066452503204346]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 74/200, Batch 45/86 [D loss: 0.6879845261573792, acc.: 54.98%] [G loss: 0.7100809216499329]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 46/86 [D loss: 0.6891330480575562, acc.: 54.69%] [G loss: 0.708310604095459]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 47/86 [D loss: 0.6889722347259521, acc.: 53.61%] [G loss: 0.7092272639274597]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 48/86 [D loss: 0.6886433959007263, acc.: 55.81%] [G loss: 0.7088418006896973]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 49/86 [D loss: 0.687345027923584, acc.: 56.64%] [G loss: 0.7062361836433411]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 50/86 [D loss: 0.6895468533039093, acc.: 54.69%] [G loss: 0.7076407670974731]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 51/86 [D loss: 0.6870838701725006, acc.: 56.54%] [G loss: 0.7086478471755981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 52/86 [D loss: 0.688850611448288, acc.: 54.35%] [G loss: 0.7078185677528381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 53/86 [D loss: 0.6882829070091248, acc.: 55.32%] [G loss: 0.7087861895561218]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 54/86 [D loss: 0.688440352678299, acc.: 56.64%] [G loss: 0.7063525319099426]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 55/86 [D loss: 0.6891891360282898, acc.: 54.79%] [G loss: 0.707713782787323]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 56/86 [D loss: 0.6882778406143188, acc.: 55.03%] [G loss: 0.7075750827789307]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 57/86 [D loss: 0.689661979675293, acc.: 53.47%] [G loss: 0.7066051363945007]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 58/86 [D loss: 0.688495934009552, acc.: 53.66%] [G loss: 0.7074978351593018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 59/86 [D loss: 0.6896148025989532, acc.: 54.54%] [G loss: 0.7073401212692261]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 60/86 [D loss: 0.6890789866447449, acc.: 55.22%] [G loss: 0.7102653980255127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 61/86 [D loss: 0.6883673071861267, acc.: 55.66%] [G loss: 0.7074403762817383]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 62/86 [D loss: 0.6888293623924255, acc.: 54.93%] [G loss: 0.7087170481681824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 63/86 [D loss: 0.6891184747219086, acc.: 55.32%] [G loss: 0.7115378379821777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 64/86 [D loss: 0.6868812143802643, acc.: 56.93%] [G loss: 0.7089167237281799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 65/86 [D loss: 0.6887615323066711, acc.: 55.13%] [G loss: 0.7089816331863403]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 66/86 [D loss: 0.6883241236209869, acc.: 56.35%] [G loss: 0.7092838883399963]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 74/200, Batch 67/86 [D loss: 0.6887768507003784, acc.: 54.74%] [G loss: 0.7106091380119324]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 68/86 [D loss: 0.6890657842159271, acc.: 54.44%] [G loss: 0.7101014852523804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 69/86 [D loss: 0.689957469701767, acc.: 54.10%] [G loss: 0.7080265283584595]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 70/86 [D loss: 0.6888833940029144, acc.: 54.88%] [G loss: 0.7101873159408569]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 71/86 [D loss: 0.688294380903244, acc.: 55.62%] [G loss: 0.7086765170097351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 72/86 [D loss: 0.6888633668422699, acc.: 54.44%] [G loss: 0.7097005248069763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 73/86 [D loss: 0.687872439622879, acc.: 56.69%] [G loss: 0.7090634107589722]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 74/86 [D loss: 0.6900525689125061, acc.: 53.37%] [G loss: 0.7087768912315369]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 75/86 [D loss: 0.6886406540870667, acc.: 55.08%] [G loss: 0.710407018661499]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 76/86 [D loss: 0.6881548464298248, acc.: 54.88%] [G loss: 0.7094488143920898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 77/86 [D loss: 0.6885398626327515, acc.: 55.57%] [G loss: 0.7102838754653931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 78/86 [D loss: 0.6882280111312866, acc.: 55.18%] [G loss: 0.7077394723892212]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 79/86 [D loss: 0.6898544132709503, acc.: 53.47%] [G loss: 0.7077808380126953]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 80/86 [D loss: 0.6882712244987488, acc.: 54.44%] [G loss: 0.7100630402565002]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 81/86 [D loss: 0.6895955502986908, acc.: 54.20%] [G loss: 0.7083340883255005]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 82/86 [D loss: 0.6890766620635986, acc.: 53.96%] [G loss: 0.7102770805358887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 83/86 [D loss: 0.6883343458175659, acc.: 55.13%] [G loss: 0.7074100375175476]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 84/86 [D loss: 0.6908193826675415, acc.: 52.34%] [G loss: 0.7098100185394287]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 74/200, Batch 85/86 [D loss: 0.6875565052032471, acc.: 55.47%] [G loss: 0.7100471258163452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 74/200, Batch 86/86 [D loss: 0.6886309683322906, acc.: 54.74%] [G loss: 0.709721565246582]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 1/86 [D loss: 0.6895627379417419, acc.: 53.08%] [G loss: 0.7102336883544922]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 2/86 [D loss: 0.686085432767868, acc.: 57.52%] [G loss: 0.7055968642234802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 3/86 [D loss: 0.6912720501422882, acc.: 52.49%] [G loss: 0.7129906415939331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 4/86 [D loss: 0.6875119209289551, acc.: 55.62%] [G loss: 0.7046506404876709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 5/86 [D loss: 0.6924876868724823, acc.: 50.93%] [G loss: 0.7092396020889282]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 6/86 [D loss: 0.6868375837802887, acc.: 57.08%] [G loss: 0.7104471921920776]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 7/86 [D loss: 0.6900350153446198, acc.: 51.95%] [G loss: 0.7061630487442017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 8/86 [D loss: 0.6900532841682434, acc.: 54.88%] [G loss: 0.7099337577819824]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 9/86 [D loss: 0.687075287103653, acc.: 56.15%] [G loss: 0.7048409581184387]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 10/86 [D loss: 0.6903089582920074, acc.: 53.61%] [G loss: 0.7066715955734253]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 11/86 [D loss: 0.6865440905094147, acc.: 58.20%] [G loss: 0.7086841464042664]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 12/86 [D loss: 0.691038966178894, acc.: 52.98%] [G loss: 0.7045920491218567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 13/86 [D loss: 0.6901877820491791, acc.: 53.42%] [G loss: 0.7093176245689392]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 14/86 [D loss: 0.6880666315555573, acc.: 54.98%] [G loss: 0.7046543955802917]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 15/86 [D loss: 0.6887378692626953, acc.: 54.25%] [G loss: 0.7085440158843994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 16/86 [D loss: 0.6871083974838257, acc.: 57.03%] [G loss: 0.7062329053878784]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 17/86 [D loss: 0.6913052499294281, acc.: 52.25%] [G loss: 0.7049932479858398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 18/86 [D loss: 0.6894335150718689, acc.: 54.15%] [G loss: 0.7100024223327637]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 19/86 [D loss: 0.688698947429657, acc.: 53.42%] [G loss: 0.7055632472038269]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 20/86 [D loss: 0.69106724858284, acc.: 53.17%] [G loss: 0.7110699415206909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 21/86 [D loss: 0.6882590353488922, acc.: 55.96%] [G loss: 0.7042960524559021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 22/86 [D loss: 0.6906432807445526, acc.: 52.49%] [G loss: 0.7077934145927429]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 23/86 [D loss: 0.6879908442497253, acc.: 55.32%] [G loss: 0.7089105844497681]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 24/86 [D loss: 0.6909653842449188, acc.: 52.25%] [G loss: 0.7064430117607117]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 25/86 [D loss: 0.6891436278820038, acc.: 54.44%] [G loss: 0.7080346941947937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 26/86 [D loss: 0.6885693073272705, acc.: 55.37%] [G loss: 0.7078567147254944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 27/86 [D loss: 0.6906851530075073, acc.: 53.66%] [G loss: 0.7088613510131836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 28/86 [D loss: 0.6868962347507477, acc.: 56.69%] [G loss: 0.7092556357383728]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 29/86 [D loss: 0.6898437142372131, acc.: 53.66%] [G loss: 0.7076854109764099]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 30/86 [D loss: 0.6887880861759186, acc.: 54.69%] [G loss: 0.7083351612091064]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 31/86 [D loss: 0.6873776614665985, acc.: 56.35%] [G loss: 0.705924391746521]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 32/86 [D loss: 0.6905324459075928, acc.: 52.15%] [G loss: 0.7052454352378845]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 33/86 [D loss: 0.6882618367671967, acc.: 56.69%] [G loss: 0.709010660648346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 34/86 [D loss: 0.6897806525230408, acc.: 53.96%] [G loss: 0.706653892993927]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 35/86 [D loss: 0.6906918287277222, acc.: 51.27%] [G loss: 0.709356963634491]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 36/86 [D loss: 0.6903021037578583, acc.: 53.81%] [G loss: 0.7057960629463196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 37/86 [D loss: 0.6903862953186035, acc.: 53.08%] [G loss: 0.7102478742599487]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 38/86 [D loss: 0.6877715587615967, acc.: 55.32%] [G loss: 0.7092124223709106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 39/86 [D loss: 0.6900951564311981, acc.: 53.12%] [G loss: 0.7068572640419006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 40/86 [D loss: 0.6877943873405457, acc.: 55.18%] [G loss: 0.7097906470298767]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 41/86 [D loss: 0.6882884800434113, acc.: 55.03%] [G loss: 0.7070815563201904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 42/86 [D loss: 0.6898031532764435, acc.: 54.49%] [G loss: 0.7081456184387207]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 43/86 [D loss: 0.6877122521400452, acc.: 54.88%] [G loss: 0.7070035934448242]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 44/86 [D loss: 0.6904327571392059, acc.: 52.49%] [G loss: 0.7083317637443542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 45/86 [D loss: 0.687452107667923, acc.: 56.30%] [G loss: 0.7098048329353333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 46/86 [D loss: 0.6883505582809448, acc.: 54.83%] [G loss: 0.7083108425140381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 47/86 [D loss: 0.6894696056842804, acc.: 55.03%] [G loss: 0.7084953784942627]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 48/86 [D loss: 0.687416672706604, acc.: 55.22%] [G loss: 0.7078613042831421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 49/86 [D loss: 0.6887734234333038, acc.: 55.32%] [G loss: 0.7085452079772949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 50/86 [D loss: 0.686605840921402, acc.: 57.32%] [G loss: 0.7091560363769531]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 51/86 [D loss: 0.6888224184513092, acc.: 54.69%] [G loss: 0.707309365272522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 52/86 [D loss: 0.6890392899513245, acc.: 54.83%] [G loss: 0.7090717554092407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 53/86 [D loss: 0.6892822682857513, acc.: 54.59%] [G loss: 0.7112208604812622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 54/86 [D loss: 0.6872524321079254, acc.: 56.84%] [G loss: 0.7094753384590149]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 55/86 [D loss: 0.6881862580776215, acc.: 54.64%] [G loss: 0.712834358215332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 56/86 [D loss: 0.6891508400440216, acc.: 54.64%] [G loss: 0.7090324759483337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 57/86 [D loss: 0.6889864802360535, acc.: 54.88%] [G loss: 0.7080914974212646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 58/86 [D loss: 0.6880660653114319, acc.: 56.64%] [G loss: 0.7101333141326904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 59/86 [D loss: 0.6877010762691498, acc.: 54.98%] [G loss: 0.7094352841377258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 60/86 [D loss: 0.6877985596656799, acc.: 56.84%] [G loss: 0.708486795425415]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 61/86 [D loss: 0.6883978545665741, acc.: 55.13%] [G loss: 0.7121373414993286]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 62/86 [D loss: 0.6886506080627441, acc.: 54.20%] [G loss: 0.7089459896087646]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 63/86 [D loss: 0.6890740990638733, acc.: 54.79%] [G loss: 0.7112873792648315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 64/86 [D loss: 0.6870926320552826, acc.: 57.62%] [G loss: 0.7112404108047485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 65/86 [D loss: 0.6898813247680664, acc.: 54.00%] [G loss: 0.7097988724708557]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 66/86 [D loss: 0.688079297542572, acc.: 55.13%] [G loss: 0.711122989654541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 67/86 [D loss: 0.6895710825920105, acc.: 53.42%] [G loss: 0.7080710530281067]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 68/86 [D loss: 0.6890074908733368, acc.: 54.49%] [G loss: 0.7108414173126221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 69/86 [D loss: 0.6895267963409424, acc.: 54.83%] [G loss: 0.7085446715354919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 70/86 [D loss: 0.690776139497757, acc.: 51.86%] [G loss: 0.7105022668838501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 71/86 [D loss: 0.6888238191604614, acc.: 56.20%] [G loss: 0.7103351354598999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 72/86 [D loss: 0.689246416091919, acc.: 52.83%] [G loss: 0.7087777853012085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 73/86 [D loss: 0.6871142983436584, acc.: 56.35%] [G loss: 0.7105345129966736]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 74/86 [D loss: 0.6875724792480469, acc.: 56.79%] [G loss: 0.708172082901001]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 75/86 [D loss: 0.6887940466403961, acc.: 54.35%] [G loss: 0.7084892392158508]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 76/86 [D loss: 0.6891151070594788, acc.: 55.52%] [G loss: 0.7099134922027588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 77/86 [D loss: 0.6882829070091248, acc.: 55.47%] [G loss: 0.707665205001831]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 78/86 [D loss: 0.6878091394901276, acc.: 56.05%] [G loss: 0.7103275656700134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 79/86 [D loss: 0.6871024370193481, acc.: 56.69%] [G loss: 0.7076542377471924]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 80/86 [D loss: 0.6889320909976959, acc.: 55.03%] [G loss: 0.710117757320404]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 81/86 [D loss: 0.6885877549648285, acc.: 55.13%] [G loss: 0.7089177966117859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 82/86 [D loss: 0.6881609857082367, acc.: 55.42%] [G loss: 0.7099148631095886]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 83/86 [D loss: 0.6875665485858917, acc.: 57.32%] [G loss: 0.7112001180648804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 75/200, Batch 84/86 [D loss: 0.6879013478755951, acc.: 56.93%] [G loss: 0.711167573928833]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 85/86 [D loss: 0.6877301037311554, acc.: 55.08%] [G loss: 0.7104755640029907]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 75/200, Batch 86/86 [D loss: 0.6872332692146301, acc.: 56.74%] [G loss: 0.708631157875061]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 1/86 [D loss: 0.6901231110095978, acc.: 53.03%] [G loss: 0.711036205291748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 2/86 [D loss: 0.6878871321678162, acc.: 56.30%] [G loss: 0.7078379988670349]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 3/86 [D loss: 0.6899703145027161, acc.: 52.59%] [G loss: 0.70875483751297]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 4/86 [D loss: 0.6895765364170074, acc.: 54.15%] [G loss: 0.71100914478302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 5/86 [D loss: 0.6897674202919006, acc.: 53.42%] [G loss: 0.7071720957756042]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 6/86 [D loss: 0.6884936392307281, acc.: 55.62%] [G loss: 0.7084356546401978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 7/86 [D loss: 0.6875404715538025, acc.: 56.49%] [G loss: 0.7064802646636963]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 8/86 [D loss: 0.6896737217903137, acc.: 54.79%] [G loss: 0.7101724147796631]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 9/86 [D loss: 0.6891264915466309, acc.: 53.56%] [G loss: 0.7114137411117554]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 10/86 [D loss: 0.6878067553043365, acc.: 56.25%] [G loss: 0.7047826051712036]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 11/86 [D loss: 0.6887666881084442, acc.: 53.81%] [G loss: 0.7093656063079834]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 12/86 [D loss: 0.6895142793655396, acc.: 54.20%] [G loss: 0.7091135382652283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 13/86 [D loss: 0.688557505607605, acc.: 53.12%] [G loss: 0.7100861072540283]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 14/86 [D loss: 0.6881172955036163, acc.: 56.88%] [G loss: 0.7103332281112671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 15/86 [D loss: 0.6892658174037933, acc.: 53.66%] [G loss: 0.7088736295700073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 16/86 [D loss: 0.6895458698272705, acc.: 54.49%] [G loss: 0.7089332938194275]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 17/86 [D loss: 0.6895298063755035, acc.: 53.86%] [G loss: 0.7083175778388977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 18/86 [D loss: 0.6866799592971802, acc.: 57.62%] [G loss: 0.7124712467193604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 19/86 [D loss: 0.6885111629962921, acc.: 54.79%] [G loss: 0.7094036340713501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 20/86 [D loss: 0.6889116168022156, acc.: 54.93%] [G loss: 0.7102260589599609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 21/86 [D loss: 0.6887369751930237, acc.: 54.93%] [G loss: 0.7114876508712769]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 22/86 [D loss: 0.6884939670562744, acc.: 54.98%] [G loss: 0.7081355452537537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 23/86 [D loss: 0.6889053285121918, acc.: 53.27%] [G loss: 0.7105705738067627]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 24/86 [D loss: 0.6862273514270782, acc.: 57.47%] [G loss: 0.7073363661766052]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 25/86 [D loss: 0.6903351843357086, acc.: 53.76%] [G loss: 0.7121917605400085]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 26/86 [D loss: 0.6868135333061218, acc.: 56.74%] [G loss: 0.7112042307853699]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 27/86 [D loss: 0.69035804271698, acc.: 52.39%] [G loss: 0.7103134393692017]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 28/86 [D loss: 0.6889671087265015, acc.: 53.81%] [G loss: 0.7118062973022461]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 29/86 [D loss: 0.6882317066192627, acc.: 56.10%] [G loss: 0.7057965397834778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 30/86 [D loss: 0.6902510523796082, acc.: 52.64%] [G loss: 0.713373601436615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 31/86 [D loss: 0.6877838671207428, acc.: 56.01%] [G loss: 0.7072004675865173]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 32/86 [D loss: 0.6917508244514465, acc.: 52.10%] [G loss: 0.7099438905715942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 33/86 [D loss: 0.68638476729393, acc.: 56.98%] [G loss: 0.7085632085800171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 34/86 [D loss: 0.6918396055698395, acc.: 50.88%] [G loss: 0.7073978781700134]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 35/86 [D loss: 0.6861425936222076, acc.: 58.06%] [G loss: 0.7109557390213013]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 36/86 [D loss: 0.6904957294464111, acc.: 53.32%] [G loss: 0.6999320387840271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 37/86 [D loss: 0.6912810802459717, acc.: 51.66%] [G loss: 0.7138356566429138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 38/86 [D loss: 0.6864855587482452, acc.: 57.28%] [G loss: 0.6992597579956055]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 39/86 [D loss: 0.695543497800827, acc.: 47.66%] [G loss: 0.7109470367431641]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 40/86 [D loss: 0.6845757067203522, acc.: 59.52%] [G loss: 0.7032313942909241]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 41/86 [D loss: 0.6947649419307709, acc.: 47.66%] [G loss: 0.6984899640083313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 42/86 [D loss: 0.690296083688736, acc.: 52.25%] [G loss: 0.7150522470474243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 43/86 [D loss: 0.6847434043884277, acc.: 60.01%] [G loss: 0.7016595602035522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 44/86 [D loss: 0.6978275179862976, acc.: 46.14%] [G loss: 0.7027243375778198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 45/86 [D loss: 0.6861567795276642, acc.: 58.84%] [G loss: 0.7079020738601685]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 46/86 [D loss: 0.6947261393070221, acc.: 47.41%] [G loss: 0.6977102756500244]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 47/86 [D loss: 0.6908195316791534, acc.: 52.44%] [G loss: 0.7118769884109497]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 48/86 [D loss: 0.6860902309417725, acc.: 58.64%] [G loss: 0.7035773992538452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 49/86 [D loss: 0.6951601803302765, acc.: 47.85%] [G loss: 0.6993755102157593]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 50/86 [D loss: 0.6885997653007507, acc.: 55.08%] [G loss: 0.7083398699760437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 51/86 [D loss: 0.6906442642211914, acc.: 52.34%] [G loss: 0.7015128135681152]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 52/86 [D loss: 0.6900413632392883, acc.: 52.34%] [G loss: 0.7083179354667664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 53/86 [D loss: 0.6861717104911804, acc.: 57.57%] [G loss: 0.7046681046485901]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 54/86 [D loss: 0.6924295723438263, acc.: 50.44%] [G loss: 0.7017917037010193]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 55/86 [D loss: 0.6888058185577393, acc.: 55.22%] [G loss: 0.7101540565490723]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 56/86 [D loss: 0.6883645057678223, acc.: 55.22%] [G loss: 0.704223096370697]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 57/86 [D loss: 0.6905593276023865, acc.: 51.56%] [G loss: 0.7083917856216431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 58/86 [D loss: 0.6887770891189575, acc.: 54.20%] [G loss: 0.7114064693450928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 59/86 [D loss: 0.6876530647277832, acc.: 56.15%] [G loss: 0.7041978240013123]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 60/86 [D loss: 0.6900848150253296, acc.: 52.88%] [G loss: 0.7070882320404053]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 61/86 [D loss: 0.686305046081543, acc.: 58.06%] [G loss: 0.7133660912513733]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 62/86 [D loss: 0.6878856420516968, acc.: 56.30%] [G loss: 0.7067672610282898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 63/86 [D loss: 0.6895549595355988, acc.: 54.10%] [G loss: 0.7120073437690735]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 64/86 [D loss: 0.6868712604045868, acc.: 55.57%] [G loss: 0.7083515524864197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 65/86 [D loss: 0.6904078722000122, acc.: 53.76%] [G loss: 0.7073979377746582]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 66/86 [D loss: 0.6882636547088623, acc.: 55.42%] [G loss: 0.7095888257026672]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 67/86 [D loss: 0.6883702278137207, acc.: 54.44%] [G loss: 0.7101547718048096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 68/86 [D loss: 0.6887670457363129, acc.: 54.98%] [G loss: 0.7114548087120056]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 69/86 [D loss: 0.6874972283840179, acc.: 57.28%] [G loss: 0.7083754539489746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 70/86 [D loss: 0.6901195049285889, acc.: 52.98%] [G loss: 0.7085941433906555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 71/86 [D loss: 0.6876834630966187, acc.: 55.42%] [G loss: 0.7117565274238586]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 72/86 [D loss: 0.6888828277587891, acc.: 54.39%] [G loss: 0.7108761072158813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 73/86 [D loss: 0.6883475482463837, acc.: 55.32%] [G loss: 0.7111895680427551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 74/86 [D loss: 0.6885986030101776, acc.: 55.52%] [G loss: 0.7116631269454956]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 75/86 [D loss: 0.6893641352653503, acc.: 54.35%] [G loss: 0.7116802930831909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 76/86 [D loss: 0.6886318624019623, acc.: 55.03%] [G loss: 0.7125160694122314]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 77/86 [D loss: 0.6874514818191528, acc.: 56.05%] [G loss: 0.7117898464202881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 78/86 [D loss: 0.6891142129898071, acc.: 53.61%] [G loss: 0.7098559737205505]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 76/200, Batch 79/86 [D loss: 0.6884709894657135, acc.: 55.71%] [G loss: 0.7113534212112427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 80/86 [D loss: 0.6872022449970245, acc.: 56.64%] [G loss: 0.7110574245452881]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 81/86 [D loss: 0.6893119513988495, acc.: 53.91%] [G loss: 0.7126431465148926]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 82/86 [D loss: 0.6879554092884064, acc.: 55.52%] [G loss: 0.7105550169944763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 83/86 [D loss: 0.6882611811161041, acc.: 55.22%] [G loss: 0.7123317122459412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 84/86 [D loss: 0.6879632472991943, acc.: 55.52%] [G loss: 0.710590124130249]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 85/86 [D loss: 0.6870330274105072, acc.: 56.30%] [G loss: 0.7109596729278564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 76/200, Batch 86/86 [D loss: 0.6892163455486298, acc.: 54.79%] [G loss: 0.7103126049041748]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 1/86 [D loss: 0.6882431507110596, acc.: 55.96%] [G loss: 0.7123062610626221]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 2/86 [D loss: 0.6888167858123779, acc.: 54.05%] [G loss: 0.7097659111022949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 3/86 [D loss: 0.6886900067329407, acc.: 55.37%] [G loss: 0.7111684679985046]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 4/86 [D loss: 0.685969740152359, acc.: 58.20%] [G loss: 0.7125306129455566]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 5/86 [D loss: 0.69019615650177, acc.: 53.17%] [G loss: 0.7075070142745972]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 6/86 [D loss: 0.688675045967102, acc.: 55.13%] [G loss: 0.7100714445114136]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 7/86 [D loss: 0.689206600189209, acc.: 54.10%] [G loss: 0.7097102403640747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 8/86 [D loss: 0.687974601984024, acc.: 55.18%] [G loss: 0.7095832824707031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 9/86 [D loss: 0.6876022815704346, acc.: 55.42%] [G loss: 0.7109779715538025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 10/86 [D loss: 0.6880261301994324, acc.: 54.49%] [G loss: 0.7113732695579529]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 11/86 [D loss: 0.6889540255069733, acc.: 54.69%] [G loss: 0.7104740738868713]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 12/86 [D loss: 0.6864768862724304, acc.: 56.49%] [G loss: 0.7094482779502869]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 13/86 [D loss: 0.6886942386627197, acc.: 53.91%] [G loss: 0.7099118232727051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 14/86 [D loss: 0.6877449750900269, acc.: 56.74%] [G loss: 0.7115975618362427]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 15/86 [D loss: 0.6885956525802612, acc.: 54.39%] [G loss: 0.7096582651138306]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 16/86 [D loss: 0.6897412836551666, acc.: 54.30%] [G loss: 0.7096351981163025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 17/86 [D loss: 0.6875412166118622, acc.: 56.15%] [G loss: 0.7096641659736633]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 18/86 [D loss: 0.6884627044200897, acc.: 54.35%] [G loss: 0.710793673992157]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 19/86 [D loss: 0.6856225430965424, acc.: 58.84%] [G loss: 0.7105191349983215]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 20/86 [D loss: 0.6874944269657135, acc.: 57.37%] [G loss: 0.7093185186386108]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 21/86 [D loss: 0.6880304217338562, acc.: 54.69%] [G loss: 0.7121036648750305]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 22/86 [D loss: 0.6866359412670135, acc.: 57.57%] [G loss: 0.7072715759277344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 23/86 [D loss: 0.6895894110202789, acc.: 54.74%] [G loss: 0.708689272403717]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 24/86 [D loss: 0.6863961815834045, acc.: 56.74%] [G loss: 0.7111817002296448]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 25/86 [D loss: 0.688274472951889, acc.: 54.83%] [G loss: 0.7081845998764038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 26/86 [D loss: 0.6904947757720947, acc.: 52.15%] [G loss: 0.7128987312316895]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 27/86 [D loss: 0.6873635053634644, acc.: 54.83%] [G loss: 0.7068105340003967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 28/86 [D loss: 0.6904698014259338, acc.: 53.22%] [G loss: 0.7066250443458557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 29/86 [D loss: 0.6886266469955444, acc.: 54.83%] [G loss: 0.7101887464523315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 30/86 [D loss: 0.6853297054767609, acc.: 59.28%] [G loss: 0.705085277557373]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 31/86 [D loss: 0.6927627921104431, acc.: 50.93%] [G loss: 0.7126675248146057]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 32/86 [D loss: 0.6839515268802643, acc.: 59.38%] [G loss: 0.7084239721298218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 33/86 [D loss: 0.6944931447505951, acc.: 46.88%] [G loss: 0.7033601999282837]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 34/86 [D loss: 0.690873384475708, acc.: 51.51%] [G loss: 0.7171456813812256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 35/86 [D loss: 0.6880335509777069, acc.: 55.18%] [G loss: 0.7001845240592957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 36/86 [D loss: 0.6973294317722321, acc.: 45.61%] [G loss: 0.7116114497184753]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 37/86 [D loss: 0.6824891567230225, acc.: 61.72%] [G loss: 0.7067924737930298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 38/86 [D loss: 0.6956462562084198, acc.: 46.39%] [G loss: 0.6968450546264648]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 39/86 [D loss: 0.6902203857898712, acc.: 52.49%] [G loss: 0.7148970365524292]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 40/86 [D loss: 0.6845827996730804, acc.: 59.96%] [G loss: 0.7000439167022705]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 41/86 [D loss: 0.6980355381965637, acc.: 45.61%] [G loss: 0.6959050893783569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 42/86 [D loss: 0.6881793141365051, acc.: 54.93%] [G loss: 0.711802065372467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 43/86 [D loss: 0.691007673740387, acc.: 52.00%] [G loss: 0.6953873634338379]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 44/86 [D loss: 0.6926375925540924, acc.: 50.73%] [G loss: 0.706322431564331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 45/86 [D loss: 0.6835622489452362, acc.: 59.96%] [G loss: 0.7099071741104126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 46/86 [D loss: 0.6917385756969452, acc.: 51.86%] [G loss: 0.6992212533950806]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 47/86 [D loss: 0.6910481750965118, acc.: 52.44%] [G loss: 0.7074950933456421]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 48/86 [D loss: 0.6847318410873413, acc.: 59.67%] [G loss: 0.7065265774726868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 49/86 [D loss: 0.6895360052585602, acc.: 53.76%] [G loss: 0.7036845088005066]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 50/86 [D loss: 0.6904237568378448, acc.: 53.17%] [G loss: 0.7089164853096008]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 51/86 [D loss: 0.6870200037956238, acc.: 55.37%] [G loss: 0.7086880207061768]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 52/86 [D loss: 0.6910040378570557, acc.: 52.73%] [G loss: 0.7079191207885742]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 53/86 [D loss: 0.6878025829792023, acc.: 55.27%] [G loss: 0.7100338339805603]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 54/86 [D loss: 0.687643438577652, acc.: 55.52%] [G loss: 0.7057159543037415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 55/86 [D loss: 0.6890232861042023, acc.: 55.08%] [G loss: 0.70741868019104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 56/86 [D loss: 0.6875122785568237, acc.: 57.42%] [G loss: 0.7089133262634277]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 57/86 [D loss: 0.6885198056697845, acc.: 56.01%] [G loss: 0.7075575590133667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 58/86 [D loss: 0.6886394917964935, acc.: 54.00%] [G loss: 0.7101403474807739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 59/86 [D loss: 0.687980443239212, acc.: 55.57%] [G loss: 0.7114620804786682]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 60/86 [D loss: 0.6872907876968384, acc.: 56.54%] [G loss: 0.7094111442565918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 61/86 [D loss: 0.6873148381710052, acc.: 57.18%] [G loss: 0.7105894684791565]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 62/86 [D loss: 0.6869796812534332, acc.: 56.35%] [G loss: 0.7117881774902344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 63/86 [D loss: 0.688738614320755, acc.: 53.81%] [G loss: 0.7105826139450073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 64/86 [D loss: 0.6883939802646637, acc.: 55.47%] [G loss: 0.710877001285553]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 65/86 [D loss: 0.6880049109458923, acc.: 54.69%] [G loss: 0.7098692059516907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 66/86 [D loss: 0.6873906552791595, acc.: 55.81%] [G loss: 0.710315465927124]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 67/86 [D loss: 0.687830924987793, acc.: 54.00%] [G loss: 0.7121739983558655]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 68/86 [D loss: 0.6872957944869995, acc.: 55.76%] [G loss: 0.7096848487854004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 69/86 [D loss: 0.6880969703197479, acc.: 55.08%] [G loss: 0.7102715969085693]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 70/86 [D loss: 0.6871775984764099, acc.: 55.91%] [G loss: 0.7110356688499451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 71/86 [D loss: 0.6882602274417877, acc.: 55.66%] [G loss: 0.7097406387329102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 72/86 [D loss: 0.6879412531852722, acc.: 57.03%] [G loss: 0.7104684710502625]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 73/86 [D loss: 0.686088889837265, acc.: 57.42%] [G loss: 0.7128785252571106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 74/86 [D loss: 0.6869757771492004, acc.: 55.52%] [G loss: 0.7090516090393066]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 75/86 [D loss: 0.6877594888210297, acc.: 54.39%] [G loss: 0.7125723361968994]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 76/86 [D loss: 0.6888707280158997, acc.: 55.27%] [G loss: 0.7108035683631897]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 77/86 [D loss: 0.6893860697746277, acc.: 53.71%] [G loss: 0.7124217748641968]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 78/86 [D loss: 0.687693178653717, acc.: 55.62%] [G loss: 0.7109361886978149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 79/86 [D loss: 0.6872300207614899, acc.: 55.91%] [G loss: 0.7102311849594116]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 80/86 [D loss: 0.6865743696689606, acc.: 56.69%] [G loss: 0.7116715312004089]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 81/86 [D loss: 0.6875331997871399, acc.: 56.74%] [G loss: 0.7109992504119873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 82/86 [D loss: 0.6871972978115082, acc.: 56.84%] [G loss: 0.7123737931251526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 83/86 [D loss: 0.6893601715564728, acc.: 53.52%] [G loss: 0.7109848260879517]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 84/86 [D loss: 0.6873510181903839, acc.: 56.20%] [G loss: 0.7119792103767395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 77/200, Batch 85/86 [D loss: 0.6863978803157806, acc.: 56.93%] [G loss: 0.7131572365760803]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 77/200, Batch 86/86 [D loss: 0.6876244843006134, acc.: 56.59%] [G loss: 0.7097416520118713]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 1/86 [D loss: 0.6876793503761292, acc.: 56.15%] [G loss: 0.7137591242790222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 2/86 [D loss: 0.6872916221618652, acc.: 55.76%] [G loss: 0.7119126319885254]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 3/86 [D loss: 0.6867750287055969, acc.: 55.91%] [G loss: 0.7131170034408569]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 4/86 [D loss: 0.6876892149448395, acc.: 55.57%] [G loss: 0.714125394821167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 5/86 [D loss: 0.6865290999412537, acc.: 56.59%] [G loss: 0.710817277431488]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 6/86 [D loss: 0.6889973878860474, acc.: 54.20%] [G loss: 0.7108327746391296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 7/86 [D loss: 0.686707615852356, acc.: 56.84%] [G loss: 0.7126303911209106]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 8/86 [D loss: 0.688075065612793, acc.: 56.54%] [G loss: 0.711243748664856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 9/86 [D loss: 0.6884920299053192, acc.: 55.32%] [G loss: 0.7110630869865417]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 10/86 [D loss: 0.6870939135551453, acc.: 55.42%] [G loss: 0.712026059627533]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 11/86 [D loss: 0.6873742938041687, acc.: 55.86%] [G loss: 0.7117030620574951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 12/86 [D loss: 0.6867983937263489, acc.: 55.81%] [G loss: 0.7099367380142212]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 13/86 [D loss: 0.6897984445095062, acc.: 52.29%] [G loss: 0.7081369161605835]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 14/86 [D loss: 0.6878848075866699, acc.: 56.20%] [G loss: 0.7114806771278381]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 15/86 [D loss: 0.6855846345424652, acc.: 58.20%] [G loss: 0.709865152835846]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 16/86 [D loss: 0.6890011131763458, acc.: 54.74%] [G loss: 0.7091776132583618]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 17/86 [D loss: 0.688724547624588, acc.: 55.76%] [G loss: 0.711005687713623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 18/86 [D loss: 0.6880460381507874, acc.: 56.69%] [G loss: 0.7076587080955505]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 19/86 [D loss: 0.6894842684268951, acc.: 52.98%] [G loss: 0.7122513651847839]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 20/86 [D loss: 0.6877621710300446, acc.: 55.37%] [G loss: 0.7115578651428223]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 21/86 [D loss: 0.6906335949897766, acc.: 53.56%] [G loss: 0.7085034251213074]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 22/86 [D loss: 0.6871337890625, acc.: 56.88%] [G loss: 0.7128309011459351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 23/86 [D loss: 0.6871789395809174, acc.: 55.62%] [G loss: 0.7059271335601807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 24/86 [D loss: 0.69168820977211, acc.: 52.78%] [G loss: 0.7103285789489746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 25/86 [D loss: 0.6861158311367035, acc.: 56.54%] [G loss: 0.7107121348381042]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 26/86 [D loss: 0.6890519857406616, acc.: 54.20%] [G loss: 0.7026150226593018]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 27/86 [D loss: 0.6924430727958679, acc.: 50.44%] [G loss: 0.7166565656661987]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 28/86 [D loss: 0.684892863035202, acc.: 58.98%] [G loss: 0.7038789391517639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 29/86 [D loss: 0.6905207633972168, acc.: 50.88%] [G loss: 0.7030288577079773]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 30/86 [D loss: 0.6872667968273163, acc.: 56.15%] [G loss: 0.7117561101913452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 31/86 [D loss: 0.6871250867843628, acc.: 56.05%] [G loss: 0.7046180963516235]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 32/86 [D loss: 0.6928264200687408, acc.: 50.68%] [G loss: 0.7105357646942139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 33/86 [D loss: 0.685570478439331, acc.: 59.62%] [G loss: 0.7093408703804016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 34/86 [D loss: 0.691940575838089, acc.: 50.88%] [G loss: 0.7057750225067139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 35/86 [D loss: 0.6878497004508972, acc.: 55.47%] [G loss: 0.7129068970680237]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 36/86 [D loss: 0.6858706772327423, acc.: 59.03%] [G loss: 0.7075380086898804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 37/86 [D loss: 0.6924068033695221, acc.: 50.05%] [G loss: 0.7020201683044434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 38/86 [D loss: 0.6878571510314941, acc.: 55.52%] [G loss: 0.7130473852157593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 39/86 [D loss: 0.6879319250583649, acc.: 55.42%] [G loss: 0.7056738138198853]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 40/86 [D loss: 0.6905372738838196, acc.: 52.73%] [G loss: 0.7094770073890686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 41/86 [D loss: 0.6857272684574127, acc.: 56.93%] [G loss: 0.7087825536727905]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 42/86 [D loss: 0.6883662641048431, acc.: 55.66%] [G loss: 0.7039797306060791]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 43/86 [D loss: 0.6891892552375793, acc.: 52.64%] [G loss: 0.7089595794677734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 44/86 [D loss: 0.6873508393764496, acc.: 55.37%] [G loss: 0.7091896533966064]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 45/86 [D loss: 0.6885705292224884, acc.: 54.25%] [G loss: 0.7055224776268005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 46/86 [D loss: 0.6883499920368195, acc.: 54.74%] [G loss: 0.7142122387886047]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 47/86 [D loss: 0.6868440806865692, acc.: 56.30%] [G loss: 0.7095990180969238]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 48/86 [D loss: 0.6906571388244629, acc.: 51.76%] [G loss: 0.7090866565704346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 49/86 [D loss: 0.6874881982803345, acc.: 56.93%] [G loss: 0.7129996418952942]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 50/86 [D loss: 0.6898689866065979, acc.: 52.49%] [G loss: 0.7094020843505859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 51/86 [D loss: 0.6889284253120422, acc.: 53.66%] [G loss: 0.7126432061195374]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 52/86 [D loss: 0.6873962879180908, acc.: 56.49%] [G loss: 0.7097476720809937]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 53/86 [D loss: 0.689816951751709, acc.: 53.32%] [G loss: 0.7099490761756897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 54/86 [D loss: 0.6883897185325623, acc.: 54.88%] [G loss: 0.7138243317604065]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 55/86 [D loss: 0.6866253018379211, acc.: 56.10%] [G loss: 0.7076468467712402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 56/86 [D loss: 0.6883015632629395, acc.: 54.83%] [G loss: 0.7098936438560486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 57/86 [D loss: 0.6872294247150421, acc.: 56.59%] [G loss: 0.711559534072876]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 58/86 [D loss: 0.6892243325710297, acc.: 54.30%] [G loss: 0.7086161971092224]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 59/86 [D loss: 0.6877367496490479, acc.: 56.01%] [G loss: 0.7109045386314392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 60/86 [D loss: 0.6871442794799805, acc.: 55.81%] [G loss: 0.7087587118148804]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 61/86 [D loss: 0.688561737537384, acc.: 55.03%] [G loss: 0.710397481918335]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 62/86 [D loss: 0.6875382959842682, acc.: 56.64%] [G loss: 0.7114744186401367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 63/86 [D loss: 0.68797767162323, acc.: 55.22%] [G loss: 0.7113882303237915]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 64/86 [D loss: 0.6887238621711731, acc.: 54.88%] [G loss: 0.7104362845420837]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 65/86 [D loss: 0.6871355473995209, acc.: 56.69%] [G loss: 0.7115368247032166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 66/86 [D loss: 0.6868308782577515, acc.: 56.25%] [G loss: 0.71156245470047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 67/86 [D loss: 0.6878926753997803, acc.: 55.42%] [G loss: 0.7111414670944214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 68/86 [D loss: 0.6881733238697052, acc.: 56.35%] [G loss: 0.7111108303070068]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 69/86 [D loss: 0.688802570104599, acc.: 54.15%] [G loss: 0.7135592103004456]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 70/86 [D loss: 0.6862338483333588, acc.: 57.37%] [G loss: 0.7106751203536987]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 71/86 [D loss: 0.6882340312004089, acc.: 55.96%] [G loss: 0.7101726531982422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 72/86 [D loss: 0.6864712238311768, acc.: 56.64%] [G loss: 0.7122408747673035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 73/86 [D loss: 0.6871528029441833, acc.: 56.40%] [G loss: 0.7115824818611145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 74/86 [D loss: 0.6866115927696228, acc.: 56.69%] [G loss: 0.7126898765563965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 75/86 [D loss: 0.6876927614212036, acc.: 55.91%] [G loss: 0.7114278674125671]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 76/86 [D loss: 0.6869506239891052, acc.: 57.52%] [G loss: 0.7126315236091614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 77/86 [D loss: 0.6860105097293854, acc.: 58.69%] [G loss: 0.7115143537521362]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 78/86 [D loss: 0.6886178553104401, acc.: 55.22%] [G loss: 0.713005542755127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 79/86 [D loss: 0.6882336139678955, acc.: 55.08%] [G loss: 0.7131711840629578]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 80/86 [D loss: 0.6876140236854553, acc.: 55.76%] [G loss: 0.7131249904632568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 81/86 [D loss: 0.6870504021644592, acc.: 56.74%] [G loss: 0.7102869153022766]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 82/86 [D loss: 0.6876219511032104, acc.: 56.35%] [G loss: 0.7116739749908447]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 83/86 [D loss: 0.686211347579956, acc.: 58.74%] [G loss: 0.7128049731254578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 84/86 [D loss: 0.6871711313724518, acc.: 54.20%] [G loss: 0.709314227104187]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 78/200, Batch 85/86 [D loss: 0.6883975565433502, acc.: 53.91%] [G loss: 0.7119173407554626]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 78/200, Batch 86/86 [D loss: 0.6856254637241364, acc.: 57.32%] [G loss: 0.7117033004760742]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 1/86 [D loss: 0.6865487396717072, acc.: 57.57%] [G loss: 0.7099822759628296]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 2/86 [D loss: 0.6883569359779358, acc.: 53.91%] [G loss: 0.7095996737480164]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 3/86 [D loss: 0.6860935688018799, acc.: 57.37%] [G loss: 0.7100698947906494]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 4/86 [D loss: 0.687335342168808, acc.: 56.64%] [G loss: 0.7129164934158325]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 5/86 [D loss: 0.6868496537208557, acc.: 58.25%] [G loss: 0.7128506898880005]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 6/86 [D loss: 0.687044620513916, acc.: 57.08%] [G loss: 0.7101447582244873]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 7/86 [D loss: 0.6881852447986603, acc.: 56.54%] [G loss: 0.7110052704811096]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 8/86 [D loss: 0.6851690411567688, acc.: 59.03%] [G loss: 0.7115843296051025]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 9/86 [D loss: 0.687142014503479, acc.: 55.71%] [G loss: 0.7114886045455933]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 10/86 [D loss: 0.6879228055477142, acc.: 56.20%] [G loss: 0.7113502621650696]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 11/86 [D loss: 0.6890636086463928, acc.: 54.54%] [G loss: 0.7101284265518188]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 12/86 [D loss: 0.6879863739013672, acc.: 55.62%] [G loss: 0.7134256362915039]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 13/86 [D loss: 0.6887127161026001, acc.: 53.66%] [G loss: 0.7113168239593506]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 14/86 [D loss: 0.6891507208347321, acc.: 53.91%] [G loss: 0.7103532552719116]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 15/86 [D loss: 0.6847108602523804, acc.: 59.72%] [G loss: 0.712311863899231]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 16/86 [D loss: 0.6877497136592865, acc.: 55.37%] [G loss: 0.7084552645683289]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 17/86 [D loss: 0.6879067718982697, acc.: 54.88%] [G loss: 0.7130908966064453]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 18/86 [D loss: 0.6871691942214966, acc.: 56.49%] [G loss: 0.7071895003318787]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 19/86 [D loss: 0.6888183355331421, acc.: 55.52%] [G loss: 0.7106854915618896]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 20/86 [D loss: 0.6867154538631439, acc.: 56.93%] [G loss: 0.7069423198699951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 21/86 [D loss: 0.6881348788738251, acc.: 54.05%] [G loss: 0.7084313631057739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 22/86 [D loss: 0.6859090924263, acc.: 57.47%] [G loss: 0.7155841588973999]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 23/86 [D loss: 0.6882122457027435, acc.: 53.61%] [G loss: 0.7069538831710815]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 24/86 [D loss: 0.6893835067749023, acc.: 53.22%] [G loss: 0.7151746153831482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 25/86 [D loss: 0.6864016652107239, acc.: 57.23%] [G loss: 0.7077370882034302]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 26/86 [D loss: 0.6899047195911407, acc.: 52.69%] [G loss: 0.7100720405578613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 27/86 [D loss: 0.6873228549957275, acc.: 56.10%] [G loss: 0.7110385298728943]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 28/86 [D loss: 0.6872686445713043, acc.: 56.15%] [G loss: 0.7050451636314392]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 29/86 [D loss: 0.6895718276500702, acc.: 53.22%] [G loss: 0.7146328091621399]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 30/86 [D loss: 0.6854892075061798, acc.: 57.23%] [G loss: 0.7074876427650452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 31/86 [D loss: 0.6900681555271149, acc.: 52.93%] [G loss: 0.7085807919502258]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 32/86 [D loss: 0.6857891380786896, acc.: 56.59%] [G loss: 0.712860107421875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 33/86 [D loss: 0.6884816884994507, acc.: 54.20%] [G loss: 0.7048326730728149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 34/86 [D loss: 0.6903927028179169, acc.: 52.83%] [G loss: 0.7128547430038452]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 35/86 [D loss: 0.6856765449047089, acc.: 57.86%] [G loss: 0.7047848105430603]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 36/86 [D loss: 0.6905494332313538, acc.: 52.64%] [G loss: 0.7061049938201904]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 37/86 [D loss: 0.6877414584159851, acc.: 56.05%] [G loss: 0.7118590474128723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 38/86 [D loss: 0.6870453357696533, acc.: 56.98%] [G loss: 0.7093342542648315]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 39/86 [D loss: 0.6916448473930359, acc.: 53.61%] [G loss: 0.7096428275108337]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 40/86 [D loss: 0.6834433674812317, acc.: 59.86%] [G loss: 0.7137949466705322]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 41/86 [D loss: 0.6915623545646667, acc.: 51.66%] [G loss: 0.7054721117019653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 42/86 [D loss: 0.6890912055969238, acc.: 53.66%] [G loss: 0.7117328643798828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 43/86 [D loss: 0.6853793263435364, acc.: 58.15%] [G loss: 0.707237720489502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 44/86 [D loss: 0.6929357051849365, acc.: 51.12%] [G loss: 0.7099495530128479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 45/86 [D loss: 0.6853868961334229, acc.: 58.79%] [G loss: 0.7125605940818787]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 46/86 [D loss: 0.692541092634201, acc.: 50.10%] [G loss: 0.7042825222015381]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 47/86 [D loss: 0.6872516572475433, acc.: 55.03%] [G loss: 0.7122453451156616]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 48/86 [D loss: 0.6858758628368378, acc.: 56.40%] [G loss: 0.7074701189994812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 49/86 [D loss: 0.6896151602268219, acc.: 53.71%] [G loss: 0.7056583166122437]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 50/86 [D loss: 0.6884035766124725, acc.: 54.59%] [G loss: 0.7110572457313538]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 51/86 [D loss: 0.6876193583011627, acc.: 55.37%] [G loss: 0.7052428126335144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 52/86 [D loss: 0.6928209960460663, acc.: 50.63%] [G loss: 0.7097593545913696]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 53/86 [D loss: 0.6872397661209106, acc.: 55.42%] [G loss: 0.7126319408416748]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 54/86 [D loss: 0.6890781223773956, acc.: 54.25%] [G loss: 0.7052049040794373]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 55/86 [D loss: 0.6884205937385559, acc.: 55.81%] [G loss: 0.7108830809593201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 56/86 [D loss: 0.6856210231781006, acc.: 57.86%] [G loss: 0.7091274261474609]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 57/86 [D loss: 0.6878767311573029, acc.: 54.88%] [G loss: 0.7105429172515869]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 58/86 [D loss: 0.6877759993076324, acc.: 55.18%] [G loss: 0.7118853330612183]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 59/86 [D loss: 0.6868118345737457, acc.: 56.49%] [G loss: 0.7104097008705139]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 60/86 [D loss: 0.6891835331916809, acc.: 54.39%] [G loss: 0.7114318013191223]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 61/86 [D loss: 0.6865591406822205, acc.: 57.13%] [G loss: 0.7114804983139038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 62/86 [D loss: 0.6876056492328644, acc.: 55.47%] [G loss: 0.7138153314590454]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 63/86 [D loss: 0.6874062716960907, acc.: 56.15%] [G loss: 0.7116937041282654]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 64/86 [D loss: 0.6869407892227173, acc.: 56.40%] [G loss: 0.7108417749404907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 65/86 [D loss: 0.6874609887599945, acc.: 55.47%] [G loss: 0.7132931351661682]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 66/86 [D loss: 0.6876612603664398, acc.: 55.42%] [G loss: 0.7120538949966431]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 67/86 [D loss: 0.6855838000774384, acc.: 57.76%] [G loss: 0.7126954197883606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 68/86 [D loss: 0.6871458888053894, acc.: 55.81%] [G loss: 0.713309109210968]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 69/86 [D loss: 0.6865592002868652, acc.: 56.15%] [G loss: 0.7123313546180725]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 70/86 [D loss: 0.6858095228672028, acc.: 58.30%] [G loss: 0.7139098644256592]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 71/86 [D loss: 0.6876441240310669, acc.: 57.23%] [G loss: 0.7114079594612122]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 72/86 [D loss: 0.687804251909256, acc.: 55.03%] [G loss: 0.7123773694038391]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 73/86 [D loss: 0.6873605847358704, acc.: 55.37%] [G loss: 0.7117623090744019]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 74/86 [D loss: 0.6853589117527008, acc.: 57.76%] [G loss: 0.713098406791687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 75/86 [D loss: 0.6871778964996338, acc.: 54.98%] [G loss: 0.7129367589950562]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 79/200, Batch 76/86 [D loss: 0.6873321235179901, acc.: 56.01%] [G loss: 0.7131338715553284]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 77/86 [D loss: 0.6890024840831757, acc.: 55.57%] [G loss: 0.7117474675178528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 78/86 [D loss: 0.6855268776416779, acc.: 59.42%] [G loss: 0.7123128771781921]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 79/200, Batch 79/86 [D loss: 0.6870878040790558, acc.: 56.05%] [G loss: 0.7137385606765747]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 80/86 [D loss: 0.6862922310829163, acc.: 56.93%] [G loss: 0.7145733833312988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 81/86 [D loss: 0.6861790120601654, acc.: 57.57%] [G loss: 0.7112146019935608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 82/86 [D loss: 0.6878000795841217, acc.: 56.40%] [G loss: 0.7144281268119812]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 83/86 [D loss: 0.68654465675354, acc.: 57.76%] [G loss: 0.7123532295227051]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 84/86 [D loss: 0.686549037694931, acc.: 55.08%] [G loss: 0.7143075466156006]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 85/86 [D loss: 0.6855623126029968, acc.: 58.15%] [G loss: 0.7161511182785034]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 79/200, Batch 86/86 [D loss: 0.6856635510921478, acc.: 58.50%] [G loss: 0.7096260786056519]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 1/86 [D loss: 0.6884592771530151, acc.: 54.98%] [G loss: 0.7140517234802246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 2/86 [D loss: 0.6860664784908295, acc.: 58.06%] [G loss: 0.7107293605804443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 3/86 [D loss: 0.6870610117912292, acc.: 55.52%] [G loss: 0.7091183662414551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 4/86 [D loss: 0.6872154176235199, acc.: 54.39%] [G loss: 0.7139188051223755]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 5/86 [D loss: 0.6883411109447479, acc.: 54.35%] [G loss: 0.7127892971038818]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 6/86 [D loss: 0.6884300112724304, acc.: 53.86%] [G loss: 0.7149343490600586]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 7/86 [D loss: 0.6858045160770416, acc.: 58.01%] [G loss: 0.7133002877235413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 8/86 [D loss: 0.6879251301288605, acc.: 56.25%] [G loss: 0.7138470411300659]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 9/86 [D loss: 0.686605304479599, acc.: 56.40%] [G loss: 0.711114227771759]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 10/86 [D loss: 0.6896259188652039, acc.: 54.00%] [G loss: 0.7172927856445312]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 11/86 [D loss: 0.6853547394275665, acc.: 58.20%] [G loss: 0.7138450741767883]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 12/86 [D loss: 0.6864465475082397, acc.: 56.88%] [G loss: 0.7117530107498169]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 13/86 [D loss: 0.6888046562671661, acc.: 54.69%] [G loss: 0.7118048071861267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 14/86 [D loss: 0.6861766874790192, acc.: 56.45%] [G loss: 0.7128798365592957]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 15/86 [D loss: 0.6875505745410919, acc.: 55.13%] [G loss: 0.7154380083084106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 16/86 [D loss: 0.6866995990276337, acc.: 57.18%] [G loss: 0.7137514352798462]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 17/86 [D loss: 0.6863301396369934, acc.: 56.74%] [G loss: 0.7137224078178406]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 18/86 [D loss: 0.6882483065128326, acc.: 55.42%] [G loss: 0.7177459001541138]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 19/86 [D loss: 0.6870449483394623, acc.: 55.71%] [G loss: 0.7168689370155334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 20/86 [D loss: 0.6867289841175079, acc.: 55.96%] [G loss: 0.712607741355896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 21/86 [D loss: 0.6872895658016205, acc.: 56.15%] [G loss: 0.7126882672309875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 22/86 [D loss: 0.685107558965683, acc.: 59.18%] [G loss: 0.7104904055595398]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 23/86 [D loss: 0.686901867389679, acc.: 55.66%] [G loss: 0.7126005291938782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 24/86 [D loss: 0.6876066327095032, acc.: 55.47%] [G loss: 0.7124295830726624]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 25/86 [D loss: 0.6870397627353668, acc.: 55.81%] [G loss: 0.7126943469047546]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 26/86 [D loss: 0.6859310567378998, acc.: 56.64%] [G loss: 0.7086951732635498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 27/86 [D loss: 0.6891032755374908, acc.: 53.66%] [G loss: 0.7125648260116577]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 28/86 [D loss: 0.6847268640995026, acc.: 58.35%] [G loss: 0.7132823467254639]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 29/86 [D loss: 0.6866122484207153, acc.: 57.18%] [G loss: 0.7155334949493408]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 30/86 [D loss: 0.6887312531471252, acc.: 54.05%] [G loss: 0.7134820818901062]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 31/86 [D loss: 0.6881157457828522, acc.: 55.71%] [G loss: 0.7106902003288269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 32/86 [D loss: 0.6872730553150177, acc.: 56.79%] [G loss: 0.7109704613685608]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 33/86 [D loss: 0.6856509447097778, acc.: 57.03%] [G loss: 0.7130380272865295]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 34/86 [D loss: 0.6875750124454498, acc.: 55.91%] [G loss: 0.7130815982818604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 35/86 [D loss: 0.6882551610469818, acc.: 54.44%] [G loss: 0.7113742828369141]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 36/86 [D loss: 0.6888696551322937, acc.: 55.71%] [G loss: 0.7154068350791931]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 37/86 [D loss: 0.6871339380741119, acc.: 56.10%] [G loss: 0.7150288820266724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 38/86 [D loss: 0.6871476769447327, acc.: 56.88%] [G loss: 0.7140727043151855]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 39/86 [D loss: 0.688307136297226, acc.: 55.32%] [G loss: 0.7138851881027222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 40/86 [D loss: 0.6864296495914459, acc.: 57.71%] [G loss: 0.7133370637893677]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 41/86 [D loss: 0.6845220923423767, acc.: 58.20%] [G loss: 0.7124823927879333]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 42/86 [D loss: 0.6862854063510895, acc.: 55.62%] [G loss: 0.7119218111038208]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 43/86 [D loss: 0.6860058307647705, acc.: 57.52%] [G loss: 0.7138947248458862]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 44/86 [D loss: 0.687211662530899, acc.: 55.86%] [G loss: 0.7088474631309509]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 45/86 [D loss: 0.6862423121929169, acc.: 55.96%] [G loss: 0.7122586369514465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 46/86 [D loss: 0.6872385442256927, acc.: 55.03%] [G loss: 0.7111137509346008]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 47/86 [D loss: 0.6872411966323853, acc.: 55.81%] [G loss: 0.710956871509552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 48/86 [D loss: 0.6861921548843384, acc.: 57.32%] [G loss: 0.7127981185913086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 49/86 [D loss: 0.6875952780246735, acc.: 55.57%] [G loss: 0.7118964195251465]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 50/86 [D loss: 0.6872229874134064, acc.: 56.64%] [G loss: 0.7108860611915588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 51/86 [D loss: 0.6872327327728271, acc.: 56.25%] [G loss: 0.7121665477752686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 52/86 [D loss: 0.688120424747467, acc.: 55.42%] [G loss: 0.7120404243469238]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 53/86 [D loss: 0.6894917786121368, acc.: 53.96%] [G loss: 0.712120532989502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 54/86 [D loss: 0.6848479807376862, acc.: 58.30%] [G loss: 0.7114081978797913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 55/86 [D loss: 0.6877390146255493, acc.: 55.71%] [G loss: 0.7149366736412048]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 56/86 [D loss: 0.6876736283302307, acc.: 53.56%] [G loss: 0.7113661766052246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 57/86 [D loss: 0.686838686466217, acc.: 56.88%] [G loss: 0.7135656476020813]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 58/86 [D loss: 0.6853593289852142, acc.: 57.03%] [G loss: 0.7135675549507141]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 59/86 [D loss: 0.6873190402984619, acc.: 55.76%] [G loss: 0.7103294134140015]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 60/86 [D loss: 0.6885837316513062, acc.: 54.39%] [G loss: 0.7110514044761658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 61/86 [D loss: 0.685981273651123, acc.: 56.69%] [G loss: 0.7129421234130859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 62/86 [D loss: 0.6863380074501038, acc.: 57.23%] [G loss: 0.7128219604492188]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 63/86 [D loss: 0.6864053010940552, acc.: 56.64%] [G loss: 0.7112939357757568]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 64/86 [D loss: 0.6873715221881866, acc.: 54.83%] [G loss: 0.7140909433364868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 65/86 [D loss: 0.6873224377632141, acc.: 54.69%] [G loss: 0.7123519778251648]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 66/86 [D loss: 0.6861439645290375, acc.: 57.57%] [G loss: 0.7113375067710876]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 67/86 [D loss: 0.6871049404144287, acc.: 55.22%] [G loss: 0.7093149423599243]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 68/86 [D loss: 0.6872130930423737, acc.: 55.81%] [G loss: 0.7147417068481445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 69/86 [D loss: 0.6876495778560638, acc.: 54.54%] [G loss: 0.7127128839492798]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 70/86 [D loss: 0.6856592893600464, acc.: 57.42%] [G loss: 0.7116618752479553]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 71/86 [D loss: 0.687730073928833, acc.: 54.49%] [G loss: 0.7152722477912903]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 72/86 [D loss: 0.6862660050392151, acc.: 57.52%] [G loss: 0.7133083343505859]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 73/86 [D loss: 0.6868084371089935, acc.: 56.84%] [G loss: 0.7147836685180664]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 74/86 [D loss: 0.6869539618492126, acc.: 56.98%] [G loss: 0.7146860361099243]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 75/86 [D loss: 0.6873309314250946, acc.: 57.18%] [G loss: 0.7135288119316101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 76/86 [D loss: 0.6873433887958527, acc.: 56.79%] [G loss: 0.7144401669502258]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 77/86 [D loss: 0.6857589781284332, acc.: 57.08%] [G loss: 0.7127649188041687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 78/86 [D loss: 0.687247097492218, acc.: 55.52%] [G loss: 0.7125365734100342]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 79/86 [D loss: 0.6871088445186615, acc.: 55.03%] [G loss: 0.7136138677597046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 80/86 [D loss: 0.6877578794956207, acc.: 55.27%] [G loss: 0.7134116888046265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 81/86 [D loss: 0.6865483522415161, acc.: 56.69%] [G loss: 0.7089129090309143]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 82/86 [D loss: 0.6876421570777893, acc.: 55.37%] [G loss: 0.7128216028213501]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 83/86 [D loss: 0.6869932413101196, acc.: 56.30%] [G loss: 0.7129168510437012]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 80/200, Batch 84/86 [D loss: 0.6867652833461761, acc.: 56.01%] [G loss: 0.7127418518066406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 85/86 [D loss: 0.6876171827316284, acc.: 55.81%] [G loss: 0.7153561115264893]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 80/200, Batch 86/86 [D loss: 0.6881038546562195, acc.: 55.62%] [G loss: 0.715401828289032]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 1/86 [D loss: 0.6880413889884949, acc.: 53.47%] [G loss: 0.7129942178726196]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 2/86 [D loss: 0.68686643242836, acc.: 54.69%] [G loss: 0.7122020721435547]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 3/86 [D loss: 0.685726672410965, acc.: 57.47%] [G loss: 0.713002622127533]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 4/86 [D loss: 0.6898655593395233, acc.: 53.22%] [G loss: 0.7128506898880005]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 5/86 [D loss: 0.685835063457489, acc.: 56.79%] [G loss: 0.7107704281806946]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 6/86 [D loss: 0.688505083322525, acc.: 53.96%] [G loss: 0.7104994654655457]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 7/86 [D loss: 0.6874152719974518, acc.: 56.25%] [G loss: 0.7125192284584045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 8/86 [D loss: 0.6862254738807678, acc.: 55.91%] [G loss: 0.7089401483535767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 9/86 [D loss: 0.688659280538559, acc.: 54.00%] [G loss: 0.7140384316444397]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 10/86 [D loss: 0.686530351638794, acc.: 56.59%] [G loss: 0.7107970714569092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 11/86 [D loss: 0.689224362373352, acc.: 53.42%] [G loss: 0.714584469795227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 12/86 [D loss: 0.6876429319381714, acc.: 54.35%] [G loss: 0.7161243557929993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 13/86 [D loss: 0.6876862943172455, acc.: 55.37%] [G loss: 0.7103527188301086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 14/86 [D loss: 0.6871597468852997, acc.: 55.86%] [G loss: 0.7130982875823975]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 15/86 [D loss: 0.6867086291313171, acc.: 57.32%] [G loss: 0.7092183232307434]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 16/86 [D loss: 0.6903219819068909, acc.: 53.66%] [G loss: 0.711713969707489]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 17/86 [D loss: 0.6852039694786072, acc.: 58.64%] [G loss: 0.7137437462806702]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 18/86 [D loss: 0.6895573139190674, acc.: 52.73%] [G loss: 0.7115963101387024]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 19/86 [D loss: 0.6899581849575043, acc.: 53.22%] [G loss: 0.7154773473739624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 20/86 [D loss: 0.6874108016490936, acc.: 56.15%] [G loss: 0.707952082157135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 21/86 [D loss: 0.6908833384513855, acc.: 53.08%] [G loss: 0.7152883410453796]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 22/86 [D loss: 0.6850688457489014, acc.: 56.88%] [G loss: 0.7096734642982483]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 23/86 [D loss: 0.6906899213790894, acc.: 51.76%] [G loss: 0.7096999287605286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 24/86 [D loss: 0.6885898411273956, acc.: 55.08%] [G loss: 0.7159909605979919]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 25/86 [D loss: 0.6880854666233063, acc.: 54.59%] [G loss: 0.7042471170425415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 26/86 [D loss: 0.6904168426990509, acc.: 52.05%] [G loss: 0.7188913822174072]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 27/86 [D loss: 0.6865702569484711, acc.: 56.79%] [G loss: 0.706136167049408]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 28/86 [D loss: 0.6913018524646759, acc.: 53.22%] [G loss: 0.7129857540130615]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 29/86 [D loss: 0.6843124330043793, acc.: 57.86%] [G loss: 0.7137322425842285]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 30/86 [D loss: 0.6932848691940308, acc.: 50.44%] [G loss: 0.7079316973686218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 31/86 [D loss: 0.6843617558479309, acc.: 57.76%] [G loss: 0.7081348896026611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 32/86 [D loss: 0.6905598342418671, acc.: 52.00%] [G loss: 0.705436646938324]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 33/86 [D loss: 0.6872007846832275, acc.: 55.08%] [G loss: 0.7181157469749451]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 34/86 [D loss: 0.6856814026832581, acc.: 56.93%] [G loss: 0.7014033794403076]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 35/86 [D loss: 0.6938705742359161, acc.: 51.42%] [G loss: 0.7152044773101807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 36/86 [D loss: 0.6849772930145264, acc.: 57.67%] [G loss: 0.7047879695892334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 37/86 [D loss: 0.6917842626571655, acc.: 52.59%] [G loss: 0.7062126994132996]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 38/86 [D loss: 0.6886653900146484, acc.: 54.79%] [G loss: 0.7124979496002197]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 39/86 [D loss: 0.6865969896316528, acc.: 55.42%] [G loss: 0.7039417028427124]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 40/86 [D loss: 0.6947818994522095, acc.: 48.83%] [G loss: 0.7145336866378784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 41/86 [D loss: 0.6857446432113647, acc.: 57.28%] [G loss: 0.7070801854133606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 42/86 [D loss: 0.6955444514751434, acc.: 47.12%] [G loss: 0.7037703990936279]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 81/200, Batch 43/86 [D loss: 0.6864733099937439, acc.: 56.30%] [G loss: 0.719764769077301]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 44/86 [D loss: 0.6872281134128571, acc.: 55.62%] [G loss: 0.7046394348144531]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 45/86 [D loss: 0.6923825740814209, acc.: 50.00%] [G loss: 0.7119916677474976]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 46/86 [D loss: 0.683989405632019, acc.: 58.89%] [G loss: 0.709395170211792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 47/86 [D loss: 0.6929121613502502, acc.: 50.29%] [G loss: 0.7082517147064209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 48/86 [D loss: 0.6892462968826294, acc.: 54.15%] [G loss: 0.7159862518310547]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 49/86 [D loss: 0.6857922375202179, acc.: 56.40%] [G loss: 0.7074022889137268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 50/86 [D loss: 0.6931764781475067, acc.: 49.66%] [G loss: 0.7113023400306702]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 51/86 [D loss: 0.6854643523693085, acc.: 58.15%] [G loss: 0.7124646902084351]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 52/86 [D loss: 0.6903519928455353, acc.: 53.08%] [G loss: 0.7069377899169922]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 53/86 [D loss: 0.6887896955013275, acc.: 54.25%] [G loss: 0.7108623385429382]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 54/86 [D loss: 0.684314101934433, acc.: 58.69%] [G loss: 0.7086775898933411]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 55/86 [D loss: 0.6911242604255676, acc.: 52.34%] [G loss: 0.7102813124656677]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 56/86 [D loss: 0.685163825750351, acc.: 58.98%] [G loss: 0.712535560131073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 57/86 [D loss: 0.6862350404262543, acc.: 56.01%] [G loss: 0.7098050117492676]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 58/86 [D loss: 0.6883241534233093, acc.: 53.42%] [G loss: 0.7139098644256592]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 59/86 [D loss: 0.6882359087467194, acc.: 54.59%] [G loss: 0.7088476419448853]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 60/86 [D loss: 0.6911942362785339, acc.: 51.86%] [G loss: 0.7140474319458008]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 61/86 [D loss: 0.6861824989318848, acc.: 56.88%] [G loss: 0.7139397859573364]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 62/86 [D loss: 0.6869669258594513, acc.: 56.05%] [G loss: 0.7109142541885376]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 63/86 [D loss: 0.6883503794670105, acc.: 54.59%] [G loss: 0.7131376266479492]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 64/86 [D loss: 0.6867315769195557, acc.: 56.49%] [G loss: 0.7131620645523071]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 65/86 [D loss: 0.6869112253189087, acc.: 54.74%] [G loss: 0.7118159532546997]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 66/86 [D loss: 0.6881098747253418, acc.: 56.20%] [G loss: 0.7124886512756348]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 67/86 [D loss: 0.6865866780281067, acc.: 55.18%] [G loss: 0.7118536233901978]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 68/86 [D loss: 0.6895557940006256, acc.: 53.66%] [G loss: 0.71168053150177]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 69/86 [D loss: 0.6849998533725739, acc.: 56.84%] [G loss: 0.7131187319755554]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 70/86 [D loss: 0.6865613758563995, acc.: 55.47%] [G loss: 0.7141448259353638]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 71/86 [D loss: 0.6875718235969543, acc.: 55.62%] [G loss: 0.7128596901893616]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 72/86 [D loss: 0.6873732209205627, acc.: 56.20%] [G loss: 0.7143983840942383]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 73/86 [D loss: 0.6876209080219269, acc.: 54.44%] [G loss: 0.7125272750854492]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 74/86 [D loss: 0.6866238415241241, acc.: 56.69%] [G loss: 0.713796079158783]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 75/86 [D loss: 0.6855194270610809, acc.: 58.01%] [G loss: 0.7106003165245056]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 76/86 [D loss: 0.687819093465805, acc.: 55.27%] [G loss: 0.7150292992591858]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 77/86 [D loss: 0.6873815059661865, acc.: 55.27%] [G loss: 0.7131446599960327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 78/86 [D loss: 0.6878428161144257, acc.: 54.49%] [G loss: 0.7158704400062561]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 79/86 [D loss: 0.6851241290569305, acc.: 58.64%] [G loss: 0.7154755592346191]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 80/86 [D loss: 0.6896860897541046, acc.: 53.47%] [G loss: 0.7141628265380859]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 81/86 [D loss: 0.6881256997585297, acc.: 56.10%] [G loss: 0.7118780016899109]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 82/86 [D loss: 0.6852361559867859, acc.: 58.11%] [G loss: 0.7137399911880493]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 83/86 [D loss: 0.6860233247280121, acc.: 55.76%] [G loss: 0.7145220637321472]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 84/86 [D loss: 0.6860894560813904, acc.: 57.32%] [G loss: 0.7164009213447571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 81/200, Batch 85/86 [D loss: 0.686407059431076, acc.: 55.32%] [G loss: 0.7150482535362244]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 81/200, Batch 86/86 [D loss: 0.6858297884464264, acc.: 57.96%] [G loss: 0.7164218425750732]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 1/86 [D loss: 0.6891167163848877, acc.: 53.96%] [G loss: 0.7130414843559265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 2/86 [D loss: 0.6845993399620056, acc.: 57.91%] [G loss: 0.7160972356796265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 3/86 [D loss: 0.6863035261631012, acc.: 56.84%] [G loss: 0.7153651714324951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 4/86 [D loss: 0.6875308752059937, acc.: 54.25%] [G loss: 0.7154648303985596]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 5/86 [D loss: 0.68579962849617, acc.: 56.98%] [G loss: 0.7140102386474609]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 6/86 [D loss: 0.6859429478645325, acc.: 57.57%] [G loss: 0.7131091952323914]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 82/200, Batch 7/86 [D loss: 0.6879340410232544, acc.: 54.25%] [G loss: 0.714864194393158]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 8/86 [D loss: 0.6857459247112274, acc.: 57.08%] [G loss: 0.7130196690559387]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 9/86 [D loss: 0.6875872015953064, acc.: 55.76%] [G loss: 0.7159032225608826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 10/86 [D loss: 0.6852860748767853, acc.: 57.47%] [G loss: 0.7132103443145752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 11/86 [D loss: 0.6867347359657288, acc.: 56.10%] [G loss: 0.7113255262374878]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 12/86 [D loss: 0.6865340769290924, acc.: 55.62%] [G loss: 0.7133197784423828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 13/86 [D loss: 0.6853547096252441, acc.: 57.28%] [G loss: 0.7132046818733215]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 14/86 [D loss: 0.6886670589447021, acc.: 54.74%] [G loss: 0.7119594216346741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 15/86 [D loss: 0.6860552728176117, acc.: 57.03%] [G loss: 0.7127079367637634]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 16/86 [D loss: 0.6888974606990814, acc.: 53.61%] [G loss: 0.7111223936080933]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 82/200, Batch 17/86 [D loss: 0.6869342029094696, acc.: 56.45%] [G loss: 0.7158195972442627]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 18/86 [D loss: 0.6846287250518799, acc.: 58.35%] [G loss: 0.7128329873085022]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 19/86 [D loss: 0.6885805130004883, acc.: 54.59%] [G loss: 0.7143977284431458]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 20/86 [D loss: 0.6852867901325226, acc.: 57.67%] [G loss: 0.7114921808242798]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 82/200, Batch 21/86 [D loss: 0.6874232292175293, acc.: 55.08%] [G loss: 0.7145569920539856]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 22/86 [D loss: 0.6870382726192474, acc.: 55.42%] [G loss: 0.714470624923706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 23/86 [D loss: 0.6872988641262054, acc.: 54.74%] [G loss: 0.7100679874420166]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 24/86 [D loss: 0.6907475292682648, acc.: 51.86%] [G loss: 0.7168816924095154]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 25/86 [D loss: 0.6850084662437439, acc.: 57.32%] [G loss: 0.7143651247024536]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 26/86 [D loss: 0.6886750757694244, acc.: 54.59%] [G loss: 0.7122958898544312]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 27/86 [D loss: 0.6872642934322357, acc.: 56.30%] [G loss: 0.7181524634361267]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 28/86 [D loss: 0.6854229271411896, acc.: 57.91%] [G loss: 0.7100908756256104]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 29/86 [D loss: 0.6888907253742218, acc.: 53.81%] [G loss: 0.7166473865509033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 30/86 [D loss: 0.6847887933254242, acc.: 58.69%] [G loss: 0.7115950584411621]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 31/86 [D loss: 0.6886457204818726, acc.: 52.98%] [G loss: 0.713249683380127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 32/86 [D loss: 0.6860719323158264, acc.: 56.49%] [G loss: 0.715526282787323]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 33/86 [D loss: 0.687152236700058, acc.: 55.71%] [G loss: 0.7090703248977661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 34/86 [D loss: 0.6909417510032654, acc.: 52.83%] [G loss: 0.7187831997871399]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 35/86 [D loss: 0.6850467324256897, acc.: 58.25%] [G loss: 0.7086345553398132]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 36/86 [D loss: 0.6924022138118744, acc.: 50.68%] [G loss: 0.7106778621673584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 37/86 [D loss: 0.6863681972026825, acc.: 56.74%] [G loss: 0.7198960781097412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 38/86 [D loss: 0.6860890984535217, acc.: 57.52%] [G loss: 0.707882821559906]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 39/86 [D loss: 0.693615734577179, acc.: 50.29%] [G loss: 0.7181111574172974]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 40/86 [D loss: 0.6828296184539795, acc.: 60.30%] [G loss: 0.7089638113975525]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 41/86 [D loss: 0.6933338642120361, acc.: 48.88%] [G loss: 0.7114038467407227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 42/86 [D loss: 0.688165158033371, acc.: 54.88%] [G loss: 0.7210310697555542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 43/86 [D loss: 0.6859898269176483, acc.: 57.91%] [G loss: 0.7070444226264954]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 44/86 [D loss: 0.6932138800621033, acc.: 49.85%] [G loss: 0.7177491784095764]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 45/86 [D loss: 0.6830308437347412, acc.: 60.16%] [G loss: 0.7156368494033813]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 46/86 [D loss: 0.6926685273647308, acc.: 50.24%] [G loss: 0.7067770957946777]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 47/86 [D loss: 0.6874198317527771, acc.: 54.35%] [G loss: 0.7154759764671326]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 48/86 [D loss: 0.6856408715248108, acc.: 57.08%] [G loss: 0.7064810991287231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 49/86 [D loss: 0.693314403295517, acc.: 48.68%] [G loss: 0.7198742628097534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 50/86 [D loss: 0.6839037239551544, acc.: 57.91%] [G loss: 0.7162885069847107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 51/86 [D loss: 0.6929207146167755, acc.: 50.49%] [G loss: 0.705744743347168]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 52/86 [D loss: 0.6881736814975739, acc.: 54.30%] [G loss: 0.7162685990333557]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 53/86 [D loss: 0.6846485435962677, acc.: 56.54%] [G loss: 0.7081298232078552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 54/86 [D loss: 0.6924043893814087, acc.: 49.95%] [G loss: 0.7085451483726501]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 55/86 [D loss: 0.6851849257946014, acc.: 58.94%] [G loss: 0.7191662192344666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 56/86 [D loss: 0.688644528388977, acc.: 53.56%] [G loss: 0.7056551575660706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 57/86 [D loss: 0.6886903941631317, acc.: 54.74%] [G loss: 0.7147394418716431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 58/86 [D loss: 0.6859422326087952, acc.: 57.57%] [G loss: 0.7153726816177368]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 59/86 [D loss: 0.6898753345012665, acc.: 53.17%] [G loss: 0.708832859992981]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 60/86 [D loss: 0.6886802613735199, acc.: 53.52%] [G loss: 0.7158811092376709]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 61/86 [D loss: 0.6861551702022552, acc.: 55.91%] [G loss: 0.7085784077644348]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 62/86 [D loss: 0.6897233724594116, acc.: 52.98%] [G loss: 0.71437668800354]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 63/86 [D loss: 0.6854150295257568, acc.: 57.42%] [G loss: 0.7134085893630981]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 64/86 [D loss: 0.6869125366210938, acc.: 56.69%] [G loss: 0.7096091508865356]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 65/86 [D loss: 0.6875541508197784, acc.: 55.96%] [G loss: 0.714689314365387]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 66/86 [D loss: 0.686487466096878, acc.: 55.71%] [G loss: 0.7098223567008972]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 67/86 [D loss: 0.6892378628253937, acc.: 53.08%] [G loss: 0.7104689478874207]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 68/86 [D loss: 0.6859485507011414, acc.: 57.96%] [G loss: 0.715918242931366]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 69/86 [D loss: 0.6876175701618195, acc.: 53.66%] [G loss: 0.7121039032936096]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 70/86 [D loss: 0.6881193220615387, acc.: 54.54%] [G loss: 0.7139706611633301]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 71/86 [D loss: 0.68597811460495, acc.: 55.66%] [G loss: 0.712580144405365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 72/86 [D loss: 0.6870594024658203, acc.: 55.86%] [G loss: 0.7135802507400513]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 73/86 [D loss: 0.6879527866840363, acc.: 54.93%] [G loss: 0.7160465717315674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 74/86 [D loss: 0.6859679222106934, acc.: 56.64%] [G loss: 0.7142491936683655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 75/86 [D loss: 0.6862478256225586, acc.: 56.59%] [G loss: 0.7141133546829224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 76/86 [D loss: 0.6853972375392914, acc.: 57.62%] [G loss: 0.7147544026374817]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 77/86 [D loss: 0.6875912249088287, acc.: 55.76%] [G loss: 0.7142837643623352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 78/86 [D loss: 0.6880078911781311, acc.: 54.20%] [G loss: 0.7135100960731506]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 79/86 [D loss: 0.6864160895347595, acc.: 57.18%] [G loss: 0.713405191898346]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 80/86 [D loss: 0.6880750358104706, acc.: 54.15%] [G loss: 0.7164783477783203]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 81/86 [D loss: 0.6864281892776489, acc.: 56.25%] [G loss: 0.7157142162322998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 82/86 [D loss: 0.6849384009838104, acc.: 57.13%] [G loss: 0.7127280235290527]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 83/86 [D loss: 0.6885280609130859, acc.: 53.91%] [G loss: 0.7172921299934387]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 84/86 [D loss: 0.6845168173313141, acc.: 57.62%] [G loss: 0.7152551412582397]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 82/200, Batch 85/86 [D loss: 0.6849897503852844, acc.: 56.20%] [G loss: 0.7150848507881165]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 82/200, Batch 86/86 [D loss: 0.6848697662353516, acc.: 57.71%] [G loss: 0.7173972725868225]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 1/86 [D loss: 0.6876808404922485, acc.: 55.37%] [G loss: 0.7154160141944885]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 2/86 [D loss: 0.6849800944328308, acc.: 55.76%] [G loss: 0.7184178233146667]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 3/86 [D loss: 0.6862479746341705, acc.: 55.47%] [G loss: 0.7167571783065796]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 4/86 [D loss: 0.6861959099769592, acc.: 55.76%] [G loss: 0.7182791233062744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 5/86 [D loss: 0.6845836937427521, acc.: 55.71%] [G loss: 0.7167145013809204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 6/86 [D loss: 0.6845089793205261, acc.: 57.23%] [G loss: 0.7159228324890137]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 7/86 [D loss: 0.6869245767593384, acc.: 56.84%] [G loss: 0.7166696190834045]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 8/86 [D loss: 0.6866792142391205, acc.: 56.79%] [G loss: 0.71639084815979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 9/86 [D loss: 0.6867194473743439, acc.: 57.37%] [G loss: 0.7152528762817383]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 10/86 [D loss: 0.6881012916564941, acc.: 55.62%] [G loss: 0.7169880867004395]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 11/86 [D loss: 0.6860836744308472, acc.: 56.20%] [G loss: 0.716783344745636]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 12/86 [D loss: 0.6877315044403076, acc.: 54.69%] [G loss: 0.7162160277366638]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 13/86 [D loss: 0.686426192522049, acc.: 55.62%] [G loss: 0.7172815799713135]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 14/86 [D loss: 0.6873228549957275, acc.: 55.22%] [G loss: 0.7154253721237183]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 15/86 [D loss: 0.6854041218757629, acc.: 56.20%] [G loss: 0.7165017127990723]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 16/86 [D loss: 0.6876140534877777, acc.: 54.35%] [G loss: 0.7137641906738281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 17/86 [D loss: 0.6862210035324097, acc.: 56.15%] [G loss: 0.7159680128097534]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 18/86 [D loss: 0.6879567503929138, acc.: 54.64%] [G loss: 0.714935839176178]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 19/86 [D loss: 0.6854201257228851, acc.: 57.32%] [G loss: 0.7167747616767883]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 20/86 [D loss: 0.6843868494033813, acc.: 58.11%] [G loss: 0.7107360363006592]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 21/86 [D loss: 0.6860689520835876, acc.: 56.40%] [G loss: 0.7156043648719788]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 22/86 [D loss: 0.6868618130683899, acc.: 55.42%] [G loss: 0.7137663960456848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 23/86 [D loss: 0.6864585280418396, acc.: 55.76%] [G loss: 0.7141550183296204]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 24/86 [D loss: 0.6868587136268616, acc.: 55.76%] [G loss: 0.7147819995880127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 25/86 [D loss: 0.6878506541252136, acc.: 55.08%] [G loss: 0.7131011486053467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 26/86 [D loss: 0.6870084702968597, acc.: 55.03%] [G loss: 0.7153329849243164]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 27/86 [D loss: 0.6855928003787994, acc.: 57.32%] [G loss: 0.7135949730873108]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 28/86 [D loss: 0.6885923147201538, acc.: 53.42%] [G loss: 0.7120850086212158]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 29/86 [D loss: 0.6836078763008118, acc.: 58.20%] [G loss: 0.7155718803405762]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 30/86 [D loss: 0.6862461268901825, acc.: 56.54%] [G loss: 0.7099955081939697]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 31/86 [D loss: 0.6888658702373505, acc.: 52.25%] [G loss: 0.7191805243492126]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 32/86 [D loss: 0.6845914423465729, acc.: 58.64%] [G loss: 0.7135313749313354]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 33/86 [D loss: 0.6892200410366058, acc.: 52.73%] [G loss: 0.7148617506027222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 34/86 [D loss: 0.6854038834571838, acc.: 57.08%] [G loss: 0.7164335250854492]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 35/86 [D loss: 0.688130110502243, acc.: 55.37%] [G loss: 0.7111557126045227]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 36/86 [D loss: 0.6892518401145935, acc.: 54.00%] [G loss: 0.7147418856620789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 37/86 [D loss: 0.6853419542312622, acc.: 57.47%] [G loss: 0.7089330554008484]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 38/86 [D loss: 0.6870988607406616, acc.: 56.10%] [G loss: 0.7104565501213074]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 39/86 [D loss: 0.6843494176864624, acc.: 57.23%] [G loss: 0.7172253727912903]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 40/86 [D loss: 0.68805992603302, acc.: 54.79%] [G loss: 0.7040649652481079]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 41/86 [D loss: 0.6924943923950195, acc.: 51.86%] [G loss: 0.7174824476242065]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 42/86 [D loss: 0.6828395128250122, acc.: 58.59%] [G loss: 0.7050111293792725]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 43/86 [D loss: 0.6924655437469482, acc.: 51.32%] [G loss: 0.7057124376296997]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 44/86 [D loss: 0.6872448921203613, acc.: 54.93%] [G loss: 0.7223140597343445]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 45/86 [D loss: 0.6833504438400269, acc.: 59.38%] [G loss: 0.7066077589988708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 46/86 [D loss: 0.6959660649299622, acc.: 48.83%] [G loss: 0.7155085802078247]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 47/86 [D loss: 0.6824997961521149, acc.: 59.08%] [G loss: 0.7124977707862854]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 48/86 [D loss: 0.6942681074142456, acc.: 48.97%] [G loss: 0.6997263431549072]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 49/86 [D loss: 0.6874417066574097, acc.: 55.03%] [G loss: 0.7191185355186462]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 50/86 [D loss: 0.6827386319637299, acc.: 58.98%] [G loss: 0.7060046792030334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 51/86 [D loss: 0.6955821514129639, acc.: 47.41%] [G loss: 0.710043728351593]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 52/86 [D loss: 0.682040274143219, acc.: 60.30%] [G loss: 0.712978720664978]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 53/86 [D loss: 0.6895280182361603, acc.: 52.88%] [G loss: 0.7009880542755127]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 54/86 [D loss: 0.6909343004226685, acc.: 52.44%] [G loss: 0.7139081954956055]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 55/86 [D loss: 0.6832760572433472, acc.: 59.77%] [G loss: 0.7106671333312988]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 56/86 [D loss: 0.693042665719986, acc.: 50.15%] [G loss: 0.7048254609107971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 57/86 [D loss: 0.6874023675918579, acc.: 55.37%] [G loss: 0.715745747089386]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 58/86 [D loss: 0.6867246329784393, acc.: 55.86%] [G loss: 0.7096855044364929]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 59/86 [D loss: 0.6886771619319916, acc.: 52.78%] [G loss: 0.7114179730415344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 60/86 [D loss: 0.6865453124046326, acc.: 56.45%] [G loss: 0.7182224988937378]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 61/86 [D loss: 0.6854741871356964, acc.: 56.69%] [G loss: 0.7129247188568115]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 83/200, Batch 62/86 [D loss: 0.6906879842281342, acc.: 53.22%] [G loss: 0.7149489521980286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 63/86 [D loss: 0.6842890083789825, acc.: 57.52%] [G loss: 0.7142277359962463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 64/86 [D loss: 0.6881972849369049, acc.: 54.49%] [G loss: 0.7128962874412537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 65/86 [D loss: 0.6852068603038788, acc.: 56.64%] [G loss: 0.7156700491905212]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 66/86 [D loss: 0.6869524717330933, acc.: 56.30%] [G loss: 0.7161240577697754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 67/86 [D loss: 0.6874277591705322, acc.: 55.76%] [G loss: 0.7149065136909485]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 68/86 [D loss: 0.685383677482605, acc.: 56.20%] [G loss: 0.7153482437133789]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 69/86 [D loss: 0.6876234710216522, acc.: 55.08%] [G loss: 0.7148212790489197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 70/86 [D loss: 0.6865230798721313, acc.: 56.10%] [G loss: 0.7162400484085083]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 71/86 [D loss: 0.6844423711299896, acc.: 57.62%] [G loss: 0.7146790027618408]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 72/86 [D loss: 0.6867098808288574, acc.: 55.27%] [G loss: 0.7152138948440552]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 73/86 [D loss: 0.6824494898319244, acc.: 59.91%] [G loss: 0.7160496711730957]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 74/86 [D loss: 0.6863143146038055, acc.: 56.40%] [G loss: 0.7135916948318481]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 75/86 [D loss: 0.6872650384902954, acc.: 54.64%] [G loss: 0.7132374048233032]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 76/86 [D loss: 0.6855775117874146, acc.: 56.74%] [G loss: 0.7167046070098877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 77/86 [D loss: 0.6855596303939819, acc.: 56.79%] [G loss: 0.7135658860206604]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 78/86 [D loss: 0.686755359172821, acc.: 55.52%] [G loss: 0.715496301651001]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 79/86 [D loss: 0.683957427740097, acc.: 58.84%] [G loss: 0.7152441740036011]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 80/86 [D loss: 0.6861733794212341, acc.: 56.10%] [G loss: 0.714853823184967]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 81/86 [D loss: 0.6858089864253998, acc.: 55.42%] [G loss: 0.7148700952529907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 82/86 [D loss: 0.6854017078876495, acc.: 57.91%] [G loss: 0.716218888759613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 83/86 [D loss: 0.6858098804950714, acc.: 58.15%] [G loss: 0.7135952115058899]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 84/86 [D loss: 0.6874276697635651, acc.: 55.13%] [G loss: 0.7144090533256531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 83/200, Batch 85/86 [D loss: 0.6882768273353577, acc.: 54.88%] [G loss: 0.7157326936721802]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 83/200, Batch 86/86 [D loss: 0.6858241558074951, acc.: 57.76%] [G loss: 0.7159072756767273]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 1/86 [D loss: 0.686655730009079, acc.: 56.15%] [G loss: 0.7167306542396545]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 2/86 [D loss: 0.686516523361206, acc.: 55.18%] [G loss: 0.7134120464324951]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 3/86 [D loss: 0.6854044497013092, acc.: 55.76%] [G loss: 0.71330726146698]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 4/86 [D loss: 0.6858615279197693, acc.: 56.30%] [G loss: 0.7183530926704407]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 5/86 [D loss: 0.6871412992477417, acc.: 54.79%] [G loss: 0.7177003026008606]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 6/86 [D loss: 0.6851134300231934, acc.: 56.69%] [G loss: 0.716524600982666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 7/86 [D loss: 0.6857604682445526, acc.: 56.59%] [G loss: 0.7173017859458923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 8/86 [D loss: 0.6848595142364502, acc.: 55.71%] [G loss: 0.7177242040634155]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 9/86 [D loss: 0.686822921037674, acc.: 54.88%] [G loss: 0.7143946886062622]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 10/86 [D loss: 0.6867504417896271, acc.: 54.88%] [G loss: 0.7188376188278198]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 11/86 [D loss: 0.6874719560146332, acc.: 55.08%] [G loss: 0.711902379989624]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 12/86 [D loss: 0.6903598308563232, acc.: 52.83%] [G loss: 0.7116492390632629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 13/86 [D loss: 0.6868003904819489, acc.: 55.18%] [G loss: 0.716249942779541]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 14/86 [D loss: 0.6864262819290161, acc.: 55.42%] [G loss: 0.7121104001998901]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 15/86 [D loss: 0.6874562501907349, acc.: 55.08%] [G loss: 0.7168313264846802]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 16/86 [D loss: 0.6839702129364014, acc.: 57.91%] [G loss: 0.7171117067337036]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 17/86 [D loss: 0.6878341138362885, acc.: 54.98%] [G loss: 0.7113112211227417]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 18/86 [D loss: 0.6862552762031555, acc.: 55.71%] [G loss: 0.7157618999481201]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 19/86 [D loss: 0.6849301755428314, acc.: 57.08%] [G loss: 0.7114406824111938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 20/86 [D loss: 0.6875911951065063, acc.: 54.79%] [G loss: 0.7143797874450684]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 21/86 [D loss: 0.6853339672088623, acc.: 57.42%] [G loss: 0.7157070636749268]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 22/86 [D loss: 0.6857026219367981, acc.: 55.76%] [G loss: 0.7126246094703674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 23/86 [D loss: 0.6887007057666779, acc.: 53.47%] [G loss: 0.7147770524024963]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 24/86 [D loss: 0.6856462955474854, acc.: 56.35%] [G loss: 0.7125709056854248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 25/86 [D loss: 0.6882160604000092, acc.: 53.52%] [G loss: 0.7135224342346191]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 26/86 [D loss: 0.6869333982467651, acc.: 55.18%] [G loss: 0.7126443982124329]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 27/86 [D loss: 0.6867372393608093, acc.: 56.01%] [G loss: 0.7115192413330078]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 28/86 [D loss: 0.6869121193885803, acc.: 54.54%] [G loss: 0.7154378294944763]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 29/86 [D loss: 0.6831964254379272, acc.: 59.67%] [G loss: 0.714522123336792]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 30/86 [D loss: 0.6883533298969269, acc.: 55.13%] [G loss: 0.7147246599197388]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 31/86 [D loss: 0.6852682828903198, acc.: 56.59%] [G loss: 0.7139850854873657]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 32/86 [D loss: 0.6882157325744629, acc.: 54.10%] [G loss: 0.7100716233253479]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 33/86 [D loss: 0.6887992024421692, acc.: 52.98%] [G loss: 0.7153458595275879]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 34/86 [D loss: 0.6857289671897888, acc.: 55.71%] [G loss: 0.7131855487823486]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 35/86 [D loss: 0.6876725852489471, acc.: 53.86%] [G loss: 0.7172586917877197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 36/86 [D loss: 0.6866225600242615, acc.: 55.22%] [G loss: 0.7159899473190308]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 37/86 [D loss: 0.683692455291748, acc.: 58.84%] [G loss: 0.7133205533027649]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 38/86 [D loss: 0.6868920922279358, acc.: 55.37%] [G loss: 0.7173097133636475]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 39/86 [D loss: 0.6847117841243744, acc.: 57.91%] [G loss: 0.7144300937652588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 40/86 [D loss: 0.6898779273033142, acc.: 52.34%] [G loss: 0.7105389833450317]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 41/86 [D loss: 0.6852834224700928, acc.: 56.64%] [G loss: 0.7174007296562195]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 42/86 [D loss: 0.6842561364173889, acc.: 58.59%] [G loss: 0.7123057842254639]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 43/86 [D loss: 0.6890809834003448, acc.: 53.86%] [G loss: 0.7175290584564209]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 44/86 [D loss: 0.6849521398544312, acc.: 58.01%] [G loss: 0.7147610783576965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 45/86 [D loss: 0.6882548928260803, acc.: 53.66%] [G loss: 0.710912823677063]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 46/86 [D loss: 0.6881279647350311, acc.: 55.32%] [G loss: 0.7200110554695129]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 47/86 [D loss: 0.6861476302146912, acc.: 56.05%] [G loss: 0.7071495056152344]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 48/86 [D loss: 0.691536009311676, acc.: 50.63%] [G loss: 0.7143857479095459]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 49/86 [D loss: 0.6828515529632568, acc.: 59.96%] [G loss: 0.7130815982818604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 50/86 [D loss: 0.689170777797699, acc.: 51.95%] [G loss: 0.7063374519348145]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 51/86 [D loss: 0.6889825463294983, acc.: 53.12%] [G loss: 0.7192167043685913]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 52/86 [D loss: 0.6833404898643494, acc.: 59.81%] [G loss: 0.7079014778137207]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 53/86 [D loss: 0.6935146152973175, acc.: 50.15%] [G loss: 0.7103685140609741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 54/86 [D loss: 0.6830517947673798, acc.: 58.98%] [G loss: 0.7109704613685608]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 55/86 [D loss: 0.6907064020633698, acc.: 51.61%] [G loss: 0.7040573358535767]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 56/86 [D loss: 0.6875607073307037, acc.: 54.44%] [G loss: 0.717322826385498]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 57/86 [D loss: 0.6823486983776093, acc.: 60.11%] [G loss: 0.7088810205459595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 58/86 [D loss: 0.6956148147583008, acc.: 47.90%] [G loss: 0.7050685882568359]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 59/86 [D loss: 0.6852535605430603, acc.: 58.64%] [G loss: 0.7163522243499756]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 60/86 [D loss: 0.6906609535217285, acc.: 52.83%] [G loss: 0.7047751545906067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 61/86 [D loss: 0.6913098096847534, acc.: 50.68%] [G loss: 0.7155608534812927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 62/86 [D loss: 0.6833762526512146, acc.: 58.30%] [G loss: 0.7117957472801208]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 63/86 [D loss: 0.6883941888809204, acc.: 55.13%] [G loss: 0.7065110206604004]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 64/86 [D loss: 0.6871612370014191, acc.: 56.88%] [G loss: 0.7167550921440125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 65/86 [D loss: 0.6858816146850586, acc.: 54.74%] [G loss: 0.7110031247138977]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 66/86 [D loss: 0.6881076395511627, acc.: 54.35%] [G loss: 0.709734320640564]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 67/86 [D loss: 0.6854115724563599, acc.: 55.71%] [G loss: 0.7171564698219299]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 68/86 [D loss: 0.6845891177654266, acc.: 56.98%] [G loss: 0.7133108377456665]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 69/86 [D loss: 0.6878156661987305, acc.: 54.93%] [G loss: 0.71204674243927]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 70/86 [D loss: 0.6856007874011993, acc.: 56.15%] [G loss: 0.7136895060539246]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 71/86 [D loss: 0.6886619925498962, acc.: 54.49%] [G loss: 0.712936520576477]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 72/86 [D loss: 0.6863387823104858, acc.: 54.79%] [G loss: 0.715811014175415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 73/86 [D loss: 0.6844130456447601, acc.: 57.91%] [G loss: 0.7160824537277222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 74/86 [D loss: 0.6871861815452576, acc.: 54.39%] [G loss: 0.7117941379547119]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 75/86 [D loss: 0.6871209740638733, acc.: 56.20%] [G loss: 0.7162265181541443]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 76/86 [D loss: 0.6844321489334106, acc.: 57.08%] [G loss: 0.7161738276481628]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 77/86 [D loss: 0.6860984265804291, acc.: 55.81%] [G loss: 0.7151502966880798]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 78/86 [D loss: 0.6861365437507629, acc.: 55.42%] [G loss: 0.716742992401123]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 79/86 [D loss: 0.6866399347782135, acc.: 55.32%] [G loss: 0.7147564888000488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 80/86 [D loss: 0.6881999373435974, acc.: 55.96%] [G loss: 0.7172374725341797]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 81/86 [D loss: 0.6868548393249512, acc.: 55.42%] [G loss: 0.7167882919311523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 82/86 [D loss: 0.6861115097999573, acc.: 55.52%] [G loss: 0.7147141695022583]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 83/86 [D loss: 0.6840951442718506, acc.: 56.40%] [G loss: 0.7183330655097961]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 84/200, Batch 84/86 [D loss: 0.6867389380931854, acc.: 54.49%] [G loss: 0.7181810736656189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 85/86 [D loss: 0.6849859058856964, acc.: 57.47%] [G loss: 0.7161906957626343]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 84/200, Batch 86/86 [D loss: 0.6845769882202148, acc.: 57.37%] [G loss: 0.7146121859550476]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 15ms/step\n",
      "Epoch 85/200, Batch 1/86 [D loss: 0.6867741942405701, acc.: 55.57%] [G loss: 0.7130861282348633]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 2/86 [D loss: 0.6847335696220398, acc.: 57.71%] [G loss: 0.7193396687507629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 3/86 [D loss: 0.6851774156093597, acc.: 57.52%] [G loss: 0.7181620001792908]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 4/86 [D loss: 0.6863672137260437, acc.: 55.86%] [G loss: 0.7162117958068848]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 5/86 [D loss: 0.6840973794460297, acc.: 58.11%] [G loss: 0.7181395888328552]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 6/86 [D loss: 0.687528133392334, acc.: 55.81%] [G loss: 0.7168749570846558]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 7/86 [D loss: 0.6856704950332642, acc.: 56.25%] [G loss: 0.7184386253356934]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 8/86 [D loss: 0.6856750249862671, acc.: 55.37%] [G loss: 0.7168606519699097]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 9/86 [D loss: 0.6881327629089355, acc.: 55.57%] [G loss: 0.7161169052124023]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 10/86 [D loss: 0.6852333247661591, acc.: 57.18%] [G loss: 0.7203205823898315]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 11/86 [D loss: 0.6835365891456604, acc.: 56.69%] [G loss: 0.713573157787323]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 12/86 [D loss: 0.6864645183086395, acc.: 55.66%] [G loss: 0.717745304107666]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 13/86 [D loss: 0.6872182488441467, acc.: 55.08%] [G loss: 0.7163336873054504]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 14/86 [D loss: 0.6861916482448578, acc.: 56.98%] [G loss: 0.7117958068847656]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 15/86 [D loss: 0.6856832504272461, acc.: 56.69%] [G loss: 0.7177847027778625]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 16/86 [D loss: 0.6871229410171509, acc.: 55.37%] [G loss: 0.7109347581863403]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 17/86 [D loss: 0.6850932538509369, acc.: 56.88%] [G loss: 0.7141990661621094]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 18/86 [D loss: 0.6849243342876434, acc.: 56.45%] [G loss: 0.7143164277076721]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 19/86 [D loss: 0.685979425907135, acc.: 57.18%] [G loss: 0.7150142788887024]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 20/86 [D loss: 0.6853660047054291, acc.: 56.10%] [G loss: 0.7174426317214966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 21/86 [D loss: 0.6862191557884216, acc.: 55.91%] [G loss: 0.7145850658416748]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 22/86 [D loss: 0.6856277883052826, acc.: 56.20%] [G loss: 0.7171677350997925]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 23/86 [D loss: 0.687177300453186, acc.: 54.93%] [G loss: 0.719093382358551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 24/86 [D loss: 0.6864160895347595, acc.: 56.15%] [G loss: 0.7146627306938171]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 25/86 [D loss: 0.6877836585044861, acc.: 55.37%] [G loss: 0.7152607440948486]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 26/86 [D loss: 0.6852587759494781, acc.: 56.54%] [G loss: 0.7199046611785889]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 27/86 [D loss: 0.6851515173912048, acc.: 55.81%] [G loss: 0.7138751149177551]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 28/86 [D loss: 0.6866302490234375, acc.: 54.74%] [G loss: 0.7205615639686584]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 29/86 [D loss: 0.6827034652233124, acc.: 58.45%] [G loss: 0.715400755405426]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 30/86 [D loss: 0.6889082789421082, acc.: 52.73%] [G loss: 0.7121590375900269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 31/86 [D loss: 0.686652421951294, acc.: 55.27%] [G loss: 0.7190661430358887]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 32/86 [D loss: 0.6843084096908569, acc.: 57.57%] [G loss: 0.7145714163780212]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 33/86 [D loss: 0.6889054179191589, acc.: 53.86%] [G loss: 0.7175453901290894]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 34/86 [D loss: 0.6849294006824493, acc.: 55.91%] [G loss: 0.7213548421859741]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 35/86 [D loss: 0.6860489249229431, acc.: 57.42%] [G loss: 0.7079033255577087]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 36/86 [D loss: 0.6890195906162262, acc.: 52.98%] [G loss: 0.7189761996269226]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 37/86 [D loss: 0.6845221817493439, acc.: 58.98%] [G loss: 0.713681697845459]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 38/86 [D loss: 0.6873835325241089, acc.: 54.15%] [G loss: 0.7132235765457153]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 39/86 [D loss: 0.6867042779922485, acc.: 54.64%] [G loss: 0.7200667858123779]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 40/86 [D loss: 0.6864258348941803, acc.: 55.91%] [G loss: 0.714870274066925]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 41/86 [D loss: 0.6873873174190521, acc.: 54.35%] [G loss: 0.7150360345840454]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 42/86 [D loss: 0.6836016774177551, acc.: 58.15%] [G loss: 0.7150143384933472]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 43/86 [D loss: 0.6853917837142944, acc.: 56.64%] [G loss: 0.7123883962631226]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 44/86 [D loss: 0.6863053739070892, acc.: 55.37%] [G loss: 0.7157537937164307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 45/86 [D loss: 0.6830780208110809, acc.: 58.11%] [G loss: 0.714553952217102]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 46/86 [D loss: 0.6877691447734833, acc.: 54.30%] [G loss: 0.7144413590431213]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 47/86 [D loss: 0.684366762638092, acc.: 58.11%] [G loss: 0.7196479439735413]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 48/86 [D loss: 0.6868893206119537, acc.: 55.18%] [G loss: 0.7143166661262512]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 49/86 [D loss: 0.6872462630271912, acc.: 52.64%] [G loss: 0.7175658345222473]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 50/86 [D loss: 0.6870865523815155, acc.: 54.88%] [G loss: 0.7173977494239807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 51/86 [D loss: 0.6868804097175598, acc.: 56.54%] [G loss: 0.713613748550415]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 52/86 [D loss: 0.6856574714183807, acc.: 57.86%] [G loss: 0.7171432971954346]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 53/86 [D loss: 0.6824804544448853, acc.: 59.08%] [G loss: 0.714438796043396]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 54/86 [D loss: 0.6878846287727356, acc.: 55.52%] [G loss: 0.7161755561828613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 55/86 [D loss: 0.6852610409259796, acc.: 55.71%] [G loss: 0.7176456451416016]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 56/86 [D loss: 0.6852234303951263, acc.: 57.76%] [G loss: 0.7177684307098389]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 57/86 [D loss: 0.6863702237606049, acc.: 56.01%] [G loss: 0.7167244553565979]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 58/86 [D loss: 0.6840045154094696, acc.: 56.98%] [G loss: 0.7193642258644104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 59/86 [D loss: 0.6866686046123505, acc.: 54.79%] [G loss: 0.7175745964050293]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 60/86 [D loss: 0.6880204975605011, acc.: 54.88%] [G loss: 0.7195277214050293]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 61/86 [D loss: 0.6833643317222595, acc.: 58.84%] [G loss: 0.7160300016403198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 62/86 [D loss: 0.6865674555301666, acc.: 55.66%] [G loss: 0.7164192199707031]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 63/86 [D loss: 0.6832478046417236, acc.: 58.11%] [G loss: 0.7148406505584717]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 64/86 [D loss: 0.6862834095954895, acc.: 55.96%] [G loss: 0.7156563401222229]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 65/86 [D loss: 0.6850390136241913, acc.: 57.71%] [G loss: 0.7146239876747131]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 66/86 [D loss: 0.6832901835441589, acc.: 59.42%] [G loss: 0.7175165414810181]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 67/86 [D loss: 0.689132571220398, acc.: 53.22%] [G loss: 0.7168708443641663]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 68/86 [D loss: 0.6855791211128235, acc.: 57.32%] [G loss: 0.7172033190727234]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 69/86 [D loss: 0.686585396528244, acc.: 55.71%] [G loss: 0.7179660201072693]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 70/86 [D loss: 0.684538334608078, acc.: 58.01%] [G loss: 0.7179570198059082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 71/86 [D loss: 0.685511440038681, acc.: 56.79%] [G loss: 0.7180076241493225]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 72/86 [D loss: 0.6841323375701904, acc.: 56.64%] [G loss: 0.7148961424827576]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 73/86 [D loss: 0.6859703361988068, acc.: 55.13%] [G loss: 0.7170189619064331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 74/86 [D loss: 0.6850647032260895, acc.: 57.08%] [G loss: 0.7166559100151062]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 75/86 [D loss: 0.6848946213722229, acc.: 56.79%] [G loss: 0.7193853855133057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 76/86 [D loss: 0.6864914894104004, acc.: 56.74%] [G loss: 0.7159057855606079]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 77/86 [D loss: 0.6842314004898071, acc.: 57.52%] [G loss: 0.7155617475509644]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 78/86 [D loss: 0.684852659702301, acc.: 57.47%] [G loss: 0.7176012992858887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 79/86 [D loss: 0.6864365637302399, acc.: 56.15%] [G loss: 0.7142254710197449]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 80/86 [D loss: 0.686939001083374, acc.: 55.57%] [G loss: 0.7169474959373474]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 81/86 [D loss: 0.6857554316520691, acc.: 56.84%] [G loss: 0.7150419354438782]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 82/86 [D loss: 0.6864993870258331, acc.: 56.74%] [G loss: 0.7129398584365845]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 83/86 [D loss: 0.6850564479827881, acc.: 57.42%] [G loss: 0.7143855094909668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 84/86 [D loss: 0.6860983967781067, acc.: 56.15%] [G loss: 0.7125625610351562]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 85/200, Batch 85/86 [D loss: 0.6860122680664062, acc.: 55.32%] [G loss: 0.7142230272293091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 85/200, Batch 86/86 [D loss: 0.6844459772109985, acc.: 57.57%] [G loss: 0.7148311138153076]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 1/86 [D loss: 0.6846950650215149, acc.: 58.01%] [G loss: 0.7160757780075073]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 2/86 [D loss: 0.6840454638004303, acc.: 58.01%] [G loss: 0.7178863286972046]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 3/86 [D loss: 0.6863722503185272, acc.: 56.49%] [G loss: 0.717700183391571]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 4/86 [D loss: 0.6850304305553436, acc.: 56.88%] [G loss: 0.716865062713623]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 5/86 [D loss: 0.6845871210098267, acc.: 57.47%] [G loss: 0.7162411212921143]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 6/86 [D loss: 0.6868718862533569, acc.: 55.91%] [G loss: 0.7154655456542969]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 7/86 [D loss: 0.6841014921665192, acc.: 58.20%] [G loss: 0.7162759304046631]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 8/86 [D loss: 0.6871350109577179, acc.: 56.05%] [G loss: 0.7183853387832642]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 9/86 [D loss: 0.6842645108699799, acc.: 56.84%] [G loss: 0.7175120711326599]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 10/86 [D loss: 0.6859555542469025, acc.: 55.42%] [G loss: 0.7188869714736938]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 11/86 [D loss: 0.6858221292495728, acc.: 56.30%] [G loss: 0.7208606004714966]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 12/86 [D loss: 0.6840287148952484, acc.: 58.54%] [G loss: 0.71547931432724]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 13/86 [D loss: 0.6851614415645599, acc.: 56.35%] [G loss: 0.7192309498786926]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 14/86 [D loss: 0.6848693788051605, acc.: 56.79%] [G loss: 0.7190978527069092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 15/86 [D loss: 0.6866583824157715, acc.: 56.69%] [G loss: 0.7119113206863403]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 16/86 [D loss: 0.6868268251419067, acc.: 54.79%] [G loss: 0.7185261249542236]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 17/86 [D loss: 0.6848689615726471, acc.: 55.57%] [G loss: 0.7147820591926575]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 18/86 [D loss: 0.6837361752986908, acc.: 59.42%] [G loss: 0.7161015868186951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 19/86 [D loss: 0.6839596629142761, acc.: 57.47%] [G loss: 0.7194479703903198]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 20/86 [D loss: 0.6859223544597626, acc.: 55.81%] [G loss: 0.7161255478858948]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 21/86 [D loss: 0.6837240755558014, acc.: 57.91%] [G loss: 0.7204894423484802]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 22/86 [D loss: 0.6840359270572662, acc.: 56.69%] [G loss: 0.7138625383377075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 23/86 [D loss: 0.6867924630641937, acc.: 55.62%] [G loss: 0.7146710157394409]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 24/86 [D loss: 0.6838840246200562, acc.: 59.42%] [G loss: 0.7151675224304199]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 25/86 [D loss: 0.6861613392829895, acc.: 57.28%] [G loss: 0.7138912081718445]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 26/86 [D loss: 0.6840849816799164, acc.: 58.50%] [G loss: 0.7190954685211182]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 27/86 [D loss: 0.684111624956131, acc.: 56.74%] [G loss: 0.7164293527603149]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 28/86 [D loss: 0.6857037544250488, acc.: 54.59%] [G loss: 0.716949999332428]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 29/86 [D loss: 0.6848488748073578, acc.: 57.96%] [G loss: 0.7185372114181519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 30/86 [D loss: 0.6864169538021088, acc.: 55.71%] [G loss: 0.7183132767677307]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 31/86 [D loss: 0.6855766177177429, acc.: 56.54%] [G loss: 0.7161983251571655]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 32/86 [D loss: 0.6860804259777069, acc.: 55.86%] [G loss: 0.7162752747535706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 33/86 [D loss: 0.686030387878418, acc.: 55.52%] [G loss: 0.7179343700408936]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 34/86 [D loss: 0.6850376725196838, acc.: 56.49%] [G loss: 0.714935302734375]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 35/86 [D loss: 0.6863049566745758, acc.: 56.20%] [G loss: 0.7180379629135132]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 36/86 [D loss: 0.6862956583499908, acc.: 54.74%] [G loss: 0.716934084892273]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 37/86 [D loss: 0.6859269738197327, acc.: 54.15%] [G loss: 0.7169325351715088]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 38/86 [D loss: 0.6873327195644379, acc.: 54.15%] [G loss: 0.7182119488716125]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 39/86 [D loss: 0.6849872171878815, acc.: 56.88%] [G loss: 0.7193806767463684]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 40/86 [D loss: 0.6877652406692505, acc.: 54.25%] [G loss: 0.7188819646835327]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 41/86 [D loss: 0.6860391497612, acc.: 55.86%] [G loss: 0.7193105220794678]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 42/86 [D loss: 0.6853958964347839, acc.: 56.74%] [G loss: 0.7131524085998535]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 43/86 [D loss: 0.6878803372383118, acc.: 54.64%] [G loss: 0.7162764668464661]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 44/86 [D loss: 0.6831121146678925, acc.: 58.79%] [G loss: 0.715990424156189]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 45/86 [D loss: 0.6877760887145996, acc.: 54.83%] [G loss: 0.7151705026626587]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 46/86 [D loss: 0.6847251951694489, acc.: 57.62%] [G loss: 0.7201361656188965]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 47/86 [D loss: 0.6858413815498352, acc.: 55.32%] [G loss: 0.7150733470916748]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 48/86 [D loss: 0.6881073713302612, acc.: 54.44%] [G loss: 0.7196391224861145]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 49/86 [D loss: 0.6839210987091064, acc.: 57.86%] [G loss: 0.713864803314209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 50/86 [D loss: 0.6908913552761078, acc.: 52.44%] [G loss: 0.7146207690238953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 51/86 [D loss: 0.6854562163352966, acc.: 55.91%] [G loss: 0.7159255146980286]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 52/86 [D loss: 0.6867709457874298, acc.: 54.64%] [G loss: 0.7113610506057739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 53/86 [D loss: 0.6860645115375519, acc.: 55.03%] [G loss: 0.7191116809844971]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 54/86 [D loss: 0.6853287816047668, acc.: 57.18%] [G loss: 0.7096836566925049]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 55/86 [D loss: 0.6924137473106384, acc.: 51.12%] [G loss: 0.720136821269989]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 56/86 [D loss: 0.6832139194011688, acc.: 57.32%] [G loss: 0.714867115020752]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 57/86 [D loss: 0.6906079053878784, acc.: 53.27%] [G loss: 0.7146685123443604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 58/86 [D loss: 0.6837462782859802, acc.: 57.86%] [G loss: 0.7211194634437561]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 59/86 [D loss: 0.6861042082309723, acc.: 55.27%] [G loss: 0.7093096971511841]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 60/86 [D loss: 0.6870661973953247, acc.: 55.52%] [G loss: 0.7230274677276611]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 61/86 [D loss: 0.6843582391738892, acc.: 56.74%] [G loss: 0.716144323348999]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 62/86 [D loss: 0.687035322189331, acc.: 54.64%] [G loss: 0.7162041664123535]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 63/86 [D loss: 0.6858243644237518, acc.: 56.64%] [G loss: 0.7210818529129028]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 64/86 [D loss: 0.6868795454502106, acc.: 54.88%] [G loss: 0.7109524011611938]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 65/86 [D loss: 0.6892121434211731, acc.: 53.86%] [G loss: 0.7189782857894897]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 66/86 [D loss: 0.6862948536872864, acc.: 55.66%] [G loss: 0.7156319618225098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 67/86 [D loss: 0.6888925135135651, acc.: 53.22%] [G loss: 0.7163906097412109]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 68/86 [D loss: 0.684058427810669, acc.: 57.81%] [G loss: 0.7193233370780945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 69/86 [D loss: 0.6842606365680695, acc.: 57.03%] [G loss: 0.7085505127906799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 70/86 [D loss: 0.6897089779376984, acc.: 52.93%] [G loss: 0.7147530913352966]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 71/86 [D loss: 0.6840955913066864, acc.: 57.37%] [G loss: 0.7164185047149658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 72/86 [D loss: 0.6886399984359741, acc.: 51.95%] [G loss: 0.7119852304458618]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 73/86 [D loss: 0.6857850849628448, acc.: 55.76%] [G loss: 0.7185518145561218]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 74/86 [D loss: 0.6864335238933563, acc.: 55.42%] [G loss: 0.7164715528488159]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 75/86 [D loss: 0.6870397627353668, acc.: 54.44%] [G loss: 0.715703010559082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 76/86 [D loss: 0.6830921471118927, acc.: 58.45%] [G loss: 0.7156618237495422]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 77/86 [D loss: 0.6867515444755554, acc.: 55.76%] [G loss: 0.7142860889434814]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 78/86 [D loss: 0.6876170933246613, acc.: 55.08%] [G loss: 0.7177198529243469]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 79/86 [D loss: 0.6852146983146667, acc.: 56.15%] [G loss: 0.716303288936615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 80/86 [D loss: 0.6858685314655304, acc.: 55.91%] [G loss: 0.7163230180740356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 81/86 [D loss: 0.685033917427063, acc.: 56.59%] [G loss: 0.71844482421875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 82/86 [D loss: 0.6861083209514618, acc.: 55.62%] [G loss: 0.7199761867523193]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 86/200, Batch 83/86 [D loss: 0.6858484745025635, acc.: 55.66%] [G loss: 0.7191062569618225]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 86/200, Batch 84/86 [D loss: 0.6850304305553436, acc.: 56.30%] [G loss: 0.7178007960319519]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 85/86 [D loss: 0.6871644854545593, acc.: 55.52%] [G loss: 0.7168199419975281]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 86/200, Batch 86/86 [D loss: 0.6863234043121338, acc.: 54.64%] [G loss: 0.717189610004425]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 1/86 [D loss: 0.6844545900821686, acc.: 57.08%] [G loss: 0.715004563331604]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 2/86 [D loss: 0.6876198649406433, acc.: 55.37%] [G loss: 0.7180453538894653]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 3/86 [D loss: 0.6834228336811066, acc.: 58.59%] [G loss: 0.7184994220733643]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 4/86 [D loss: 0.6862095296382904, acc.: 55.37%] [G loss: 0.7115786075592041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 5/86 [D loss: 0.6878859996795654, acc.: 54.49%] [G loss: 0.717585563659668]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 6/86 [D loss: 0.6869222521781921, acc.: 53.96%] [G loss: 0.7113512754440308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 7/86 [D loss: 0.6884883344173431, acc.: 52.59%] [G loss: 0.7180010080337524]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 8/86 [D loss: 0.6815389692783356, acc.: 59.52%] [G loss: 0.7173483967781067]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 9/86 [D loss: 0.6888138651847839, acc.: 53.22%] [G loss: 0.7167777419090271]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 10/86 [D loss: 0.6848474144935608, acc.: 57.47%] [G loss: 0.7170896530151367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 11/86 [D loss: 0.6853908896446228, acc.: 57.18%] [G loss: 0.7104299068450928]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 12/86 [D loss: 0.6864842176437378, acc.: 55.03%] [G loss: 0.7201524972915649]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 13/86 [D loss: 0.6831952035427094, acc.: 58.30%] [G loss: 0.7181339859962463]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 14/86 [D loss: 0.6899040639400482, acc.: 52.05%] [G loss: 0.7142125964164734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 15/86 [D loss: 0.6854020059108734, acc.: 56.88%] [G loss: 0.7210122346878052]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 16/86 [D loss: 0.6846812665462494, acc.: 55.96%] [G loss: 0.7121003270149231]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 17/86 [D loss: 0.6856606900691986, acc.: 55.42%] [G loss: 0.7163570523262024]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 87/200, Batch 18/86 [D loss: 0.6859934329986572, acc.: 57.03%] [G loss: 0.7216506600379944]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 19/86 [D loss: 0.6869502663612366, acc.: 56.59%] [G loss: 0.7115033268928528]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 20/86 [D loss: 0.6875066161155701, acc.: 54.00%] [G loss: 0.7197644710540771]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 21/86 [D loss: 0.6848822832107544, acc.: 57.37%] [G loss: 0.7153857946395874]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 22/86 [D loss: 0.6894907355308533, acc.: 52.73%] [G loss: 0.7174773216247559]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 23/86 [D loss: 0.6827133297920227, acc.: 58.11%] [G loss: 0.7173318862915039]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 24/86 [D loss: 0.6886963546276093, acc.: 53.86%] [G loss: 0.7120445966720581]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 25/86 [D loss: 0.6853013038635254, acc.: 56.64%] [G loss: 0.7206560969352722]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 26/86 [D loss: 0.6856576800346375, acc.: 56.74%] [G loss: 0.7124330401420593]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 27/86 [D loss: 0.6894164383411407, acc.: 53.56%] [G loss: 0.718536376953125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 28/86 [D loss: 0.6847372353076935, acc.: 56.01%] [G loss: 0.715939998626709]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 29/86 [D loss: 0.6864528656005859, acc.: 55.37%] [G loss: 0.7085197567939758]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 30/86 [D loss: 0.6879889070987701, acc.: 53.66%] [G loss: 0.7169463038444519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 31/86 [D loss: 0.6841107308864594, acc.: 55.86%] [G loss: 0.7134382128715515]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 32/86 [D loss: 0.6855828166007996, acc.: 56.49%] [G loss: 0.715043306350708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 33/86 [D loss: 0.6823940873146057, acc.: 58.59%] [G loss: 0.7251324653625488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 34/86 [D loss: 0.6864569783210754, acc.: 55.62%] [G loss: 0.7111542224884033]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 35/86 [D loss: 0.685017466545105, acc.: 56.69%] [G loss: 0.7176600098609924]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 36/86 [D loss: 0.6851730346679688, acc.: 57.08%] [G loss: 0.7112870216369629]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 37/86 [D loss: 0.6879493892192841, acc.: 53.42%] [G loss: 0.7162349224090576]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 38/86 [D loss: 0.6826009750366211, acc.: 57.57%] [G loss: 0.7164839506149292]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 39/86 [D loss: 0.6861855387687683, acc.: 55.76%] [G loss: 0.7169399261474609]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 40/86 [D loss: 0.6857995092868805, acc.: 55.27%] [G loss: 0.718462347984314]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 41/86 [D loss: 0.688050776720047, acc.: 54.88%] [G loss: 0.7158102989196777]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 87/200, Batch 42/86 [D loss: 0.6855809390544891, acc.: 56.01%] [G loss: 0.7184394598007202]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 43/86 [D loss: 0.6849627494812012, acc.: 55.96%] [G loss: 0.7161574363708496]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 44/86 [D loss: 0.6854838728904724, acc.: 56.15%] [G loss: 0.7164722681045532]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 45/86 [D loss: 0.6836914718151093, acc.: 58.84%] [G loss: 0.7163230180740356]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 46/86 [D loss: 0.6852754056453705, acc.: 55.47%] [G loss: 0.7152301073074341]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 47/86 [D loss: 0.6862006187438965, acc.: 56.30%] [G loss: 0.7187176942825317]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 48/86 [D loss: 0.6867935657501221, acc.: 54.05%] [G loss: 0.7189700603485107]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 49/86 [D loss: 0.6863220036029816, acc.: 55.52%] [G loss: 0.7191256284713745]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 50/86 [D loss: 0.6857911050319672, acc.: 55.37%] [G loss: 0.7185708284378052]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 51/86 [D loss: 0.6833082437515259, acc.: 57.42%] [G loss: 0.7171906232833862]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 52/86 [D loss: 0.6851989328861237, acc.: 56.05%] [G loss: 0.72073894739151]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 53/86 [D loss: 0.6845535933971405, acc.: 56.49%] [G loss: 0.7228814363479614]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 54/86 [D loss: 0.6852503716945648, acc.: 56.10%] [G loss: 0.7156264781951904]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 55/86 [D loss: 0.6856119334697723, acc.: 54.93%] [G loss: 0.7213827967643738]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 56/86 [D loss: 0.6840523481369019, acc.: 56.79%] [G loss: 0.7189255356788635]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 57/86 [D loss: 0.6846292614936829, acc.: 56.79%] [G loss: 0.718329131603241]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 58/86 [D loss: 0.6846134066581726, acc.: 57.62%] [G loss: 0.7184486389160156]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 59/86 [D loss: 0.6856945753097534, acc.: 56.54%] [G loss: 0.7187448143959045]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 60/86 [D loss: 0.6866117119789124, acc.: 55.91%] [G loss: 0.7200719118118286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 61/86 [D loss: 0.6839390993118286, acc.: 58.98%] [G loss: 0.7182239294052124]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 62/86 [D loss: 0.6872222721576691, acc.: 55.81%] [G loss: 0.7177027463912964]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 63/86 [D loss: 0.6830584704875946, acc.: 57.67%] [G loss: 0.718876838684082]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 64/86 [D loss: 0.6864128410816193, acc.: 55.96%] [G loss: 0.7179132699966431]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 65/86 [D loss: 0.6860160827636719, acc.: 55.91%] [G loss: 0.7226535081863403]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 66/86 [D loss: 0.683901309967041, acc.: 57.03%] [G loss: 0.7203314304351807]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 67/86 [D loss: 0.6871998012065887, acc.: 55.52%] [G loss: 0.7215986847877502]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 68/86 [D loss: 0.6839536428451538, acc.: 57.13%] [G loss: 0.7225689888000488]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 69/86 [D loss: 0.6847539246082306, acc.: 56.01%] [G loss: 0.7172262668609619]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 70/86 [D loss: 0.6853882372379303, acc.: 55.37%] [G loss: 0.7218354940414429]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 71/86 [D loss: 0.6839044392108917, acc.: 57.67%] [G loss: 0.7223689556121826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 72/86 [D loss: 0.6880218684673309, acc.: 54.15%] [G loss: 0.720133900642395]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 73/86 [D loss: 0.6840240061283112, acc.: 55.71%] [G loss: 0.7196193337440491]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 74/86 [D loss: 0.6851058304309845, acc.: 56.79%] [G loss: 0.7200811505317688]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 75/86 [D loss: 0.6853899359703064, acc.: 56.15%] [G loss: 0.7201645970344543]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 76/86 [D loss: 0.6831543147563934, acc.: 58.45%] [G loss: 0.7187123894691467]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 77/86 [D loss: 0.6875252723693848, acc.: 54.69%] [G loss: 0.7178054451942444]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 78/86 [D loss: 0.6875237822532654, acc.: 54.39%] [G loss: 0.7182630300521851]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 79/86 [D loss: 0.6835757791996002, acc.: 58.69%] [G loss: 0.7167099118232727]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 80/86 [D loss: 0.6883218586444855, acc.: 55.27%] [G loss: 0.7180228233337402]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 81/86 [D loss: 0.6835090816020966, acc.: 58.45%] [G loss: 0.7209026217460632]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 82/86 [D loss: 0.6843727231025696, acc.: 55.96%] [G loss: 0.7164627313613892]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 83/86 [D loss: 0.6869178712368011, acc.: 56.15%] [G loss: 0.7187293171882629]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 87/200, Batch 84/86 [D loss: 0.6838355958461761, acc.: 56.69%] [G loss: 0.7217625975608826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 85/86 [D loss: 0.6861979365348816, acc.: 55.42%] [G loss: 0.7240122556686401]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 87/200, Batch 86/86 [D loss: 0.6812281012535095, acc.: 60.40%] [G loss: 0.7177951335906982]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 1/86 [D loss: 0.6864217519760132, acc.: 56.59%] [G loss: 0.7145318984985352]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 2/86 [D loss: 0.6882894933223724, acc.: 53.22%] [G loss: 0.7181369662284851]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 3/86 [D loss: 0.6849648654460907, acc.: 56.25%] [G loss: 0.718864917755127]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 4/86 [D loss: 0.6854727864265442, acc.: 55.91%] [G loss: 0.7201409935951233]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 5/86 [D loss: 0.6828613579273224, acc.: 59.13%] [G loss: 0.7152162194252014]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 6/86 [D loss: 0.6854186952114105, acc.: 55.71%] [G loss: 0.7174769639968872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 7/86 [D loss: 0.6842735111713409, acc.: 57.62%] [G loss: 0.7213713526725769]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 8/86 [D loss: 0.6852604150772095, acc.: 55.76%] [G loss: 0.7145334482192993]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 9/86 [D loss: 0.6867544949054718, acc.: 54.54%] [G loss: 0.7172771692276001]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 10/86 [D loss: 0.6828681826591492, acc.: 57.52%] [G loss: 0.716763973236084]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 11/86 [D loss: 0.6871138215065002, acc.: 53.52%] [G loss: 0.7175919413566589]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 12/86 [D loss: 0.6860453188419342, acc.: 55.03%] [G loss: 0.7188940048217773]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 13/86 [D loss: 0.6840801239013672, acc.: 57.23%] [G loss: 0.7140030860900879]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 14/86 [D loss: 0.6889780461788177, acc.: 52.10%] [G loss: 0.7199395298957825]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 15/86 [D loss: 0.6841181218624115, acc.: 56.93%] [G loss: 0.7177914381027222]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 16/86 [D loss: 0.687746673822403, acc.: 53.32%] [G loss: 0.7121545672416687]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 17/86 [D loss: 0.6873776018619537, acc.: 54.20%] [G loss: 0.7214431166648865]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 18/86 [D loss: 0.6821740865707397, acc.: 59.08%] [G loss: 0.7185249924659729]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 19/86 [D loss: 0.6917176842689514, acc.: 52.59%] [G loss: 0.714323103427887]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 20/86 [D loss: 0.6839422285556793, acc.: 58.35%] [G loss: 0.7162907123565674]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 21/86 [D loss: 0.6866203248500824, acc.: 56.35%] [G loss: 0.7089296579360962]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 22/86 [D loss: 0.688233882188797, acc.: 52.69%] [G loss: 0.7195074558258057]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 23/86 [D loss: 0.6801989376544952, acc.: 60.74%] [G loss: 0.715966522693634]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 24/86 [D loss: 0.6938373744487762, acc.: 50.20%] [G loss: 0.715437650680542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 25/86 [D loss: 0.6828183531761169, acc.: 58.15%] [G loss: 0.7162891030311584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 26/86 [D loss: 0.6899854242801666, acc.: 52.93%] [G loss: 0.7068890333175659]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 27/86 [D loss: 0.6856601536273956, acc.: 55.71%] [G loss: 0.7198150753974915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 28/86 [D loss: 0.6823669672012329, acc.: 57.57%] [G loss: 0.7129165530204773]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 29/86 [D loss: 0.6910536885261536, acc.: 52.69%] [G loss: 0.7137289643287659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 30/86 [D loss: 0.682931661605835, acc.: 58.94%] [G loss: 0.7145946621894836]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 31/86 [D loss: 0.6887584328651428, acc.: 53.37%] [G loss: 0.7092846632003784]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 32/86 [D loss: 0.6865279972553253, acc.: 55.96%] [G loss: 0.7211259007453918]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 33/86 [D loss: 0.6819860339164734, acc.: 59.47%] [G loss: 0.7131381034851074]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 34/86 [D loss: 0.6898757219314575, acc.: 52.78%] [G loss: 0.7172330021858215]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 35/86 [D loss: 0.68222975730896, acc.: 59.28%] [G loss: 0.7177940607070923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 36/86 [D loss: 0.6903314590454102, acc.: 52.34%] [G loss: 0.7110801935195923]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 37/86 [D loss: 0.6867158114910126, acc.: 54.74%] [G loss: 0.7221326231956482]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 38/86 [D loss: 0.6809731721878052, acc.: 59.86%] [G loss: 0.7178866267204285]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 39/86 [D loss: 0.6915484368801117, acc.: 52.20%] [G loss: 0.7186144590377808]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 40/86 [D loss: 0.6823227107524872, acc.: 58.35%] [G loss: 0.7182245850563049]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 41/86 [D loss: 0.687581866979599, acc.: 54.69%] [G loss: 0.7128152847290039]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 42/86 [D loss: 0.6869018375873566, acc.: 53.86%] [G loss: 0.7222934365272522]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 43/86 [D loss: 0.6819820702075958, acc.: 58.74%] [G loss: 0.7179480195045471]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 44/86 [D loss: 0.6923333704471588, acc.: 50.59%] [G loss: 0.714569091796875]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 45/86 [D loss: 0.6850452423095703, acc.: 56.59%] [G loss: 0.7228033542633057]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 46/86 [D loss: 0.6854203343391418, acc.: 55.66%] [G loss: 0.7148247957229614]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 47/86 [D loss: 0.6865949928760529, acc.: 54.64%] [G loss: 0.7194532155990601]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 48/86 [D loss: 0.6841179728507996, acc.: 57.67%] [G loss: 0.7218298316001892]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 49/86 [D loss: 0.6858842968940735, acc.: 55.86%] [G loss: 0.7166591882705688]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 88/200, Batch 50/86 [D loss: 0.6860845386981964, acc.: 56.49%] [G loss: 0.7228533029556274]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 51/86 [D loss: 0.6845088303089142, acc.: 58.30%] [G loss: 0.7186136245727539]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 52/86 [D loss: 0.6846368908882141, acc.: 56.98%] [G loss: 0.7166754007339478]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 53/86 [D loss: 0.685449630022049, acc.: 56.69%] [G loss: 0.7185847759246826]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 54/86 [D loss: 0.6844545602798462, acc.: 57.28%] [G loss: 0.7186828851699829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 55/86 [D loss: 0.6867415010929108, acc.: 55.66%] [G loss: 0.7217384576797485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 56/86 [D loss: 0.6837973296642303, acc.: 58.06%] [G loss: 0.717760443687439]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 57/86 [D loss: 0.6859913766384125, acc.: 56.20%] [G loss: 0.7169037461280823]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 58/86 [D loss: 0.6837002635002136, acc.: 57.47%] [G loss: 0.720188319683075]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 59/86 [D loss: 0.6855108141899109, acc.: 56.20%] [G loss: 0.7198147773742676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 60/86 [D loss: 0.6840856075286865, acc.: 56.40%] [G loss: 0.7189697623252869]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 61/86 [D loss: 0.68442502617836, acc.: 57.52%] [G loss: 0.7224366664886475]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 62/86 [D loss: 0.684523731470108, acc.: 56.35%] [G loss: 0.7201876044273376]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 63/86 [D loss: 0.6846712529659271, acc.: 56.93%] [G loss: 0.7214438915252686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 64/86 [D loss: 0.6832473874092102, acc.: 57.32%] [G loss: 0.7207494974136353]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 65/86 [D loss: 0.6835205256938934, acc.: 56.79%] [G loss: 0.7202900052070618]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 66/86 [D loss: 0.6842888593673706, acc.: 57.28%] [G loss: 0.7186117172241211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 67/86 [D loss: 0.6848902404308319, acc.: 56.15%] [G loss: 0.7202624082565308]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 68/86 [D loss: 0.6838090121746063, acc.: 56.74%] [G loss: 0.7201031446456909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 69/86 [D loss: 0.6846746206283569, acc.: 57.42%] [G loss: 0.7221274971961975]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 70/86 [D loss: 0.6861909627914429, acc.: 53.71%] [G loss: 0.7196141481399536]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 71/86 [D loss: 0.6843985319137573, acc.: 57.57%] [G loss: 0.7189767360687256]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 72/86 [D loss: 0.6830872595310211, acc.: 56.54%] [G loss: 0.7188677191734314]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 73/86 [D loss: 0.6842891275882721, acc.: 58.30%] [G loss: 0.7192913889884949]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 74/86 [D loss: 0.6851284801959991, acc.: 56.20%] [G loss: 0.7215197682380676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 75/86 [D loss: 0.683973491191864, acc.: 57.57%] [G loss: 0.7167922854423523]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 76/86 [D loss: 0.6862260103225708, acc.: 56.05%] [G loss: 0.7184616923332214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 77/86 [D loss: 0.6848291456699371, acc.: 56.69%] [G loss: 0.7199575304985046]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 78/86 [D loss: 0.6869140565395355, acc.: 54.59%] [G loss: 0.7225842475891113]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 79/86 [D loss: 0.6828209161758423, acc.: 58.25%] [G loss: 0.7211304306983948]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 80/86 [D loss: 0.6832473874092102, acc.: 57.57%] [G loss: 0.7187244892120361]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 81/86 [D loss: 0.68720343708992, acc.: 54.64%] [G loss: 0.7235040664672852]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 82/86 [D loss: 0.6829308569431305, acc.: 59.08%] [G loss: 0.7182343006134033]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 83/86 [D loss: 0.6859691441059113, acc.: 55.32%] [G loss: 0.7199503183364868]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 88/200, Batch 84/86 [D loss: 0.6853788793087006, acc.: 57.03%] [G loss: 0.7197859287261963]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 85/86 [D loss: 0.6842837631702423, acc.: 56.64%] [G loss: 0.7189728617668152]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 88/200, Batch 86/86 [D loss: 0.6848161518573761, acc.: 56.69%] [G loss: 0.7226249575614929]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 1/86 [D loss: 0.6844435036182404, acc.: 56.59%] [G loss: 0.7204137444496155]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 2/86 [D loss: 0.6857180297374725, acc.: 56.64%] [G loss: 0.7177582383155823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 3/86 [D loss: 0.6844711303710938, acc.: 55.96%] [G loss: 0.7199978828430176]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 4/86 [D loss: 0.683598130941391, acc.: 58.64%] [G loss: 0.7187104225158691]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 5/86 [D loss: 0.6865480542182922, acc.: 54.30%] [G loss: 0.7207337617874146]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 6/86 [D loss: 0.6855882704257965, acc.: 56.69%] [G loss: 0.7221558094024658]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 7/86 [D loss: 0.6846722364425659, acc.: 56.30%] [G loss: 0.7193915843963623]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 8/86 [D loss: 0.6869587302207947, acc.: 54.93%] [G loss: 0.7219420671463013]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 9/86 [D loss: 0.6863496601581573, acc.: 55.27%] [G loss: 0.7208028435707092]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 10/86 [D loss: 0.6843277812004089, acc.: 57.37%] [G loss: 0.7200679779052734]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 11/86 [D loss: 0.6844609677791595, acc.: 56.88%] [G loss: 0.7196166515350342]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 12/86 [D loss: 0.6826078593730927, acc.: 59.47%] [G loss: 0.7202479243278503]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 13/86 [D loss: 0.6856322586536407, acc.: 55.47%] [G loss: 0.7208447456359863]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 14/86 [D loss: 0.6827256381511688, acc.: 58.79%] [G loss: 0.719055712223053]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 15/86 [D loss: 0.6861270070075989, acc.: 56.40%] [G loss: 0.7175325751304626]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 16/86 [D loss: 0.6842374205589294, acc.: 58.15%] [G loss: 0.7204791307449341]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 17/86 [D loss: 0.6862505674362183, acc.: 54.74%] [G loss: 0.720766544342041]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 18/86 [D loss: 0.685492217540741, acc.: 56.98%] [G loss: 0.7207158803939819]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 19/86 [D loss: 0.6836084425449371, acc.: 56.64%] [G loss: 0.7220063805580139]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 20/86 [D loss: 0.6852296590805054, acc.: 56.98%] [G loss: 0.7199375033378601]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 21/86 [D loss: 0.6848503649234772, acc.: 56.45%] [G loss: 0.7199500799179077]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 22/86 [D loss: 0.6843521296977997, acc.: 56.88%] [G loss: 0.7233215570449829]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 23/86 [D loss: 0.6851462423801422, acc.: 55.81%] [G loss: 0.7195929288864136]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 24/86 [D loss: 0.6845390200614929, acc.: 56.40%] [G loss: 0.7199172377586365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 25/86 [D loss: 0.6854847073554993, acc.: 56.69%] [G loss: 0.7217206358909607]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 26/86 [D loss: 0.6840302348136902, acc.: 57.18%] [G loss: 0.719106912612915]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 27/86 [D loss: 0.685803234577179, acc.: 56.59%] [G loss: 0.7196860313415527]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 28/86 [D loss: 0.685743898153305, acc.: 55.71%] [G loss: 0.7198611497879028]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 29/86 [D loss: 0.6836846470832825, acc.: 57.18%] [G loss: 0.7212649583816528]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 30/86 [D loss: 0.6847482621669769, acc.: 55.57%] [G loss: 0.7224164605140686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 31/86 [D loss: 0.6848920583724976, acc.: 56.05%] [G loss: 0.7163928747177124]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 32/86 [D loss: 0.6839024722576141, acc.: 57.28%] [G loss: 0.7191205620765686]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 33/86 [D loss: 0.6847053468227386, acc.: 56.54%] [G loss: 0.7201540470123291]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 34/86 [D loss: 0.683368593454361, acc.: 57.13%] [G loss: 0.7222914099693298]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 35/86 [D loss: 0.6850629448890686, acc.: 56.35%] [G loss: 0.7187367081642151]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 36/86 [D loss: 0.6840912997722626, acc.: 57.13%] [G loss: 0.7214377522468567]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 37/86 [D loss: 0.6829854846000671, acc.: 57.32%] [G loss: 0.7230944037437439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 38/86 [D loss: 0.6835969984531403, acc.: 56.45%] [G loss: 0.7205327153205872]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 39/86 [D loss: 0.6861176788806915, acc.: 55.13%] [G loss: 0.7209272384643555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 40/86 [D loss: 0.6840126514434814, acc.: 56.59%] [G loss: 0.7217373847961426]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 41/86 [D loss: 0.685202032327652, acc.: 56.25%] [G loss: 0.7206992506980896]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 42/86 [D loss: 0.684166669845581, acc.: 57.52%] [G loss: 0.7197352647781372]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 43/86 [D loss: 0.6861574053764343, acc.: 55.57%] [G loss: 0.7216562032699585]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 44/86 [D loss: 0.6857176423072815, acc.: 56.59%] [G loss: 0.7234503030776978]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 45/86 [D loss: 0.6836108267307281, acc.: 57.23%] [G loss: 0.7200074195861816]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 46/86 [D loss: 0.6842705607414246, acc.: 55.47%] [G loss: 0.7191265821456909]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 47/86 [D loss: 0.6852038204669952, acc.: 56.54%] [G loss: 0.7181144952774048]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 48/86 [D loss: 0.68450927734375, acc.: 56.40%] [G loss: 0.7147393226623535]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 49/86 [D loss: 0.6860097050666809, acc.: 56.74%] [G loss: 0.7191454768180847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 50/86 [D loss: 0.6831881701946259, acc.: 58.45%] [G loss: 0.7186875343322754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 51/86 [D loss: 0.6872170865535736, acc.: 54.69%] [G loss: 0.7164811491966248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 52/86 [D loss: 0.685410350561142, acc.: 57.42%] [G loss: 0.7169952392578125]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 53/86 [D loss: 0.6864527463912964, acc.: 55.37%] [G loss: 0.7169085741043091]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 54/86 [D loss: 0.6891171932220459, acc.: 52.20%] [G loss: 0.7225087285041809]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 55/86 [D loss: 0.6851552426815033, acc.: 56.64%] [G loss: 0.7179688215255737]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 89/200, Batch 56/86 [D loss: 0.6875743269920349, acc.: 53.27%] [G loss: 0.7192363142967224]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 57/86 [D loss: 0.6871316134929657, acc.: 53.32%] [G loss: 0.7210423350334167]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 58/86 [D loss: 0.6849312484264374, acc.: 56.64%] [G loss: 0.7131515145301819]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 59/86 [D loss: 0.6867561042308807, acc.: 55.03%] [G loss: 0.729775071144104]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 60/86 [D loss: 0.6831028461456299, acc.: 58.15%] [G loss: 0.7143965363502502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 61/86 [D loss: 0.6908035278320312, acc.: 51.12%] [G loss: 0.7178734540939331]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 62/86 [D loss: 0.6830517053604126, acc.: 57.32%] [G loss: 0.7235561013221741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 63/86 [D loss: 0.6879257261753082, acc.: 54.00%] [G loss: 0.7156990766525269]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 64/86 [D loss: 0.6868237555027008, acc.: 56.49%] [G loss: 0.7192403674125671]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 65/86 [D loss: 0.6864686906337738, acc.: 54.88%] [G loss: 0.7104660272598267]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 66/86 [D loss: 0.687054306268692, acc.: 53.32%] [G loss: 0.7176233530044556]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 67/86 [D loss: 0.6833194494247437, acc.: 55.66%] [G loss: 0.7181943655014038]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 68/86 [D loss: 0.6920419335365295, acc.: 50.49%] [G loss: 0.7134345173835754]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 69/86 [D loss: 0.6829044818878174, acc.: 57.42%] [G loss: 0.7212063670158386]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 70/86 [D loss: 0.6908370852470398, acc.: 52.49%] [G loss: 0.7082288861274719]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 71/86 [D loss: 0.6877486109733582, acc.: 53.96%] [G loss: 0.7210730314254761]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 72/86 [D loss: 0.6827508211135864, acc.: 58.20%] [G loss: 0.7069578170776367]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 73/86 [D loss: 0.6974168419837952, acc.: 46.88%] [G loss: 0.7230836749076843]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 74/86 [D loss: 0.6776339411735535, acc.: 61.67%] [G loss: 0.7089417576789856]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 75/86 [D loss: 0.6933011710643768, acc.: 50.29%] [G loss: 0.7072524428367615]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 76/86 [D loss: 0.684913694858551, acc.: 56.30%] [G loss: 0.7226187586784363]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 77/86 [D loss: 0.6806725561618805, acc.: 59.13%] [G loss: 0.7128127813339233]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 78/86 [D loss: 0.6925521194934845, acc.: 51.03%] [G loss: 0.7179960608482361]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 79/86 [D loss: 0.6815516650676727, acc.: 59.52%] [G loss: 0.7154273986816406]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 80/86 [D loss: 0.6935765445232391, acc.: 49.61%] [G loss: 0.7057084441184998]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 81/86 [D loss: 0.6872190535068512, acc.: 54.20%] [G loss: 0.7233432531356812]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 82/86 [D loss: 0.6818543374538422, acc.: 58.94%] [G loss: 0.7164496779441833]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 83/86 [D loss: 0.6945282518863678, acc.: 50.05%] [G loss: 0.7183557748794556]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 84/86 [D loss: 0.6833871603012085, acc.: 57.62%] [G loss: 0.7176403999328613]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 89/200, Batch 85/86 [D loss: 0.6876545548439026, acc.: 54.49%] [G loss: 0.7075817584991455]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 89/200, Batch 86/86 [D loss: 0.6859908103942871, acc.: 55.13%] [G loss: 0.7222389578819275]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 1/86 [D loss: 0.6804295480251312, acc.: 60.06%] [G loss: 0.7164947390556335]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 2/86 [D loss: 0.6937871873378754, acc.: 50.29%] [G loss: 0.7113621234893799]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 3/86 [D loss: 0.6845851838588715, acc.: 56.64%] [G loss: 0.7239494323730469]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 4/86 [D loss: 0.6865725517272949, acc.: 54.54%] [G loss: 0.712431788444519]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 5/86 [D loss: 0.6870361268520355, acc.: 54.98%] [G loss: 0.7236875295639038]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 6/86 [D loss: 0.6825718581676483, acc.: 57.18%] [G loss: 0.7208111882209778]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 7/86 [D loss: 0.6891893744468689, acc.: 53.91%] [G loss: 0.7155154347419739]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 8/86 [D loss: 0.6851939260959625, acc.: 57.47%] [G loss: 0.7223485708236694]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 9/86 [D loss: 0.6870477795600891, acc.: 55.37%] [G loss: 0.7178294658660889]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 10/86 [D loss: 0.6865282654762268, acc.: 55.86%] [G loss: 0.7200380563735962]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 11/86 [D loss: 0.6828044652938843, acc.: 58.20%] [G loss: 0.7182617783546448]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 12/86 [D loss: 0.6852232813835144, acc.: 56.64%] [G loss: 0.7176653742790222]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 13/86 [D loss: 0.683679610490799, acc.: 57.13%] [G loss: 0.7230827212333679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 14/86 [D loss: 0.6831922233104706, acc.: 57.76%] [G loss: 0.7195096015930176]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 15/86 [D loss: 0.6850430965423584, acc.: 57.23%] [G loss: 0.7193264961242676]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 16/86 [D loss: 0.6857340633869171, acc.: 55.08%] [G loss: 0.7211817502975464]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 17/86 [D loss: 0.6835631728172302, acc.: 58.11%] [G loss: 0.719435453414917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 18/86 [D loss: 0.6839604675769806, acc.: 57.32%] [G loss: 0.719244122505188]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 19/86 [D loss: 0.683464765548706, acc.: 56.20%] [G loss: 0.7198307514190674]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 20/86 [D loss: 0.6848711967468262, acc.: 56.84%] [G loss: 0.716484546661377]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 21/86 [D loss: 0.6848291754722595, acc.: 56.79%] [G loss: 0.7182137966156006]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 22/86 [D loss: 0.6829643845558167, acc.: 57.47%] [G loss: 0.718959629535675]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 23/86 [D loss: 0.6862240135669708, acc.: 54.74%] [G loss: 0.717285692691803]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 24/86 [D loss: 0.683736652135849, acc.: 57.62%] [G loss: 0.7185449600219727]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 25/86 [D loss: 0.6842943131923676, acc.: 56.74%] [G loss: 0.7179017066955566]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 26/86 [D loss: 0.6848699450492859, acc.: 55.91%] [G loss: 0.7168382406234741]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 27/86 [D loss: 0.6831566691398621, acc.: 57.08%] [G loss: 0.7216616272926331]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 28/86 [D loss: 0.68595951795578, acc.: 55.81%] [G loss: 0.7185649275779724]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 29/86 [D loss: 0.6858092546463013, acc.: 55.91%] [G loss: 0.7198606729507446]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 30/86 [D loss: 0.684680849313736, acc.: 56.05%] [G loss: 0.721893310546875]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 31/86 [D loss: 0.6839242577552795, acc.: 55.52%] [G loss: 0.7194260954856873]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 32/86 [D loss: 0.683927059173584, acc.: 56.49%] [G loss: 0.7228960990905762]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 33/86 [D loss: 0.6845141649246216, acc.: 55.81%] [G loss: 0.7196855545043945]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 34/86 [D loss: 0.6861604154109955, acc.: 55.22%] [G loss: 0.7227113842964172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 35/86 [D loss: 0.6805345416069031, acc.: 59.47%] [G loss: 0.7219910025596619]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 36/86 [D loss: 0.6849022805690765, acc.: 55.57%] [G loss: 0.7208254933357239]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 37/86 [D loss: 0.682155042886734, acc.: 56.35%] [G loss: 0.7225752472877502]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 38/86 [D loss: 0.6837754845619202, acc.: 56.40%] [G loss: 0.7198749780654907]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 39/86 [D loss: 0.68514284491539, acc.: 55.71%] [G loss: 0.7233454585075378]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 40/86 [D loss: 0.6823360323905945, acc.: 58.25%] [G loss: 0.7218483686447144]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 41/86 [D loss: 0.6848657429218292, acc.: 55.91%] [G loss: 0.7216596603393555]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 42/86 [D loss: 0.6852456629276276, acc.: 56.54%] [G loss: 0.7259819507598877]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 43/86 [D loss: 0.6833820641040802, acc.: 58.25%] [G loss: 0.7207356095314026]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 44/86 [D loss: 0.6845496594905853, acc.: 56.69%] [G loss: 0.7244364023208618]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 45/86 [D loss: 0.682762861251831, acc.: 57.57%] [G loss: 0.7224700450897217]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 46/86 [D loss: 0.6853445172309875, acc.: 56.20%] [G loss: 0.7183368802070618]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 47/86 [D loss: 0.6842368841171265, acc.: 56.59%] [G loss: 0.7195793390274048]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 48/86 [D loss: 0.6835885047912598, acc.: 57.32%] [G loss: 0.719774603843689]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 49/86 [D loss: 0.6865753531455994, acc.: 55.32%] [G loss: 0.7230436205863953]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 50/86 [D loss: 0.6808931231498718, acc.: 59.77%] [G loss: 0.7194386720657349]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 51/86 [D loss: 0.6869516670703888, acc.: 55.81%] [G loss: 0.7190134525299072]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 52/86 [D loss: 0.6842465996742249, acc.: 56.98%] [G loss: 0.721879780292511]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 53/86 [D loss: 0.6835476756095886, acc.: 56.93%] [G loss: 0.7151408195495605]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 54/86 [D loss: 0.6882710456848145, acc.: 54.05%] [G loss: 0.7224420309066772]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 55/86 [D loss: 0.681349366903305, acc.: 59.33%] [G loss: 0.7165042757987976]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 56/86 [D loss: 0.688001960515976, acc.: 54.30%] [G loss: 0.7153845429420471]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 57/86 [D loss: 0.685234397649765, acc.: 56.45%] [G loss: 0.7232438325881958]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 58/86 [D loss: 0.6828030347824097, acc.: 57.91%] [G loss: 0.7126122117042542]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 59/86 [D loss: 0.6888402700424194, acc.: 53.32%] [G loss: 0.7234945297241211]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 60/86 [D loss: 0.6813571155071259, acc.: 59.18%] [G loss: 0.7146802544593811]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 61/86 [D loss: 0.6909540295600891, acc.: 52.10%] [G loss: 0.7118039131164551]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 62/86 [D loss: 0.6853339970111847, acc.: 54.25%] [G loss: 0.7261251211166382]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 63/86 [D loss: 0.6817239224910736, acc.: 58.40%] [G loss: 0.7072084546089172]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 64/86 [D loss: 0.6944268643856049, acc.: 49.32%] [G loss: 0.7209486961364746]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 65/86 [D loss: 0.6780765056610107, acc.: 60.21%] [G loss: 0.7175098061561584]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 66/86 [D loss: 0.6918087601661682, acc.: 50.73%] [G loss: 0.7107307314872742]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 67/86 [D loss: 0.6874698400497437, acc.: 53.03%] [G loss: 0.7248586416244507]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 68/86 [D loss: 0.6803810298442841, acc.: 60.25%] [G loss: 0.711902916431427]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 69/86 [D loss: 0.6955114603042603, acc.: 48.58%] [G loss: 0.7191014289855957]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 70/86 [D loss: 0.6785595417022705, acc.: 60.45%] [G loss: 0.7192803621292114]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 71/86 [D loss: 0.6923581659793854, acc.: 51.51%] [G loss: 0.7062789797782898]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 72/86 [D loss: 0.6865251958370209, acc.: 54.98%] [G loss: 0.7214820384979248]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 73/86 [D loss: 0.6798099875450134, acc.: 59.86%] [G loss: 0.711250364780426]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 74/86 [D loss: 0.6926800906658173, acc.: 50.39%] [G loss: 0.7103548049926758]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 75/86 [D loss: 0.6845867931842804, acc.: 55.57%] [G loss: 0.7224367260932922]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 76/86 [D loss: 0.686305969953537, acc.: 54.35%] [G loss: 0.7063672542572021]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 77/86 [D loss: 0.6867939829826355, acc.: 53.56%] [G loss: 0.719103991985321]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 78/86 [D loss: 0.6802632212638855, acc.: 59.08%] [G loss: 0.7162012457847595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 79/86 [D loss: 0.6908330917358398, acc.: 51.61%] [G loss: 0.7116536498069763]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 80/86 [D loss: 0.6852012872695923, acc.: 56.01%] [G loss: 0.7219460010528564]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 81/86 [D loss: 0.6861835718154907, acc.: 54.93%] [G loss: 0.7130048871040344]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 82/86 [D loss: 0.6846969127655029, acc.: 55.37%] [G loss: 0.7193500399589539]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 83/86 [D loss: 0.6847727596759796, acc.: 57.03%] [G loss: 0.7198814153671265]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 90/200, Batch 84/86 [D loss: 0.683834046125412, acc.: 58.30%] [G loss: 0.7155779004096985]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 85/86 [D loss: 0.6845309436321259, acc.: 56.93%] [G loss: 0.72044438123703]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 90/200, Batch 86/86 [D loss: 0.6855500638484955, acc.: 56.25%] [G loss: 0.7199567556381226]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 1/86 [D loss: 0.6858889162540436, acc.: 53.66%] [G loss: 0.7193899154663086]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 2/86 [D loss: 0.6832405924797058, acc.: 58.74%] [G loss: 0.721463680267334]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 3/86 [D loss: 0.6830960512161255, acc.: 57.28%] [G loss: 0.720268964767456]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 4/86 [D loss: 0.6853133141994476, acc.: 56.54%] [G loss: 0.7222661972045898]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 5/86 [D loss: 0.6836768686771393, acc.: 56.93%] [G loss: 0.722073495388031]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 6/86 [D loss: 0.6837745904922485, acc.: 57.13%] [G loss: 0.7209079265594482]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 7/86 [D loss: 0.6845177710056305, acc.: 56.74%] [G loss: 0.7224432229995728]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 8/86 [D loss: 0.6859158575534821, acc.: 55.13%] [G loss: 0.7233593463897705]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 9/86 [D loss: 0.6835964918136597, acc.: 57.67%] [G loss: 0.7196148633956909]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 10/86 [D loss: 0.6863492131233215, acc.: 54.64%] [G loss: 0.7236067652702332]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 11/86 [D loss: 0.6836029291152954, acc.: 58.06%] [G loss: 0.718981146812439]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 12/86 [D loss: 0.683456301689148, acc.: 57.76%] [G loss: 0.7180370092391968]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 13/86 [D loss: 0.6842707693576813, acc.: 56.05%] [G loss: 0.7218316197395325]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 14/86 [D loss: 0.684583306312561, acc.: 57.08%] [G loss: 0.7219036817550659]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 15/86 [D loss: 0.6834483742713928, acc.: 57.37%] [G loss: 0.7199723720550537]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 16/86 [D loss: 0.683939516544342, acc.: 57.13%] [G loss: 0.7209844589233398]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 17/86 [D loss: 0.6841844320297241, acc.: 56.10%] [G loss: 0.7199172377586365]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 18/86 [D loss: 0.6860533952713013, acc.: 54.98%] [G loss: 0.7218335866928101]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 19/86 [D loss: 0.6854982376098633, acc.: 55.71%] [G loss: 0.7224606871604919]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 20/86 [D loss: 0.6823138296604156, acc.: 57.96%] [G loss: 0.7219873070716858]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 21/86 [D loss: 0.68678417801857, acc.: 55.27%] [G loss: 0.7219403386116028]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 22/86 [D loss: 0.683307558298111, acc.: 55.42%] [G loss: 0.7209228277206421]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 23/86 [D loss: 0.6851764023303986, acc.: 55.47%] [G loss: 0.7219065427780151]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 24/86 [D loss: 0.686182290315628, acc.: 55.13%] [G loss: 0.7214829921722412]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 25/86 [D loss: 0.6821798384189606, acc.: 58.01%] [G loss: 0.7215338945388794]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 26/86 [D loss: 0.6834012866020203, acc.: 55.96%] [G loss: 0.7202354669570923]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 27/86 [D loss: 0.6843386292457581, acc.: 57.47%] [G loss: 0.721872091293335]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 28/86 [D loss: 0.6847794950008392, acc.: 56.69%] [G loss: 0.7224853038787842]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 29/86 [D loss: 0.6840466856956482, acc.: 56.45%] [G loss: 0.7187056541442871]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 30/86 [D loss: 0.6839073598384857, acc.: 57.76%] [G loss: 0.7253092527389526]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 31/86 [D loss: 0.6833207607269287, acc.: 57.32%] [G loss: 0.7218008041381836]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 32/86 [D loss: 0.6845232248306274, acc.: 56.05%] [G loss: 0.7238866090774536]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 33/86 [D loss: 0.6842953562736511, acc.: 54.79%] [G loss: 0.7232812643051147]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 34/86 [D loss: 0.6855016052722931, acc.: 55.66%] [G loss: 0.7224678993225098]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 35/86 [D loss: 0.6858003437519073, acc.: 56.84%] [G loss: 0.7215385437011719]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 36/86 [D loss: 0.6822578608989716, acc.: 57.67%] [G loss: 0.7238099575042725]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 37/86 [D loss: 0.6846623718738556, acc.: 56.59%] [G loss: 0.7212758660316467]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 38/86 [D loss: 0.6833313703536987, acc.: 56.84%] [G loss: 0.7197780609130859]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 39/86 [D loss: 0.6829963624477386, acc.: 58.84%] [G loss: 0.720572292804718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 40/86 [D loss: 0.6859821975231171, acc.: 54.88%] [G loss: 0.7230013012886047]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 41/86 [D loss: 0.683655709028244, acc.: 56.74%] [G loss: 0.7236653566360474]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 42/86 [D loss: 0.685517430305481, acc.: 57.08%] [G loss: 0.7237966060638428]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 43/86 [D loss: 0.6838991045951843, acc.: 56.79%] [G loss: 0.721406102180481]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 44/86 [D loss: 0.6826435923576355, acc.: 57.28%] [G loss: 0.7203165888786316]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 45/86 [D loss: 0.6860446035861969, acc.: 54.98%] [G loss: 0.7192214131355286]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 46/86 [D loss: 0.6843335032463074, acc.: 56.10%] [G loss: 0.7220455408096313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 47/86 [D loss: 0.6848027408123016, acc.: 56.20%] [G loss: 0.7172865867614746]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 48/86 [D loss: 0.6863469481468201, acc.: 55.47%] [G loss: 0.7191059589385986]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 49/86 [D loss: 0.6823140680789948, acc.: 57.08%] [G loss: 0.7172254323959351]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 50/86 [D loss: 0.6828398704528809, acc.: 57.52%] [G loss: 0.7216112017631531]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 51/86 [D loss: 0.6827050745487213, acc.: 58.59%] [G loss: 0.7232564687728882]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 52/86 [D loss: 0.6836057007312775, acc.: 56.64%] [G loss: 0.7175896763801575]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 53/86 [D loss: 0.6855462193489075, acc.: 55.81%] [G loss: 0.7206698060035706]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 54/86 [D loss: 0.6823307871818542, acc.: 58.20%] [G loss: 0.7214418053627014]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 55/86 [D loss: 0.685762345790863, acc.: 54.30%] [G loss: 0.7199174165725708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 56/86 [D loss: 0.6867771148681641, acc.: 55.81%] [G loss: 0.7248350381851196]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 57/86 [D loss: 0.6823663115501404, acc.: 57.23%] [G loss: 0.7232213020324707]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 58/86 [D loss: 0.6865279376506805, acc.: 53.81%] [G loss: 0.7162492871284485]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 59/86 [D loss: 0.6851297914981842, acc.: 57.03%] [G loss: 0.7217575907707214]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 60/86 [D loss: 0.6866052746772766, acc.: 54.49%] [G loss: 0.7166597247123718]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 61/86 [D loss: 0.686646580696106, acc.: 54.25%] [G loss: 0.7196893692016602]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 62/86 [D loss: 0.6829011738300323, acc.: 57.03%] [G loss: 0.7240700721740723]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 63/86 [D loss: 0.6835847496986389, acc.: 56.79%] [G loss: 0.7176862359046936]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 64/86 [D loss: 0.6827681958675385, acc.: 57.18%] [G loss: 0.7222407460212708]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 65/86 [D loss: 0.6833427846431732, acc.: 57.03%] [G loss: 0.7172849178314209]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 66/86 [D loss: 0.6870748698711395, acc.: 53.61%] [G loss: 0.7180742621421814]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 67/86 [D loss: 0.6860367953777313, acc.: 54.69%] [G loss: 0.7246674299240112]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 68/86 [D loss: 0.6849671006202698, acc.: 55.42%] [G loss: 0.7179158926010132]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 69/86 [D loss: 0.6867249608039856, acc.: 54.69%] [G loss: 0.7248840928077698]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 70/86 [D loss: 0.6830137073993683, acc.: 57.23%] [G loss: 0.7202613949775696]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 71/86 [D loss: 0.6887357234954834, acc.: 53.12%] [G loss: 0.7193028926849365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 72/86 [D loss: 0.6832334101200104, acc.: 57.81%] [G loss: 0.7266827821731567]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 73/86 [D loss: 0.6846257448196411, acc.: 55.66%] [G loss: 0.7154276371002197]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 74/86 [D loss: 0.6870520412921906, acc.: 54.20%] [G loss: 0.722220778465271]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 75/86 [D loss: 0.6806888580322266, acc.: 58.30%] [G loss: 0.7152841687202454]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 76/86 [D loss: 0.6867332756519318, acc.: 54.79%] [G loss: 0.7195463180541992]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 77/86 [D loss: 0.6871514320373535, acc.: 53.66%] [G loss: 0.726146936416626]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 78/86 [D loss: 0.6829160749912262, acc.: 57.91%] [G loss: 0.7177156209945679]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 79/86 [D loss: 0.6876972615718842, acc.: 53.96%] [G loss: 0.7270355820655823]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 80/86 [D loss: 0.6802845299243927, acc.: 58.98%] [G loss: 0.7204099893569946]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 81/86 [D loss: 0.690123587846756, acc.: 52.54%] [G loss: 0.7153226137161255]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 82/86 [D loss: 0.6842232942581177, acc.: 55.96%] [G loss: 0.7285006046295166]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 91/200, Batch 83/86 [D loss: 0.6836227178573608, acc.: 56.88%] [G loss: 0.7152283191680908]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 84/86 [D loss: 0.6914601624011993, acc.: 52.05%] [G loss: 0.7214502692222595]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 85/86 [D loss: 0.682541161775589, acc.: 57.23%] [G loss: 0.7211184501647949]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 91/200, Batch 86/86 [D loss: 0.6880137622356415, acc.: 53.71%] [G loss: 0.717096745967865]\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 1/86 [D loss: 0.6826935112476349, acc.: 56.84%] [G loss: 0.7241911292076111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 2/86 [D loss: 0.6819841861724854, acc.: 58.50%] [G loss: 0.7185742855072021]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 3/86 [D loss: 0.6908809244632721, acc.: 52.83%] [G loss: 0.7198052406311035]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 4/86 [D loss: 0.6825348734855652, acc.: 56.25%] [G loss: 0.7201687097549438]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 5/86 [D loss: 0.6850080788135529, acc.: 54.54%] [G loss: 0.7139267921447754]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 6/86 [D loss: 0.6868722438812256, acc.: 54.93%] [G loss: 0.7230145335197449]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 7/86 [D loss: 0.6800668835639954, acc.: 58.25%] [G loss: 0.719459593296051]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 8/86 [D loss: 0.687819093465805, acc.: 53.12%] [G loss: 0.7149602770805359]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 9/86 [D loss: 0.6840987503528595, acc.: 57.08%] [G loss: 0.7232935428619385]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 10/86 [D loss: 0.6835914850234985, acc.: 56.93%] [G loss: 0.7142548561096191]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 11/86 [D loss: 0.6862199902534485, acc.: 55.37%] [G loss: 0.7215908765792847]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 12/86 [D loss: 0.6819643378257751, acc.: 58.15%] [G loss: 0.7226715683937073]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 13/86 [D loss: 0.6878780424594879, acc.: 54.49%] [G loss: 0.7142794728279114]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 14/86 [D loss: 0.6886590719223022, acc.: 53.22%] [G loss: 0.7273436784744263]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 92/200, Batch 15/86 [D loss: 0.6816459596157074, acc.: 59.08%] [G loss: 0.7204233407974243]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 16/86 [D loss: 0.6857273280620575, acc.: 55.66%] [G loss: 0.7201573848724365]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 17/86 [D loss: 0.6836503446102142, acc.: 57.32%] [G loss: 0.7253549098968506]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 18/86 [D loss: 0.6819893717765808, acc.: 58.89%] [G loss: 0.7205225825309753]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 19/86 [D loss: 0.6892763674259186, acc.: 53.08%] [G loss: 0.722335934638977]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 20/86 [D loss: 0.6828226447105408, acc.: 57.86%] [G loss: 0.7236777544021606]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 21/86 [D loss: 0.6853106021881104, acc.: 55.57%] [G loss: 0.7195004820823669]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 22/86 [D loss: 0.6850422024726868, acc.: 55.18%] [G loss: 0.7230419516563416]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 23/86 [D loss: 0.6802711188793182, acc.: 58.79%] [G loss: 0.7240858674049377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 24/86 [D loss: 0.6896642446517944, acc.: 52.83%] [G loss: 0.7194882035255432]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 25/86 [D loss: 0.6818522214889526, acc.: 58.50%] [G loss: 0.7262383103370667]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 26/86 [D loss: 0.6845089197158813, acc.: 56.10%] [G loss: 0.7180730104446411]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 27/86 [D loss: 0.6848811507225037, acc.: 56.15%] [G loss: 0.7235768437385559]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 28/86 [D loss: 0.6828444302082062, acc.: 58.94%] [G loss: 0.7250527739524841]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 29/86 [D loss: 0.6863011419773102, acc.: 54.69%] [G loss: 0.7226450443267822]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 30/86 [D loss: 0.68406081199646, acc.: 56.35%] [G loss: 0.7265073657035828]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 31/86 [D loss: 0.6849468052387238, acc.: 55.66%] [G loss: 0.7189421653747559]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 32/86 [D loss: 0.6856925189495087, acc.: 55.42%] [G loss: 0.7252465486526489]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 33/86 [D loss: 0.682910829782486, acc.: 57.13%] [G loss: 0.7216789722442627]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 34/86 [D loss: 0.6848742365837097, acc.: 55.86%] [G loss: 0.7218923568725586]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 35/86 [D loss: 0.6860924661159515, acc.: 55.91%] [G loss: 0.722703754901886]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 36/86 [D loss: 0.6824641227722168, acc.: 56.59%] [G loss: 0.7256443500518799]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 37/86 [D loss: 0.6840980052947998, acc.: 55.52%] [G loss: 0.726454496383667]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 38/86 [D loss: 0.6821835339069366, acc.: 57.96%] [G loss: 0.7244560718536377]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 39/86 [D loss: 0.6829414963722229, acc.: 57.57%] [G loss: 0.7200192213058472]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 40/86 [D loss: 0.685307115316391, acc.: 56.59%] [G loss: 0.7233942747116089]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 41/86 [D loss: 0.6839154958724976, acc.: 57.03%] [G loss: 0.7225983142852783]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 42/86 [D loss: 0.6854429841041565, acc.: 54.79%] [G loss: 0.7224697470664978]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 43/86 [D loss: 0.6836880743503571, acc.: 56.05%] [G loss: 0.7196024060249329]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 44/86 [D loss: 0.6810154020786285, acc.: 57.91%] [G loss: 0.72209233045578]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 45/86 [D loss: 0.6861453056335449, acc.: 54.64%] [G loss: 0.7203740477561951]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 46/86 [D loss: 0.6832273006439209, acc.: 57.23%] [G loss: 0.7231022715568542]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 47/86 [D loss: 0.683748722076416, acc.: 55.81%] [G loss: 0.7199996709823608]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 48/86 [D loss: 0.684422492980957, acc.: 56.10%] [G loss: 0.7207088470458984]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 49/86 [D loss: 0.681763082742691, acc.: 58.98%] [G loss: 0.7219852805137634]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 50/86 [D loss: 0.6846479177474976, acc.: 56.10%] [G loss: 0.7222400903701782]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 51/86 [D loss: 0.6841985881328583, acc.: 56.20%] [G loss: 0.7214437127113342]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 52/86 [D loss: 0.682377964258194, acc.: 59.33%] [G loss: 0.7210205793380737]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 53/86 [D loss: 0.6837868690490723, acc.: 56.25%] [G loss: 0.7221947312355042]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 54/86 [D loss: 0.6821650266647339, acc.: 56.30%] [G loss: 0.7199394702911377]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 55/86 [D loss: 0.6844869256019592, acc.: 56.30%] [G loss: 0.720285177230835]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 56/86 [D loss: 0.6842409372329712, acc.: 55.96%] [G loss: 0.7203571200370789]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 57/86 [D loss: 0.6823097467422485, acc.: 58.20%] [G loss: 0.7210385203361511]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 58/86 [D loss: 0.6841664910316467, acc.: 56.64%] [G loss: 0.7242385149002075]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 59/86 [D loss: 0.6825290024280548, acc.: 57.71%] [G loss: 0.725834846496582]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 60/86 [D loss: 0.6826176643371582, acc.: 57.28%] [G loss: 0.720806360244751]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 61/86 [D loss: 0.6859007775783539, acc.: 55.52%] [G loss: 0.7232505083084106]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 62/86 [D loss: 0.6834122538566589, acc.: 56.45%] [G loss: 0.7192243933677673]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 63/86 [D loss: 0.6852043569087982, acc.: 55.18%] [G loss: 0.7222613096237183]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 64/86 [D loss: 0.6822943687438965, acc.: 58.15%] [G loss: 0.7210077047348022]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 65/86 [D loss: 0.6836352944374084, acc.: 57.18%] [G loss: 0.7181656360626221]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 66/86 [D loss: 0.6857621967792511, acc.: 54.54%] [G loss: 0.7209955453872681]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 67/86 [D loss: 0.6838731169700623, acc.: 56.35%] [G loss: 0.7241929769515991]\n",
      "32/32 [==============================] - 0s 14ms/step\n",
      "Epoch 92/200, Batch 68/86 [D loss: 0.684017539024353, acc.: 56.74%] [G loss: 0.720148503780365]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 69/86 [D loss: 0.6856274902820587, acc.: 54.59%] [G loss: 0.7230730652809143]\n",
      "32/32 [==============================] - 0s 13ms/step\n",
      "Epoch 92/200, Batch 70/86 [D loss: 0.6837368905544281, acc.: 56.93%] [G loss: 0.7250946164131165]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 71/86 [D loss: 0.6848764717578888, acc.: 55.42%] [G loss: 0.723613977432251]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 72/86 [D loss: 0.6837178468704224, acc.: 57.23%] [G loss: 0.7225162386894226]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 73/86 [D loss: 0.6842981278896332, acc.: 57.57%] [G loss: 0.7192355990409851]\n",
      "32/32 [==============================] - 0s 12ms/step\n",
      "Epoch 92/200, Batch 74/86 [D loss: 0.6854965686798096, acc.: 53.81%] [G loss: 0.723610520362854]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 75/86 [D loss: 0.6828937828540802, acc.: 57.37%] [G loss: 0.723456084728241]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 76/86 [D loss: 0.685806542634964, acc.: 55.66%] [G loss: 0.7175219058990479]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 77/86 [D loss: 0.6861549913883209, acc.: 56.40%] [G loss: 0.7256072163581848]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 78/86 [D loss: 0.6823281943798065, acc.: 57.76%] [G loss: 0.7202218770980835]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 79/86 [D loss: 0.6865347921848297, acc.: 53.61%] [G loss: 0.7218965291976929]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 80/86 [D loss: 0.6835323572158813, acc.: 57.13%] [G loss: 0.7236652374267578]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 81/86 [D loss: 0.6848816275596619, acc.: 55.22%] [G loss: 0.7183371186256409]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 82/86 [D loss: 0.6858214139938354, acc.: 56.59%] [G loss: 0.7249992489814758]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 83/86 [D loss: 0.6807776987552643, acc.: 58.59%] [G loss: 0.7234275341033936]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 92/200, Batch 84/86 [D loss: 0.6856541037559509, acc.: 54.93%] [G loss: 0.7201686501502991]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 85/86 [D loss: 0.6838648617267609, acc.: 56.45%] [G loss: 0.726333498954773]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 92/200, Batch 86/86 [D loss: 0.6846025586128235, acc.: 55.91%] [G loss: 0.7187730669975281]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 1/86 [D loss: 0.68614262342453, acc.: 55.37%] [G loss: 0.7274912595748901]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 93/200, Batch 2/86 [D loss: 0.6821997165679932, acc.: 58.69%] [G loss: 0.7193819284439087]\n",
      "32/32 [==============================] - 0s 10ms/step\n",
      "Epoch 93/200, Batch 3/86 [D loss: 0.6885183155536652, acc.: 53.71%] [G loss: 0.7184445261955261]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 93/200, Batch 4/86 [D loss: 0.6852069199085236, acc.: 55.47%] [G loss: 0.7243431806564331]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 93/200, Batch 5/86 [D loss: 0.6826633214950562, acc.: 57.67%] [G loss: 0.7189701795578003]\n",
      "32/32 [==============================] - 0s 11ms/step\n",
      "Epoch 93/200, Batch 6/86 [D loss: 0.6877978146076202, acc.: 55.66%] [G loss: 0.7231931686401367]\n",
      "32/32 [==============================] - 2s 62ms/step\n",
      "Epoch 93/200, Batch 7/86 [D loss: 0.6828177571296692, acc.: 56.98%] [G loss: 0.7208967208862305]\n",
      "32/32 [==============================] - 2s 63ms/step\n",
      "Epoch 93/200, Batch 8/86 [D loss: 0.685038149356842, acc.: 55.37%] [G loss: 0.7200210690498352]\n",
      "32/32 [==============================] - 2s 60ms/step\n",
      "Epoch 93/200, Batch 9/86 [D loss: 0.6836909353733063, acc.: 56.88%] [G loss: 0.7253385782241821]\n",
      "32/32 [==============================] - 2s 61ms/step\n",
      "Epoch 93/200, Batch 10/86 [D loss: 0.6840038001537323, acc.: 56.84%] [G loss: 0.7219340801239014]\n",
      "32/32 [==============================] - 2s 58ms/step\n",
      "Epoch 93/200, Batch 11/86 [D loss: 0.6834175288677216, acc.: 57.62%] [G loss: 0.7236998081207275]\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "Epoch 93/200, Batch 12/86 [D loss: 0.6827953457832336, acc.: 58.15%] [G loss: 0.7203148603439331]\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "Epoch 93/200, Batch 13/86 [D loss: 0.684017688035965, acc.: 55.81%] [G loss: 0.7224608063697815]\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "Epoch 93/200, Batch 14/86 [D loss: 0.6845234334468842, acc.: 55.03%] [G loss: 0.7253027558326721]\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "Epoch 93/200, Batch 15/86 [D loss: 0.683801531791687, acc.: 56.25%] [G loss: 0.7211416959762573]\n",
      "32/32 [==============================] - 2s 59ms/step\n",
      "Epoch 93/200, Batch 16/86 [D loss: 0.6857593655586243, acc.: 54.98%] [G loss: 0.7234910726547241]\n",
      "32/32 [==============================] - 2s 60ms/step\n",
      "Epoch 93/200, Batch 17/86 [D loss: 0.6843360662460327, acc.: 56.74%] [G loss: 0.7249844074249268]\n",
      "32/32 [==============================] - 1s 39ms/step\n",
      "Epoch 93/200, Batch 18/86 [D loss: 0.6823195815086365, acc.: 57.32%] [G loss: 0.7192556858062744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 19/86 [D loss: 0.6830719709396362, acc.: 56.35%] [G loss: 0.7306292653083801]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 20/86 [D loss: 0.6817585229873657, acc.: 58.50%] [G loss: 0.7217010855674744]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 21/86 [D loss: 0.6844930946826935, acc.: 55.86%] [G loss: 0.7207055687904358]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 22/86 [D loss: 0.6841281354427338, acc.: 57.08%] [G loss: 0.7233366370201111]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 23/86 [D loss: 0.6860218644142151, acc.: 56.05%] [G loss: 0.7188225388526917]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 24/86 [D loss: 0.6827009916305542, acc.: 56.69%] [G loss: 0.7226558923721313]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 25/86 [D loss: 0.6833316087722778, acc.: 56.69%] [G loss: 0.7201458811759949]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 26/86 [D loss: 0.6861290633678436, acc.: 54.74%] [G loss: 0.722712516784668]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 27/86 [D loss: 0.6843459904193878, acc.: 56.20%] [G loss: 0.7230088114738464]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 28/86 [D loss: 0.6843936443328857, acc.: 55.62%] [G loss: 0.7243177890777588]\n",
      "32/32 [==============================] - 0s 9ms/step\n",
      "Epoch 93/200, Batch 29/86 [D loss: 0.6844875812530518, acc.: 57.03%] [G loss: 0.725267231464386]\n",
      "32/32 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "cgan = CGAN(img_rows, img_cols, channels)\n",
    "cgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:    \n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "            self.img_rows = rows\n",
    "            self.img_cols = cols\n",
    "            self.channels = channels\n",
    "            self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "            self.latent_dim = z\n",
    "            self.num_classes = num_classes\n",
    "            optimizer = Adam(0.00002, 0.5)\n",
    "            self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "            self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "            z = Input(shape=(self.latent_dim,))\n",
    "            label = Input(shape=(1,))\n",
    "            img = self.generator([z, label])\n",
    "            self.discriminator.trainable = False\n",
    "            valid = self.discriminator([img, label])\n",
    "            self.combined = Model([z, label], valid)\n",
    "            self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    # define the standalone discriminator model\n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # image input\n",
    "        in_image = Input(shape=in_shape)\n",
    "        # downsample to 14x14\n",
    "        fe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        # normal\n",
    "        fe = Conv2D(64, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        # downsample to 7x7\n",
    "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        # normal\n",
    "        fe = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        # flatten feature maps\n",
    "        fe = Flatten()(fe)\n",
    "        # real/fake output\n",
    "        out1 = Dense(1, activation='sigmoid')(fe)\n",
    "        # class label output\n",
    "        out2 = Dense(n_classes, activation='softmax')(fe)\n",
    "        # define model\n",
    "        model = Model(in_image, [out1, out2])\n",
    "        # compile model\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    # define the standalone generator model\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # label input\n",
    "        in_label = Input(shape=(1,))\n",
    "        # embedding for categorical input\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        # linear multiplication\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "        # reshape to additional channel\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        # image generator input\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        # foundation for 7x7 image\n",
    "        n_nodes = 384 * 7 * 7\n",
    "        gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Reshape((7, 7, 384))(gen)\n",
    "        # merge image gen and label input\n",
    "        merge = Concatenate()([gen, li])\n",
    "        # upsample to 14x14\n",
    "        gen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "        gen = BatchNormalization()(gen)\n",
    "        gen = Activation('relu')(gen)\n",
    "        # upsample to 28x28\n",
    "        gen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "        out_layer = Activation('tanh')(gen)\n",
    "        # define model\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes  # Ensure labels are within valid range\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].set_title(chr(sampled_labels[cnt][0] + 65))  # Convert labels to characters (A-Z)\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"ACGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('ACGAN_mnist', exist_ok=True)\n",
    "        fig.savefig(\"ACGAN_mnist/ACGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=1024, save_interval=1, gen_steps=1):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                # Select a random batch of real images and labels\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels = y_train[idx]\n",
    "                valid = np.ones((batch_size, 1))\n",
    "\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                fake = np.zeros((batch_size, 1))\n",
    "\n",
    "                # Train the discriminator (real images)\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, labels], [valid, labels])\n",
    "                # Train the discriminator (fake images)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, gen_labels], [fake, gen_labels])\n",
    "                # Calculate the average discriminator loss\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([noise, gen_labels], valid_y)\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "            if (epoch + 1) % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 123ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_6\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(1024, 28, 28, 1) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(1024,) dtype=int64>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate and train the DCGAN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m acgan \u001b[38;5;241m=\u001b[39m ACGAN(img_rows, img_cols, channels)\n\u001b[1;32m----> 6\u001b[0m \u001b[43macgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 129\u001b[0m, in \u001b[0;36mACGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval, gen_steps)\u001b[0m\n\u001b[0;32m    126\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mpredict([noise, gen_labels])\n\u001b[0;32m    127\u001b[0m labels_fake \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Fake labels\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch([gen_imgs, gen_labels], labels_fake)\n\u001b[0;32m    131\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39madd(d_loss_real, d_loss_fake)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2381\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2379\u001b[0m     )\n\u001b[0;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2381\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2383\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filef6elets5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:1146\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m   1143\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[0;32m   1145\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1146\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1148\u001b[0m     outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m )\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:1135\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1135\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:993\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# Run forward pass.\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m--> 993\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    994\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\input_spec.py:216\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs to a layer should be tensors. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_6\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(1024, 28, 28, 1) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(1024,) dtype=int64>]\n"
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "acgan = ACGAN(img_rows, img_cols, channels)\n",
    "acgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
