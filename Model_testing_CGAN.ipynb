{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, Conv2DTranspose, PReLU\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Concatenate\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, HTML\n",
    "import glob\n",
    "from keras.layers import AveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Add\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import Input\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install scikit-image\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Reshape, UpSampling2D, \\\n",
    "    BatchNormalization, Activation, Input, LeakyReLU, ZeroPadding2D, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "#import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('emnist-letters-train.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {1: 0, \n",
    "           2: 1, \n",
    "           3: 2, \n",
    "           4: 3, \n",
    "           5: 4, \n",
    "           6: 5, \n",
    "           7: 6, \n",
    "           8: 7, \n",
    "           9: 8, \n",
    "           10: 9, \n",
    "           11: 10, \n",
    "           12: 11, \n",
    "           13: 12, \n",
    "           14: 13, \n",
    "           15: 14, \n",
    "           16: 15, \n",
    "           17: 16, \n",
    "           18: 17, \n",
    "           19: 18, \n",
    "           20: 19, \n",
    "           21: 20, \n",
    "           22: 21, \n",
    "           23: 22, \n",
    "           24: 23, \n",
    "           25: 24, \n",
    "           26: 25, \n",
    "           27: 26}\n",
    "\n",
    "        # Map the labels column to its corresponding value\n",
    "df[0] = df[0].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = np.array(df.iloc[:,0].values)\n",
    "y_pre = pd.Categorical(y_pre)\n",
    "X = np.array(df.iloc[:,1:].values)\n",
    "X = X.reshape(-1,28,28,1)\n",
    "preprocessed = []\n",
    "for image in X:\n",
    "    rotated_image = rotate(image, 90, reshape=False)\n",
    "    flipped_image = np.flipud(rotated_image)\n",
    "    preprocessed.append(flipped_image)\n",
    "X_pre = np.array(preprocessed)\n",
    "X = X_pre\n",
    "X = X.astype('float32')\n",
    "X_pre = (X - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = Model([z, label], valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = in_shape[0] * in_shape[1]\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        merge = Concatenate()([in_image, li])\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        fe = Dropout(0.4)(fe)\n",
    "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "        model = Model([in_image, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 128 * 7 * 7\n",
    "        gen = Dense(n_nodes)(in_lat)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Reshape((7, 7, 128))(gen) \n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(merge)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes  # Ensure labels are within valid range\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"CGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('CGAN_mnist', exist_ok=True)\n",
    "        fig.savefig(\"CGAN_mnist/CGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=1024, save_interval=1, gen_steps=1):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, y_train[idx]], labels_real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, gen_labels], labels_fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([noise, gen_labels], valid_y)\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "            if (epoch) % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 1/86 [D loss: 0.728632241487503, acc.: 5.81%] [G loss: 0.6920509934425354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 2/86 [D loss: 0.6823033094406128, acc.: 38.18%] [G loss: 0.690722644329071]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 3/86 [D loss: 0.6423549354076385, acc.: 49.95%] [G loss: 0.6874240636825562]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 4/86 [D loss: 0.6066926717758179, acc.: 50.00%] [G loss: 0.6789748668670654]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 5/86 [D loss: 0.5747033357620239, acc.: 50.00%] [G loss: 0.6618641018867493]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 6/86 [D loss: 0.5544271767139435, acc.: 50.00%] [G loss: 0.6316642165184021]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 7/86 [D loss: 0.551781177520752, acc.: 50.00%] [G loss: 0.5885984897613525]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 8/86 [D loss: 0.5753931254148483, acc.: 50.00%] [G loss: 0.5406744480133057]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 9/86 [D loss: 0.6124674752354622, acc.: 50.00%] [G loss: 0.5081720352172852]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 10/86 [D loss: 0.6311993598937988, acc.: 50.00%] [G loss: 0.5172932147979736]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 11/86 [D loss: 0.6128180474042892, acc.: 50.00%] [G loss: 0.5782179832458496]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 12/86 [D loss: 0.5690795034170151, acc.: 50.05%] [G loss: 0.6788807511329651]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 13/86 [D loss: 0.5165100991725922, acc.: 67.04%] [G loss: 0.8154112696647644]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 14/86 [D loss: 0.47061707079410553, acc.: 98.39%] [G loss: 0.963974118232727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 15/86 [D loss: 0.4253856986761093, acc.: 99.51%] [G loss: 1.1229513883590698]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 16/86 [D loss: 0.39430853724479675, acc.: 97.80%] [G loss: 1.2645220756530762]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 17/86 [D loss: 0.35490627586841583, acc.: 97.17%] [G loss: 1.404860019683838]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 1/200, Batch 18/86 [D loss: 0.325075164437294, acc.: 96.00%] [G loss: 1.5321872234344482]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 19/86 [D loss: 0.29272955656051636, acc.: 96.48%] [G loss: 1.6532045602798462]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 20/86 [D loss: 0.2621074765920639, acc.: 96.92%] [G loss: 1.7723945379257202]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 21/86 [D loss: 0.23223543912172318, acc.: 97.31%] [G loss: 1.8903231620788574]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 22/86 [D loss: 0.2232203185558319, acc.: 96.58%] [G loss: 1.9685114622116089]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 23/86 [D loss: 0.19494200497865677, acc.: 97.02%] [G loss: 2.068681001663208]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 24/86 [D loss: 0.17854517698287964, acc.: 97.56%] [G loss: 2.171356678009033]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 25/86 [D loss: 0.1588173732161522, acc.: 97.75%] [G loss: 2.2848095893859863]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 26/86 [D loss: 0.14294756576418877, acc.: 98.34%] [G loss: 2.37388014793396]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 27/86 [D loss: 0.12764813750982285, acc.: 98.68%] [G loss: 2.4627597332000732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 28/86 [D loss: 0.11672256141901016, acc.: 98.39%] [G loss: 2.5489730834960938]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 29/86 [D loss: 0.10832580178976059, acc.: 98.54%] [G loss: 2.6216750144958496]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 30/86 [D loss: 0.09797786921262741, acc.: 98.73%] [G loss: 2.7187798023223877]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 31/86 [D loss: 0.09739884361624718, acc.: 98.14%] [G loss: 2.7614974975585938]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 32/86 [D loss: 0.08693331852555275, acc.: 98.83%] [G loss: 2.8247525691986084]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 33/86 [D loss: 0.08278323337435722, acc.: 98.93%] [G loss: 2.881162643432617]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 34/86 [D loss: 0.0762563943862915, acc.: 99.17%] [G loss: 2.9420766830444336]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 35/86 [D loss: 0.07586617022752762, acc.: 98.97%] [G loss: 2.984776020050049]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 36/86 [D loss: 0.07727222517132759, acc.: 98.54%] [G loss: 3.009871482849121]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 1/200, Batch 37/86 [D loss: 0.08585378155112267, acc.: 97.75%] [G loss: 3.0980868339538574]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 38/86 [D loss: 0.10493620857596397, acc.: 96.92%] [G loss: 3.1250429153442383]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 39/86 [D loss: 0.13610536605119705, acc.: 95.95%] [G loss: 3.1170806884765625]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 40/86 [D loss: 0.20264721661806107, acc.: 93.95%] [G loss: 3.032147169113159]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 41/86 [D loss: 0.28639310598373413, acc.: 91.16%] [G loss: 2.9904229640960693]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 42/86 [D loss: 0.39360351860523224, acc.: 88.23%] [G loss: 2.664034366607666]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 43/86 [D loss: 0.4312223494052887, acc.: 85.94%] [G loss: 1.7887979745864868]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 44/86 [D loss: 0.8402308374643326, acc.: 47.46%] [G loss: 0.5383606553077698]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 45/86 [D loss: 1.8278572410345078, acc.: 47.02%] [G loss: 0.10896911472082138]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 46/86 [D loss: 1.714010238647461, acc.: 47.56%] [G loss: 0.13386574387550354]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 1/200, Batch 47/86 [D loss: 1.307356458157301, acc.: 49.37%] [G loss: 0.22405681014060974]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 48/86 [D loss: 0.9882315248250961, acc.: 50.00%] [G loss: 0.38694092631340027]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 49/86 [D loss: 0.6592806875705719, acc.: 53.27%] [G loss: 0.7054184079170227]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 50/86 [D loss: 0.32141184248030186, acc.: 85.99%] [G loss: 1.2809171676635742]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 51/86 [D loss: 0.16487803682684898, acc.: 100.00%] [G loss: 1.7435353994369507]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 52/86 [D loss: 0.1456306166946888, acc.: 100.00%] [G loss: 1.7217953205108643]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 53/86 [D loss: 0.1775094298645854, acc.: 100.00%] [G loss: 1.4035528898239136]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 54/86 [D loss: 0.23186492267996073, acc.: 100.00%] [G loss: 1.1131267547607422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 55/86 [D loss: 0.27853849716484547, acc.: 100.00%] [G loss: 0.9061509370803833]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 56/86 [D loss: 0.31624490581452847, acc.: 99.37%] [G loss: 0.7982752323150635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 57/86 [D loss: 0.3458793144673109, acc.: 86.23%] [G loss: 0.7324663996696472]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 58/86 [D loss: 0.3770475904457271, acc.: 75.15%] [G loss: 0.6790376305580139]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 59/86 [D loss: 0.42379199247807264, acc.: 65.33%] [G loss: 0.6354750394821167]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 60/86 [D loss: 0.4628494270145893, acc.: 60.79%] [G loss: 0.5951542854309082]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 61/86 [D loss: 0.4980712756514549, acc.: 58.98%] [G loss: 0.5723602175712585]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 62/86 [D loss: 0.5289006289094687, acc.: 58.84%] [G loss: 0.5790495872497559]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 63/86 [D loss: 0.516680795699358, acc.: 58.45%] [G loss: 0.6134179830551147]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 64/86 [D loss: 0.4933534562587738, acc.: 63.23%] [G loss: 0.6708782911300659]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 65/86 [D loss: 0.4697874188423157, acc.: 70.21%] [G loss: 0.7492176294326782]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 66/86 [D loss: 0.44703924655914307, acc.: 78.12%] [G loss: 0.8232035636901855]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 67/86 [D loss: 0.42194056510925293, acc.: 90.62%] [G loss: 0.8864120841026306]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 68/86 [D loss: 0.3969154879450798, acc.: 94.92%] [G loss: 0.943143367767334]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 69/86 [D loss: 0.37611010670661926, acc.: 94.97%] [G loss: 0.9867342114448547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 70/86 [D loss: 0.37638138979673386, acc.: 92.38%] [G loss: 0.9931207895278931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 71/86 [D loss: 0.3993021994829178, acc.: 86.23%] [G loss: 0.9444838166236877]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 72/86 [D loss: 0.43182144314050674, acc.: 78.96%] [G loss: 0.8952353000640869]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 73/86 [D loss: 0.46577049791812897, acc.: 70.46%] [G loss: 0.825107991695404]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 74/86 [D loss: 0.5046834424138069, acc.: 64.55%] [G loss: 0.7663689255714417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 75/86 [D loss: 0.5237923711538315, acc.: 60.69%] [G loss: 0.7278289198875427]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 76/86 [D loss: 0.5243650153279305, acc.: 59.86%] [G loss: 0.7152512073516846]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 77/86 [D loss: 0.518845334649086, acc.: 61.47%] [G loss: 0.7202983498573303]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 78/86 [D loss: 0.513831615447998, acc.: 62.06%] [G loss: 0.7304670214653015]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 1/200, Batch 79/86 [D loss: 0.49902819097042084, acc.: 65.38%] [G loss: 0.753878653049469]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 80/86 [D loss: 0.4767807498574257, acc.: 70.46%] [G loss: 0.7782914638519287]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 81/86 [D loss: 0.45400185137987137, acc.: 76.95%] [G loss: 0.816824197769165]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 82/86 [D loss: 0.4510027915239334, acc.: 79.35%] [G loss: 0.8271504044532776]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 83/86 [D loss: 0.44438473880290985, acc.: 80.13%] [G loss: 0.8421510457992554]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 84/86 [D loss: 0.44745123386383057, acc.: 80.76%] [G loss: 0.8402247428894043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 85/86 [D loss: 0.46581434458494186, acc.: 76.42%] [G loss: 0.8324419856071472]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200, Batch 86/86 [D loss: 0.4796334207057953, acc.: 72.75%] [G loss: 0.8301780819892883]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 1/86 [D loss: 0.49098895490169525, acc.: 72.85%] [G loss: 0.8389800786972046]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 2/86 [D loss: 0.5054440647363663, acc.: 74.37%] [G loss: 0.8459023237228394]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 3/86 [D loss: 0.494434118270874, acc.: 76.71%] [G loss: 0.878943920135498]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 4/86 [D loss: 0.49843907356262207, acc.: 77.59%] [G loss: 0.9160322546958923]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 5/86 [D loss: 0.4905104786157608, acc.: 77.69%] [G loss: 0.977810263633728]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 6/86 [D loss: 0.4744161516427994, acc.: 82.37%] [G loss: 1.0462558269500732]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 7/86 [D loss: 0.45599114894866943, acc.: 85.16%] [G loss: 1.1152909994125366]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 8/86 [D loss: 0.4389234483242035, acc.: 89.94%] [G loss: 1.1710944175720215]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 9/86 [D loss: 0.4342320114374161, acc.: 89.70%] [G loss: 1.2368273735046387]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 10/86 [D loss: 0.4640534818172455, acc.: 87.01%] [G loss: 1.2564349174499512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 11/86 [D loss: 0.514687642455101, acc.: 79.30%] [G loss: 1.2855520248413086]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 12/86 [D loss: 0.5680285394191742, acc.: 74.90%] [G loss: 1.3245649337768555]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 2/200, Batch 13/86 [D loss: 0.6046185493469238, acc.: 70.26%] [G loss: 1.3543593883514404]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 14/86 [D loss: 0.5802589952945709, acc.: 73.83%] [G loss: 1.3649402856826782]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 15/86 [D loss: 0.5646257698535919, acc.: 75.15%] [G loss: 1.3941619396209717]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 16/86 [D loss: 0.5385504066944122, acc.: 75.88%] [G loss: 1.4050061702728271]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 17/86 [D loss: 0.5008439123630524, acc.: 79.74%] [G loss: 1.4709450006484985]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 18/86 [D loss: 0.49638357758522034, acc.: 78.91%] [G loss: 1.429673671722412]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 19/86 [D loss: 0.48053693771362305, acc.: 80.47%] [G loss: 1.434832215309143]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 20/86 [D loss: 0.4660097062587738, acc.: 81.20%] [G loss: 1.438539743423462]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 21/86 [D loss: 0.4673227071762085, acc.: 81.84%] [G loss: 1.4250974655151367]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 22/86 [D loss: 0.457026943564415, acc.: 84.18%] [G loss: 1.430514931678772]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 23/86 [D loss: 0.4573865532875061, acc.: 85.35%] [G loss: 1.3810489177703857]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 24/86 [D loss: 0.45605552196502686, acc.: 85.16%] [G loss: 1.3720935583114624]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 25/86 [D loss: 0.4592122584581375, acc.: 85.79%] [G loss: 1.3392658233642578]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 26/86 [D loss: 0.4681828171014786, acc.: 84.72%] [G loss: 1.299368143081665]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 27/86 [D loss: 0.48723435401916504, acc.: 83.74%] [G loss: 1.2563719749450684]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 28/86 [D loss: 0.5123137980699539, acc.: 80.57%] [G loss: 1.2279108762741089]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 29/86 [D loss: 0.5698748826980591, acc.: 71.04%] [G loss: 1.1183090209960938]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 30/86 [D loss: 0.6560770273208618, acc.: 60.50%] [G loss: 1.0203536748886108]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 31/86 [D loss: 0.7571662366390228, acc.: 53.66%] [G loss: 0.9580143094062805]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 32/86 [D loss: 0.8057947158813477, acc.: 51.61%] [G loss: 1.0561275482177734]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 33/86 [D loss: 0.7321960031986237, acc.: 53.61%] [G loss: 1.1276795864105225]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 34/86 [D loss: 0.6794745326042175, acc.: 59.67%] [G loss: 1.1593713760375977]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 35/86 [D loss: 0.6041769981384277, acc.: 66.41%] [G loss: 1.1628276109695435]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 36/86 [D loss: 0.5942772924900055, acc.: 66.94%] [G loss: 1.0837894678115845]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 37/86 [D loss: 0.593777596950531, acc.: 67.38%] [G loss: 1.092902421951294]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 38/86 [D loss: 0.5851893424987793, acc.: 68.60%] [G loss: 1.0702087879180908]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 39/86 [D loss: 0.6083877086639404, acc.: 67.33%] [G loss: 1.042303442955017]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 40/86 [D loss: 0.6131467223167419, acc.: 66.55%] [G loss: 1.0594075918197632]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 41/86 [D loss: 0.6141394674777985, acc.: 68.26%] [G loss: 1.081517219543457]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 42/86 [D loss: 0.6078663468360901, acc.: 68.99%] [G loss: 1.0911885499954224]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 43/86 [D loss: 0.6193232238292694, acc.: 66.85%] [G loss: 1.0392255783081055]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 44/86 [D loss: 0.633320689201355, acc.: 64.26%] [G loss: 1.023634910583496]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 45/86 [D loss: 0.6673706471920013, acc.: 57.28%] [G loss: 0.9691096544265747]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 46/86 [D loss: 0.7133740186691284, acc.: 51.22%] [G loss: 0.8926068544387817]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 47/86 [D loss: 0.7550691664218903, acc.: 46.88%] [G loss: 0.8697863817214966]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 48/86 [D loss: 0.762755423784256, acc.: 46.97%] [G loss: 0.8802616596221924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 49/86 [D loss: 0.7428261935710907, acc.: 50.73%] [G loss: 0.9161766767501831]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 50/86 [D loss: 0.7320002019405365, acc.: 52.78%] [G loss: 0.9626350998878479]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 51/86 [D loss: 0.7137448191642761, acc.: 54.79%] [G loss: 0.9542142152786255]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 52/86 [D loss: 0.7242467999458313, acc.: 54.39%] [G loss: 0.9476206302642822]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 53/86 [D loss: 0.7139169573783875, acc.: 55.76%] [G loss: 0.9224125742912292]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 54/86 [D loss: 0.7272769212722778, acc.: 53.71%] [G loss: 0.9365128874778748]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 55/86 [D loss: 0.7152866423130035, acc.: 53.71%] [G loss: 0.954825758934021]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 56/86 [D loss: 0.7020610570907593, acc.: 54.98%] [G loss: 0.9601557850837708]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 57/86 [D loss: 0.6934238076210022, acc.: 57.08%] [G loss: 0.964558482170105]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 58/86 [D loss: 0.6758377552032471, acc.: 59.13%] [G loss: 0.983718752861023]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 59/86 [D loss: 0.6601960062980652, acc.: 60.84%] [G loss: 1.001466989517212]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 60/86 [D loss: 0.6672372221946716, acc.: 59.47%] [G loss: 0.9774779677391052]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 61/86 [D loss: 0.6500753462314606, acc.: 61.38%] [G loss: 0.9841474294662476]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 62/86 [D loss: 0.6483928263187408, acc.: 60.94%] [G loss: 0.9676724076271057]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 63/86 [D loss: 0.6536742746829987, acc.: 60.55%] [G loss: 0.9452609419822693]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 64/86 [D loss: 0.6652304530143738, acc.: 58.45%] [G loss: 0.9454990029335022]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 65/86 [D loss: 0.691737711429596, acc.: 53.71%] [G loss: 0.8863999247550964]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 66/86 [D loss: 0.7178758382797241, acc.: 51.32%] [G loss: 0.8487423658370972]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 67/86 [D loss: 0.7694491744041443, acc.: 45.65%] [G loss: 0.8254624009132385]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 68/86 [D loss: 0.7790941596031189, acc.: 40.53%] [G loss: 0.783930242061615]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 69/86 [D loss: 0.8030675053596497, acc.: 38.43%] [G loss: 0.7619591951370239]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 70/86 [D loss: 0.8378343880176544, acc.: 33.40%] [G loss: 0.7576947808265686]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 71/86 [D loss: 0.8402020931243896, acc.: 32.67%] [G loss: 0.7428480386734009]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 72/86 [D loss: 0.8483724594116211, acc.: 31.74%] [G loss: 0.7422040700912476]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 73/86 [D loss: 0.8549001812934875, acc.: 29.59%] [G loss: 0.7329923510551453]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 74/86 [D loss: 0.8649233877658844, acc.: 27.69%] [G loss: 0.728865385055542]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 75/86 [D loss: 0.8494112491607666, acc.: 29.44%] [G loss: 0.7523781061172485]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 76/86 [D loss: 0.8413287699222565, acc.: 29.05%] [G loss: 0.7452927231788635]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 77/86 [D loss: 0.8324628174304962, acc.: 29.83%] [G loss: 0.742350161075592]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 78/86 [D loss: 0.8254991471767426, acc.: 29.49%] [G loss: 0.7465149164199829]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 79/86 [D loss: 0.8214337527751923, acc.: 30.13%] [G loss: 0.7594816088676453]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 80/86 [D loss: 0.800036609172821, acc.: 33.45%] [G loss: 0.7618569135665894]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 81/86 [D loss: 0.7916801869869232, acc.: 35.74%] [G loss: 0.7695015668869019]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 82/86 [D loss: 0.7899254560470581, acc.: 35.55%] [G loss: 0.779296875]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 83/86 [D loss: 0.7854596078395844, acc.: 36.67%] [G loss: 0.7753828167915344]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 2/200, Batch 84/86 [D loss: 0.7661841213703156, acc.: 39.94%] [G loss: 0.7880328297615051]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 85/86 [D loss: 0.7553176283836365, acc.: 42.09%] [G loss: 0.799302339553833]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 2/200, Batch 86/86 [D loss: 0.7470804750919342, acc.: 44.68%] [G loss: 0.817947268486023]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 1/86 [D loss: 0.7365307509899139, acc.: 47.75%] [G loss: 0.8163325786590576]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 2/86 [D loss: 0.7303261458873749, acc.: 46.92%] [G loss: 0.8180993795394897]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 3/86 [D loss: 0.7172527611255646, acc.: 51.12%] [G loss: 0.8108865022659302]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 4/86 [D loss: 0.7041184902191162, acc.: 52.83%] [G loss: 0.8452463746070862]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 5/86 [D loss: 0.7067044377326965, acc.: 51.76%] [G loss: 0.8485289216041565]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 6/86 [D loss: 0.6991631090641022, acc.: 54.00%] [G loss: 0.8506374359130859]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 7/86 [D loss: 0.7005663216114044, acc.: 53.47%] [G loss: 0.8508199453353882]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 8/86 [D loss: 0.7057914435863495, acc.: 52.25%] [G loss: 0.8484448790550232]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 9/86 [D loss: 0.6872562766075134, acc.: 55.91%] [G loss: 0.8625995516777039]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 10/86 [D loss: 0.6930379569530487, acc.: 55.71%] [G loss: 0.8671792149543762]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 11/86 [D loss: 0.6761583685874939, acc.: 58.35%] [G loss: 0.856105387210846]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 12/86 [D loss: 0.6772740483283997, acc.: 58.11%] [G loss: 0.8641409277915955]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 13/86 [D loss: 0.6841170787811279, acc.: 56.74%] [G loss: 0.858668327331543]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 14/86 [D loss: 0.6858809292316437, acc.: 55.62%] [G loss: 0.8757990002632141]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 15/86 [D loss: 0.6699609458446503, acc.: 60.16%] [G loss: 0.8680854439735413]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 16/86 [D loss: 0.6697435081005096, acc.: 59.18%] [G loss: 0.8771709203720093]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 17/86 [D loss: 0.6818755567073822, acc.: 58.54%] [G loss: 0.8774703741073608]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 18/86 [D loss: 0.6743338704109192, acc.: 58.69%] [G loss: 0.8839163184165955]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 19/86 [D loss: 0.6759338974952698, acc.: 58.35%] [G loss: 0.8662315011024475]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 20/86 [D loss: 0.6608783900737762, acc.: 60.84%] [G loss: 0.8821902275085449]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 21/86 [D loss: 0.6742532849311829, acc.: 59.28%] [G loss: 0.8768714070320129]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 22/86 [D loss: 0.6841003894805908, acc.: 55.47%] [G loss: 0.8577275276184082]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 23/86 [D loss: 0.6754919588565826, acc.: 58.45%] [G loss: 0.8541232347488403]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 24/86 [D loss: 0.6820314228534698, acc.: 56.20%] [G loss: 0.8607557415962219]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 25/86 [D loss: 0.693918913602829, acc.: 54.25%] [G loss: 0.847175657749176]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 26/86 [D loss: 0.6908091008663177, acc.: 53.17%] [G loss: 0.8583859205245972]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 27/86 [D loss: 0.6844526529312134, acc.: 53.86%] [G loss: 0.8401976227760315]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 3/200, Batch 28/86 [D loss: 0.7004677355289459, acc.: 52.69%] [G loss: 0.8348197937011719]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 29/86 [D loss: 0.6924873292446136, acc.: 53.32%] [G loss: 0.8212904930114746]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 30/86 [D loss: 0.7000371813774109, acc.: 53.17%] [G loss: 0.8127086758613586]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 31/86 [D loss: 0.7094342410564423, acc.: 50.54%] [G loss: 0.8209236860275269]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 32/86 [D loss: 0.7084744572639465, acc.: 50.68%] [G loss: 0.8097058534622192]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 33/86 [D loss: 0.7198567092418671, acc.: 47.90%] [G loss: 0.8234790563583374]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 34/86 [D loss: 0.7190488278865814, acc.: 47.22%] [G loss: 0.797035276889801]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 35/86 [D loss: 0.7216342091560364, acc.: 48.73%] [G loss: 0.8069565296173096]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 36/86 [D loss: 0.7214610278606415, acc.: 47.75%] [G loss: 0.7954409718513489]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 37/86 [D loss: 0.7315084040164948, acc.: 45.26%] [G loss: 0.7923398613929749]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 3/200, Batch 38/86 [D loss: 0.7332748770713806, acc.: 45.46%] [G loss: 0.7832878828048706]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 39/86 [D loss: 0.7428650259971619, acc.: 43.75%] [G loss: 0.7731408476829529]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 40/86 [D loss: 0.7511424422264099, acc.: 40.72%] [G loss: 0.7666518688201904]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 41/86 [D loss: 0.7438004910945892, acc.: 43.70%] [G loss: 0.7585175037384033]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 42/86 [D loss: 0.7561402320861816, acc.: 40.67%] [G loss: 0.767759382724762]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 3/200, Batch 43/86 [D loss: 0.7500511407852173, acc.: 41.31%] [G loss: 0.7697144746780396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 44/86 [D loss: 0.762015700340271, acc.: 40.28%] [G loss: 0.7620536088943481]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 45/86 [D loss: 0.75446817278862, acc.: 38.87%] [G loss: 0.7495906352996826]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 46/86 [D loss: 0.7741060853004456, acc.: 36.57%] [G loss: 0.7449513077735901]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 47/86 [D loss: 0.7651865482330322, acc.: 37.45%] [G loss: 0.7470778226852417]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 48/86 [D loss: 0.7786065638065338, acc.: 35.30%] [G loss: 0.7380539178848267]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 49/86 [D loss: 0.7699752748012543, acc.: 37.84%] [G loss: 0.7415814399719238]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 50/86 [D loss: 0.7759241759777069, acc.: 37.35%] [G loss: 0.7357509136199951]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 51/86 [D loss: 0.7745261788368225, acc.: 36.33%] [G loss: 0.7234600186347961]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 52/86 [D loss: 0.7826024889945984, acc.: 35.21%] [G loss: 0.734392523765564]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 53/86 [D loss: 0.7758419811725616, acc.: 36.72%] [G loss: 0.7248474359512329]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 54/86 [D loss: 0.7882300317287445, acc.: 33.98%] [G loss: 0.7267619371414185]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 55/86 [D loss: 0.7809350192546844, acc.: 35.64%] [G loss: 0.7161131501197815]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 56/86 [D loss: 0.7866097390651703, acc.: 33.20%] [G loss: 0.7131100296974182]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 57/86 [D loss: 0.7888553440570831, acc.: 33.35%] [G loss: 0.7106040716171265]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 58/86 [D loss: 0.7826310992240906, acc.: 33.15%] [G loss: 0.7132053375244141]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 59/86 [D loss: 0.7969821095466614, acc.: 30.81%] [G loss: 0.721617579460144]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 60/86 [D loss: 0.8004086017608643, acc.: 30.03%] [G loss: 0.7101501226425171]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 61/86 [D loss: 0.8026685416698456, acc.: 30.57%] [G loss: 0.7065443992614746]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 62/86 [D loss: 0.8035411238670349, acc.: 30.37%] [G loss: 0.7034480571746826]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 63/86 [D loss: 0.7971447110176086, acc.: 29.93%] [G loss: 0.7054489850997925]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 64/86 [D loss: 0.8024490475654602, acc.: 29.30%] [G loss: 0.714803159236908]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 65/86 [D loss: 0.7983187735080719, acc.: 30.08%] [G loss: 0.7022446393966675]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 66/86 [D loss: 0.8081448376178741, acc.: 28.61%] [G loss: 0.7141066789627075]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 67/86 [D loss: 0.7946758270263672, acc.: 29.64%] [G loss: 0.7067899703979492]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 3/200, Batch 68/86 [D loss: 0.7949927151203156, acc.: 30.62%] [G loss: 0.7068564295768738]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 69/86 [D loss: 0.7919532060623169, acc.: 30.22%] [G loss: 0.6896670460700989]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 70/86 [D loss: 0.8015727698802948, acc.: 27.39%] [G loss: 0.6886880993843079]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 71/86 [D loss: 0.8005501925945282, acc.: 25.59%] [G loss: 0.6988823413848877]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 72/86 [D loss: 0.7892648875713348, acc.: 29.54%] [G loss: 0.6934494972229004]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 3/200, Batch 73/86 [D loss: 0.789189338684082, acc.: 27.78%] [G loss: 0.7067916989326477]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 74/86 [D loss: 0.783476710319519, acc.: 28.37%] [G loss: 0.6967901587486267]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 75/86 [D loss: 0.775532454252243, acc.: 30.37%] [G loss: 0.6934890151023865]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 76/86 [D loss: 0.7793177962303162, acc.: 29.30%] [G loss: 0.6954451203346252]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 77/86 [D loss: 0.7805462181568146, acc.: 29.05%] [G loss: 0.7067139148712158]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 3/200, Batch 78/86 [D loss: 0.7675181031227112, acc.: 32.37%] [G loss: 0.7091496586799622]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 79/86 [D loss: 0.7613005340099335, acc.: 34.08%] [G loss: 0.7125257253646851]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 80/86 [D loss: 0.7611498236656189, acc.: 32.13%] [G loss: 0.7229806184768677]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 81/86 [D loss: 0.7521054744720459, acc.: 36.72%] [G loss: 0.7228797674179077]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 82/86 [D loss: 0.7466853857040405, acc.: 36.67%] [G loss: 0.735603928565979]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 3/200, Batch 83/86 [D loss: 0.7405742108821869, acc.: 37.74%] [G loss: 0.7371283173561096]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 84/86 [D loss: 0.7370842695236206, acc.: 38.04%] [G loss: 0.7532017827033997]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 85/86 [D loss: 0.7256333231925964, acc.: 41.16%] [G loss: 0.7642552852630615]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 3/200, Batch 86/86 [D loss: 0.7227242887020111, acc.: 43.46%] [G loss: 0.7692518830299377]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 1/86 [D loss: 0.7138553559780121, acc.: 44.97%] [G loss: 0.76531982421875]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 2/86 [D loss: 0.7106896638870239, acc.: 46.53%] [G loss: 0.7722652554512024]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 3/86 [D loss: 0.7055366933345795, acc.: 47.71%] [G loss: 0.7744630575180054]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 4/86 [D loss: 0.700334906578064, acc.: 49.27%] [G loss: 0.7837311029434204]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 5/86 [D loss: 0.6954736113548279, acc.: 51.37%] [G loss: 0.7799994945526123]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 6/86 [D loss: 0.6892647743225098, acc.: 53.76%] [G loss: 0.7873378992080688]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 7/86 [D loss: 0.6874469518661499, acc.: 52.78%] [G loss: 0.7940425276756287]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 8/86 [D loss: 0.6801228821277618, acc.: 56.64%] [G loss: 0.8009665608406067]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 9/86 [D loss: 0.6802157163619995, acc.: 55.71%] [G loss: 0.8016430139541626]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 10/86 [D loss: 0.6783951818943024, acc.: 56.59%] [G loss: 0.8072320818901062]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 11/86 [D loss: 0.675693541765213, acc.: 57.86%] [G loss: 0.8054092526435852]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 12/86 [D loss: 0.6705604195594788, acc.: 59.03%] [G loss: 0.7989062070846558]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 13/86 [D loss: 0.672086775302887, acc.: 58.79%] [G loss: 0.808201253414154]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 14/86 [D loss: 0.6656944751739502, acc.: 60.79%] [G loss: 0.8120294809341431]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 15/86 [D loss: 0.6635569334030151, acc.: 60.74%] [G loss: 0.8105114698410034]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 16/86 [D loss: 0.6650652587413788, acc.: 62.26%] [G loss: 0.8081566095352173]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 17/86 [D loss: 0.6614561975002289, acc.: 63.13%] [G loss: 0.8085994720458984]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 18/86 [D loss: 0.660946249961853, acc.: 62.65%] [G loss: 0.8147501945495605]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 19/86 [D loss: 0.6638529896736145, acc.: 62.11%] [G loss: 0.8135989308357239]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 20/86 [D loss: 0.6598457098007202, acc.: 62.70%] [G loss: 0.8096540570259094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 21/86 [D loss: 0.6594510972499847, acc.: 61.87%] [G loss: 0.8158814311027527]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 22/86 [D loss: 0.6558270454406738, acc.: 63.77%] [G loss: 0.819402813911438]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 23/86 [D loss: 0.6537409424781799, acc.: 64.75%] [G loss: 0.8157821893692017]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 24/86 [D loss: 0.6567212045192719, acc.: 63.62%] [G loss: 0.8150455355644226]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 25/86 [D loss: 0.6553225219249725, acc.: 62.74%] [G loss: 0.798201322555542]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 26/86 [D loss: 0.6520019769668579, acc.: 63.48%] [G loss: 0.8164359331130981]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 27/86 [D loss: 0.6578861474990845, acc.: 63.96%] [G loss: 0.8191730380058289]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 28/86 [D loss: 0.6597078740596771, acc.: 62.16%] [G loss: 0.8176949620246887]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 29/86 [D loss: 0.6652133166790009, acc.: 60.40%] [G loss: 0.8124308586120605]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 4/200, Batch 30/86 [D loss: 0.661342203617096, acc.: 61.67%] [G loss: 0.8132204413414001]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 31/86 [D loss: 0.6623411774635315, acc.: 60.69%] [G loss: 0.8174348473548889]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 32/86 [D loss: 0.667849600315094, acc.: 59.03%] [G loss: 0.8124942779541016]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 33/86 [D loss: 0.6627591252326965, acc.: 60.74%] [G loss: 0.8065615892410278]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 34/86 [D loss: 0.6733339130878448, acc.: 57.96%] [G loss: 0.8043925166130066]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 35/86 [D loss: 0.6678919196128845, acc.: 60.11%] [G loss: 0.8094275593757629]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 36/86 [D loss: 0.6737178862094879, acc.: 57.13%] [G loss: 0.7996071577072144]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 37/86 [D loss: 0.6712401509284973, acc.: 58.06%] [G loss: 0.8054009675979614]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 38/86 [D loss: 0.6738920509815216, acc.: 57.57%] [G loss: 0.7980881929397583]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 39/86 [D loss: 0.6808253526687622, acc.: 55.91%] [G loss: 0.7990233898162842]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 40/86 [D loss: 0.6795821189880371, acc.: 56.59%] [G loss: 0.7985872626304626]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 41/86 [D loss: 0.6835721135139465, acc.: 55.27%] [G loss: 0.7909445762634277]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 42/86 [D loss: 0.6861226260662079, acc.: 53.91%] [G loss: 0.7895742654800415]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 43/86 [D loss: 0.6788002550601959, acc.: 57.28%] [G loss: 0.7959096431732178]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 44/86 [D loss: 0.6813872754573822, acc.: 55.71%] [G loss: 0.7839558124542236]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 45/86 [D loss: 0.6825753450393677, acc.: 57.18%] [G loss: 0.7850942015647888]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 46/86 [D loss: 0.6844865083694458, acc.: 55.32%] [G loss: 0.7794178128242493]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 47/86 [D loss: 0.6852171123027802, acc.: 53.66%] [G loss: 0.7785366177558899]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 48/86 [D loss: 0.6875253021717072, acc.: 55.08%] [G loss: 0.7866255044937134]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 49/86 [D loss: 0.6865594387054443, acc.: 54.88%] [G loss: 0.7730494737625122]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 50/86 [D loss: 0.6819121837615967, acc.: 56.49%] [G loss: 0.780025839805603]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 51/86 [D loss: 0.6857303380966187, acc.: 55.91%] [G loss: 0.783404529094696]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 52/86 [D loss: 0.6805634200572968, acc.: 57.03%] [G loss: 0.7841830849647522]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 53/86 [D loss: 0.6918945610523224, acc.: 53.47%] [G loss: 0.7825503349304199]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 54/86 [D loss: 0.6837009489536285, acc.: 56.10%] [G loss: 0.7726397514343262]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 55/86 [D loss: 0.6919401288032532, acc.: 53.42%] [G loss: 0.7851346135139465]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 56/86 [D loss: 0.6873369812965393, acc.: 55.96%] [G loss: 0.7853510975837708]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 57/86 [D loss: 0.6912393569946289, acc.: 54.44%] [G loss: 0.7825523018836975]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 58/86 [D loss: 0.6904374063014984, acc.: 54.74%] [G loss: 0.7904152870178223]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 59/86 [D loss: 0.6905695199966431, acc.: 54.39%] [G loss: 0.7945131659507751]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 4/200, Batch 60/86 [D loss: 0.6866173446178436, acc.: 54.83%] [G loss: 0.7987756729125977]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 61/86 [D loss: 0.6849579811096191, acc.: 56.98%] [G loss: 0.8009693026542664]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 62/86 [D loss: 0.6859749853610992, acc.: 56.01%] [G loss: 0.8000782132148743]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 63/86 [D loss: 0.6851207613945007, acc.: 57.18%] [G loss: 0.7986273169517517]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 64/86 [D loss: 0.6891540288925171, acc.: 55.57%] [G loss: 0.8024285435676575]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 65/86 [D loss: 0.6871751844882965, acc.: 56.01%] [G loss: 0.7958356142044067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 66/86 [D loss: 0.6869821846485138, acc.: 56.25%] [G loss: 0.7859534025192261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 67/86 [D loss: 0.6812001168727875, acc.: 58.25%] [G loss: 0.7932658195495605]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 68/86 [D loss: 0.6868568062782288, acc.: 55.86%] [G loss: 0.790567934513092]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 69/86 [D loss: 0.687719315290451, acc.: 56.59%] [G loss: 0.7874442934989929]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 70/86 [D loss: 0.687841385602951, acc.: 57.42%] [G loss: 0.7843070030212402]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 71/86 [D loss: 0.6808443367481232, acc.: 56.98%] [G loss: 0.7837203741073608]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 72/86 [D loss: 0.676519513130188, acc.: 59.03%] [G loss: 0.7862473726272583]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 73/86 [D loss: 0.6800449788570404, acc.: 59.13%] [G loss: 0.7878667712211609]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 74/86 [D loss: 0.6752626597881317, acc.: 60.01%] [G loss: 0.7934769988059998]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 4/200, Batch 75/86 [D loss: 0.6779378950595856, acc.: 58.59%] [G loss: 0.7835587859153748]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 76/86 [D loss: 0.6687292754650116, acc.: 61.13%] [G loss: 0.7869604825973511]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 77/86 [D loss: 0.6714209616184235, acc.: 61.72%] [G loss: 0.7945361733436584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 78/86 [D loss: 0.668292909860611, acc.: 61.52%] [G loss: 0.7996418476104736]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 79/86 [D loss: 0.668590247631073, acc.: 61.96%] [G loss: 0.7991293668746948]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 80/86 [D loss: 0.6683870553970337, acc.: 62.16%] [G loss: 0.8030303716659546]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 81/86 [D loss: 0.6622511744499207, acc.: 62.45%] [G loss: 0.8109613656997681]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 82/86 [D loss: 0.6650956571102142, acc.: 62.45%] [G loss: 0.8193119764328003]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 83/86 [D loss: 0.6623231470584869, acc.: 63.23%] [G loss: 0.8193050026893616]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 4/200, Batch 84/86 [D loss: 0.6607275307178497, acc.: 64.11%] [G loss: 0.8227967023849487]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 85/86 [D loss: 0.6645672917366028, acc.: 62.40%] [G loss: 0.8171745538711548]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 4/200, Batch 86/86 [D loss: 0.662038117647171, acc.: 63.92%] [G loss: 0.8212794065475464]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 1/86 [D loss: 0.6562953591346741, acc.: 64.65%] [G loss: 0.8261547684669495]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 2/86 [D loss: 0.6592954695224762, acc.: 63.53%] [G loss: 0.8331494331359863]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 3/86 [D loss: 0.665636420249939, acc.: 62.01%] [G loss: 0.8134863972663879]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 4/86 [D loss: 0.6638369858264923, acc.: 63.28%] [G loss: 0.8072409629821777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 5/86 [D loss: 0.6629313230514526, acc.: 62.50%] [G loss: 0.806614339351654]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 5/200, Batch 6/86 [D loss: 0.6726914644241333, acc.: 59.57%] [G loss: 0.7970665097236633]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 7/86 [D loss: 0.6716310381889343, acc.: 60.01%] [G loss: 0.7919573783874512]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 8/86 [D loss: 0.6753987371921539, acc.: 59.42%] [G loss: 0.7881922721862793]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 9/86 [D loss: 0.6692777276039124, acc.: 59.72%] [G loss: 0.7792724967002869]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 10/86 [D loss: 0.6697160601615906, acc.: 61.47%] [G loss: 0.7820245623588562]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 11/86 [D loss: 0.6776154935359955, acc.: 59.03%] [G loss: 0.7812824249267578]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 12/86 [D loss: 0.6754252314567566, acc.: 58.69%] [G loss: 0.766398012638092]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 13/86 [D loss: 0.6751846969127655, acc.: 59.18%] [G loss: 0.7663499712944031]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 14/86 [D loss: 0.673980325460434, acc.: 58.01%] [G loss: 0.7717639803886414]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 15/86 [D loss: 0.677539736032486, acc.: 58.74%] [G loss: 0.766853392124176]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 16/86 [D loss: 0.6852346956729889, acc.: 54.93%] [G loss: 0.7685196995735168]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 17/86 [D loss: 0.6851346492767334, acc.: 55.81%] [G loss: 0.7591047286987305]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 18/86 [D loss: 0.6802479028701782, acc.: 57.28%] [G loss: 0.7670931816101074]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 19/86 [D loss: 0.6847043037414551, acc.: 56.15%] [G loss: 0.7623909115791321]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 20/86 [D loss: 0.6859551668167114, acc.: 55.62%] [G loss: 0.769281804561615]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 21/86 [D loss: 0.6831274926662445, acc.: 55.32%] [G loss: 0.765831470489502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 22/86 [D loss: 0.6919930279254913, acc.: 52.64%] [G loss: 0.7646678686141968]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 23/86 [D loss: 0.6874118149280548, acc.: 53.96%] [G loss: 0.7711168527603149]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 24/86 [D loss: 0.6864205896854401, acc.: 54.35%] [G loss: 0.771826982498169]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 25/86 [D loss: 0.6833197772502899, acc.: 55.91%] [G loss: 0.7701194286346436]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 26/86 [D loss: 0.6887497305870056, acc.: 54.30%] [G loss: 0.7775863409042358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 27/86 [D loss: 0.6881352066993713, acc.: 54.64%] [G loss: 0.7705936431884766]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 28/86 [D loss: 0.6897867619991302, acc.: 53.37%] [G loss: 0.7627514004707336]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 5/200, Batch 29/86 [D loss: 0.6835503578186035, acc.: 55.66%] [G loss: 0.775067150592804]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 30/86 [D loss: 0.6838399171829224, acc.: 54.88%] [G loss: 0.7742452621459961]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 31/86 [D loss: 0.6853570640087128, acc.: 56.59%] [G loss: 0.7653355598449707]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 32/86 [D loss: 0.6804013848304749, acc.: 55.96%] [G loss: 0.7717355489730835]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 33/86 [D loss: 0.6814695000648499, acc.: 56.93%] [G loss: 0.7696505188941956]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 34/86 [D loss: 0.6839264929294586, acc.: 55.13%] [G loss: 0.765295684337616]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 35/86 [D loss: 0.6838327348232269, acc.: 54.88%] [G loss: 0.7655569314956665]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 36/86 [D loss: 0.6758816838264465, acc.: 56.49%] [G loss: 0.7586089968681335]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 37/86 [D loss: 0.6830628216266632, acc.: 54.59%] [G loss: 0.7588526010513306]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 38/86 [D loss: 0.6798806488513947, acc.: 56.59%] [G loss: 0.7564144730567932]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 39/86 [D loss: 0.6821625530719757, acc.: 54.59%] [G loss: 0.7569624781608582]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 40/86 [D loss: 0.6791511178016663, acc.: 55.62%] [G loss: 0.7512056827545166]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 41/86 [D loss: 0.6771289706230164, acc.: 56.88%] [G loss: 0.7659463286399841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 42/86 [D loss: 0.6837502419948578, acc.: 54.98%] [G loss: 0.7581518888473511]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 43/86 [D loss: 0.6809219121932983, acc.: 55.71%] [G loss: 0.7573615908622742]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 44/86 [D loss: 0.6844260692596436, acc.: 55.08%] [G loss: 0.7537261843681335]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 45/86 [D loss: 0.6811814904212952, acc.: 57.37%] [G loss: 0.7596811056137085]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 46/86 [D loss: 0.6826423704624176, acc.: 54.93%] [G loss: 0.7470924854278564]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 47/86 [D loss: 0.6865765750408173, acc.: 53.52%] [G loss: 0.756173849105835]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 48/86 [D loss: 0.6881020367145538, acc.: 54.44%] [G loss: 0.7566884756088257]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 49/86 [D loss: 0.693791389465332, acc.: 53.61%] [G loss: 0.7617238163948059]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 50/86 [D loss: 0.6960501968860626, acc.: 52.44%] [G loss: 0.7578824758529663]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 51/86 [D loss: 0.6886204183101654, acc.: 53.08%] [G loss: 0.765958309173584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 52/86 [D loss: 0.7020286917686462, acc.: 50.63%] [G loss: 0.7584385275840759]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 53/86 [D loss: 0.6925197243690491, acc.: 53.52%] [G loss: 0.7448610067367554]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 5/200, Batch 54/86 [D loss: 0.6971695721149445, acc.: 50.10%] [G loss: 0.7464773654937744]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 55/86 [D loss: 0.6973788738250732, acc.: 50.20%] [G loss: 0.7380814552307129]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 56/86 [D loss: 0.6943906247615814, acc.: 50.73%] [G loss: 0.7265050411224365]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 57/86 [D loss: 0.7009199559688568, acc.: 50.49%] [G loss: 0.7298377752304077]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 58/86 [D loss: 0.7090368568897247, acc.: 47.85%] [G loss: 0.7269895076751709]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 5/200, Batch 59/86 [D loss: 0.6988005340099335, acc.: 50.05%] [G loss: 0.7207856178283691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 60/86 [D loss: 0.698797196149826, acc.: 50.39%] [G loss: 0.7221885919570923]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 61/86 [D loss: 0.7011074423789978, acc.: 49.32%] [G loss: 0.7195756435394287]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 62/86 [D loss: 0.7057848274707794, acc.: 48.05%] [G loss: 0.7181815505027771]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 63/86 [D loss: 0.7036499381065369, acc.: 48.63%] [G loss: 0.716561496257782]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 5/200, Batch 64/86 [D loss: 0.700740247964859, acc.: 48.44%] [G loss: 0.7193630933761597]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 65/86 [D loss: 0.7057719528675079, acc.: 47.07%] [G loss: 0.7232240438461304]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 66/86 [D loss: 0.7040433585643768, acc.: 49.12%] [G loss: 0.7260184288024902]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 67/86 [D loss: 0.6997353434562683, acc.: 49.61%] [G loss: 0.7356553673744202]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 68/86 [D loss: 0.6995696127414703, acc.: 50.24%] [G loss: 0.746949315071106]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 5/200, Batch 69/86 [D loss: 0.6987999081611633, acc.: 51.46%] [G loss: 0.748227596282959]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 70/86 [D loss: 0.6977618336677551, acc.: 50.49%] [G loss: 0.7544381618499756]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 71/86 [D loss: 0.6949325799942017, acc.: 50.44%] [G loss: 0.7514116764068604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 72/86 [D loss: 0.68959641456604, acc.: 53.47%] [G loss: 0.7577089667320251]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 73/86 [D loss: 0.6913323402404785, acc.: 53.42%] [G loss: 0.7488358616828918]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 5/200, Batch 74/86 [D loss: 0.6901676654815674, acc.: 51.90%] [G loss: 0.7572100162506104]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 75/86 [D loss: 0.6907746493816376, acc.: 51.56%] [G loss: 0.7517901659011841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 76/86 [D loss: 0.6882586479187012, acc.: 53.37%] [G loss: 0.7495447397232056]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 77/86 [D loss: 0.6878365874290466, acc.: 53.17%] [G loss: 0.7525633573532104]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 78/86 [D loss: 0.6852148175239563, acc.: 53.52%] [G loss: 0.7499901056289673]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 79/86 [D loss: 0.6895891726016998, acc.: 53.17%] [G loss: 0.7397396564483643]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 80/86 [D loss: 0.6902892589569092, acc.: 52.15%] [G loss: 0.7344219088554382]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 81/86 [D loss: 0.6863411068916321, acc.: 53.12%] [G loss: 0.7325183153152466]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 82/86 [D loss: 0.6961744725704193, acc.: 51.46%] [G loss: 0.7321874499320984]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 5/200, Batch 83/86 [D loss: 0.693287581205368, acc.: 50.73%] [G loss: 0.7324109077453613]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 5/200, Batch 84/86 [D loss: 0.6941505670547485, acc.: 51.03%] [G loss: 0.7176916599273682]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 85/86 [D loss: 0.6998870968818665, acc.: 49.71%] [G loss: 0.7271414399147034]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 86/86 [D loss: 0.6975747346878052, acc.: 51.71%] [G loss: 0.7289449572563171]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 1/86 [D loss: 0.7003816068172455, acc.: 50.59%] [G loss: 0.7320179343223572]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 2/86 [D loss: 0.6987507939338684, acc.: 49.80%] [G loss: 0.7322090268135071]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 3/86 [D loss: 0.7066710591316223, acc.: 46.83%] [G loss: 0.7382326722145081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 4/86 [D loss: 0.7112993001937866, acc.: 46.83%] [G loss: 0.7347984313964844]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 5/86 [D loss: 0.7073381543159485, acc.: 46.24%] [G loss: 0.7360895872116089]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 6/86 [D loss: 0.7130946516990662, acc.: 46.29%] [G loss: 0.7236143350601196]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 7/86 [D loss: 0.711791604757309, acc.: 46.24%] [G loss: 0.7194720506668091]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 8/86 [D loss: 0.7093409299850464, acc.: 46.39%] [G loss: 0.7135399580001831]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 9/86 [D loss: 0.714451938867569, acc.: 44.58%] [G loss: 0.7015214562416077]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 10/86 [D loss: 0.7132242321968079, acc.: 44.97%] [G loss: 0.7001779675483704]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 11/86 [D loss: 0.7117871344089508, acc.: 45.12%] [G loss: 0.6959585547447205]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 12/86 [D loss: 0.7134819030761719, acc.: 44.29%] [G loss: 0.7027666568756104]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 13/86 [D loss: 0.7119929194450378, acc.: 44.19%] [G loss: 0.6927664279937744]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 14/86 [D loss: 0.7092092931270599, acc.: 45.46%] [G loss: 0.691042959690094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 15/86 [D loss: 0.7037688791751862, acc.: 47.46%] [G loss: 0.6960987448692322]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 16/86 [D loss: 0.7043479681015015, acc.: 46.00%] [G loss: 0.6975275278091431]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 17/86 [D loss: 0.7094696462154388, acc.: 45.70%] [G loss: 0.6931975483894348]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 18/86 [D loss: 0.7057689130306244, acc.: 47.71%] [G loss: 0.7031368613243103]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 19/86 [D loss: 0.7065073251724243, acc.: 45.90%] [G loss: 0.703035831451416]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 20/86 [D loss: 0.7087411880493164, acc.: 45.51%] [G loss: 0.715332567691803]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 21/86 [D loss: 0.7014441192150116, acc.: 47.36%] [G loss: 0.7351818680763245]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 22/86 [D loss: 0.6987164318561554, acc.: 50.63%] [G loss: 0.7471147775650024]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 6/200, Batch 23/86 [D loss: 0.6905833780765533, acc.: 53.37%] [G loss: 0.7680253982543945]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 24/86 [D loss: 0.689532071352005, acc.: 51.46%] [G loss: 0.7674535512924194]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 25/86 [D loss: 0.6896296739578247, acc.: 53.32%] [G loss: 0.7745374441146851]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 26/86 [D loss: 0.6878788769245148, acc.: 52.69%] [G loss: 0.7619489431381226]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 27/86 [D loss: 0.6940344870090485, acc.: 51.17%] [G loss: 0.7313696146011353]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 28/86 [D loss: 0.7020688951015472, acc.: 48.44%] [G loss: 0.7197583913803101]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 29/86 [D loss: 0.7094240188598633, acc.: 44.09%] [G loss: 0.7018632292747498]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 30/86 [D loss: 0.7008328437805176, acc.: 46.04%] [G loss: 0.6970706582069397]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 31/86 [D loss: 0.6952572464942932, acc.: 48.14%] [G loss: 0.6978822350502014]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 6/200, Batch 32/86 [D loss: 0.6886110305786133, acc.: 49.66%] [G loss: 0.7048485279083252]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 33/86 [D loss: 0.6815449893474579, acc.: 53.37%] [G loss: 0.7111569046974182]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 34/86 [D loss: 0.6732555627822876, acc.: 57.13%] [G loss: 0.712304413318634]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 35/86 [D loss: 0.6742353141307831, acc.: 57.08%] [G loss: 0.7033010721206665]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 36/86 [D loss: 0.6841790676116943, acc.: 54.30%] [G loss: 0.6850934624671936]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 6/200, Batch 37/86 [D loss: 0.7030717134475708, acc.: 49.07%] [G loss: 0.6657724380493164]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 38/86 [D loss: 0.724189043045044, acc.: 45.51%] [G loss: 0.628817617893219]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 39/86 [D loss: 0.7525869011878967, acc.: 41.50%] [G loss: 0.6275458335876465]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 40/86 [D loss: 0.7527642846107483, acc.: 39.70%] [G loss: 0.6542378067970276]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 41/86 [D loss: 0.7256222069263458, acc.: 43.95%] [G loss: 0.7279930114746094]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 6/200, Batch 42/86 [D loss: 0.6949421763420105, acc.: 50.93%] [G loss: 0.7977907657623291]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 43/86 [D loss: 0.6902930736541748, acc.: 52.59%] [G loss: 0.8126272559165955]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 44/86 [D loss: 0.7009700834751129, acc.: 50.15%] [G loss: 0.7775198221206665]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 45/86 [D loss: 0.7170948088169098, acc.: 46.14%] [G loss: 0.7377510070800781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 46/86 [D loss: 0.7364162802696228, acc.: 38.87%] [G loss: 0.6881505846977234]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 47/86 [D loss: 0.7413462996482849, acc.: 35.84%] [G loss: 0.6627469658851624]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 48/86 [D loss: 0.741019070148468, acc.: 33.94%] [G loss: 0.6694443821907043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 49/86 [D loss: 0.723572164773941, acc.: 39.70%] [G loss: 0.6731305718421936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 50/86 [D loss: 0.7158478498458862, acc.: 41.60%] [G loss: 0.6928085088729858]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 51/86 [D loss: 0.694237232208252, acc.: 50.73%] [G loss: 0.7055196166038513]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 52/86 [D loss: 0.6885515451431274, acc.: 52.93%] [G loss: 0.7155967950820923]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 53/86 [D loss: 0.6842752695083618, acc.: 55.81%] [G loss: 0.7147570848464966]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 54/86 [D loss: 0.6884091198444366, acc.: 52.64%] [G loss: 0.7097089290618896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 55/86 [D loss: 0.695173054933548, acc.: 51.95%] [G loss: 0.6885044574737549]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 56/86 [D loss: 0.7107319831848145, acc.: 48.63%] [G loss: 0.6708313226699829]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 57/86 [D loss: 0.7236884534358978, acc.: 45.51%] [G loss: 0.6654331684112549]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 58/86 [D loss: 0.7211196422576904, acc.: 45.80%] [G loss: 0.6906448006629944]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 59/86 [D loss: 0.7061401307582855, acc.: 50.10%] [G loss: 0.7283861637115479]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 60/86 [D loss: 0.6871342062950134, acc.: 54.69%] [G loss: 0.7778272032737732]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 61/86 [D loss: 0.6740674376487732, acc.: 59.42%] [G loss: 0.8181812167167664]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 62/86 [D loss: 0.6726050972938538, acc.: 59.28%] [G loss: 0.8121842741966248]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 63/86 [D loss: 0.6787869036197662, acc.: 58.01%] [G loss: 0.7991946935653687]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 64/86 [D loss: 0.686982125043869, acc.: 55.32%] [G loss: 0.7579770684242249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 65/86 [D loss: 0.6962261497974396, acc.: 52.49%] [G loss: 0.7294352054595947]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 66/86 [D loss: 0.7046521008014679, acc.: 46.73%] [G loss: 0.7191723585128784]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 67/86 [D loss: 0.6988871991634369, acc.: 49.27%] [G loss: 0.7108831405639648]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 68/86 [D loss: 0.6943714320659637, acc.: 51.07%] [G loss: 0.7092474102973938]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 69/86 [D loss: 0.6887162029743195, acc.: 53.96%] [G loss: 0.7113432884216309]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 70/86 [D loss: 0.6813872754573822, acc.: 57.52%] [G loss: 0.7171904444694519]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 71/86 [D loss: 0.6784407794475555, acc.: 58.74%] [G loss: 0.7261788845062256]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 72/86 [D loss: 0.6777819693088531, acc.: 58.20%] [G loss: 0.7199910283088684]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 73/86 [D loss: 0.6774492561817169, acc.: 58.98%] [G loss: 0.7076242566108704]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 74/86 [D loss: 0.6865969598293304, acc.: 54.15%] [G loss: 0.6911497712135315]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 75/86 [D loss: 0.7025956809520721, acc.: 50.05%] [G loss: 0.6696475744247437]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 76/86 [D loss: 0.7136520445346832, acc.: 49.27%] [G loss: 0.6623742580413818]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 77/86 [D loss: 0.7219663262367249, acc.: 45.02%] [G loss: 0.6576467752456665]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 78/86 [D loss: 0.7229354679584503, acc.: 43.60%] [G loss: 0.683366060256958]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 79/86 [D loss: 0.7068367004394531, acc.: 46.63%] [G loss: 0.7249448895454407]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 80/86 [D loss: 0.6960477232933044, acc.: 49.66%] [G loss: 0.7662939429283142]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 81/86 [D loss: 0.6876863837242126, acc.: 52.88%] [G loss: 0.7869942784309387]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 6/200, Batch 82/86 [D loss: 0.6932238936424255, acc.: 51.56%] [G loss: 0.7874706983566284]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 83/86 [D loss: 0.6984311044216156, acc.: 50.83%] [G loss: 0.7570088505744934]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 84/86 [D loss: 0.7071343958377838, acc.: 47.90%] [G loss: 0.7322601079940796]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 85/86 [D loss: 0.7195626497268677, acc.: 41.60%] [G loss: 0.7012628316879272]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 6/200, Batch 86/86 [D loss: 0.7220770418643951, acc.: 40.33%] [G loss: 0.6826674342155457]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 1/86 [D loss: 0.7163143754005432, acc.: 41.65%] [G loss: 0.6808679103851318]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 2/86 [D loss: 0.713186115026474, acc.: 41.55%] [G loss: 0.6880676746368408]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 3/86 [D loss: 0.7075024545192719, acc.: 44.34%] [G loss: 0.6938743591308594]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 4/86 [D loss: 0.6967211961746216, acc.: 49.71%] [G loss: 0.7046706676483154]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 5/86 [D loss: 0.6874617636203766, acc.: 52.83%] [G loss: 0.7097270488739014]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 6/86 [D loss: 0.6824981868267059, acc.: 54.10%] [G loss: 0.7177092432975769]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 7/86 [D loss: 0.679203599691391, acc.: 55.66%] [G loss: 0.7165209650993347]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 8/86 [D loss: 0.6817750334739685, acc.: 55.27%] [G loss: 0.7063343524932861]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 9/86 [D loss: 0.6933649480342865, acc.: 51.51%] [G loss: 0.6923015117645264]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 10/86 [D loss: 0.7022846937179565, acc.: 46.44%] [G loss: 0.6699416637420654]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 11/86 [D loss: 0.7141031920909882, acc.: 44.29%] [G loss: 0.6574362516403198]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 12/86 [D loss: 0.7251547574996948, acc.: 42.33%] [G loss: 0.6661104559898376]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 13/86 [D loss: 0.7191888988018036, acc.: 43.31%] [G loss: 0.6922070384025574]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 7/200, Batch 14/86 [D loss: 0.7041933536529541, acc.: 48.78%] [G loss: 0.7418406009674072]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 15/86 [D loss: 0.6868908107280731, acc.: 54.35%] [G loss: 0.794298529624939]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 16/86 [D loss: 0.6770885288715363, acc.: 57.81%] [G loss: 0.8099526762962341]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 17/86 [D loss: 0.6776034533977509, acc.: 57.37%] [G loss: 0.8003650903701782]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 18/86 [D loss: 0.6831142902374268, acc.: 56.74%] [G loss: 0.7712602019309998]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 19/86 [D loss: 0.6950100660324097, acc.: 52.59%] [G loss: 0.7398132681846619]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 20/86 [D loss: 0.7079662680625916, acc.: 48.00%] [G loss: 0.7077027559280396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 21/86 [D loss: 0.7116566300392151, acc.: 44.34%] [G loss: 0.6936368346214294]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 22/86 [D loss: 0.7147137224674225, acc.: 42.77%] [G loss: 0.6905475854873657]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 23/86 [D loss: 0.7080808877944946, acc.: 43.90%] [G loss: 0.6921113729476929]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 7/200, Batch 24/86 [D loss: 0.6983231008052826, acc.: 48.34%] [G loss: 0.7067473530769348]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 25/86 [D loss: 0.6889426112174988, acc.: 52.05%] [G loss: 0.7143526673316956]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 26/86 [D loss: 0.6778312921524048, acc.: 56.64%] [G loss: 0.7220443487167358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 27/86 [D loss: 0.6724806427955627, acc.: 59.96%] [G loss: 0.724265992641449]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 28/86 [D loss: 0.6712210774421692, acc.: 60.40%] [G loss: 0.7290638089179993]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 29/86 [D loss: 0.6744464933872223, acc.: 57.62%] [G loss: 0.7104014158248901]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 30/86 [D loss: 0.6842715740203857, acc.: 53.56%] [G loss: 0.7011055946350098]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 31/86 [D loss: 0.6983108818531036, acc.: 49.66%] [G loss: 0.6750545501708984]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 32/86 [D loss: 0.7119917869567871, acc.: 44.78%] [G loss: 0.6581565737724304]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 33/86 [D loss: 0.7290960252285004, acc.: 41.50%] [G loss: 0.6584954261779785]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 34/86 [D loss: 0.7226341664791107, acc.: 44.24%] [G loss: 0.6728650331497192]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 35/86 [D loss: 0.7122610211372375, acc.: 45.61%] [G loss: 0.7213155031204224]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 36/86 [D loss: 0.6902027130126953, acc.: 52.93%] [G loss: 0.7651075720787048]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 37/86 [D loss: 0.6819654107093811, acc.: 56.30%] [G loss: 0.8035570979118347]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 38/86 [D loss: 0.6723798513412476, acc.: 59.91%] [G loss: 0.810912013053894]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 39/86 [D loss: 0.6784588694572449, acc.: 59.28%] [G loss: 0.7899190187454224]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 40/86 [D loss: 0.6861933171749115, acc.: 57.08%] [G loss: 0.758368730545044]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 41/86 [D loss: 0.7063766717910767, acc.: 48.39%] [G loss: 0.7307490706443787]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 42/86 [D loss: 0.7155019044876099, acc.: 43.65%] [G loss: 0.7010588049888611]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 43/86 [D loss: 0.7168327867984772, acc.: 41.94%] [G loss: 0.6958983540534973]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 44/86 [D loss: 0.7171631157398224, acc.: 41.11%] [G loss: 0.6919348239898682]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 45/86 [D loss: 0.7090065777301788, acc.: 43.55%] [G loss: 0.6978437900543213]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 46/86 [D loss: 0.7009174227714539, acc.: 46.92%] [G loss: 0.7062373757362366]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 47/86 [D loss: 0.6911203861236572, acc.: 51.56%] [G loss: 0.7165907621383667]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 48/86 [D loss: 0.681034117937088, acc.: 56.49%] [G loss: 0.7229557037353516]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 49/86 [D loss: 0.6808907985687256, acc.: 57.37%] [G loss: 0.7316091060638428]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 50/86 [D loss: 0.6790451109409332, acc.: 57.67%] [G loss: 0.731709361076355]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 51/86 [D loss: 0.6743857860565186, acc.: 59.47%] [G loss: 0.7209512591362]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 52/86 [D loss: 0.6883175373077393, acc.: 53.32%] [G loss: 0.7093359231948853]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 53/86 [D loss: 0.6970793306827545, acc.: 51.17%] [G loss: 0.6913356781005859]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 54/86 [D loss: 0.7064277529716492, acc.: 47.27%] [G loss: 0.6753103137016296]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 55/86 [D loss: 0.7214060723781586, acc.: 44.73%] [G loss: 0.6633918285369873]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 56/86 [D loss: 0.7225949764251709, acc.: 44.14%] [G loss: 0.6795876622200012]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 57/86 [D loss: 0.722849428653717, acc.: 43.36%] [G loss: 0.7018707394599915]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 58/86 [D loss: 0.7094205617904663, acc.: 45.61%] [G loss: 0.7464368343353271]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 59/86 [D loss: 0.6923505067825317, acc.: 53.47%] [G loss: 0.7857094407081604]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 60/86 [D loss: 0.6863311529159546, acc.: 54.98%] [G loss: 0.8044306635856628]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 61/86 [D loss: 0.6756360828876495, acc.: 59.08%] [G loss: 0.8127791285514832]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 62/86 [D loss: 0.6871176064014435, acc.: 55.03%] [G loss: 0.8103526830673218]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 63/86 [D loss: 0.6924873888492584, acc.: 54.88%] [G loss: 0.7806448340415955]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 64/86 [D loss: 0.704880952835083, acc.: 49.90%] [G loss: 0.7517355680465698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 65/86 [D loss: 0.7159638702869415, acc.: 44.14%] [G loss: 0.7220363616943359]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 66/86 [D loss: 0.712756484746933, acc.: 44.43%] [G loss: 0.7094826698303223]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 67/86 [D loss: 0.7162877321243286, acc.: 42.29%] [G loss: 0.7008323073387146]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 68/86 [D loss: 0.7136189341545105, acc.: 42.72%] [G loss: 0.7009278535842896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 69/86 [D loss: 0.7071142494678497, acc.: 45.85%] [G loss: 0.7025347352027893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 70/86 [D loss: 0.701223611831665, acc.: 47.46%] [G loss: 0.7094990611076355]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 71/86 [D loss: 0.6976516246795654, acc.: 49.66%] [G loss: 0.7155036926269531]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 72/86 [D loss: 0.6900770664215088, acc.: 53.27%] [G loss: 0.7209392786026001]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 73/86 [D loss: 0.6815218031406403, acc.: 57.23%] [G loss: 0.7282879948616028]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 74/86 [D loss: 0.6802985966205597, acc.: 57.28%] [G loss: 0.7270089983940125]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 75/86 [D loss: 0.6820457279682159, acc.: 56.59%] [G loss: 0.7212315201759338]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 76/86 [D loss: 0.6828421056270599, acc.: 56.54%] [G loss: 0.7131180167198181]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 77/86 [D loss: 0.6959967911243439, acc.: 51.42%] [G loss: 0.6963980197906494]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 78/86 [D loss: 0.7013184428215027, acc.: 48.63%] [G loss: 0.6841189861297607]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 79/86 [D loss: 0.7228457629680634, acc.: 42.24%] [G loss: 0.6725403070449829]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 80/86 [D loss: 0.7288418710231781, acc.: 41.16%] [G loss: 0.6653211116790771]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 81/86 [D loss: 0.7308982312679291, acc.: 38.62%] [G loss: 0.6747738122940063]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 82/86 [D loss: 0.727295994758606, acc.: 38.96%] [G loss: 0.6958968639373779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 7/200, Batch 83/86 [D loss: 0.7194100022315979, acc.: 42.19%] [G loss: 0.721229076385498]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 84/86 [D loss: 0.703790009021759, acc.: 46.19%] [G loss: 0.7628793716430664]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 7/200, Batch 85/86 [D loss: 0.6965018212795258, acc.: 49.90%] [G loss: 0.7857083082199097]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 86/86 [D loss: 0.6918960511684418, acc.: 52.93%] [G loss: 0.8004187345504761]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 1/86 [D loss: 0.693876713514328, acc.: 50.93%] [G loss: 0.7945862412452698]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 2/86 [D loss: 0.697633683681488, acc.: 51.51%] [G loss: 0.7807358503341675]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 3/86 [D loss: 0.7048375606536865, acc.: 49.56%] [G loss: 0.7542981505393982]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 4/86 [D loss: 0.7157440185546875, acc.: 44.78%] [G loss: 0.7328980565071106]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 5/86 [D loss: 0.7181384563446045, acc.: 43.07%] [G loss: 0.7117087244987488]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 6/86 [D loss: 0.7235454022884369, acc.: 41.11%] [G loss: 0.7008466124534607]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 7/86 [D loss: 0.7258755564689636, acc.: 38.87%] [G loss: 0.6923081874847412]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 8/86 [D loss: 0.7155969738960266, acc.: 42.48%] [G loss: 0.6899479031562805]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 8/200, Batch 9/86 [D loss: 0.7160637974739075, acc.: 40.87%] [G loss: 0.6945427060127258]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 10/86 [D loss: 0.7058775424957275, acc.: 45.70%] [G loss: 0.7007230520248413]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 11/86 [D loss: 0.6995427012443542, acc.: 49.32%] [G loss: 0.7077247500419617]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 12/86 [D loss: 0.6910389363765717, acc.: 52.78%] [G loss: 0.713957667350769]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 13/86 [D loss: 0.6906313896179199, acc.: 51.90%] [G loss: 0.7187345623970032]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 14/86 [D loss: 0.6838789582252502, acc.: 56.45%] [G loss: 0.7164608836174011]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 15/86 [D loss: 0.6836465895175934, acc.: 56.05%] [G loss: 0.715253472328186]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 16/86 [D loss: 0.6856957674026489, acc.: 54.83%] [G loss: 0.7038886547088623]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 17/86 [D loss: 0.6939087808132172, acc.: 51.22%] [G loss: 0.6963423490524292]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 18/86 [D loss: 0.7044894695281982, acc.: 47.17%] [G loss: 0.6827631592750549]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 8/200, Batch 19/86 [D loss: 0.7062650322914124, acc.: 47.75%] [G loss: 0.6786742210388184]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 20/86 [D loss: 0.7156191766262054, acc.: 43.31%] [G loss: 0.6742515563964844]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 21/86 [D loss: 0.7237220704555511, acc.: 40.58%] [G loss: 0.6806399822235107]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 22/86 [D loss: 0.7182311415672302, acc.: 41.50%] [G loss: 0.702944815158844]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 23/86 [D loss: 0.7092439830303192, acc.: 45.31%] [G loss: 0.7402709126472473]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 24/86 [D loss: 0.6964350640773773, acc.: 50.10%] [G loss: 0.7877234220504761]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 25/86 [D loss: 0.681443452835083, acc.: 55.96%] [G loss: 0.8160809278488159]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 26/86 [D loss: 0.6674307286739349, acc.: 59.57%] [G loss: 0.8498520255088806]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 27/86 [D loss: 0.6617710292339325, acc.: 61.87%] [G loss: 0.8586140871047974]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 8/200, Batch 28/86 [D loss: 0.669084757566452, acc.: 60.64%] [G loss: 0.8420654535293579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 29/86 [D loss: 0.6770275235176086, acc.: 59.81%] [G loss: 0.8178667426109314]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 30/86 [D loss: 0.6826058328151703, acc.: 57.96%] [G loss: 0.7822960615158081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 31/86 [D loss: 0.6930747926235199, acc.: 54.98%] [G loss: 0.7539852857589722]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 32/86 [D loss: 0.7029471695423126, acc.: 49.95%] [G loss: 0.7270370721817017]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 8/200, Batch 33/86 [D loss: 0.7071764767169952, acc.: 47.02%] [G loss: 0.7189427018165588]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 34/86 [D loss: 0.7046807110309601, acc.: 47.02%] [G loss: 0.720026969909668]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 35/86 [D loss: 0.6988120973110199, acc.: 48.73%] [G loss: 0.715681791305542]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 36/86 [D loss: 0.6897858381271362, acc.: 52.83%] [G loss: 0.7249597311019897]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 37/86 [D loss: 0.679316520690918, acc.: 57.23%] [G loss: 0.7355139255523682]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 8/200, Batch 38/86 [D loss: 0.6688258647918701, acc.: 65.09%] [G loss: 0.7483360767364502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 39/86 [D loss: 0.6589727997779846, acc.: 69.43%] [G loss: 0.7620556950569153]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 40/86 [D loss: 0.6492514908313751, acc.: 74.71%] [G loss: 0.777798056602478]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 41/86 [D loss: 0.640926331281662, acc.: 76.95%] [G loss: 0.7795734405517578]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 42/86 [D loss: 0.6421190798282623, acc.: 73.58%] [G loss: 0.7731890082359314]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 43/86 [D loss: 0.6482318639755249, acc.: 68.16%] [G loss: 0.7516278624534607]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 44/86 [D loss: 0.6706431806087494, acc.: 58.79%] [G loss: 0.716055154800415]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 45/86 [D loss: 0.6922974586486816, acc.: 49.90%] [G loss: 0.6720625162124634]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 46/86 [D loss: 0.7225721478462219, acc.: 43.90%] [G loss: 0.634784996509552]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 47/86 [D loss: 0.7467750012874603, acc.: 39.50%] [G loss: 0.6155474185943604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 48/86 [D loss: 0.7597158253192902, acc.: 36.91%] [G loss: 0.6155259609222412]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 49/86 [D loss: 0.7526296973228455, acc.: 36.18%] [G loss: 0.6572234034538269]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 50/86 [D loss: 0.7318661212921143, acc.: 40.38%] [G loss: 0.7273622751235962]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 51/86 [D loss: 0.7003195881843567, acc.: 48.00%] [G loss: 0.7984775304794312]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 52/86 [D loss: 0.6770822703838348, acc.: 55.42%] [G loss: 0.860273003578186]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 53/86 [D loss: 0.6685404181480408, acc.: 57.13%] [G loss: 0.8821309208869934]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 54/86 [D loss: 0.6673372685909271, acc.: 60.01%] [G loss: 0.8609984517097473]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 55/86 [D loss: 0.6814256310462952, acc.: 56.98%] [G loss: 0.8144297003746033]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 56/86 [D loss: 0.7025889754295349, acc.: 49.80%] [G loss: 0.7715378403663635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 57/86 [D loss: 0.7180654406547546, acc.: 44.29%] [G loss: 0.7237237691879272]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 58/86 [D loss: 0.7291624546051025, acc.: 40.77%] [G loss: 0.6986382007598877]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 8/200, Batch 59/86 [D loss: 0.7339341938495636, acc.: 36.96%] [G loss: 0.6845496892929077]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 60/86 [D loss: 0.7327809035778046, acc.: 36.57%] [G loss: 0.6757365465164185]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 61/86 [D loss: 0.7326769828796387, acc.: 36.13%] [G loss: 0.6784638166427612]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 62/86 [D loss: 0.7213132977485657, acc.: 39.06%] [G loss: 0.6859407424926758]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 63/86 [D loss: 0.7118662595748901, acc.: 43.65%] [G loss: 0.696918785572052]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 64/86 [D loss: 0.7050748765468597, acc.: 46.48%] [G loss: 0.7101460099220276]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 65/86 [D loss: 0.6926268041133881, acc.: 53.22%] [G loss: 0.7250086069107056]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 66/86 [D loss: 0.6865859627723694, acc.: 56.25%] [G loss: 0.7325674891471863]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 67/86 [D loss: 0.6785166263580322, acc.: 58.69%] [G loss: 0.7444457411766052]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 68/86 [D loss: 0.6819760501384735, acc.: 56.84%] [G loss: 0.7364092469215393]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 69/86 [D loss: 0.6788792908191681, acc.: 57.67%] [G loss: 0.7391491532325745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 70/86 [D loss: 0.6804136335849762, acc.: 55.66%] [G loss: 0.7176154851913452]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 71/86 [D loss: 0.6973466575145721, acc.: 48.44%] [G loss: 0.7033429741859436]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 72/86 [D loss: 0.7051371335983276, acc.: 48.39%] [G loss: 0.6918278336524963]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 73/86 [D loss: 0.7143237590789795, acc.: 45.70%] [G loss: 0.6802439093589783]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 74/86 [D loss: 0.721426248550415, acc.: 42.58%] [G loss: 0.6768242120742798]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 75/86 [D loss: 0.7220038175582886, acc.: 41.85%] [G loss: 0.6854739785194397]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 76/86 [D loss: 0.7170903384685516, acc.: 41.65%] [G loss: 0.718521237373352]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 77/86 [D loss: 0.7034630179405212, acc.: 46.48%] [G loss: 0.7449774742126465]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 78/86 [D loss: 0.6888100206851959, acc.: 51.66%] [G loss: 0.7955791354179382]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 79/86 [D loss: 0.6804391443729401, acc.: 53.86%] [G loss: 0.8329389095306396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 80/86 [D loss: 0.665754646062851, acc.: 59.23%] [G loss: 0.8539884090423584]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 81/86 [D loss: 0.6591338217258453, acc.: 63.28%] [G loss: 0.8610689640045166]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 82/86 [D loss: 0.6642172336578369, acc.: 63.13%] [G loss: 0.8535867929458618]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 8/200, Batch 83/86 [D loss: 0.6686395704746246, acc.: 62.45%] [G loss: 0.8253681063652039]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 84/86 [D loss: 0.6850206255912781, acc.: 58.79%] [G loss: 0.798193097114563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 85/86 [D loss: 0.6921654641628265, acc.: 55.52%] [G loss: 0.7702259421348572]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 8/200, Batch 86/86 [D loss: 0.6973206400871277, acc.: 53.47%] [G loss: 0.7504696249961853]\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 1/86 [D loss: 0.7037658393383026, acc.: 50.54%] [G loss: 0.729550838470459]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 2/86 [D loss: 0.7054182291030884, acc.: 47.12%] [G loss: 0.714936375617981]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 3/86 [D loss: 0.7057289481163025, acc.: 46.34%] [G loss: 0.7117648720741272]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 4/86 [D loss: 0.6999193429946899, acc.: 50.05%] [G loss: 0.711974024772644]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 5/86 [D loss: 0.6981693506240845, acc.: 49.85%] [G loss: 0.7148913145065308]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 6/86 [D loss: 0.6939364075660706, acc.: 51.71%] [G loss: 0.7211689949035645]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 7/86 [D loss: 0.6861369609832764, acc.: 56.54%] [G loss: 0.7287955284118652]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 8/86 [D loss: 0.6785285174846649, acc.: 61.38%] [G loss: 0.7411824464797974]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 9/86 [D loss: 0.6726792454719543, acc.: 62.79%] [G loss: 0.7501706480979919]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 10/86 [D loss: 0.6701951324939728, acc.: 63.87%] [G loss: 0.7590039372444153]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 11/86 [D loss: 0.6665681600570679, acc.: 64.84%] [G loss: 0.7517095804214478]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 12/86 [D loss: 0.6701568365097046, acc.: 61.62%] [G loss: 0.7441429495811462]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 13/86 [D loss: 0.6700587868690491, acc.: 60.35%] [G loss: 0.7365735769271851]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 14/86 [D loss: 0.681650847196579, acc.: 56.59%] [G loss: 0.726925253868103]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 15/86 [D loss: 0.6931709349155426, acc.: 50.24%] [G loss: 0.7066594958305359]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 16/86 [D loss: 0.7012264132499695, acc.: 47.90%] [G loss: 0.6855375170707703]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 17/86 [D loss: 0.7134634256362915, acc.: 43.36%] [G loss: 0.6688804626464844]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 18/86 [D loss: 0.7237773835659027, acc.: 39.89%] [G loss: 0.6704532504081726]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 19/86 [D loss: 0.7290965020656586, acc.: 37.06%] [G loss: 0.6734996438026428]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 20/86 [D loss: 0.7260318398475647, acc.: 36.52%] [G loss: 0.6913182735443115]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 21/86 [D loss: 0.7172319293022156, acc.: 41.55%] [G loss: 0.7241389751434326]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 22/86 [D loss: 0.7011671662330627, acc.: 49.56%] [G loss: 0.7552796602249146]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 23/86 [D loss: 0.6848211586475372, acc.: 55.27%] [G loss: 0.8057309985160828]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 24/86 [D loss: 0.674472838640213, acc.: 58.50%] [G loss: 0.8345680236816406]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 25/86 [D loss: 0.6697283089160919, acc.: 61.77%] [G loss: 0.8485187292098999]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 9/200, Batch 26/86 [D loss: 0.6684055328369141, acc.: 62.21%] [G loss: 0.845525324344635]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 27/86 [D loss: 0.6746558248996735, acc.: 60.99%] [G loss: 0.8305938243865967]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 28/86 [D loss: 0.6840525567531586, acc.: 58.59%] [G loss: 0.7994098663330078]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 29/86 [D loss: 0.6933325827121735, acc.: 55.71%] [G loss: 0.771031379699707]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 30/86 [D loss: 0.703052818775177, acc.: 51.61%] [G loss: 0.742396891117096]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 9/200, Batch 31/86 [D loss: 0.7114543914794922, acc.: 46.39%] [G loss: 0.7212790250778198]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 32/86 [D loss: 0.7143995463848114, acc.: 43.31%] [G loss: 0.704785943031311]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 33/86 [D loss: 0.7144238650798798, acc.: 42.72%] [G loss: 0.696720540523529]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 34/86 [D loss: 0.7078456580638885, acc.: 45.36%] [G loss: 0.6991748809814453]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 35/86 [D loss: 0.7067735493183136, acc.: 45.85%] [G loss: 0.7077227830886841]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 36/86 [D loss: 0.7006580829620361, acc.: 47.66%] [G loss: 0.7175251841545105]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 37/86 [D loss: 0.6876541972160339, acc.: 53.42%] [G loss: 0.7269895076751709]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 38/86 [D loss: 0.6756622791290283, acc.: 61.52%] [G loss: 0.7412906885147095]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 39/86 [D loss: 0.666301965713501, acc.: 66.89%] [G loss: 0.7546665668487549]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 40/86 [D loss: 0.657264918088913, acc.: 73.24%] [G loss: 0.7687333822250366]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 41/86 [D loss: 0.6461531817913055, acc.: 75.59%] [G loss: 0.7815402746200562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 42/86 [D loss: 0.6438945233821869, acc.: 74.95%] [G loss: 0.7840944528579712]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 43/86 [D loss: 0.6404006481170654, acc.: 74.32%] [G loss: 0.7875775098800659]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 44/86 [D loss: 0.6445010602474213, acc.: 70.70%] [G loss: 0.7727428078651428]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 45/86 [D loss: 0.648082047700882, acc.: 67.92%] [G loss: 0.7572166323661804]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 46/86 [D loss: 0.6597668528556824, acc.: 62.60%] [G loss: 0.7226049304008484]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 47/86 [D loss: 0.6830528378486633, acc.: 53.71%] [G loss: 0.6943196058273315]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 48/86 [D loss: 0.699612557888031, acc.: 47.85%] [G loss: 0.6619471311569214]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 49/86 [D loss: 0.7149536311626434, acc.: 44.34%] [G loss: 0.6370629668235779]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 50/86 [D loss: 0.7349906861782074, acc.: 38.96%] [G loss: 0.6398741006851196]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 51/86 [D loss: 0.7368529736995697, acc.: 37.35%] [G loss: 0.6543560028076172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 52/86 [D loss: 0.72598597407341, acc.: 39.40%] [G loss: 0.695914626121521]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 53/86 [D loss: 0.7023027837276459, acc.: 48.14%] [G loss: 0.7553708553314209]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 54/86 [D loss: 0.6758514046669006, acc.: 56.35%] [G loss: 0.8256877660751343]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 55/86 [D loss: 0.6522121727466583, acc.: 65.23%] [G loss: 0.8939909934997559]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 56/86 [D loss: 0.6388916969299316, acc.: 67.97%] [G loss: 0.9327915906906128]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 57/86 [D loss: 0.6407860815525055, acc.: 67.43%] [G loss: 0.9276157021522522]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 58/86 [D loss: 0.6452370285987854, acc.: 68.90%] [G loss: 0.8982892632484436]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 59/86 [D loss: 0.6605404913425446, acc.: 66.11%] [G loss: 0.8464035987854004]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 60/86 [D loss: 0.6779018640518188, acc.: 60.25%] [G loss: 0.792462944984436]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 61/86 [D loss: 0.6950590014457703, acc.: 54.20%] [G loss: 0.7513973116874695]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 62/86 [D loss: 0.7133238911628723, acc.: 46.14%] [G loss: 0.7119488716125488]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 63/86 [D loss: 0.7279919683933258, acc.: 40.28%] [G loss: 0.6894049644470215]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 64/86 [D loss: 0.7245230078697205, acc.: 40.38%] [G loss: 0.6731075048446655]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 9/200, Batch 65/86 [D loss: 0.7267507910728455, acc.: 38.48%] [G loss: 0.6721600294113159]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 66/86 [D loss: 0.7184351682662964, acc.: 39.45%] [G loss: 0.675195038318634]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 67/86 [D loss: 0.7111563682556152, acc.: 42.68%] [G loss: 0.6823381781578064]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 68/86 [D loss: 0.70180544257164, acc.: 45.90%] [G loss: 0.6874024271965027]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 69/86 [D loss: 0.6950284838676453, acc.: 48.24%] [G loss: 0.7039622068405151]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 70/86 [D loss: 0.6826053559780121, acc.: 55.96%] [G loss: 0.7203308939933777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 71/86 [D loss: 0.6743198037147522, acc.: 59.81%] [G loss: 0.7311175465583801]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 72/86 [D loss: 0.6675612926483154, acc.: 64.60%] [G loss: 0.7454162836074829]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 73/86 [D loss: 0.6611923575401306, acc.: 67.14%] [G loss: 0.7457371354103088]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 74/86 [D loss: 0.6563802659511566, acc.: 69.19%] [G loss: 0.7503624558448792]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 75/86 [D loss: 0.6595643162727356, acc.: 66.50%] [G loss: 0.7375551462173462]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 76/86 [D loss: 0.6649908423423767, acc.: 61.82%] [G loss: 0.7181136608123779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 77/86 [D loss: 0.6757478415966034, acc.: 56.54%] [G loss: 0.6998112797737122]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 78/86 [D loss: 0.6942022442817688, acc.: 48.63%] [G loss: 0.6713254451751709]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 79/86 [D loss: 0.717132568359375, acc.: 42.33%] [G loss: 0.6449565291404724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 80/86 [D loss: 0.7334988713264465, acc.: 37.94%] [G loss: 0.631511390209198]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 81/86 [D loss: 0.7424928247928619, acc.: 35.30%] [G loss: 0.6318284869194031]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 9/200, Batch 82/86 [D loss: 0.7425230741500854, acc.: 35.84%] [G loss: 0.6378195285797119]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 83/86 [D loss: 0.7377342581748962, acc.: 34.62%] [G loss: 0.6610929369926453]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 9/200, Batch 84/86 [D loss: 0.724875807762146, acc.: 37.89%] [G loss: 0.7026526927947998]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 85/86 [D loss: 0.7041346430778503, acc.: 47.02%] [G loss: 0.7493046522140503]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 86/86 [D loss: 0.6879072189331055, acc.: 51.90%] [G loss: 0.7982115745544434]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 1/86 [D loss: 0.6745062470436096, acc.: 58.79%] [G loss: 0.831757664680481]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 2/86 [D loss: 0.6640401482582092, acc.: 61.96%] [G loss: 0.8535487651824951]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 10/200, Batch 3/86 [D loss: 0.6621450781822205, acc.: 62.11%] [G loss: 0.8626666069030762]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 4/86 [D loss: 0.6674843430519104, acc.: 61.38%] [G loss: 0.8521287441253662]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 5/86 [D loss: 0.6718193292617798, acc.: 62.16%] [G loss: 0.8282390236854553]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 6/86 [D loss: 0.6807476878166199, acc.: 60.50%] [G loss: 0.8024144172668457]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 7/86 [D loss: 0.6929035782814026, acc.: 55.71%] [G loss: 0.7684317231178284]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 8/86 [D loss: 0.7020482420921326, acc.: 51.86%] [G loss: 0.7480481863021851]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 9/86 [D loss: 0.7060113251209259, acc.: 48.78%] [G loss: 0.7320532202720642]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 10/86 [D loss: 0.7081424593925476, acc.: 46.29%] [G loss: 0.7182971835136414]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 11/86 [D loss: 0.7074130475521088, acc.: 45.70%] [G loss: 0.7150222659111023]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 12/86 [D loss: 0.7032303512096405, acc.: 46.97%] [G loss: 0.7140458226203918]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 13/86 [D loss: 0.7031294703483582, acc.: 46.68%] [G loss: 0.7167454957962036]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 14/86 [D loss: 0.6977199018001556, acc.: 50.10%] [G loss: 0.7223461866378784]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 15/86 [D loss: 0.688472181558609, acc.: 56.30%] [G loss: 0.7344943284988403]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 16/86 [D loss: 0.6791459023952484, acc.: 60.74%] [G loss: 0.7413122653961182]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 10/200, Batch 17/86 [D loss: 0.6716301143169403, acc.: 64.55%] [G loss: 0.7552230358123779]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 18/86 [D loss: 0.6675809323787689, acc.: 65.43%] [G loss: 0.7629658579826355]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 19/86 [D loss: 0.6616823971271515, acc.: 67.09%] [G loss: 0.7628562450408936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 20/86 [D loss: 0.660435289144516, acc.: 66.50%] [G loss: 0.7643325328826904]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 21/86 [D loss: 0.6615834832191467, acc.: 66.02%] [G loss: 0.7619875073432922]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 22/86 [D loss: 0.6653818786144257, acc.: 62.94%] [G loss: 0.7503026723861694]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 23/86 [D loss: 0.6711810529232025, acc.: 61.18%] [G loss: 0.7378535270690918]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 24/86 [D loss: 0.6809475719928741, acc.: 55.86%] [G loss: 0.7225478887557983]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 25/86 [D loss: 0.6947879791259766, acc.: 50.29%] [G loss: 0.7017449736595154]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 26/86 [D loss: 0.7072746455669403, acc.: 45.56%] [G loss: 0.6800529956817627]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 27/86 [D loss: 0.7136786878108978, acc.: 42.58%] [G loss: 0.6776674389839172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 28/86 [D loss: 0.7168316543102264, acc.: 42.04%] [G loss: 0.6813458800315857]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 29/86 [D loss: 0.7198222577571869, acc.: 40.43%] [G loss: 0.6918187737464905]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 10/200, Batch 30/86 [D loss: 0.7123599648475647, acc.: 42.14%] [G loss: 0.7103389501571655]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 31/86 [D loss: 0.7024350464344025, acc.: 45.61%] [G loss: 0.7335355877876282]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 32/86 [D loss: 0.69550621509552, acc.: 50.49%] [G loss: 0.7692781686782837]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 33/86 [D loss: 0.6857410371303558, acc.: 54.98%] [G loss: 0.7982796430587769]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 34/86 [D loss: 0.6764255166053772, acc.: 58.50%] [G loss: 0.8210729956626892]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 35/86 [D loss: 0.6664881408214569, acc.: 60.94%] [G loss: 0.8375071287155151]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 36/86 [D loss: 0.6643038690090179, acc.: 62.74%] [G loss: 0.84971022605896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 37/86 [D loss: 0.665443480014801, acc.: 62.84%] [G loss: 0.8435419201850891]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 38/86 [D loss: 0.6688135862350464, acc.: 62.89%] [G loss: 0.8300129175186157]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 39/86 [D loss: 0.6794803142547607, acc.: 59.62%] [G loss: 0.8013989329338074]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 40/86 [D loss: 0.6869007050991058, acc.: 58.25%] [G loss: 0.7758652567863464]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 41/86 [D loss: 0.6942038536071777, acc.: 54.88%] [G loss: 0.7538187503814697]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 42/86 [D loss: 0.7053596377372742, acc.: 49.71%] [G loss: 0.7329062223434448]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 43/86 [D loss: 0.7083712220191956, acc.: 46.09%] [G loss: 0.7199187278747559]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 44/86 [D loss: 0.7121692299842834, acc.: 43.02%] [G loss: 0.7066855430603027]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 45/86 [D loss: 0.7104969024658203, acc.: 42.97%] [G loss: 0.6958997249603271]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 10/200, Batch 46/86 [D loss: 0.7112832069396973, acc.: 41.11%] [G loss: 0.7010206580162048]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 47/86 [D loss: 0.7029278576374054, acc.: 43.80%] [G loss: 0.7035835385322571]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 48/86 [D loss: 0.6963576376438141, acc.: 48.34%] [G loss: 0.70763099193573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 49/86 [D loss: 0.691887766122818, acc.: 50.88%] [G loss: 0.7220118045806885]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 50/86 [D loss: 0.6839234828948975, acc.: 57.67%] [G loss: 0.7321839928627014]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 51/86 [D loss: 0.6690085530281067, acc.: 65.23%] [G loss: 0.7443364262580872]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 10/200, Batch 52/86 [D loss: 0.6607162654399872, acc.: 69.92%] [G loss: 0.7590208649635315]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 53/86 [D loss: 0.6569678783416748, acc.: 71.53%] [G loss: 0.7708690166473389]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 54/86 [D loss: 0.6511340439319611, acc.: 72.17%] [G loss: 0.7784269452095032]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 55/86 [D loss: 0.6483070254325867, acc.: 71.58%] [G loss: 0.7815941572189331]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 56/86 [D loss: 0.6439887285232544, acc.: 72.41%] [G loss: 0.7775639295578003]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 10/200, Batch 57/86 [D loss: 0.6465300619602203, acc.: 68.51%] [G loss: 0.7674398422241211]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 58/86 [D loss: 0.6551977694034576, acc.: 64.26%] [G loss: 0.7531102299690247]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 59/86 [D loss: 0.6689203381538391, acc.: 59.08%] [G loss: 0.7237235903739929]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 60/86 [D loss: 0.6834181249141693, acc.: 54.00%] [G loss: 0.689556360244751]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 61/86 [D loss: 0.7045547366142273, acc.: 47.75%] [G loss: 0.6643667221069336]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 10/200, Batch 62/86 [D loss: 0.7238523960113525, acc.: 42.09%] [G loss: 0.6427075862884521]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 63/86 [D loss: 0.7371219396591187, acc.: 38.48%] [G loss: 0.6355465650558472]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 64/86 [D loss: 0.7466301620006561, acc.: 36.04%] [G loss: 0.6384053230285645]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 65/86 [D loss: 0.7369982302188873, acc.: 35.25%] [G loss: 0.6664692163467407]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 66/86 [D loss: 0.7149739265441895, acc.: 42.24%] [G loss: 0.7239554524421692]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 67/86 [D loss: 0.6934808194637299, acc.: 50.78%] [G loss: 0.7906263470649719]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 68/86 [D loss: 0.6650383770465851, acc.: 62.94%] [G loss: 0.8588457703590393]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 69/86 [D loss: 0.6436037719249725, acc.: 67.29%] [G loss: 0.9174105525016785]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 70/86 [D loss: 0.6296637058258057, acc.: 69.14%] [G loss: 0.9426473379135132]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 71/86 [D loss: 0.6245708465576172, acc.: 72.90%] [G loss: 0.9368417263031006]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 10/200, Batch 72/86 [D loss: 0.6399379670619965, acc.: 69.09%] [G loss: 0.8936901092529297]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 73/86 [D loss: 0.6541005969047546, acc.: 68.90%] [G loss: 0.8497984409332275]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 74/86 [D loss: 0.6749673783779144, acc.: 62.01%] [G loss: 0.7887837290763855]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 75/86 [D loss: 0.6902883648872375, acc.: 55.57%] [G loss: 0.7426853775978088]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 76/86 [D loss: 0.7114976346492767, acc.: 46.00%] [G loss: 0.7000605463981628]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 77/86 [D loss: 0.7186956703662872, acc.: 42.48%] [G loss: 0.6814754605293274]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 78/86 [D loss: 0.7207377254962921, acc.: 39.06%] [G loss: 0.6669718623161316]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 79/86 [D loss: 0.7208921015262604, acc.: 39.31%] [G loss: 0.6634125113487244]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 80/86 [D loss: 0.7116710543632507, acc.: 42.24%] [G loss: 0.6734718680381775]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 81/86 [D loss: 0.7059773206710815, acc.: 42.92%] [G loss: 0.683370053768158]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 10/200, Batch 82/86 [D loss: 0.6943360269069672, acc.: 48.00%] [G loss: 0.699895977973938]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 83/86 [D loss: 0.6805805265903473, acc.: 57.47%] [G loss: 0.7135775089263916]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 84/86 [D loss: 0.6703505516052246, acc.: 63.62%] [G loss: 0.7392886281013489]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 10/200, Batch 85/86 [D loss: 0.6561310291290283, acc.: 73.34%] [G loss: 0.7581327557563782]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 86/86 [D loss: 0.6463939845561981, acc.: 76.71%] [G loss: 0.773450493812561]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 1/86 [D loss: 0.6362590789794922, acc.: 78.56%] [G loss: 0.7883807420730591]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 2/86 [D loss: 0.6292269825935364, acc.: 79.83%] [G loss: 0.8017361164093018]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 3/86 [D loss: 0.6307198703289032, acc.: 75.93%] [G loss: 0.8026031851768494]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 4/86 [D loss: 0.6297695636749268, acc.: 73.83%] [G loss: 0.7892951965332031]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 5/86 [D loss: 0.6427400410175323, acc.: 69.38%] [G loss: 0.7617659568786621]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 6/86 [D loss: 0.6566163599491119, acc.: 60.99%] [G loss: 0.7282688617706299]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 7/86 [D loss: 0.6791670620441437, acc.: 53.56%] [G loss: 0.6936228275299072]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 8/86 [D loss: 0.7025861740112305, acc.: 47.31%] [G loss: 0.650666356086731]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 9/86 [D loss: 0.7292692363262177, acc.: 40.82%] [G loss: 0.6213292479515076]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 10/86 [D loss: 0.742408275604248, acc.: 38.23%] [G loss: 0.6061491370201111]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 11/200, Batch 11/86 [D loss: 0.7509782016277313, acc.: 36.38%] [G loss: 0.609861433506012]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 12/86 [D loss: 0.7451044023036957, acc.: 36.43%] [G loss: 0.6338268518447876]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 13/86 [D loss: 0.7317198812961578, acc.: 37.30%] [G loss: 0.6821020841598511]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 14/86 [D loss: 0.7046694159507751, acc.: 46.78%] [G loss: 0.7390838861465454]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 15/86 [D loss: 0.6807231903076172, acc.: 56.59%] [G loss: 0.7977314591407776]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 16/86 [D loss: 0.6643050909042358, acc.: 62.60%] [G loss: 0.8352208733558655]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 17/86 [D loss: 0.6553559303283691, acc.: 64.70%] [G loss: 0.8684552907943726]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 18/86 [D loss: 0.6522303223609924, acc.: 66.60%] [G loss: 0.8725777864456177]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 19/86 [D loss: 0.6591741144657135, acc.: 65.62%] [G loss: 0.8425233364105225]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 20/86 [D loss: 0.6686028838157654, acc.: 64.99%] [G loss: 0.8151307106018066]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 21/86 [D loss: 0.6812435686588287, acc.: 59.23%] [G loss: 0.7744311094284058]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 22/86 [D loss: 0.6955164074897766, acc.: 53.47%] [G loss: 0.7365041971206665]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 23/86 [D loss: 0.7081860303878784, acc.: 48.93%] [G loss: 0.710588812828064]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 24/86 [D loss: 0.711449533700943, acc.: 44.04%] [G loss: 0.6954683065414429]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 25/86 [D loss: 0.7172797620296478, acc.: 39.55%] [G loss: 0.6854864954948425]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 11/200, Batch 26/86 [D loss: 0.7133386135101318, acc.: 42.63%] [G loss: 0.6827973127365112]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 27/86 [D loss: 0.7112134695053101, acc.: 41.60%] [G loss: 0.6862252950668335]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 28/86 [D loss: 0.7042036354541779, acc.: 44.73%] [G loss: 0.6885345578193665]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 29/86 [D loss: 0.701409637928009, acc.: 45.36%] [G loss: 0.7000694274902344]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 30/86 [D loss: 0.6924972534179688, acc.: 51.90%] [G loss: 0.7108636498451233]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 31/86 [D loss: 0.6834791004657745, acc.: 57.52%] [G loss: 0.7252737283706665]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 32/86 [D loss: 0.671845942735672, acc.: 63.72%] [G loss: 0.7326345443725586]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 33/86 [D loss: 0.6720655858516693, acc.: 62.74%] [G loss: 0.7406033277511597]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 34/86 [D loss: 0.6682709753513336, acc.: 64.06%] [G loss: 0.7508625984191895]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 35/86 [D loss: 0.6634481251239777, acc.: 64.50%] [G loss: 0.7508828639984131]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 36/86 [D loss: 0.6649519205093384, acc.: 63.53%] [G loss: 0.7527676224708557]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 37/86 [D loss: 0.6659810543060303, acc.: 61.87%] [G loss: 0.7375732660293579]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 38/86 [D loss: 0.673171192407608, acc.: 58.59%] [G loss: 0.7227250337600708]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 39/86 [D loss: 0.6811219155788422, acc.: 53.91%] [G loss: 0.7125360369682312]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 40/86 [D loss: 0.68922358751297, acc.: 51.95%] [G loss: 0.6970146894454956]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 41/86 [D loss: 0.6959304809570312, acc.: 48.83%] [G loss: 0.68415766954422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 42/86 [D loss: 0.7032592594623566, acc.: 45.85%] [G loss: 0.681977391242981]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 43/86 [D loss: 0.7092824280261993, acc.: 44.14%] [G loss: 0.682273805141449]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 44/86 [D loss: 0.7107729613780975, acc.: 43.46%] [G loss: 0.6869043111801147]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 45/86 [D loss: 0.7054994404315948, acc.: 46.09%] [G loss: 0.7015390396118164]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 46/86 [D loss: 0.6985151767730713, acc.: 49.12%] [G loss: 0.7176748514175415]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 47/86 [D loss: 0.6949899792671204, acc.: 50.39%] [G loss: 0.7432173490524292]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 48/86 [D loss: 0.683721125125885, acc.: 57.47%] [G loss: 0.7666738033294678]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 49/86 [D loss: 0.6785441637039185, acc.: 57.28%] [G loss: 0.7823818922042847]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 50/86 [D loss: 0.6719991266727448, acc.: 62.11%] [G loss: 0.8039443492889404]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 51/86 [D loss: 0.6703823804855347, acc.: 63.33%] [G loss: 0.805604100227356]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 52/86 [D loss: 0.6717622578144073, acc.: 63.72%] [G loss: 0.8035651445388794]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 53/86 [D loss: 0.6743451356887817, acc.: 61.77%] [G loss: 0.7903394103050232]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 54/86 [D loss: 0.6786741316318512, acc.: 61.57%] [G loss: 0.7773443460464478]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 55/86 [D loss: 0.6840703785419464, acc.: 59.03%] [G loss: 0.7554068565368652]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 56/86 [D loss: 0.6936202347278595, acc.: 54.30%] [G loss: 0.7420063018798828]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 57/86 [D loss: 0.6961957514286041, acc.: 53.27%] [G loss: 0.7216137051582336]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 58/86 [D loss: 0.7038642466068268, acc.: 48.24%] [G loss: 0.7113312482833862]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 59/86 [D loss: 0.7016895711421967, acc.: 48.24%] [G loss: 0.7043836116790771]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 60/86 [D loss: 0.7022367417812347, acc.: 45.85%] [G loss: 0.7044252157211304]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 61/86 [D loss: 0.7022171020507812, acc.: 46.48%] [G loss: 0.7038322687149048]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 62/86 [D loss: 0.6987487077713013, acc.: 47.17%] [G loss: 0.7061560750007629]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 63/86 [D loss: 0.6954319179058075, acc.: 49.32%] [G loss: 0.7063864469528198]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 64/86 [D loss: 0.690984457731247, acc.: 53.17%] [G loss: 0.7132017612457275]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 65/86 [D loss: 0.6821412444114685, acc.: 57.81%] [G loss: 0.7201581001281738]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 66/86 [D loss: 0.6770617067813873, acc.: 62.06%] [G loss: 0.7263227701187134]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 67/86 [D loss: 0.673152506351471, acc.: 63.09%] [G loss: 0.7319834232330322]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 68/86 [D loss: 0.6672753989696503, acc.: 66.41%] [G loss: 0.7344034314155579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 69/86 [D loss: 0.6663995385169983, acc.: 66.31%] [G loss: 0.7424977421760559]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 70/86 [D loss: 0.6623612642288208, acc.: 66.26%] [G loss: 0.7408195734024048]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 71/86 [D loss: 0.6620081961154938, acc.: 65.58%] [G loss: 0.7412999272346497]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 72/86 [D loss: 0.6650655567646027, acc.: 64.94%] [G loss: 0.7297205328941345]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 73/86 [D loss: 0.6692858040332794, acc.: 60.30%] [G loss: 0.7228337526321411]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 74/86 [D loss: 0.6725045144557953, acc.: 59.03%] [G loss: 0.7085922956466675]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 75/86 [D loss: 0.6870470643043518, acc.: 53.08%] [G loss: 0.6956503987312317]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 76/86 [D loss: 0.6956604719161987, acc.: 48.54%] [G loss: 0.6871205568313599]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 77/86 [D loss: 0.6977083683013916, acc.: 47.85%] [G loss: 0.6804201006889343]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 78/86 [D loss: 0.7041051089763641, acc.: 45.75%] [G loss: 0.6818580627441406]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 79/86 [D loss: 0.703720211982727, acc.: 46.09%] [G loss: 0.6942400932312012]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 80/86 [D loss: 0.6982461512088776, acc.: 47.61%] [G loss: 0.7085884213447571]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 11/200, Batch 81/86 [D loss: 0.6895290613174438, acc.: 54.39%] [G loss: 0.7328003644943237]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 82/86 [D loss: 0.6837002635002136, acc.: 55.96%] [G loss: 0.7567894458770752]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 83/86 [D loss: 0.6713887751102448, acc.: 62.40%] [G loss: 0.7888880968093872]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 11/200, Batch 84/86 [D loss: 0.6587329506874084, acc.: 66.89%] [G loss: 0.8174225091934204]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 85/86 [D loss: 0.6584144830703735, acc.: 67.24%] [G loss: 0.8269216418266296]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 11/200, Batch 86/86 [D loss: 0.6572085618972778, acc.: 67.68%] [G loss: 0.8306775093078613]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 1/86 [D loss: 0.6606042683124542, acc.: 67.58%] [G loss: 0.8248143196105957]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 2/86 [D loss: 0.6678575873374939, acc.: 64.65%] [G loss: 0.8056924343109131]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 3/86 [D loss: 0.6715570986270905, acc.: 64.84%] [G loss: 0.7756461501121521]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 4/86 [D loss: 0.6848190128803253, acc.: 59.08%] [G loss: 0.7483054995536804]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 12/200, Batch 5/86 [D loss: 0.6916502714157104, acc.: 56.64%] [G loss: 0.7314946055412292]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 6/86 [D loss: 0.6982668936252594, acc.: 52.29%] [G loss: 0.7147807478904724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 7/86 [D loss: 0.7001982629299164, acc.: 48.54%] [G loss: 0.6964181661605835]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 8/86 [D loss: 0.7067801058292389, acc.: 45.56%] [G loss: 0.691188395023346]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 9/86 [D loss: 0.7029758989810944, acc.: 45.17%] [G loss: 0.6860201954841614]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 10/86 [D loss: 0.701066255569458, acc.: 47.02%] [G loss: 0.6847921013832092]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 11/86 [D loss: 0.6978209018707275, acc.: 47.46%] [G loss: 0.6890795230865479]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 12/86 [D loss: 0.6903149485588074, acc.: 51.61%] [G loss: 0.696370542049408]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 13/86 [D loss: 0.6843933761119843, acc.: 55.57%] [G loss: 0.705877423286438]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 14/86 [D loss: 0.680614709854126, acc.: 56.54%] [G loss: 0.7125522494316101]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 15/86 [D loss: 0.6757428646087646, acc.: 62.40%] [G loss: 0.7185381650924683]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 16/86 [D loss: 0.6716577410697937, acc.: 62.55%] [G loss: 0.7229222059249878]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 17/86 [D loss: 0.6666885316371918, acc.: 65.77%] [G loss: 0.7256006598472595]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 18/86 [D loss: 0.6651186347007751, acc.: 64.45%] [G loss: 0.7245745658874512]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 12/200, Batch 19/86 [D loss: 0.6697241067886353, acc.: 61.87%] [G loss: 0.7195441722869873]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 20/86 [D loss: 0.675011545419693, acc.: 59.33%] [G loss: 0.7065574526786804]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 21/86 [D loss: 0.6833531856536865, acc.: 56.15%] [G loss: 0.6907796263694763]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 22/86 [D loss: 0.6907608509063721, acc.: 52.44%] [G loss: 0.6782206296920776]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 23/86 [D loss: 0.7009549736976624, acc.: 48.49%] [G loss: 0.6611315011978149]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 12/200, Batch 24/86 [D loss: 0.7106662094593048, acc.: 44.24%] [G loss: 0.6507197618484497]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 25/86 [D loss: 0.716384083032608, acc.: 45.41%] [G loss: 0.6491670608520508]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 26/86 [D loss: 0.717685341835022, acc.: 44.63%] [G loss: 0.6571211814880371]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 27/86 [D loss: 0.7120019495487213, acc.: 44.04%] [G loss: 0.6820266246795654]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 28/86 [D loss: 0.7041574120521545, acc.: 46.97%] [G loss: 0.7016187310218811]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 12/200, Batch 29/86 [D loss: 0.6917065978050232, acc.: 52.83%] [G loss: 0.7322173118591309]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 30/86 [D loss: 0.6803020238876343, acc.: 58.74%] [G loss: 0.7587016224861145]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 31/86 [D loss: 0.6720190644264221, acc.: 62.01%] [G loss: 0.7833536267280579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 32/86 [D loss: 0.6646675169467926, acc.: 66.06%] [G loss: 0.8010440468788147]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 33/86 [D loss: 0.6646358370780945, acc.: 64.65%] [G loss: 0.8072748184204102]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 34/86 [D loss: 0.6683720648288727, acc.: 65.43%] [G loss: 0.7907532453536987]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 35/86 [D loss: 0.6751184165477753, acc.: 62.79%] [G loss: 0.7784376740455627]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 36/86 [D loss: 0.6801077127456665, acc.: 60.35%] [G loss: 0.7590622305870056]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 37/86 [D loss: 0.6845301985740662, acc.: 58.98%] [G loss: 0.7400054931640625]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 12/200, Batch 38/86 [D loss: 0.6921564936637878, acc.: 54.15%] [G loss: 0.7262612581253052]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 39/86 [D loss: 0.6927384436130524, acc.: 52.49%] [G loss: 0.7172136306762695]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 40/86 [D loss: 0.6978514790534973, acc.: 49.95%] [G loss: 0.7096889615058899]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 41/86 [D loss: 0.6936595141887665, acc.: 51.71%] [G loss: 0.704035758972168]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 42/86 [D loss: 0.6928103566169739, acc.: 51.46%] [G loss: 0.7048118114471436]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 43/86 [D loss: 0.692685455083847, acc.: 51.32%] [G loss: 0.7032569646835327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 44/86 [D loss: 0.6872859299182892, acc.: 54.74%] [G loss: 0.7109817862510681]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 45/86 [D loss: 0.6821544468402863, acc.: 57.96%] [G loss: 0.7182541489601135]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 46/86 [D loss: 0.6770119667053223, acc.: 60.84%] [G loss: 0.7258105874061584]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 47/86 [D loss: 0.6730475723743439, acc.: 62.84%] [G loss: 0.730817973613739]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 48/86 [D loss: 0.6697000563144684, acc.: 64.40%] [G loss: 0.738288402557373]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 49/86 [D loss: 0.6665751338005066, acc.: 65.43%] [G loss: 0.7424282431602478]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 50/86 [D loss: 0.6647084355354309, acc.: 64.99%] [G loss: 0.7394174337387085]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 51/86 [D loss: 0.6706024706363678, acc.: 61.38%] [G loss: 0.7322026491165161]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 52/86 [D loss: 0.6716702878475189, acc.: 60.50%] [G loss: 0.7278624773025513]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 53/86 [D loss: 0.6751477420330048, acc.: 59.08%] [G loss: 0.7078773975372314]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 54/86 [D loss: 0.6901323795318604, acc.: 52.25%] [G loss: 0.6946654915809631]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 55/86 [D loss: 0.6932331323623657, acc.: 50.93%] [G loss: 0.680935263633728]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 56/86 [D loss: 0.6998163461685181, acc.: 47.85%] [G loss: 0.6811234951019287]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 57/86 [D loss: 0.7029041945934296, acc.: 47.12%] [G loss: 0.6737505197525024]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 58/86 [D loss: 0.7062572240829468, acc.: 46.14%] [G loss: 0.6861901879310608]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 59/86 [D loss: 0.7042100429534912, acc.: 46.29%] [G loss: 0.6963223814964294]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 60/86 [D loss: 0.7006850838661194, acc.: 47.56%] [G loss: 0.7090163826942444]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 61/86 [D loss: 0.694873183965683, acc.: 50.24%] [G loss: 0.7290215492248535]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 62/86 [D loss: 0.688361257314682, acc.: 53.08%] [G loss: 0.7483894228935242]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 63/86 [D loss: 0.6820214092731476, acc.: 59.03%] [G loss: 0.7628730535507202]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 64/86 [D loss: 0.6784063577651978, acc.: 59.03%] [G loss: 0.7734628319740295]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 65/86 [D loss: 0.6764735579490662, acc.: 61.04%] [G loss: 0.7781990170478821]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 66/86 [D loss: 0.6779281497001648, acc.: 61.13%] [G loss: 0.776708722114563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 67/86 [D loss: 0.6803989112377167, acc.: 59.67%] [G loss: 0.7641095519065857]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 68/86 [D loss: 0.6792506873607635, acc.: 60.60%] [G loss: 0.7580901384353638]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 69/86 [D loss: 0.6874518692493439, acc.: 56.15%] [G loss: 0.7432856559753418]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 70/86 [D loss: 0.6916501820087433, acc.: 55.13%] [G loss: 0.7357969880104065]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 71/86 [D loss: 0.6935006976127625, acc.: 52.69%] [G loss: 0.7211741209030151]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 72/86 [D loss: 0.69759401679039, acc.: 49.37%] [G loss: 0.7116792798042297]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 73/86 [D loss: 0.6987947225570679, acc.: 49.37%] [G loss: 0.7104904055595398]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 74/86 [D loss: 0.699760764837265, acc.: 48.24%] [G loss: 0.712337076663971]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 75/86 [D loss: 0.6986112892627716, acc.: 48.63%] [G loss: 0.7033066749572754]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 76/86 [D loss: 0.6941428184509277, acc.: 51.56%] [G loss: 0.7088922262191772]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 77/86 [D loss: 0.6876617968082428, acc.: 55.18%] [G loss: 0.7130606174468994]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 78/86 [D loss: 0.6826961934566498, acc.: 57.67%] [G loss: 0.722125768661499]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 79/86 [D loss: 0.6800606548786163, acc.: 59.18%] [G loss: 0.730824887752533]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 80/86 [D loss: 0.6740517616271973, acc.: 63.09%] [G loss: 0.7344822883605957]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 81/86 [D loss: 0.6726678907871246, acc.: 63.28%] [G loss: 0.739281415939331]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 82/86 [D loss: 0.669393002986908, acc.: 64.65%] [G loss: 0.7403160333633423]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 12/200, Batch 83/86 [D loss: 0.6691841185092926, acc.: 65.48%] [G loss: 0.7416399717330933]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 84/86 [D loss: 0.6652642786502838, acc.: 66.85%] [G loss: 0.7376378774642944]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 12/200, Batch 85/86 [D loss: 0.6668443977832794, acc.: 65.19%] [G loss: 0.7380781769752502]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 12/200, Batch 86/86 [D loss: 0.673276275396347, acc.: 62.79%] [G loss: 0.7270854711532593]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 1/86 [D loss: 0.6745886504650116, acc.: 59.91%] [G loss: 0.7184383273124695]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 2/86 [D loss: 0.6793409287929535, acc.: 57.71%] [G loss: 0.711126983165741]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 3/86 [D loss: 0.6847619414329529, acc.: 53.52%] [G loss: 0.7053548693656921]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 4/86 [D loss: 0.6906432509422302, acc.: 52.25%] [G loss: 0.7019637823104858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 5/86 [D loss: 0.6924388706684113, acc.: 51.37%] [G loss: 0.7092477083206177]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 6/86 [D loss: 0.6924155950546265, acc.: 51.76%] [G loss: 0.7123918533325195]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 7/86 [D loss: 0.6896844506263733, acc.: 52.59%] [G loss: 0.727324366569519]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 8/86 [D loss: 0.6849620342254639, acc.: 55.42%] [G loss: 0.735419511795044]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 9/86 [D loss: 0.6784795522689819, acc.: 59.91%] [G loss: 0.7539926767349243]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 10/86 [D loss: 0.6741431951522827, acc.: 62.45%] [G loss: 0.7726535201072693]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 11/86 [D loss: 0.6708126962184906, acc.: 61.82%] [G loss: 0.7884256839752197]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 12/86 [D loss: 0.6696203947067261, acc.: 62.99%] [G loss: 0.7978045344352722]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 13/86 [D loss: 0.6708208620548248, acc.: 62.21%] [G loss: 0.7958430051803589]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 14/86 [D loss: 0.6700474917888641, acc.: 64.45%] [G loss: 0.7945294976234436]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 15/86 [D loss: 0.6751196980476379, acc.: 62.94%] [G loss: 0.7834187150001526]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 16/86 [D loss: 0.6773807108402252, acc.: 61.08%] [G loss: 0.7729501724243164]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 13/200, Batch 17/86 [D loss: 0.6842743754386902, acc.: 60.50%] [G loss: 0.7571063041687012]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 18/86 [D loss: 0.6900169551372528, acc.: 55.37%] [G loss: 0.7347015142440796]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 19/86 [D loss: 0.6908194720745087, acc.: 54.69%] [G loss: 0.7245134115219116]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 20/86 [D loss: 0.6963697373867035, acc.: 51.56%] [G loss: 0.7168816328048706]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 21/86 [D loss: 0.7052369117736816, acc.: 47.22%] [G loss: 0.7083625793457031]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 13/200, Batch 22/86 [D loss: 0.6969491839408875, acc.: 51.22%] [G loss: 0.7081830501556396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 23/86 [D loss: 0.6959406137466431, acc.: 51.17%] [G loss: 0.7044715285301208]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 24/86 [D loss: 0.6963512301445007, acc.: 49.95%] [G loss: 0.7086436748504639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 25/86 [D loss: 0.6895386576652527, acc.: 54.59%] [G loss: 0.7153236865997314]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 26/86 [D loss: 0.6874847710132599, acc.: 55.08%] [G loss: 0.7196409702301025]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 27/86 [D loss: 0.6827137768268585, acc.: 58.64%] [G loss: 0.7289332151412964]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 28/86 [D loss: 0.6772525608539581, acc.: 60.60%] [G loss: 0.7320829629898071]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 29/86 [D loss: 0.6765733361244202, acc.: 59.67%] [G loss: 0.7331841588020325]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 30/86 [D loss: 0.6743228435516357, acc.: 62.35%] [G loss: 0.7328628897666931]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 31/86 [D loss: 0.6706912219524384, acc.: 63.28%] [G loss: 0.7349839210510254]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 32/86 [D loss: 0.6729212701320648, acc.: 60.60%] [G loss: 0.7255720496177673]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 33/86 [D loss: 0.6771689653396606, acc.: 58.30%] [G loss: 0.7212718725204468]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 34/86 [D loss: 0.683206170797348, acc.: 56.25%] [G loss: 0.7146621346473694]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 35/86 [D loss: 0.6903939843177795, acc.: 52.64%] [G loss: 0.7035279273986816]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 36/86 [D loss: 0.6930059790611267, acc.: 50.63%] [G loss: 0.6922507882118225]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 37/86 [D loss: 0.6981039941310883, acc.: 48.34%] [G loss: 0.6943187713623047]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 38/86 [D loss: 0.697327047586441, acc.: 49.37%] [G loss: 0.6940852403640747]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 39/86 [D loss: 0.6982085704803467, acc.: 49.56%] [G loss: 0.709478497505188]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 40/86 [D loss: 0.6894365549087524, acc.: 54.64%] [G loss: 0.722889244556427]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 41/86 [D loss: 0.685345470905304, acc.: 56.01%] [G loss: 0.7473564743995667]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 42/86 [D loss: 0.6788782775402069, acc.: 59.23%] [G loss: 0.7637020349502563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 43/86 [D loss: 0.674074113368988, acc.: 61.33%] [G loss: 0.7851278185844421]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 44/86 [D loss: 0.6677887737751007, acc.: 62.74%] [G loss: 0.7988747954368591]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 45/86 [D loss: 0.6651925444602966, acc.: 66.16%] [G loss: 0.8041648268699646]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 46/86 [D loss: 0.665819525718689, acc.: 66.26%] [G loss: 0.797099232673645]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 47/86 [D loss: 0.6684988737106323, acc.: 64.89%] [G loss: 0.7935454249382019]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 48/86 [D loss: 0.6730004549026489, acc.: 64.65%] [G loss: 0.774203896522522]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 49/86 [D loss: 0.6806968152523041, acc.: 59.96%] [G loss: 0.764220654964447]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 50/86 [D loss: 0.6805726587772369, acc.: 61.08%] [G loss: 0.7464603185653687]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 51/86 [D loss: 0.6857282221317291, acc.: 57.08%] [G loss: 0.738969087600708]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 52/86 [D loss: 0.690607875585556, acc.: 54.00%] [G loss: 0.7234439253807068]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 53/86 [D loss: 0.6937731802463531, acc.: 52.34%] [G loss: 0.7170906066894531]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 54/86 [D loss: 0.6921142041683197, acc.: 52.29%] [G loss: 0.7136998176574707]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 55/86 [D loss: 0.6950054168701172, acc.: 50.49%] [G loss: 0.7115161418914795]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 13/200, Batch 56/86 [D loss: 0.690591037273407, acc.: 54.15%] [G loss: 0.7138333320617676]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 57/86 [D loss: 0.6848438680171967, acc.: 55.66%] [G loss: 0.7175191044807434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 58/86 [D loss: 0.681929349899292, acc.: 57.57%] [G loss: 0.7184575796127319]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 59/86 [D loss: 0.6781933009624481, acc.: 60.60%] [G loss: 0.7283768653869629]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 60/86 [D loss: 0.6745242178440094, acc.: 62.40%] [G loss: 0.7318551540374756]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 61/86 [D loss: 0.6723100244998932, acc.: 62.94%] [G loss: 0.7333271503448486]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 62/86 [D loss: 0.6724165081977844, acc.: 61.91%] [G loss: 0.7347361445426941]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 63/86 [D loss: 0.6716536283493042, acc.: 61.91%] [G loss: 0.7304120063781738]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 64/86 [D loss: 0.6708859205245972, acc.: 61.82%] [G loss: 0.7260962128639221]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 65/86 [D loss: 0.6771343350410461, acc.: 57.47%] [G loss: 0.7130382061004639]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 66/86 [D loss: 0.6840396821498871, acc.: 54.05%] [G loss: 0.703934907913208]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 67/86 [D loss: 0.6862921118736267, acc.: 52.25%] [G loss: 0.6974890828132629]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 68/86 [D loss: 0.6983479559421539, acc.: 48.14%] [G loss: 0.6883586049079895]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 69/86 [D loss: 0.6972605288028717, acc.: 49.02%] [G loss: 0.6874990463256836]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 70/86 [D loss: 0.6980699300765991, acc.: 47.71%] [G loss: 0.6906203031539917]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 71/86 [D loss: 0.7017447352409363, acc.: 46.58%] [G loss: 0.6970021724700928]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 72/86 [D loss: 0.6958627104759216, acc.: 49.90%] [G loss: 0.7094005346298218]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 73/86 [D loss: 0.6907491087913513, acc.: 51.95%] [G loss: 0.7300529479980469]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 74/86 [D loss: 0.6847496330738068, acc.: 55.66%] [G loss: 0.7459938526153564]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 75/86 [D loss: 0.6744322776794434, acc.: 61.57%] [G loss: 0.7688500881195068]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 76/86 [D loss: 0.6685827672481537, acc.: 64.60%] [G loss: 0.7819552421569824]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 77/86 [D loss: 0.6712642014026642, acc.: 63.43%] [G loss: 0.7897799015045166]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 78/86 [D loss: 0.6688494384288788, acc.: 63.92%] [G loss: 0.7977665066719055]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 79/86 [D loss: 0.6670570075511932, acc.: 65.77%] [G loss: 0.7939987182617188]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 80/86 [D loss: 0.6729952692985535, acc.: 63.87%] [G loss: 0.7871323823928833]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 81/86 [D loss: 0.675478994846344, acc.: 62.89%] [G loss: 0.770822286605835]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 82/86 [D loss: 0.682018518447876, acc.: 60.79%] [G loss: 0.7542737722396851]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 13/200, Batch 83/86 [D loss: 0.6822355091571808, acc.: 58.94%] [G loss: 0.7455940246582031]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 84/86 [D loss: 0.6890783309936523, acc.: 55.22%] [G loss: 0.7321830987930298]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 13/200, Batch 85/86 [D loss: 0.688070684671402, acc.: 56.05%] [G loss: 0.7270869612693787]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 13/200, Batch 86/86 [D loss: 0.6940828561782837, acc.: 51.03%] [G loss: 0.720555305480957]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 1/86 [D loss: 0.6887611150741577, acc.: 52.54%] [G loss: 0.7148340344429016]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 2/86 [D loss: 0.6875991225242615, acc.: 53.66%] [G loss: 0.7169252038002014]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 3/86 [D loss: 0.6875729560852051, acc.: 54.20%] [G loss: 0.7160580158233643]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 4/86 [D loss: 0.6806682348251343, acc.: 58.06%] [G loss: 0.7199562788009644]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 5/86 [D loss: 0.676111489534378, acc.: 60.64%] [G loss: 0.7266760468482971]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 6/86 [D loss: 0.6719965934753418, acc.: 61.28%] [G loss: 0.733210027217865]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 7/86 [D loss: 0.6698166728019714, acc.: 64.11%] [G loss: 0.7371065020561218]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 8/86 [D loss: 0.6692818105220795, acc.: 62.94%] [G loss: 0.7375074625015259]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 9/86 [D loss: 0.6665439009666443, acc.: 64.99%] [G loss: 0.7374570369720459]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 10/86 [D loss: 0.6678907871246338, acc.: 63.38%] [G loss: 0.7335150241851807]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 11/86 [D loss: 0.6759659647941589, acc.: 59.33%] [G loss: 0.7243674993515015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 12/86 [D loss: 0.6834456920623779, acc.: 54.83%] [G loss: 0.713402271270752]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 13/86 [D loss: 0.6873309910297394, acc.: 52.98%] [G loss: 0.6962882876396179]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 14/86 [D loss: 0.6959774792194366, acc.: 50.39%] [G loss: 0.6897264719009399]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 15/86 [D loss: 0.7052874267101288, acc.: 45.70%] [G loss: 0.6810163259506226]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 16/86 [D loss: 0.7098356485366821, acc.: 45.12%] [G loss: 0.6805508136749268]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 17/86 [D loss: 0.7078637778759003, acc.: 44.09%] [G loss: 0.6899364590644836]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 18/86 [D loss: 0.7032687664031982, acc.: 46.53%] [G loss: 0.7050720453262329]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 14/200, Batch 19/86 [D loss: 0.6977442502975464, acc.: 47.66%] [G loss: 0.7336207032203674]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 20/86 [D loss: 0.6914394497871399, acc.: 52.73%] [G loss: 0.7542017102241516]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 21/86 [D loss: 0.6802068054676056, acc.: 58.59%] [G loss: 0.7774451375007629]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 22/86 [D loss: 0.6716005504131317, acc.: 61.23%] [G loss: 0.7998366355895996]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 23/86 [D loss: 0.6662811040878296, acc.: 64.40%] [G loss: 0.8131850957870483]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 24/86 [D loss: 0.6691378355026245, acc.: 64.16%] [G loss: 0.8165813088417053]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 25/86 [D loss: 0.670243501663208, acc.: 63.77%] [G loss: 0.8041965365409851]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 26/86 [D loss: 0.6736034750938416, acc.: 62.99%] [G loss: 0.789142370223999]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 27/86 [D loss: 0.6796071827411652, acc.: 61.43%] [G loss: 0.777633547782898]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 28/86 [D loss: 0.6859720945358276, acc.: 59.91%] [G loss: 0.7622959613800049]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 14/200, Batch 29/86 [D loss: 0.6903429627418518, acc.: 56.20%] [G loss: 0.7473496794700623]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 30/86 [D loss: 0.6928080022335052, acc.: 56.15%] [G loss: 0.7337687015533447]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 31/86 [D loss: 0.693619966506958, acc.: 54.64%] [G loss: 0.7292137145996094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 32/86 [D loss: 0.6884993314743042, acc.: 55.66%] [G loss: 0.7289339303970337]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 33/86 [D loss: 0.6884716153144836, acc.: 55.08%] [G loss: 0.7302479147911072]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 34/86 [D loss: 0.6821045577526093, acc.: 57.96%] [G loss: 0.7337996363639832]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 35/86 [D loss: 0.6772433221340179, acc.: 62.06%] [G loss: 0.7416679859161377]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 36/86 [D loss: 0.6720149517059326, acc.: 64.21%] [G loss: 0.7482991218566895]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 37/86 [D loss: 0.6649125218391418, acc.: 67.77%] [G loss: 0.750415027141571]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 38/86 [D loss: 0.6645475327968597, acc.: 66.46%] [G loss: 0.7574977278709412]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 39/86 [D loss: 0.6588022708892822, acc.: 69.82%] [G loss: 0.7567408084869385]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 40/86 [D loss: 0.6588602960109711, acc.: 67.77%] [G loss: 0.754036009311676]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 41/86 [D loss: 0.6647723019123077, acc.: 64.70%] [G loss: 0.7462360858917236]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 42/86 [D loss: 0.6702191829681396, acc.: 60.60%] [G loss: 0.7296521663665771]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 43/86 [D loss: 0.6738997995853424, acc.: 59.13%] [G loss: 0.7185606360435486]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 44/86 [D loss: 0.6854376792907715, acc.: 54.00%] [G loss: 0.7021946310997009]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 45/86 [D loss: 0.6925720274448395, acc.: 52.59%] [G loss: 0.6920839548110962]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 46/86 [D loss: 0.6968066990375519, acc.: 48.58%] [G loss: 0.6861114501953125]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 47/86 [D loss: 0.7037926912307739, acc.: 46.88%] [G loss: 0.6842988133430481]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 48/86 [D loss: 0.6992232203483582, acc.: 48.34%] [G loss: 0.688302218914032]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 49/86 [D loss: 0.7030745446681976, acc.: 46.14%] [G loss: 0.701736330986023]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 50/86 [D loss: 0.6998634338378906, acc.: 49.37%] [G loss: 0.7244062423706055]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 51/86 [D loss: 0.6921105682849884, acc.: 51.81%] [G loss: 0.7463157773017883]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 52/86 [D loss: 0.6812082529067993, acc.: 58.15%] [G loss: 0.7596087455749512]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 53/86 [D loss: 0.674783855676651, acc.: 60.89%] [G loss: 0.779819667339325]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 54/86 [D loss: 0.674012303352356, acc.: 61.72%] [G loss: 0.7865877151489258]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 55/86 [D loss: 0.6741454303264618, acc.: 61.62%] [G loss: 0.7912247180938721]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 56/86 [D loss: 0.6768640577793121, acc.: 62.35%] [G loss: 0.7794632911682129]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 57/86 [D loss: 0.6791573166847229, acc.: 59.77%] [G loss: 0.7680363059043884]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 58/86 [D loss: 0.6843355000019073, acc.: 58.11%] [G loss: 0.75786292552948]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 59/86 [D loss: 0.6891450583934784, acc.: 54.64%] [G loss: 0.7477438449859619]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 60/86 [D loss: 0.6925120055675507, acc.: 53.47%] [G loss: 0.7314594388008118]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 14/200, Batch 61/86 [D loss: 0.6907989382743835, acc.: 53.86%] [G loss: 0.7234694361686707]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 62/86 [D loss: 0.6972326040267944, acc.: 51.27%] [G loss: 0.7166988849639893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 63/86 [D loss: 0.6907616555690765, acc.: 54.05%] [G loss: 0.7193593978881836]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 64/86 [D loss: 0.6926146447658539, acc.: 52.29%] [G loss: 0.7176814079284668]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 65/86 [D loss: 0.6859010756015778, acc.: 55.47%] [G loss: 0.7193773984909058]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 14/200, Batch 66/86 [D loss: 0.6821578443050385, acc.: 58.30%] [G loss: 0.7303215265274048]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 67/86 [D loss: 0.6748418509960175, acc.: 61.82%] [G loss: 0.7372008562088013]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 68/86 [D loss: 0.6677820682525635, acc.: 67.87%] [G loss: 0.7453985214233398]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 69/86 [D loss: 0.6622532606124878, acc.: 68.07%] [G loss: 0.7516347169876099]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 70/86 [D loss: 0.6546988785266876, acc.: 71.19%] [G loss: 0.7580718398094177]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 71/86 [D loss: 0.6548068523406982, acc.: 70.12%] [G loss: 0.7599177360534668]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 72/86 [D loss: 0.6571511030197144, acc.: 68.55%] [G loss: 0.7566004991531372]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 73/86 [D loss: 0.6545378267765045, acc.: 67.72%] [G loss: 0.7510491609573364]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 74/86 [D loss: 0.65809565782547, acc.: 65.92%] [G loss: 0.7430071830749512]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 75/86 [D loss: 0.6672363579273224, acc.: 61.62%] [G loss: 0.7306289076805115]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 76/86 [D loss: 0.6685563623905182, acc.: 60.64%] [G loss: 0.727193295955658]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 77/86 [D loss: 0.6745849847793579, acc.: 57.52%] [G loss: 0.7122928500175476]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 78/86 [D loss: 0.6801638901233673, acc.: 56.01%] [G loss: 0.7067619562149048]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 79/86 [D loss: 0.6894637942314148, acc.: 53.42%] [G loss: 0.6992961764335632]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 80/86 [D loss: 0.6901009678840637, acc.: 52.88%] [G loss: 0.7082316875457764]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 81/86 [D loss: 0.6882432699203491, acc.: 53.76%] [G loss: 0.7219422459602356]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 82/86 [D loss: 0.6838735640048981, acc.: 55.76%] [G loss: 0.7379516959190369]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 83/86 [D loss: 0.6798748672008514, acc.: 58.54%] [G loss: 0.7609738111495972]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 14/200, Batch 84/86 [D loss: 0.6695789694786072, acc.: 62.89%] [G loss: 0.7792301774024963]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 14/200, Batch 85/86 [D loss: 0.6726391017436981, acc.: 60.40%] [G loss: 0.79322749376297]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 14/200, Batch 86/86 [D loss: 0.6675574481487274, acc.: 64.01%] [G loss: 0.7916051745414734]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 1/86 [D loss: 0.6724125444889069, acc.: 62.79%] [G loss: 0.7947589159011841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 2/86 [D loss: 0.6822260022163391, acc.: 57.52%] [G loss: 0.7811582684516907]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 3/86 [D loss: 0.6869197487831116, acc.: 57.08%] [G loss: 0.764266312122345]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 4/86 [D loss: 0.6944222748279572, acc.: 53.76%] [G loss: 0.747317910194397]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 5/86 [D loss: 0.7089927494525909, acc.: 46.53%] [G loss: 0.7285324335098267]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 6/86 [D loss: 0.7067813873291016, acc.: 46.78%] [G loss: 0.7112036943435669]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 7/86 [D loss: 0.7167079746723175, acc.: 42.19%] [G loss: 0.7027286887168884]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 8/86 [D loss: 0.712020069360733, acc.: 42.14%] [G loss: 0.701387882232666]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 9/86 [D loss: 0.7104218304157257, acc.: 43.80%] [G loss: 0.7036476731300354]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 10/86 [D loss: 0.7093589007854462, acc.: 44.78%] [G loss: 0.7143805623054504]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 11/86 [D loss: 0.7013489902019501, acc.: 47.12%] [G loss: 0.7285720109939575]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 12/86 [D loss: 0.6878429353237152, acc.: 55.08%] [G loss: 0.7393338680267334]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 13/86 [D loss: 0.6811299324035645, acc.: 58.74%] [G loss: 0.7540135383605957]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 14/86 [D loss: 0.6715036630630493, acc.: 65.04%] [G loss: 0.7683526873588562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 15/86 [D loss: 0.6647360026836395, acc.: 68.12%] [G loss: 0.7776172161102295]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 16/86 [D loss: 0.6604284048080444, acc.: 68.60%] [G loss: 0.7902892827987671]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 17/86 [D loss: 0.6539120972156525, acc.: 70.85%] [G loss: 0.7904980182647705]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 18/86 [D loss: 0.6547132730484009, acc.: 70.12%] [G loss: 0.7849106788635254]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 19/86 [D loss: 0.6557202339172363, acc.: 68.55%] [G loss: 0.7795568108558655]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 20/86 [D loss: 0.6630927920341492, acc.: 64.70%] [G loss: 0.7643207907676697]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 21/86 [D loss: 0.6646795868873596, acc.: 63.77%] [G loss: 0.7474566698074341]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 22/86 [D loss: 0.6792338192462921, acc.: 58.11%] [G loss: 0.7334821224212646]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 23/86 [D loss: 0.684145450592041, acc.: 54.25%] [G loss: 0.7252058982849121]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 24/86 [D loss: 0.683815062046051, acc.: 55.76%] [G loss: 0.7251842021942139]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 25/86 [D loss: 0.6885180473327637, acc.: 52.93%] [G loss: 0.7308550477027893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 26/86 [D loss: 0.6871006190776825, acc.: 55.47%] [G loss: 0.7410354614257812]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 27/86 [D loss: 0.6797443330287933, acc.: 58.06%] [G loss: 0.7628490924835205]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 28/86 [D loss: 0.6758653819561005, acc.: 60.60%] [G loss: 0.7836806178092957]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 29/86 [D loss: 0.6696018576622009, acc.: 62.60%] [G loss: 0.7997449636459351]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 30/86 [D loss: 0.6681474149227142, acc.: 62.55%] [G loss: 0.8054105639457703]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 31/86 [D loss: 0.6648320257663727, acc.: 64.01%] [G loss: 0.8098803758621216]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 32/86 [D loss: 0.6676515638828278, acc.: 64.36%] [G loss: 0.804063081741333]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 15/200, Batch 33/86 [D loss: 0.6730108261108398, acc.: 61.08%] [G loss: 0.7790629863739014]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 34/86 [D loss: 0.6888502538204193, acc.: 56.10%] [G loss: 0.7579239010810852]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 35/86 [D loss: 0.6926351189613342, acc.: 53.66%] [G loss: 0.7420251369476318]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 36/86 [D loss: 0.6979762017726898, acc.: 50.54%] [G loss: 0.7163946628570557]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 37/86 [D loss: 0.7096343040466309, acc.: 46.48%] [G loss: 0.7101481556892395]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 38/86 [D loss: 0.7145070433616638, acc.: 41.02%] [G loss: 0.6935487985610962]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 39/86 [D loss: 0.7100363969802856, acc.: 43.95%] [G loss: 0.6904982924461365]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 40/86 [D loss: 0.7149916887283325, acc.: 39.94%] [G loss: 0.6857688426971436]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 41/86 [D loss: 0.7107770442962646, acc.: 42.09%] [G loss: 0.6894466280937195]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 42/86 [D loss: 0.7039845585823059, acc.: 43.70%] [G loss: 0.6936171650886536]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 43/86 [D loss: 0.6983455419540405, acc.: 47.31%] [G loss: 0.7033329606056213]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 44/86 [D loss: 0.692367672920227, acc.: 50.63%] [G loss: 0.7132272720336914]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 45/86 [D loss: 0.6852173507213593, acc.: 54.25%] [G loss: 0.7234802842140198]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 46/86 [D loss: 0.6774924099445343, acc.: 60.45%] [G loss: 0.7313107848167419]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 47/86 [D loss: 0.6752913594245911, acc.: 60.94%] [G loss: 0.742516040802002]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 48/86 [D loss: 0.6716318726539612, acc.: 63.82%] [G loss: 0.7419049739837646]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 49/86 [D loss: 0.667651891708374, acc.: 64.36%] [G loss: 0.7426179647445679]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 50/86 [D loss: 0.6646546125411987, acc.: 64.84%] [G loss: 0.7440798282623291]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 51/86 [D loss: 0.6685776710510254, acc.: 61.52%] [G loss: 0.7367072105407715]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 52/86 [D loss: 0.6737621128559113, acc.: 60.25%] [G loss: 0.7268372774124146]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 53/86 [D loss: 0.6800456047058105, acc.: 56.35%] [G loss: 0.7254173755645752]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 54/86 [D loss: 0.6823567450046539, acc.: 55.18%] [G loss: 0.7214232683181763]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 55/86 [D loss: 0.6840518712997437, acc.: 53.96%] [G loss: 0.7172013521194458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 56/86 [D loss: 0.680753231048584, acc.: 55.86%] [G loss: 0.7249941229820251]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 57/86 [D loss: 0.68207848072052, acc.: 55.96%] [G loss: 0.7366017699241638]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 58/86 [D loss: 0.6770182847976685, acc.: 59.57%] [G loss: 0.7482057213783264]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 59/86 [D loss: 0.6698774099349976, acc.: 62.35%] [G loss: 0.7778990268707275]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 60/86 [D loss: 0.6630334854125977, acc.: 66.75%] [G loss: 0.795295774936676]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 61/86 [D loss: 0.6540683209896088, acc.: 69.78%] [G loss: 0.8128721714019775]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 62/86 [D loss: 0.650477409362793, acc.: 70.65%] [G loss: 0.8355837464332581]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 63/86 [D loss: 0.6490394771099091, acc.: 70.51%] [G loss: 0.8337106108665466]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 64/86 [D loss: 0.6511478424072266, acc.: 69.38%] [G loss: 0.8304856419563293]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 65/86 [D loss: 0.6567499339580536, acc.: 67.58%] [G loss: 0.8117499947547913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 66/86 [D loss: 0.6701222062110901, acc.: 63.77%] [G loss: 0.7979716062545776]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 67/86 [D loss: 0.6767619252204895, acc.: 60.60%] [G loss: 0.7733472585678101]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 68/86 [D loss: 0.6886255443096161, acc.: 56.10%] [G loss: 0.7423692345619202]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 69/86 [D loss: 0.6922401487827301, acc.: 52.83%] [G loss: 0.7233526110649109]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 70/86 [D loss: 0.701013058423996, acc.: 47.75%] [G loss: 0.7096148729324341]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 71/86 [D loss: 0.7007390558719635, acc.: 48.29%] [G loss: 0.7019171714782715]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 72/86 [D loss: 0.7024861574172974, acc.: 46.88%] [G loss: 0.6954092979431152]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 73/86 [D loss: 0.7000705301761627, acc.: 47.46%] [G loss: 0.6929118633270264]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 74/86 [D loss: 0.6998070180416107, acc.: 46.78%] [G loss: 0.694922924041748]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 75/86 [D loss: 0.6969780027866364, acc.: 49.27%] [G loss: 0.700055718421936]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 76/86 [D loss: 0.6926925182342529, acc.: 51.07%] [G loss: 0.7060701251029968]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 77/86 [D loss: 0.6828403472900391, acc.: 55.81%] [G loss: 0.7155023813247681]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 78/86 [D loss: 0.6839874088764191, acc.: 55.96%] [G loss: 0.7284408807754517]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 79/86 [D loss: 0.6838077306747437, acc.: 56.05%] [G loss: 0.7245992422103882]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 80/86 [D loss: 0.6814011335372925, acc.: 56.01%] [G loss: 0.7222472429275513]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 81/86 [D loss: 0.6872450113296509, acc.: 54.64%] [G loss: 0.7198353409767151]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 82/86 [D loss: 0.6917667090892792, acc.: 51.37%] [G loss: 0.7135306000709534]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 83/86 [D loss: 0.6969738304615021, acc.: 48.39%] [G loss: 0.6987747550010681]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 15/200, Batch 84/86 [D loss: 0.6997326910495758, acc.: 48.54%] [G loss: 0.6943081021308899]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 15/200, Batch 85/86 [D loss: 0.7114186584949493, acc.: 44.14%] [G loss: 0.6917239427566528]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 15/200, Batch 86/86 [D loss: 0.7126058340072632, acc.: 43.51%] [G loss: 0.697594165802002]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 1/86 [D loss: 0.7071027159690857, acc.: 45.46%] [G loss: 0.7121024131774902]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 2/86 [D loss: 0.7005660533905029, acc.: 47.56%] [G loss: 0.744340181350708]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 3/86 [D loss: 0.6850679814815521, acc.: 55.81%] [G loss: 0.7675991654396057]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 4/86 [D loss: 0.6724194288253784, acc.: 61.08%] [G loss: 0.8036523461341858]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 5/86 [D loss: 0.6648142635822296, acc.: 63.38%] [G loss: 0.8289885520935059]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 6/86 [D loss: 0.6539168059825897, acc.: 66.85%] [G loss: 0.8574087023735046]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 7/86 [D loss: 0.6501895785331726, acc.: 67.72%] [G loss: 0.8566880226135254]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 8/86 [D loss: 0.6488560736179352, acc.: 68.16%] [G loss: 0.8571491241455078]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 9/86 [D loss: 0.6529766321182251, acc.: 67.53%] [G loss: 0.8450482487678528]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 16/200, Batch 10/86 [D loss: 0.6571534872055054, acc.: 67.24%] [G loss: 0.8282147645950317]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 11/86 [D loss: 0.6632562577724457, acc.: 65.77%] [G loss: 0.8071234226226807]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 12/86 [D loss: 0.6678098738193512, acc.: 63.92%] [G loss: 0.7852826118469238]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 13/86 [D loss: 0.6706463992595673, acc.: 63.53%] [G loss: 0.774425208568573]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 14/86 [D loss: 0.6749178767204285, acc.: 61.77%] [G loss: 0.7618553638458252]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 15/86 [D loss: 0.6802120804786682, acc.: 58.45%] [G loss: 0.7505049705505371]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 16/86 [D loss: 0.672119140625, acc.: 62.65%] [G loss: 0.7440701723098755]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 17/86 [D loss: 0.6744360029697418, acc.: 61.04%] [G loss: 0.7405385375022888]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 18/86 [D loss: 0.6746929585933685, acc.: 60.74%] [G loss: 0.7390144467353821]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 19/86 [D loss: 0.6721661686897278, acc.: 60.99%] [G loss: 0.7463473081588745]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 20/86 [D loss: 0.6652188301086426, acc.: 66.02%] [G loss: 0.7462912201881409]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 21/86 [D loss: 0.6670646369457245, acc.: 62.99%] [G loss: 0.7452435493469238]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 22/86 [D loss: 0.6655104160308838, acc.: 64.55%] [G loss: 0.7413355708122253]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 23/86 [D loss: 0.667566567659378, acc.: 62.65%] [G loss: 0.7365239262580872]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 24/86 [D loss: 0.6726754307746887, acc.: 59.47%] [G loss: 0.7328528165817261]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 25/86 [D loss: 0.6798398494720459, acc.: 56.15%] [G loss: 0.7217286229133606]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 26/86 [D loss: 0.6849289834499359, acc.: 53.61%] [G loss: 0.7027164697647095]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 27/86 [D loss: 0.6978553533554077, acc.: 49.76%] [G loss: 0.6853013634681702]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 28/86 [D loss: 0.709381639957428, acc.: 46.00%] [G loss: 0.6739786267280579]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 29/86 [D loss: 0.7213095724582672, acc.: 41.46%] [G loss: 0.6620499491691589]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 30/86 [D loss: 0.7222607731819153, acc.: 41.02%] [G loss: 0.6613737940788269]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 31/86 [D loss: 0.723709762096405, acc.: 40.43%] [G loss: 0.6610338687896729]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 32/86 [D loss: 0.7207834422588348, acc.: 40.58%] [G loss: 0.6682450771331787]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 33/86 [D loss: 0.7163675427436829, acc.: 41.94%] [G loss: 0.6862204670906067]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 34/86 [D loss: 0.7120601236820221, acc.: 42.33%] [G loss: 0.7085374593734741]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 35/86 [D loss: 0.7014875710010529, acc.: 46.09%] [G loss: 0.732757568359375]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 36/86 [D loss: 0.6931003928184509, acc.: 50.83%] [G loss: 0.7532718777656555]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 37/86 [D loss: 0.6838778555393219, acc.: 55.62%] [G loss: 0.7754983305931091]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 38/86 [D loss: 0.6804821491241455, acc.: 59.42%] [G loss: 0.7796272039413452]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 39/86 [D loss: 0.6798054277896881, acc.: 59.23%] [G loss: 0.7852862477302551]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 40/86 [D loss: 0.679342657327652, acc.: 58.74%] [G loss: 0.7815590500831604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 41/86 [D loss: 0.6797715127468109, acc.: 60.06%] [G loss: 0.7811911702156067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 42/86 [D loss: 0.6840896010398865, acc.: 59.52%] [G loss: 0.7689344882965088]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 43/86 [D loss: 0.6828793883323669, acc.: 59.03%] [G loss: 0.7649827003479004]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 44/86 [D loss: 0.684160053730011, acc.: 58.64%] [G loss: 0.7589902877807617]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 45/86 [D loss: 0.6806570589542389, acc.: 59.33%] [G loss: 0.7651010751724243]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 46/86 [D loss: 0.6759143471717834, acc.: 62.30%] [G loss: 0.774730920791626]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 47/86 [D loss: 0.6675384938716888, acc.: 64.55%] [G loss: 0.785340428352356]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 48/86 [D loss: 0.6570626199245453, acc.: 70.36%] [G loss: 0.7992157936096191]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 49/86 [D loss: 0.6480359137058258, acc.: 72.75%] [G loss: 0.8110153675079346]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 50/86 [D loss: 0.6394866704940796, acc.: 77.54%] [G loss: 0.8222013711929321]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 51/86 [D loss: 0.6355249285697937, acc.: 77.69%] [G loss: 0.8325103521347046]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 52/86 [D loss: 0.6296496391296387, acc.: 79.98%] [G loss: 0.8334248661994934]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 53/86 [D loss: 0.6280253529548645, acc.: 79.59%] [G loss: 0.8322423696517944]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 54/86 [D loss: 0.6260093748569489, acc.: 79.69%] [G loss: 0.8223538398742676]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 55/86 [D loss: 0.6262646019458771, acc.: 79.05%] [G loss: 0.8112308979034424]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 56/86 [D loss: 0.6317400932312012, acc.: 76.81%] [G loss: 0.7909504175186157]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 57/86 [D loss: 0.6421555876731873, acc.: 70.36%] [G loss: 0.7713359594345093]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 58/86 [D loss: 0.6576025187969208, acc.: 62.60%] [G loss: 0.7464430332183838]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 59/86 [D loss: 0.6628924608230591, acc.: 61.38%] [G loss: 0.7260481119155884]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 60/86 [D loss: 0.682036429643631, acc.: 54.44%] [G loss: 0.7060429453849792]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 61/86 [D loss: 0.6939423978328705, acc.: 50.24%] [G loss: 0.6807262897491455]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 62/86 [D loss: 0.707306444644928, acc.: 46.34%] [G loss: 0.660542368888855]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 63/86 [D loss: 0.7153945863246918, acc.: 44.34%] [G loss: 0.6624632477760315]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 64/86 [D loss: 0.7183921039104462, acc.: 42.87%] [G loss: 0.6721483469009399]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 65/86 [D loss: 0.7203009724617004, acc.: 42.33%] [G loss: 0.6772638559341431]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 16/200, Batch 66/86 [D loss: 0.7173890769481659, acc.: 41.99%] [G loss: 0.697282075881958]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 67/86 [D loss: 0.7084047496318817, acc.: 43.75%] [G loss: 0.7191277742385864]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 68/86 [D loss: 0.7046560645103455, acc.: 46.53%] [G loss: 0.7318202257156372]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 69/86 [D loss: 0.7074973583221436, acc.: 47.07%] [G loss: 0.7384527921676636]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 70/86 [D loss: 0.7044307887554169, acc.: 47.71%] [G loss: 0.7413312196731567]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 71/86 [D loss: 0.7141415178775787, acc.: 45.61%] [G loss: 0.7459608316421509]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 72/86 [D loss: 0.7138060331344604, acc.: 44.97%] [G loss: 0.7277929782867432]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 73/86 [D loss: 0.7175087034702301, acc.: 43.90%] [G loss: 0.7258206009864807]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 74/86 [D loss: 0.7188620567321777, acc.: 43.21%] [G loss: 0.7207451462745667]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 75/86 [D loss: 0.7177914083003998, acc.: 44.09%] [G loss: 0.7273387908935547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 76/86 [D loss: 0.7142982482910156, acc.: 45.95%] [G loss: 0.7370553016662598]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 77/86 [D loss: 0.7027848958969116, acc.: 49.07%] [G loss: 0.7459684610366821]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 78/86 [D loss: 0.6898503303527832, acc.: 55.18%] [G loss: 0.7705615162849426]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 79/86 [D loss: 0.6774271726608276, acc.: 62.01%] [G loss: 0.7905166149139404]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 16/200, Batch 80/86 [D loss: 0.6608082056045532, acc.: 67.97%] [G loss: 0.8189655542373657]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 81/86 [D loss: 0.6479554176330566, acc.: 71.58%] [G loss: 0.8361564874649048]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 82/86 [D loss: 0.6347843706607819, acc.: 76.12%] [G loss: 0.850383996963501]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 83/86 [D loss: 0.6264090836048126, acc.: 80.37%] [G loss: 0.8641602396965027]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 84/86 [D loss: 0.6198757886886597, acc.: 80.81%] [G loss: 0.8730629086494446]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 16/200, Batch 85/86 [D loss: 0.6153653562068939, acc.: 81.93%] [G loss: 0.8629806041717529]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 16/200, Batch 86/86 [D loss: 0.6216223537921906, acc.: 81.20%] [G loss: 0.8566393256187439]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 1/86 [D loss: 0.6219416856765747, acc.: 80.71%] [G loss: 0.8353766798973083]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 2/86 [D loss: 0.6312865912914276, acc.: 76.42%] [G loss: 0.8137437105178833]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 3/86 [D loss: 0.636215478181839, acc.: 73.44%] [G loss: 0.7909676432609558]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 4/86 [D loss: 0.6444135904312134, acc.: 70.36%] [G loss: 0.7639626860618591]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 5/86 [D loss: 0.6589768826961517, acc.: 64.45%] [G loss: 0.7501334547996521]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 6/86 [D loss: 0.6726942360401154, acc.: 57.96%] [G loss: 0.7222497463226318]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 7/86 [D loss: 0.6743829846382141, acc.: 55.96%] [G loss: 0.7105787992477417]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 8/86 [D loss: 0.6889963448047638, acc.: 52.69%] [G loss: 0.7026666402816772]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 9/86 [D loss: 0.6944491267204285, acc.: 49.90%] [G loss: 0.7031790614128113]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 10/86 [D loss: 0.6948709785938263, acc.: 51.86%] [G loss: 0.7045883536338806]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 11/86 [D loss: 0.6894070208072662, acc.: 53.32%] [G loss: 0.7102857232093811]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 12/86 [D loss: 0.6934505999088287, acc.: 51.61%] [G loss: 0.7210045456886292]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 13/86 [D loss: 0.6964407861232758, acc.: 50.15%] [G loss: 0.7264624834060669]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 14/86 [D loss: 0.6877376139163971, acc.: 54.15%] [G loss: 0.7296097278594971]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 15/86 [D loss: 0.688816249370575, acc.: 53.81%] [G loss: 0.7385808229446411]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 16/86 [D loss: 0.6987507045269012, acc.: 50.49%] [G loss: 0.728670060634613]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 17/86 [D loss: 0.6988447904586792, acc.: 50.49%] [G loss: 0.7167780995368958]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 18/86 [D loss: 0.7037254869937897, acc.: 48.14%] [G loss: 0.7088157534599304]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 19/86 [D loss: 0.7127246558666229, acc.: 45.21%] [G loss: 0.6987905502319336]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 20/86 [D loss: 0.7158719897270203, acc.: 42.68%] [G loss: 0.6950098276138306]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 21/86 [D loss: 0.7155190110206604, acc.: 42.48%] [G loss: 0.6814733743667603]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 22/86 [D loss: 0.720040500164032, acc.: 39.06%] [G loss: 0.6804823875427246]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 23/86 [D loss: 0.7220957577228546, acc.: 39.01%] [G loss: 0.6843928098678589]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 24/86 [D loss: 0.7116933465003967, acc.: 42.87%] [G loss: 0.6935804486274719]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 25/86 [D loss: 0.7053014934062958, acc.: 44.48%] [G loss: 0.7053606510162354]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 26/86 [D loss: 0.6973445117473602, acc.: 48.97%] [G loss: 0.722375750541687]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 27/86 [D loss: 0.6834482550621033, acc.: 56.49%] [G loss: 0.7415132522583008]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 28/86 [D loss: 0.672995537519455, acc.: 62.55%] [G loss: 0.758224606513977]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 29/86 [D loss: 0.6640968322753906, acc.: 67.82%] [G loss: 0.7735514640808105]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 30/86 [D loss: 0.655340850353241, acc.: 71.63%] [G loss: 0.7877930402755737]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 31/86 [D loss: 0.6482229232788086, acc.: 74.17%] [G loss: 0.8010567426681519]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 32/86 [D loss: 0.6463883817195892, acc.: 74.02%] [G loss: 0.8040632605552673]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 33/86 [D loss: 0.6430935263633728, acc.: 73.93%] [G loss: 0.80731201171875]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 34/86 [D loss: 0.6382790803909302, acc.: 76.37%] [G loss: 0.8043264150619507]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 35/86 [D loss: 0.642299622297287, acc.: 73.63%] [G loss: 0.7904090881347656]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 36/86 [D loss: 0.6437645554542542, acc.: 72.17%] [G loss: 0.7886651754379272]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 37/86 [D loss: 0.6547343134880066, acc.: 67.09%] [G loss: 0.7682188749313354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 38/86 [D loss: 0.6575674414634705, acc.: 65.33%] [G loss: 0.7570539712905884]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 39/86 [D loss: 0.6614857316017151, acc.: 65.33%] [G loss: 0.7515617609024048]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 40/86 [D loss: 0.6658481657505035, acc.: 63.77%] [G loss: 0.747453510761261]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 41/86 [D loss: 0.6653612852096558, acc.: 62.11%] [G loss: 0.7563229203224182]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 42/86 [D loss: 0.6697364449501038, acc.: 62.06%] [G loss: 0.7559607028961182]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 43/86 [D loss: 0.6654487252235413, acc.: 62.89%] [G loss: 0.767784595489502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 44/86 [D loss: 0.66385617852211, acc.: 64.26%] [G loss: 0.7868421673774719]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 45/86 [D loss: 0.6658191680908203, acc.: 64.79%] [G loss: 0.7868845462799072]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 46/86 [D loss: 0.6645064651966095, acc.: 64.70%] [G loss: 0.7975249886512756]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 47/86 [D loss: 0.6683521270751953, acc.: 62.65%] [G loss: 0.7936134338378906]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 48/86 [D loss: 0.6748690903186798, acc.: 60.45%] [G loss: 0.7761246562004089]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 49/86 [D loss: 0.6818321049213409, acc.: 57.96%] [G loss: 0.7588039636611938]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 50/86 [D loss: 0.6883588433265686, acc.: 53.32%] [G loss: 0.7449936866760254]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 51/86 [D loss: 0.6958909928798676, acc.: 50.98%] [G loss: 0.723486602306366]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 52/86 [D loss: 0.7014036178588867, acc.: 49.12%] [G loss: 0.7071189880371094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 53/86 [D loss: 0.7116838097572327, acc.: 42.48%] [G loss: 0.7058087587356567]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 54/86 [D loss: 0.709882915019989, acc.: 43.36%] [G loss: 0.6897063255310059]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 55/86 [D loss: 0.7115124762058258, acc.: 43.60%] [G loss: 0.6925565004348755]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 56/86 [D loss: 0.712912380695343, acc.: 43.02%] [G loss: 0.6863661408424377]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 57/86 [D loss: 0.7090128660202026, acc.: 44.29%] [G loss: 0.6979720592498779]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 58/86 [D loss: 0.7008078992366791, acc.: 47.75%] [G loss: 0.7043349742889404]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 59/86 [D loss: 0.6949712634086609, acc.: 50.68%] [G loss: 0.7139816880226135]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 60/86 [D loss: 0.6901589334011078, acc.: 53.52%] [G loss: 0.7248968482017517]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 61/86 [D loss: 0.682803213596344, acc.: 57.03%] [G loss: 0.7337865233421326]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 62/86 [D loss: 0.6796940267086029, acc.: 59.33%] [G loss: 0.7411707639694214]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 63/86 [D loss: 0.6761381030082703, acc.: 58.69%] [G loss: 0.7535017132759094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 64/86 [D loss: 0.6721911430358887, acc.: 61.91%] [G loss: 0.7522662878036499]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 65/86 [D loss: 0.6712096333503723, acc.: 61.47%] [G loss: 0.7527226805686951]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 17/200, Batch 66/86 [D loss: 0.6771242618560791, acc.: 57.37%] [G loss: 0.7519603967666626]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 67/86 [D loss: 0.6743241548538208, acc.: 58.64%] [G loss: 0.7456777095794678]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 68/86 [D loss: 0.6747928261756897, acc.: 59.18%] [G loss: 0.7521887421607971]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 69/86 [D loss: 0.67352095246315, acc.: 59.03%] [G loss: 0.7517944574356079]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 70/86 [D loss: 0.6696570217609406, acc.: 62.01%] [G loss: 0.7668524384498596]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 71/86 [D loss: 0.6693267226219177, acc.: 61.77%] [G loss: 0.7732363939285278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 72/86 [D loss: 0.6667475402355194, acc.: 62.74%] [G loss: 0.7813591957092285]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 73/86 [D loss: 0.6583591997623444, acc.: 66.65%] [G loss: 0.7995778918266296]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 74/86 [D loss: 0.6570743322372437, acc.: 68.02%] [G loss: 0.8074519038200378]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 75/86 [D loss: 0.6536847352981567, acc.: 69.87%] [G loss: 0.8103998899459839]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 76/86 [D loss: 0.6524029970169067, acc.: 70.12%] [G loss: 0.8147614002227783]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 77/86 [D loss: 0.6530658900737762, acc.: 68.70%] [G loss: 0.8134362101554871]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 78/86 [D loss: 0.6572239696979523, acc.: 68.55%] [G loss: 0.8054298758506775]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 79/86 [D loss: 0.6632266342639923, acc.: 66.80%] [G loss: 0.7963082790374756]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 80/86 [D loss: 0.6658541262149811, acc.: 65.28%] [G loss: 0.7807079553604126]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 81/86 [D loss: 0.6705299615859985, acc.: 62.74%] [G loss: 0.7626922726631165]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 82/86 [D loss: 0.6771240234375, acc.: 59.28%] [G loss: 0.7416503429412842]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 83/86 [D loss: 0.6822580099105835, acc.: 56.45%] [G loss: 0.7324219346046448]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 17/200, Batch 84/86 [D loss: 0.6853784620761871, acc.: 55.52%] [G loss: 0.7241718173027039]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 17/200, Batch 85/86 [D loss: 0.6873877048492432, acc.: 54.83%] [G loss: 0.7147873044013977]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 17/200, Batch 86/86 [D loss: 0.690671980381012, acc.: 50.88%] [G loss: 0.7044147849082947]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 1/86 [D loss: 0.6939464509487152, acc.: 50.78%] [G loss: 0.699612021446228]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 2/86 [D loss: 0.6916942298412323, acc.: 52.05%] [G loss: 0.6982974410057068]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 3/86 [D loss: 0.6909248232841492, acc.: 50.63%] [G loss: 0.6997699737548828]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 4/86 [D loss: 0.6868853271007538, acc.: 52.20%] [G loss: 0.702750027179718]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 5/86 [D loss: 0.6889089345932007, acc.: 52.49%] [G loss: 0.6989904642105103]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 6/86 [D loss: 0.6867161393165588, acc.: 54.20%] [G loss: 0.6987202167510986]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 7/86 [D loss: 0.6906428337097168, acc.: 50.93%] [G loss: 0.7029719948768616]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 8/86 [D loss: 0.6911724507808685, acc.: 51.56%] [G loss: 0.6978827118873596]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 9/86 [D loss: 0.6905878186225891, acc.: 52.25%] [G loss: 0.7019175887107849]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 10/86 [D loss: 0.6943593919277191, acc.: 49.66%] [G loss: 0.701227605342865]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 11/86 [D loss: 0.6945216059684753, acc.: 49.51%] [G loss: 0.7011342644691467]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 12/86 [D loss: 0.6933752298355103, acc.: 51.27%] [G loss: 0.7070410251617432]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 13/86 [D loss: 0.6933233141899109, acc.: 49.95%] [G loss: 0.70955890417099]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 14/86 [D loss: 0.6926159262657166, acc.: 50.34%] [G loss: 0.7147760391235352]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 15/86 [D loss: 0.6872124373912811, acc.: 54.64%] [G loss: 0.7287230491638184]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 16/86 [D loss: 0.6808197796344757, acc.: 57.81%] [G loss: 0.7409806251525879]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 17/86 [D loss: 0.6797489821910858, acc.: 59.03%] [G loss: 0.7598896622657776]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 18/86 [D loss: 0.670814037322998, acc.: 64.65%] [G loss: 0.7760515213012695]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 19/86 [D loss: 0.6687214076519012, acc.: 64.16%] [G loss: 0.7935212850570679]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 20/86 [D loss: 0.6607222259044647, acc.: 67.19%] [G loss: 0.8047589063644409]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 21/86 [D loss: 0.6607397198677063, acc.: 66.75%] [G loss: 0.8147439360618591]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 22/86 [D loss: 0.6579402685165405, acc.: 67.53%] [G loss: 0.8123570084571838]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 23/86 [D loss: 0.6557851731777191, acc.: 69.34%] [G loss: 0.8163063526153564]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 24/86 [D loss: 0.6570172011852264, acc.: 69.97%] [G loss: 0.8057005405426025]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 25/86 [D loss: 0.6584367454051971, acc.: 68.70%] [G loss: 0.8023607134819031]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 26/86 [D loss: 0.6639848053455353, acc.: 66.70%] [G loss: 0.7883193492889404]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 27/86 [D loss: 0.6666578054428101, acc.: 65.92%] [G loss: 0.7798877954483032]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 28/86 [D loss: 0.6680378317832947, acc.: 64.60%] [G loss: 0.7656378746032715]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 29/86 [D loss: 0.6705569922924042, acc.: 63.57%] [G loss: 0.7535696029663086]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 30/86 [D loss: 0.6731288135051727, acc.: 62.70%] [G loss: 0.744662880897522]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 31/86 [D loss: 0.6769492626190186, acc.: 60.01%] [G loss: 0.7361112833023071]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 32/86 [D loss: 0.677150547504425, acc.: 60.94%] [G loss: 0.7325014472007751]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 33/86 [D loss: 0.6785269677639008, acc.: 58.79%] [G loss: 0.7285798192024231]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 34/86 [D loss: 0.6781474053859711, acc.: 59.52%] [G loss: 0.7228208780288696]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 35/86 [D loss: 0.6798430681228638, acc.: 58.54%] [G loss: 0.7194303274154663]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 36/86 [D loss: 0.680648684501648, acc.: 58.35%] [G loss: 0.72194904088974]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 37/86 [D loss: 0.6843656599521637, acc.: 56.40%] [G loss: 0.7158641815185547]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 38/86 [D loss: 0.6886768043041229, acc.: 54.49%] [G loss: 0.7104913592338562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 39/86 [D loss: 0.690485954284668, acc.: 52.64%] [G loss: 0.7056651711463928]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 40/86 [D loss: 0.6918206512928009, acc.: 53.66%] [G loss: 0.7054988145828247]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 41/86 [D loss: 0.6953740417957306, acc.: 50.49%] [G loss: 0.7062296867370605]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 42/86 [D loss: 0.6990451812744141, acc.: 48.63%] [G loss: 0.7009070515632629]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 43/86 [D loss: 0.6985926032066345, acc.: 50.63%] [G loss: 0.7129924297332764]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 44/86 [D loss: 0.6911528706550598, acc.: 52.78%] [G loss: 0.7176052331924438]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 45/86 [D loss: 0.6902210414409637, acc.: 53.66%] [G loss: 0.7290972471237183]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 46/86 [D loss: 0.6845903098583221, acc.: 55.66%] [G loss: 0.7440701723098755]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 47/86 [D loss: 0.6847911179065704, acc.: 54.79%] [G loss: 0.7535421848297119]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 48/86 [D loss: 0.6748725175857544, acc.: 62.11%] [G loss: 0.7714881896972656]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 49/86 [D loss: 0.675153523683548, acc.: 59.86%] [G loss: 0.7744225263595581]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 50/86 [D loss: 0.674197643995285, acc.: 60.74%] [G loss: 0.7833044528961182]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 51/86 [D loss: 0.6697019636631012, acc.: 63.28%] [G loss: 0.7874707579612732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 52/86 [D loss: 0.6640495955944061, acc.: 65.92%] [G loss: 0.7834911346435547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 53/86 [D loss: 0.6657459437847137, acc.: 66.50%] [G loss: 0.7846803069114685]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 54/86 [D loss: 0.6678928434848785, acc.: 65.43%] [G loss: 0.7791237831115723]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 55/86 [D loss: 0.6674081385135651, acc.: 65.33%] [G loss: 0.7766382694244385]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 56/86 [D loss: 0.6681132912635803, acc.: 66.16%] [G loss: 0.7608806490898132]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 57/86 [D loss: 0.6674099564552307, acc.: 66.50%] [G loss: 0.7579150795936584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 58/86 [D loss: 0.6674301624298096, acc.: 66.60%] [G loss: 0.7540475726127625]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 59/86 [D loss: 0.6712723672389984, acc.: 62.79%] [G loss: 0.7474231719970703]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 60/86 [D loss: 0.6711449027061462, acc.: 63.53%] [G loss: 0.7483967542648315]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 61/86 [D loss: 0.6700318455696106, acc.: 64.89%] [G loss: 0.7468055486679077]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 62/86 [D loss: 0.6685677468776703, acc.: 64.89%] [G loss: 0.7405626177787781]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 63/86 [D loss: 0.6717033684253693, acc.: 62.50%] [G loss: 0.7364417314529419]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 64/86 [D loss: 0.672286331653595, acc.: 62.06%] [G loss: 0.7293068766593933]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 65/86 [D loss: 0.6732045412063599, acc.: 61.28%] [G loss: 0.7269541025161743]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 66/86 [D loss: 0.6732059419155121, acc.: 61.38%] [G loss: 0.7216472625732422]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 67/86 [D loss: 0.6809183955192566, acc.: 57.18%] [G loss: 0.7180700302124023]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 68/86 [D loss: 0.6781129240989685, acc.: 58.54%] [G loss: 0.7169912457466125]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 69/86 [D loss: 0.6833607256412506, acc.: 56.69%] [G loss: 0.7129478454589844]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 70/86 [D loss: 0.6824645698070526, acc.: 55.76%] [G loss: 0.7134073376655579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 71/86 [D loss: 0.6886608600616455, acc.: 53.37%] [G loss: 0.7068068981170654]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 72/86 [D loss: 0.6880056858062744, acc.: 54.59%] [G loss: 0.7079857587814331]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 73/86 [D loss: 0.6858794987201691, acc.: 53.47%] [G loss: 0.7168734073638916]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 74/86 [D loss: 0.6894769966602325, acc.: 54.00%] [G loss: 0.7178261876106262]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 75/86 [D loss: 0.687148928642273, acc.: 55.52%] [G loss: 0.72237229347229]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 18/200, Batch 76/86 [D loss: 0.6903542876243591, acc.: 52.15%] [G loss: 0.7301633358001709]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 77/86 [D loss: 0.6868994534015656, acc.: 54.39%] [G loss: 0.7392374873161316]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 78/86 [D loss: 0.684014618396759, acc.: 56.74%] [G loss: 0.7419546842575073]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 79/86 [D loss: 0.679599791765213, acc.: 59.28%] [G loss: 0.7475449442863464]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 80/86 [D loss: 0.6844539046287537, acc.: 56.10%] [G loss: 0.7493127584457397]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 81/86 [D loss: 0.6789595782756805, acc.: 59.62%] [G loss: 0.752296507358551]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 82/86 [D loss: 0.6799628734588623, acc.: 59.47%] [G loss: 0.755472719669342]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 18/200, Batch 83/86 [D loss: 0.6800131499767303, acc.: 59.57%] [G loss: 0.7534563541412354]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 84/86 [D loss: 0.6742156744003296, acc.: 62.30%] [G loss: 0.7587140202522278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 85/86 [D loss: 0.6750485301017761, acc.: 62.99%] [G loss: 0.7540329694747925]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 18/200, Batch 86/86 [D loss: 0.6726444661617279, acc.: 62.74%] [G loss: 0.7514320611953735]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 1/86 [D loss: 0.6718452274799347, acc.: 64.55%] [G loss: 0.7560254335403442]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 2/86 [D loss: 0.6656648516654968, acc.: 67.24%] [G loss: 0.7568942904472351]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 3/86 [D loss: 0.667363852262497, acc.: 66.41%] [G loss: 0.7580851912498474]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 4/86 [D loss: 0.6654387712478638, acc.: 66.31%] [G loss: 0.7599500417709351]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 5/86 [D loss: 0.6612350344657898, acc.: 67.92%] [G loss: 0.7608972787857056]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 6/86 [D loss: 0.6649972498416901, acc.: 66.46%] [G loss: 0.7557018399238586]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 7/86 [D loss: 0.6616314947605133, acc.: 67.72%] [G loss: 0.7480090856552124]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 8/86 [D loss: 0.661533772945404, acc.: 65.97%] [G loss: 0.7496815919876099]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 9/86 [D loss: 0.668247789144516, acc.: 62.50%] [G loss: 0.7438172101974487]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 10/86 [D loss: 0.6675604581832886, acc.: 64.16%] [G loss: 0.7415472269058228]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 11/86 [D loss: 0.6727100014686584, acc.: 62.26%] [G loss: 0.730631411075592]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 12/86 [D loss: 0.6786698400974274, acc.: 59.23%] [G loss: 0.7293259501457214]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 13/86 [D loss: 0.6755940914154053, acc.: 59.86%] [G loss: 0.7326419949531555]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 14/86 [D loss: 0.6769748628139496, acc.: 59.81%] [G loss: 0.7325243353843689]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 15/86 [D loss: 0.6830736100673676, acc.: 55.57%] [G loss: 0.7301276922225952]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 16/86 [D loss: 0.6807370781898499, acc.: 57.52%] [G loss: 0.7340492010116577]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 17/86 [D loss: 0.6857612133026123, acc.: 56.40%] [G loss: 0.7334846258163452]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 18/86 [D loss: 0.6838347613811493, acc.: 55.86%] [G loss: 0.7345722913742065]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 19/86 [D loss: 0.6819154322147369, acc.: 57.62%] [G loss: 0.7406217455863953]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 20/86 [D loss: 0.6871181130409241, acc.: 54.74%] [G loss: 0.7437453269958496]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 21/86 [D loss: 0.6888464689254761, acc.: 54.49%] [G loss: 0.7371228933334351]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 22/86 [D loss: 0.6864898800849915, acc.: 54.74%] [G loss: 0.7369884848594666]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 23/86 [D loss: 0.6855369210243225, acc.: 55.62%] [G loss: 0.739501416683197]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 24/86 [D loss: 0.685091495513916, acc.: 55.08%] [G loss: 0.7403964400291443]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 25/86 [D loss: 0.6849176585674286, acc.: 56.98%] [G loss: 0.7386056184768677]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 26/86 [D loss: 0.6850622296333313, acc.: 55.62%] [G loss: 0.7384055256843567]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 27/86 [D loss: 0.6843434870243073, acc.: 55.66%] [G loss: 0.7400591373443604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 28/86 [D loss: 0.6778836846351624, acc.: 60.40%] [G loss: 0.7452632188796997]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 29/86 [D loss: 0.6771878302097321, acc.: 59.91%] [G loss: 0.7511889934539795]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 30/86 [D loss: 0.6732059419155121, acc.: 62.30%] [G loss: 0.7559162378311157]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 31/86 [D loss: 0.6733334064483643, acc.: 61.96%] [G loss: 0.753263533115387]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 32/86 [D loss: 0.6683180034160614, acc.: 64.89%] [G loss: 0.7537912130355835]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 33/86 [D loss: 0.6694048643112183, acc.: 63.77%] [G loss: 0.7589836120605469]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 34/86 [D loss: 0.6666077077388763, acc.: 64.16%] [G loss: 0.7607016563415527]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 35/86 [D loss: 0.6691595017910004, acc.: 63.28%] [G loss: 0.7595241069793701]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 36/86 [D loss: 0.6680566966533661, acc.: 63.72%] [G loss: 0.7532157301902771]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 37/86 [D loss: 0.6645295023918152, acc.: 65.19%] [G loss: 0.75160813331604]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 38/86 [D loss: 0.6683474183082581, acc.: 63.72%] [G loss: 0.7528543472290039]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 39/86 [D loss: 0.6652229726314545, acc.: 65.14%] [G loss: 0.7505722045898438]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 40/86 [D loss: 0.6694922745227814, acc.: 63.18%] [G loss: 0.7512205839157104]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 41/86 [D loss: 0.6693612337112427, acc.: 64.26%] [G loss: 0.7529867887496948]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 42/86 [D loss: 0.67424476146698, acc.: 60.74%] [G loss: 0.7523002028465271]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 43/86 [D loss: 0.6689497232437134, acc.: 63.62%] [G loss: 0.7521894574165344]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 44/86 [D loss: 0.670786052942276, acc.: 62.89%] [G loss: 0.7500169277191162]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 45/86 [D loss: 0.6731017827987671, acc.: 62.45%] [G loss: 0.7510851621627808]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 46/86 [D loss: 0.6767337918281555, acc.: 61.23%] [G loss: 0.7432880401611328]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 47/86 [D loss: 0.6788923442363739, acc.: 59.42%] [G loss: 0.7424112558364868]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 48/86 [D loss: 0.6788347363471985, acc.: 59.18%] [G loss: 0.7370319366455078]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 49/86 [D loss: 0.6804542243480682, acc.: 58.01%] [G loss: 0.7370107173919678]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 50/86 [D loss: 0.6809360682964325, acc.: 57.86%] [G loss: 0.7305347919464111]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 51/86 [D loss: 0.6822721362113953, acc.: 56.59%] [G loss: 0.7293948531150818]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 52/86 [D loss: 0.6847000122070312, acc.: 56.49%] [G loss: 0.7301706075668335]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 53/86 [D loss: 0.6857417821884155, acc.: 55.52%] [G loss: 0.7278863787651062]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 54/86 [D loss: 0.6821687817573547, acc.: 57.96%] [G loss: 0.7283617258071899]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 55/86 [D loss: 0.6856570243835449, acc.: 54.98%] [G loss: 0.7269417643547058]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 56/86 [D loss: 0.6811365783214569, acc.: 56.88%] [G loss: 0.7278475761413574]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 57/86 [D loss: 0.6821101903915405, acc.: 56.20%] [G loss: 0.7360104322433472]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 58/86 [D loss: 0.6790498495101929, acc.: 60.16%] [G loss: 0.7362446784973145]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 59/86 [D loss: 0.6773353815078735, acc.: 59.42%] [G loss: 0.7360974550247192]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 60/86 [D loss: 0.6735533177852631, acc.: 62.70%] [G loss: 0.740268886089325]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 61/86 [D loss: 0.6752946674823761, acc.: 60.35%] [G loss: 0.7430734038352966]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 62/86 [D loss: 0.6721822917461395, acc.: 63.33%] [G loss: 0.7454652786254883]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 63/86 [D loss: 0.6710613369941711, acc.: 63.33%] [G loss: 0.755328893661499]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 64/86 [D loss: 0.6699953973293304, acc.: 63.77%] [G loss: 0.7584445476531982]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 65/86 [D loss: 0.6652093827724457, acc.: 67.14%] [G loss: 0.7589704394340515]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 66/86 [D loss: 0.6668088138103485, acc.: 66.31%] [G loss: 0.764911413192749]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 67/86 [D loss: 0.6664352416992188, acc.: 65.67%] [G loss: 0.763683021068573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 68/86 [D loss: 0.6661593914031982, acc.: 65.19%] [G loss: 0.7638934254646301]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 19/200, Batch 69/86 [D loss: 0.6638782620429993, acc.: 66.31%] [G loss: 0.7664639949798584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 70/86 [D loss: 0.665223091840744, acc.: 66.60%] [G loss: 0.7612247467041016]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 71/86 [D loss: 0.6675919592380524, acc.: 64.94%] [G loss: 0.7586721181869507]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 72/86 [D loss: 0.6667716801166534, acc.: 63.72%] [G loss: 0.7565265893936157]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 73/86 [D loss: 0.6657792031764984, acc.: 65.87%] [G loss: 0.75899338722229]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 74/86 [D loss: 0.6685169339179993, acc.: 64.55%] [G loss: 0.7500739693641663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 75/86 [D loss: 0.669582724571228, acc.: 65.04%] [G loss: 0.7491961717605591]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 76/86 [D loss: 0.6720545589923859, acc.: 62.06%] [G loss: 0.7437846064567566]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 77/86 [D loss: 0.6750040352344513, acc.: 61.43%] [G loss: 0.7363643646240234]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 78/86 [D loss: 0.6771577000617981, acc.: 59.38%] [G loss: 0.7343230843544006]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 79/86 [D loss: 0.6795760989189148, acc.: 59.13%] [G loss: 0.7277305126190186]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 80/86 [D loss: 0.6805517077445984, acc.: 58.20%] [G loss: 0.7312734723091125]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 81/86 [D loss: 0.6829239428043365, acc.: 57.08%] [G loss: 0.7239286303520203]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 82/86 [D loss: 0.6827170550823212, acc.: 57.76%] [G loss: 0.7225011587142944]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 19/200, Batch 83/86 [D loss: 0.6817370355129242, acc.: 57.91%] [G loss: 0.7261611223220825]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 84/86 [D loss: 0.6848189532756805, acc.: 55.03%] [G loss: 0.7236177921295166]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 19/200, Batch 85/86 [D loss: 0.6848604381084442, acc.: 55.08%] [G loss: 0.7251999974250793]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 19/200, Batch 86/86 [D loss: 0.6821949183940887, acc.: 56.98%] [G loss: 0.7239819765090942]\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 1/86 [D loss: 0.6785802841186523, acc.: 59.23%] [G loss: 0.7263522744178772]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 2/86 [D loss: 0.681580662727356, acc.: 57.62%] [G loss: 0.7275051474571228]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 3/86 [D loss: 0.6819367706775665, acc.: 57.08%] [G loss: 0.7289515733718872]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 4/86 [D loss: 0.6797960996627808, acc.: 58.30%] [G loss: 0.7397207021713257]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 5/86 [D loss: 0.6778242588043213, acc.: 58.98%] [G loss: 0.7435429096221924]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 20/200, Batch 6/86 [D loss: 0.6779879927635193, acc.: 60.06%] [G loss: 0.7423527240753174]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 7/86 [D loss: 0.6757368445396423, acc.: 60.74%] [G loss: 0.7484129071235657]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 8/86 [D loss: 0.6742954850196838, acc.: 60.55%] [G loss: 0.750292181968689]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 9/86 [D loss: 0.6697103679180145, acc.: 64.79%] [G loss: 0.7563520073890686]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 10/86 [D loss: 0.6686660945415497, acc.: 64.40%] [G loss: 0.7560746669769287]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 11/86 [D loss: 0.6687812507152557, acc.: 64.01%] [G loss: 0.7610729932785034]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 12/86 [D loss: 0.6664460003376007, acc.: 65.43%] [G loss: 0.7570081949234009]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 13/86 [D loss: 0.6674796342849731, acc.: 65.09%] [G loss: 0.7602933645248413]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 14/86 [D loss: 0.6675113439559937, acc.: 65.38%] [G loss: 0.7572026252746582]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 15/86 [D loss: 0.6675971150398254, acc.: 64.89%] [G loss: 0.7515463829040527]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 16/86 [D loss: 0.668480634689331, acc.: 64.31%] [G loss: 0.7535256743431091]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 17/86 [D loss: 0.6667959690093994, acc.: 64.16%] [G loss: 0.7515518665313721]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 18/86 [D loss: 0.6681693196296692, acc.: 64.94%] [G loss: 0.7485199570655823]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 19/86 [D loss: 0.6691454350948334, acc.: 63.18%] [G loss: 0.7496569156646729]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 20/86 [D loss: 0.6747949123382568, acc.: 60.50%] [G loss: 0.7464045286178589]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 21/86 [D loss: 0.6746939122676849, acc.: 59.96%] [G loss: 0.739359438419342]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 22/86 [D loss: 0.6769920885562897, acc.: 58.69%] [G loss: 0.7342701554298401]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 23/86 [D loss: 0.6757640838623047, acc.: 60.64%] [G loss: 0.736857533454895]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 24/86 [D loss: 0.6788750290870667, acc.: 58.54%] [G loss: 0.7280450463294983]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 25/86 [D loss: 0.6784509718418121, acc.: 56.84%] [G loss: 0.7246284484863281]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 26/86 [D loss: 0.6821436882019043, acc.: 57.23%] [G loss: 0.7308127284049988]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 27/86 [D loss: 0.6784477233886719, acc.: 58.01%] [G loss: 0.7345728874206543]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 28/86 [D loss: 0.6812804937362671, acc.: 56.98%] [G loss: 0.7315527200698853]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 29/86 [D loss: 0.6840779781341553, acc.: 56.10%] [G loss: 0.7334496974945068]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 30/86 [D loss: 0.6779612302780151, acc.: 59.38%] [G loss: 0.7380936741828918]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 31/86 [D loss: 0.6809876561164856, acc.: 57.76%] [G loss: 0.7473918795585632]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 32/86 [D loss: 0.6823077499866486, acc.: 55.57%] [G loss: 0.7441437840461731]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 33/86 [D loss: 0.6782574653625488, acc.: 59.33%] [G loss: 0.7501504421234131]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 34/86 [D loss: 0.6779391169548035, acc.: 58.74%] [G loss: 0.7511178851127625]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 35/86 [D loss: 0.6773093640804291, acc.: 59.23%] [G loss: 0.7585238814353943]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 36/86 [D loss: 0.6730005741119385, acc.: 62.21%] [G loss: 0.7539032101631165]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 37/86 [D loss: 0.6736177802085876, acc.: 62.01%] [G loss: 0.7594606876373291]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 38/86 [D loss: 0.6732011437416077, acc.: 61.77%] [G loss: 0.7665881514549255]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 39/86 [D loss: 0.6721343994140625, acc.: 62.26%] [G loss: 0.7702323198318481]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 40/86 [D loss: 0.6679877042770386, acc.: 63.72%] [G loss: 0.7664699554443359]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 41/86 [D loss: 0.668086588382721, acc.: 64.84%] [G loss: 0.7686261534690857]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 42/86 [D loss: 0.6659059226512909, acc.: 66.06%] [G loss: 0.7635225057601929]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 43/86 [D loss: 0.6656610667705536, acc.: 66.60%] [G loss: 0.7612261176109314]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 44/86 [D loss: 0.6622978150844574, acc.: 65.58%] [G loss: 0.7637393474578857]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 45/86 [D loss: 0.6664899289608002, acc.: 64.65%] [G loss: 0.7573699355125427]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 46/86 [D loss: 0.6664068400859833, acc.: 65.67%] [G loss: 0.7557312846183777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 47/86 [D loss: 0.6677935719490051, acc.: 64.01%] [G loss: 0.7607730031013489]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 48/86 [D loss: 0.6689642369747162, acc.: 62.74%] [G loss: 0.7559446692466736]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 49/86 [D loss: 0.6706225872039795, acc.: 63.33%] [G loss: 0.754605770111084]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 50/86 [D loss: 0.6695758998394012, acc.: 62.94%] [G loss: 0.7514204978942871]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 51/86 [D loss: 0.6749222278594971, acc.: 60.84%] [G loss: 0.7484643459320068]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 52/86 [D loss: 0.6717912256717682, acc.: 61.87%] [G loss: 0.7422670722007751]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 53/86 [D loss: 0.6766696274280548, acc.: 59.96%] [G loss: 0.74294114112854]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 54/86 [D loss: 0.6765809953212738, acc.: 59.28%] [G loss: 0.7461926937103271]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 55/86 [D loss: 0.6779747009277344, acc.: 60.79%] [G loss: 0.7460842132568359]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 56/86 [D loss: 0.6820769608020782, acc.: 56.59%] [G loss: 0.7391066551208496]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 57/86 [D loss: 0.6806091666221619, acc.: 58.35%] [G loss: 0.7340556383132935]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 58/86 [D loss: 0.6803881227970123, acc.: 58.11%] [G loss: 0.742262601852417]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 59/86 [D loss: 0.6776913106441498, acc.: 59.81%] [G loss: 0.7399862408638]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 60/86 [D loss: 0.6805300116539001, acc.: 57.47%] [G loss: 0.7367479205131531]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 61/86 [D loss: 0.6776325702667236, acc.: 59.03%] [G loss: 0.7429054379463196]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 62/86 [D loss: 0.6797216832637787, acc.: 57.96%] [G loss: 0.7450590133666992]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 63/86 [D loss: 0.6765604019165039, acc.: 60.35%] [G loss: 0.7448940277099609]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 64/86 [D loss: 0.6739211678504944, acc.: 60.11%] [G loss: 0.7481064200401306]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 65/86 [D loss: 0.6700005233287811, acc.: 63.13%] [G loss: 0.7445098161697388]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 66/86 [D loss: 0.66803839802742, acc.: 63.72%] [G loss: 0.7494959235191345]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 67/86 [D loss: 0.6722282469272614, acc.: 61.96%] [G loss: 0.7559186220169067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 68/86 [D loss: 0.6695675849914551, acc.: 63.43%] [G loss: 0.7526766061782837]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 69/86 [D loss: 0.6688931584358215, acc.: 63.67%] [G loss: 0.7527104020118713]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 70/86 [D loss: 0.6679653227329254, acc.: 63.28%] [G loss: 0.757262647151947]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 71/86 [D loss: 0.6687676012516022, acc.: 64.79%] [G loss: 0.7516077160835266]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 20/200, Batch 72/86 [D loss: 0.6710690557956696, acc.: 62.45%] [G loss: 0.7509766221046448]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 73/86 [D loss: 0.6707166433334351, acc.: 62.55%] [G loss: 0.7589508891105652]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 74/86 [D loss: 0.6686058044433594, acc.: 63.67%] [G loss: 0.7602024078369141]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 75/86 [D loss: 0.6685840487480164, acc.: 63.77%] [G loss: 0.7550565004348755]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 76/86 [D loss: 0.6706140041351318, acc.: 63.18%] [G loss: 0.7545764446258545]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 20/200, Batch 77/86 [D loss: 0.6725157499313354, acc.: 61.23%] [G loss: 0.7488624453544617]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 78/86 [D loss: 0.6698932945728302, acc.: 63.82%] [G loss: 0.749928891658783]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 79/86 [D loss: 0.6752519607543945, acc.: 60.45%] [G loss: 0.7459279298782349]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 80/86 [D loss: 0.6719179153442383, acc.: 61.23%] [G loss: 0.7388917207717896]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 81/86 [D loss: 0.6727036833763123, acc.: 61.28%] [G loss: 0.7398540377616882]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 20/200, Batch 82/86 [D loss: 0.6735339462757111, acc.: 59.96%] [G loss: 0.7416982650756836]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 83/86 [D loss: 0.6732190847396851, acc.: 61.52%] [G loss: 0.7337694764137268]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 20/200, Batch 84/86 [D loss: 0.6776024103164673, acc.: 59.28%] [G loss: 0.7359798550605774]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 85/86 [D loss: 0.6765413284301758, acc.: 57.86%] [G loss: 0.7391846179962158]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 20/200, Batch 86/86 [D loss: 0.6777021884918213, acc.: 57.71%] [G loss: 0.7347782254219055]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 1/86 [D loss: 0.6791382730007172, acc.: 58.50%] [G loss: 0.7436386346817017]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 2/86 [D loss: 0.6768873333930969, acc.: 59.47%] [G loss: 0.743462324142456]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 3/86 [D loss: 0.6752176880836487, acc.: 60.69%] [G loss: 0.7433714270591736]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 4/86 [D loss: 0.6766898036003113, acc.: 59.67%] [G loss: 0.7494503855705261]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 5/86 [D loss: 0.6732989847660065, acc.: 61.18%] [G loss: 0.7411178946495056]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 6/86 [D loss: 0.6746626198291779, acc.: 59.38%] [G loss: 0.748283326625824]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 7/86 [D loss: 0.6747631132602692, acc.: 60.45%] [G loss: 0.7533034086227417]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 8/86 [D loss: 0.6710904836654663, acc.: 62.26%] [G loss: 0.7451566457748413]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 9/86 [D loss: 0.6750154495239258, acc.: 60.55%] [G loss: 0.7526757121086121]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 10/86 [D loss: 0.6721116602420807, acc.: 61.18%] [G loss: 0.7503895163536072]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 11/86 [D loss: 0.6746667921543121, acc.: 60.25%] [G loss: 0.7522251605987549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 12/86 [D loss: 0.6704435646533966, acc.: 61.62%] [G loss: 0.7487585544586182]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 13/86 [D loss: 0.6695950031280518, acc.: 63.92%] [G loss: 0.7562732696533203]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 14/86 [D loss: 0.6694663166999817, acc.: 61.18%] [G loss: 0.7517074346542358]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 15/86 [D loss: 0.6713605225086212, acc.: 62.40%] [G loss: 0.7518021464347839]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 16/86 [D loss: 0.6705144643783569, acc.: 63.13%] [G loss: 0.7566838264465332]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 17/86 [D loss: 0.6686770021915436, acc.: 63.67%] [G loss: 0.7515009641647339]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 18/86 [D loss: 0.6661988496780396, acc.: 63.67%] [G loss: 0.7509757280349731]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 19/86 [D loss: 0.6661688685417175, acc.: 64.36%] [G loss: 0.7484278082847595]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 20/86 [D loss: 0.6691223680973053, acc.: 63.28%] [G loss: 0.7552087306976318]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 21/86 [D loss: 0.6697151362895966, acc.: 62.94%] [G loss: 0.7504621148109436]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 22/86 [D loss: 0.6700391471385956, acc.: 63.38%] [G loss: 0.7557415962219238]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 23/86 [D loss: 0.671179324388504, acc.: 62.40%] [G loss: 0.7438265681266785]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 24/86 [D loss: 0.6701484620571136, acc.: 61.87%] [G loss: 0.7479966878890991]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 25/86 [D loss: 0.6706845462322235, acc.: 63.28%] [G loss: 0.7475241422653198]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 26/86 [D loss: 0.6726263463497162, acc.: 61.57%] [G loss: 0.742363452911377]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 27/86 [D loss: 0.6731657087802887, acc.: 60.55%] [G loss: 0.7473138570785522]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 28/86 [D loss: 0.6756078004837036, acc.: 60.25%] [G loss: 0.7471899390220642]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 29/86 [D loss: 0.6734488010406494, acc.: 59.91%] [G loss: 0.7408748865127563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 30/86 [D loss: 0.6752901077270508, acc.: 59.47%] [G loss: 0.7475723624229431]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 31/86 [D loss: 0.6744398176670074, acc.: 59.72%] [G loss: 0.745648980140686]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 32/86 [D loss: 0.6749264895915985, acc.: 60.06%] [G loss: 0.7489258646965027]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 33/86 [D loss: 0.6763624548912048, acc.: 59.18%] [G loss: 0.7439393401145935]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 34/86 [D loss: 0.6730816960334778, acc.: 60.89%] [G loss: 0.7397832870483398]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 35/86 [D loss: 0.6776817142963409, acc.: 58.84%] [G loss: 0.7358845472335815]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 36/86 [D loss: 0.6758296489715576, acc.: 61.04%] [G loss: 0.7406476736068726]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 37/86 [D loss: 0.6736500561237335, acc.: 61.13%] [G loss: 0.7382857203483582]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 38/86 [D loss: 0.6738370060920715, acc.: 59.77%] [G loss: 0.7404665946960449]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 39/86 [D loss: 0.6725532412528992, acc.: 61.04%] [G loss: 0.7416129112243652]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 40/86 [D loss: 0.6739091873168945, acc.: 60.74%] [G loss: 0.7494692802429199]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 41/86 [D loss: 0.6730409264564514, acc.: 60.40%] [G loss: 0.7454150915145874]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 21/200, Batch 42/86 [D loss: 0.6739048361778259, acc.: 59.47%] [G loss: 0.7480965256690979]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 43/86 [D loss: 0.6714595556259155, acc.: 61.52%] [G loss: 0.7553200721740723]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 44/86 [D loss: 0.66818568110466, acc.: 62.26%] [G loss: 0.7558314204216003]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 45/86 [D loss: 0.6705518662929535, acc.: 61.87%] [G loss: 0.759991466999054]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 46/86 [D loss: 0.6679410040378571, acc.: 62.30%] [G loss: 0.763263463973999]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 47/86 [D loss: 0.6680538058280945, acc.: 62.55%] [G loss: 0.7729672789573669]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 48/86 [D loss: 0.668489933013916, acc.: 62.94%] [G loss: 0.7639274597167969]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 49/86 [D loss: 0.6673420965671539, acc.: 63.18%] [G loss: 0.7743163704872131]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 50/86 [D loss: 0.6648260056972504, acc.: 64.11%] [G loss: 0.7821847200393677]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 51/86 [D loss: 0.6658854484558105, acc.: 64.36%] [G loss: 0.7754234671592712]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 52/86 [D loss: 0.6664496958255768, acc.: 62.06%] [G loss: 0.7817311882972717]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 53/86 [D loss: 0.6654199361801147, acc.: 64.26%] [G loss: 0.7721338272094727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 54/86 [D loss: 0.6668834388256073, acc.: 63.53%] [G loss: 0.7766168117523193]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 55/86 [D loss: 0.6673215925693512, acc.: 62.30%] [G loss: 0.7771878838539124]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 56/86 [D loss: 0.6679107546806335, acc.: 61.57%] [G loss: 0.7660702466964722]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 57/86 [D loss: 0.6743004620075226, acc.: 59.38%] [G loss: 0.7616662979125977]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 58/86 [D loss: 0.6758505403995514, acc.: 58.45%] [G loss: 0.7472378015518188]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 59/86 [D loss: 0.6722391247749329, acc.: 60.84%] [G loss: 0.750207781791687]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 60/86 [D loss: 0.6807472705841064, acc.: 57.08%] [G loss: 0.7411964535713196]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 61/86 [D loss: 0.6827383935451508, acc.: 55.18%] [G loss: 0.7397781610488892]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 62/86 [D loss: 0.6868096590042114, acc.: 55.96%] [G loss: 0.7355592846870422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 63/86 [D loss: 0.680191159248352, acc.: 57.23%] [G loss: 0.728360652923584]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 64/86 [D loss: 0.6817712783813477, acc.: 56.40%] [G loss: 0.7380605936050415]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 65/86 [D loss: 0.6839125156402588, acc.: 55.42%] [G loss: 0.7415735721588135]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 66/86 [D loss: 0.6809468865394592, acc.: 57.76%] [G loss: 0.7506454586982727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 67/86 [D loss: 0.6762290894985199, acc.: 59.42%] [G loss: 0.7525535225868225]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 68/86 [D loss: 0.6756725013256073, acc.: 59.86%] [G loss: 0.7563351392745972]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 69/86 [D loss: 0.6730384528636932, acc.: 60.16%] [G loss: 0.7669028043746948]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 70/86 [D loss: 0.6664416790008545, acc.: 63.96%] [G loss: 0.7764662504196167]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 71/86 [D loss: 0.6648421883583069, acc.: 63.23%] [G loss: 0.7846884727478027]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 72/86 [D loss: 0.6603953540325165, acc.: 65.82%] [G loss: 0.78428053855896]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 73/86 [D loss: 0.6563476324081421, acc.: 67.68%] [G loss: 0.7882643342018127]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 74/86 [D loss: 0.6599859893321991, acc.: 67.58%] [G loss: 0.7855758666992188]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 75/86 [D loss: 0.6538559198379517, acc.: 68.55%] [G loss: 0.7835463285446167]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 76/86 [D loss: 0.6544674038887024, acc.: 68.65%] [G loss: 0.7757331132888794]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 77/86 [D loss: 0.6577868461608887, acc.: 68.60%] [G loss: 0.7681275010108948]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 78/86 [D loss: 0.6593180000782013, acc.: 65.67%] [G loss: 0.7622172832489014]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 79/86 [D loss: 0.658074676990509, acc.: 65.23%] [G loss: 0.763236939907074]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 80/86 [D loss: 0.6632285714149475, acc.: 64.70%] [G loss: 0.7452551126480103]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 81/86 [D loss: 0.6670661270618439, acc.: 62.40%] [G loss: 0.7419809103012085]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 21/200, Batch 82/86 [D loss: 0.6709372997283936, acc.: 60.69%] [G loss: 0.736888587474823]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 83/86 [D loss: 0.6762130260467529, acc.: 59.18%] [G loss: 0.7281826734542847]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 21/200, Batch 84/86 [D loss: 0.679192066192627, acc.: 57.86%] [G loss: 0.736020565032959]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 85/86 [D loss: 0.6850090324878693, acc.: 55.22%] [G loss: 0.7357617616653442]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 21/200, Batch 86/86 [D loss: 0.6839005053043365, acc.: 56.79%] [G loss: 0.7374862432479858]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 1/86 [D loss: 0.6846832931041718, acc.: 55.52%] [G loss: 0.7436323165893555]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 2/86 [D loss: 0.6838095486164093, acc.: 56.59%] [G loss: 0.7382356524467468]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 3/86 [D loss: 0.6871282160282135, acc.: 55.86%] [G loss: 0.7456215620040894]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 4/86 [D loss: 0.6830539405345917, acc.: 55.76%] [G loss: 0.7461845278739929]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 5/86 [D loss: 0.6813470721244812, acc.: 57.81%] [G loss: 0.7523903846740723]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 6/86 [D loss: 0.6763589382171631, acc.: 59.81%] [G loss: 0.7615604400634766]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 7/86 [D loss: 0.6698644161224365, acc.: 62.94%] [G loss: 0.7668147087097168]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 8/86 [D loss: 0.6606776118278503, acc.: 66.31%] [G loss: 0.774503767490387]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 9/86 [D loss: 0.6586614549160004, acc.: 67.77%] [G loss: 0.7855035066604614]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 10/86 [D loss: 0.6542219519615173, acc.: 68.26%] [G loss: 0.7873430848121643]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 11/86 [D loss: 0.6525641977787018, acc.: 69.87%] [G loss: 0.797431468963623]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 12/86 [D loss: 0.6504393517971039, acc.: 70.02%] [G loss: 0.7943807244300842]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 13/86 [D loss: 0.6509090065956116, acc.: 69.19%] [G loss: 0.7965012192726135]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 14/86 [D loss: 0.6524620652198792, acc.: 68.55%] [G loss: 0.7997230887413025]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 15/86 [D loss: 0.6527230441570282, acc.: 66.89%] [G loss: 0.7971723079681396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 16/86 [D loss: 0.6508056223392487, acc.: 69.48%] [G loss: 0.7960238456726074]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 17/86 [D loss: 0.6573691070079803, acc.: 65.72%] [G loss: 0.8008888959884644]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 18/86 [D loss: 0.6597795784473419, acc.: 64.84%] [G loss: 0.7888708710670471]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 19/86 [D loss: 0.6623071730136871, acc.: 64.26%] [G loss: 0.7835444211959839]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 20/86 [D loss: 0.6700523197650909, acc.: 60.99%] [G loss: 0.7630754709243774]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 21/86 [D loss: 0.6752165853977203, acc.: 60.50%] [G loss: 0.7500360012054443]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 22/86 [D loss: 0.6843060553073883, acc.: 54.98%] [G loss: 0.7409530878067017]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 23/86 [D loss: 0.6841628849506378, acc.: 55.62%] [G loss: 0.7314949631690979]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 24/86 [D loss: 0.683768630027771, acc.: 56.40%] [G loss: 0.7212043404579163]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 25/86 [D loss: 0.6866679489612579, acc.: 54.10%] [G loss: 0.7200922966003418]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 26/86 [D loss: 0.6816305220127106, acc.: 56.59%] [G loss: 0.7218292355537415]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 27/86 [D loss: 0.6821827292442322, acc.: 57.28%] [G loss: 0.7252711653709412]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 28/86 [D loss: 0.683447539806366, acc.: 55.42%] [G loss: 0.7267457842826843]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 29/86 [D loss: 0.6789241135120392, acc.: 56.69%] [G loss: 0.7272496819496155]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 30/86 [D loss: 0.6739221513271332, acc.: 59.28%] [G loss: 0.7466939091682434]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 31/86 [D loss: 0.6675790548324585, acc.: 61.52%] [G loss: 0.7506279945373535]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 32/86 [D loss: 0.6675073802471161, acc.: 61.62%] [G loss: 0.7650957703590393]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 33/86 [D loss: 0.6606794893741608, acc.: 64.36%] [G loss: 0.7882486581802368]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 34/86 [D loss: 0.6543568670749664, acc.: 67.24%] [G loss: 0.806084930896759]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 35/86 [D loss: 0.648311972618103, acc.: 68.31%] [G loss: 0.8170389533042908]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 36/86 [D loss: 0.6474572420120239, acc.: 69.34%] [G loss: 0.8131711483001709]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 37/86 [D loss: 0.6495898962020874, acc.: 67.97%] [G loss: 0.822442889213562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 38/86 [D loss: 0.6512012481689453, acc.: 69.29%] [G loss: 0.8069026470184326]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 39/86 [D loss: 0.6517549455165863, acc.: 68.85%] [G loss: 0.786781907081604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 40/86 [D loss: 0.6566242277622223, acc.: 67.58%] [G loss: 0.767500638961792]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 41/86 [D loss: 0.6646988987922668, acc.: 63.87%] [G loss: 0.7612746357917786]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 42/86 [D loss: 0.6601282954216003, acc.: 64.94%] [G loss: 0.7562790513038635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 43/86 [D loss: 0.6672002971172333, acc.: 62.40%] [G loss: 0.7414961457252502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 44/86 [D loss: 0.6639735698699951, acc.: 62.70%] [G loss: 0.7388604283332825]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 45/86 [D loss: 0.6775172650814056, acc.: 56.79%] [G loss: 0.7312994599342346]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 46/86 [D loss: 0.6724860966205597, acc.: 59.47%] [G loss: 0.724501371383667]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 47/86 [D loss: 0.6836967766284943, acc.: 54.69%] [G loss: 0.7217659950256348]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 48/86 [D loss: 0.6845312118530273, acc.: 54.59%] [G loss: 0.7325020432472229]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 49/86 [D loss: 0.6826500594615936, acc.: 57.18%] [G loss: 0.7457570433616638]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 50/86 [D loss: 0.6788618862628937, acc.: 57.23%] [G loss: 0.7725326418876648]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 51/86 [D loss: 0.6705493330955505, acc.: 60.25%] [G loss: 0.788440465927124]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 52/86 [D loss: 0.6668727099895477, acc.: 63.43%] [G loss: 0.8063783645629883]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 53/86 [D loss: 0.6691762208938599, acc.: 60.60%] [G loss: 0.8121870756149292]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 22/200, Batch 54/86 [D loss: 0.6724260151386261, acc.: 61.23%] [G loss: 0.7960595488548279]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 55/86 [D loss: 0.6691487431526184, acc.: 62.60%] [G loss: 0.7892780900001526]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 56/86 [D loss: 0.6667510569095612, acc.: 62.65%] [G loss: 0.7929880619049072]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 57/86 [D loss: 0.6592967808246613, acc.: 67.68%] [G loss: 0.787395715713501]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 58/86 [D loss: 0.652336597442627, acc.: 69.24%] [G loss: 0.7917264699935913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 59/86 [D loss: 0.6454022526741028, acc.: 71.00%] [G loss: 0.7981652021408081]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 60/86 [D loss: 0.645026445388794, acc.: 69.48%] [G loss: 0.7920791506767273]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 61/86 [D loss: 0.6503113508224487, acc.: 65.87%] [G loss: 0.7756120562553406]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 62/86 [D loss: 0.6523507833480835, acc.: 65.67%] [G loss: 0.7683592438697815]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 63/86 [D loss: 0.66620934009552, acc.: 62.06%] [G loss: 0.7551246285438538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 64/86 [D loss: 0.667748361825943, acc.: 60.60%] [G loss: 0.7523264288902283]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 65/86 [D loss: 0.6677336990833282, acc.: 60.94%] [G loss: 0.7687305808067322]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 66/86 [D loss: 0.6630339324474335, acc.: 62.79%] [G loss: 0.7858800888061523]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 67/86 [D loss: 0.6592478156089783, acc.: 63.28%] [G loss: 0.808739960193634]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 68/86 [D loss: 0.6595494747161865, acc.: 65.04%] [G loss: 0.8177050352096558]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 69/86 [D loss: 0.6585059762001038, acc.: 64.16%] [G loss: 0.8131089210510254]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 70/86 [D loss: 0.6696441173553467, acc.: 62.26%] [G loss: 0.7801375985145569]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 71/86 [D loss: 0.6772413849830627, acc.: 60.64%] [G loss: 0.7561939358711243]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 22/200, Batch 72/86 [D loss: 0.6804348528385162, acc.: 58.79%] [G loss: 0.7478743195533752]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 73/86 [D loss: 0.6773988008499146, acc.: 58.20%] [G loss: 0.7375757694244385]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 74/86 [D loss: 0.6660071611404419, acc.: 63.67%] [G loss: 0.7526106834411621]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 75/86 [D loss: 0.6609178483486176, acc.: 65.43%] [G loss: 0.7662732005119324]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 76/86 [D loss: 0.6494521796703339, acc.: 67.82%] [G loss: 0.768670916557312]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 77/86 [D loss: 0.6479105949401855, acc.: 67.19%] [G loss: 0.7766833901405334]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 78/86 [D loss: 0.6498782932758331, acc.: 64.50%] [G loss: 0.7768846154212952]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 79/86 [D loss: 0.6606804132461548, acc.: 60.89%] [G loss: 0.7526926398277283]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 80/86 [D loss: 0.6688159108161926, acc.: 58.25%] [G loss: 0.7331392765045166]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 81/86 [D loss: 0.6700794100761414, acc.: 57.57%] [G loss: 0.7503589391708374]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 82/86 [D loss: 0.6656503975391388, acc.: 60.25%] [G loss: 0.7842361927032471]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 22/200, Batch 83/86 [D loss: 0.6499372124671936, acc.: 66.41%] [G loss: 0.8172449469566345]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 22/200, Batch 84/86 [D loss: 0.642472505569458, acc.: 67.72%] [G loss: 0.8691037893295288]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 85/86 [D loss: 0.6370363831520081, acc.: 69.92%] [G loss: 0.8797755241394043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 22/200, Batch 86/86 [D loss: 0.6468672752380371, acc.: 67.48%] [G loss: 0.8614855408668518]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 1/86 [D loss: 0.6616482436656952, acc.: 64.06%] [G loss: 0.8065992593765259]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 2/86 [D loss: 0.6714296042919159, acc.: 61.04%] [G loss: 0.7612513899803162]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 23/200, Batch 3/86 [D loss: 0.688837081193924, acc.: 53.47%] [G loss: 0.7314876914024353]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 4/86 [D loss: 0.6809592843055725, acc.: 57.13%] [G loss: 0.7330945730209351]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 5/86 [D loss: 0.6700316369533539, acc.: 61.82%] [G loss: 0.7429079413414001]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 6/86 [D loss: 0.659246027469635, acc.: 65.14%] [G loss: 0.7642987966537476]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 7/86 [D loss: 0.649095207452774, acc.: 66.80%] [G loss: 0.7817665338516235]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 8/86 [D loss: 0.648358166217804, acc.: 64.45%] [G loss: 0.7845789790153503]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 9/86 [D loss: 0.6562952995300293, acc.: 61.08%] [G loss: 0.7627346515655518]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 10/86 [D loss: 0.6773562133312225, acc.: 54.93%] [G loss: 0.7307033538818359]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 11/86 [D loss: 0.6821317672729492, acc.: 54.74%] [G loss: 0.7211199998855591]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 12/86 [D loss: 0.6901802718639374, acc.: 52.59%] [G loss: 0.7451150417327881]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 13/86 [D loss: 0.6701371073722839, acc.: 58.30%] [G loss: 0.7962772846221924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 14/86 [D loss: 0.6506076753139496, acc.: 65.23%] [G loss: 0.8740242719650269]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 15/86 [D loss: 0.6314163506031036, acc.: 69.63%] [G loss: 0.9352799654006958]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 16/86 [D loss: 0.628026008605957, acc.: 69.97%] [G loss: 0.9410447478294373]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 17/86 [D loss: 0.6391845941543579, acc.: 68.70%] [G loss: 0.8869328498840332]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 18/86 [D loss: 0.6639518737792969, acc.: 63.38%] [G loss: 0.8252429962158203]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 19/86 [D loss: 0.6772396862506866, acc.: 58.64%] [G loss: 0.7714378833770752]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 20/86 [D loss: 0.676969438791275, acc.: 59.62%] [G loss: 0.7538723945617676]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 21/86 [D loss: 0.6733888685703278, acc.: 60.60%] [G loss: 0.754716157913208]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 22/86 [D loss: 0.6586999595165253, acc.: 65.38%] [G loss: 0.7790783643722534]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 23/86 [D loss: 0.6385196447372437, acc.: 70.56%] [G loss: 0.8102184534072876]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 24/86 [D loss: 0.6283281743526459, acc.: 71.68%] [G loss: 0.8170771598815918]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 25/86 [D loss: 0.6353379786014557, acc.: 66.89%] [G loss: 0.8209313154220581]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 26/86 [D loss: 0.6372221410274506, acc.: 65.04%] [G loss: 0.7895177602767944]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 27/86 [D loss: 0.6698731780052185, acc.: 58.45%] [G loss: 0.7246655821800232]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 28/86 [D loss: 0.6938930153846741, acc.: 52.05%] [G loss: 0.691266655921936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 29/86 [D loss: 0.7143592238426208, acc.: 48.10%] [G loss: 0.7001388072967529]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 30/86 [D loss: 0.695284903049469, acc.: 52.34%] [G loss: 0.7525940537452698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 31/86 [D loss: 0.6675621271133423, acc.: 60.06%] [G loss: 0.852261483669281]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 32/86 [D loss: 0.6390695571899414, acc.: 67.92%] [G loss: 0.9314113259315491]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 33/86 [D loss: 0.6295984983444214, acc.: 68.75%] [G loss: 0.9573091268539429]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 34/86 [D loss: 0.6329240500926971, acc.: 70.56%] [G loss: 0.9173923134803772]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 35/86 [D loss: 0.6561705470085144, acc.: 66.50%] [G loss: 0.8511980175971985]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 36/86 [D loss: 0.672376275062561, acc.: 61.57%] [G loss: 0.7936932444572449]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 37/86 [D loss: 0.6791675388813019, acc.: 59.72%] [G loss: 0.7553346753120422]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 38/86 [D loss: 0.679451584815979, acc.: 58.11%] [G loss: 0.7486290335655212]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 39/86 [D loss: 0.6613189876079559, acc.: 65.82%] [G loss: 0.7688694000244141]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 40/86 [D loss: 0.6453335881233215, acc.: 69.24%] [G loss: 0.7905464768409729]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 41/86 [D loss: 0.6258207559585571, acc.: 76.51%] [G loss: 0.8126049041748047]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 42/86 [D loss: 0.6086446940898895, acc.: 79.74%] [G loss: 0.8403611779212952]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 43/86 [D loss: 0.6119039356708527, acc.: 74.12%] [G loss: 0.8378965854644775]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 44/86 [D loss: 0.6235370635986328, acc.: 68.26%] [G loss: 0.8143039345741272]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 45/86 [D loss: 0.6405712366104126, acc.: 62.84%] [G loss: 0.7558209896087646]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 46/86 [D loss: 0.6668894588947296, acc.: 55.52%] [G loss: 0.7208309769630432]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 47/86 [D loss: 0.6923136115074158, acc.: 52.93%] [G loss: 0.6872259974479675]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 48/86 [D loss: 0.6947450637817383, acc.: 52.15%] [G loss: 0.7126417756080627]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 49/86 [D loss: 0.6882435381412506, acc.: 52.34%] [G loss: 0.7681015133857727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 50/86 [D loss: 0.667042464017868, acc.: 59.77%] [G loss: 0.8531330227851868]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 51/86 [D loss: 0.6401891112327576, acc.: 67.97%] [G loss: 0.9331724643707275]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 52/86 [D loss: 0.6295888721942902, acc.: 69.04%] [G loss: 0.9411945939064026]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 53/86 [D loss: 0.640052080154419, acc.: 67.63%] [G loss: 0.8964348435401917]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 54/86 [D loss: 0.6646164953708649, acc.: 63.72%] [G loss: 0.8343210220336914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 55/86 [D loss: 0.6855232119560242, acc.: 57.13%] [G loss: 0.7870956063270569]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 56/86 [D loss: 0.6928550601005554, acc.: 54.05%] [G loss: 0.7485751509666443]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 57/86 [D loss: 0.6800178587436676, acc.: 56.98%] [G loss: 0.7526146173477173]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 58/86 [D loss: 0.670805037021637, acc.: 61.62%] [G loss: 0.7791683673858643]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 59/86 [D loss: 0.6445393562316895, acc.: 71.29%] [G loss: 0.8095321655273438]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 60/86 [D loss: 0.6263290643692017, acc.: 75.00%] [G loss: 0.8531030416488647]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 61/86 [D loss: 0.6077042520046234, acc.: 78.22%] [G loss: 0.8756856918334961]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 62/86 [D loss: 0.6015249192714691, acc.: 76.66%] [G loss: 0.8786321878433228]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 63/86 [D loss: 0.6167683899402618, acc.: 71.44%] [G loss: 0.8512030839920044]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 64/86 [D loss: 0.6299518942832947, acc.: 66.60%] [G loss: 0.7961465716362]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 65/86 [D loss: 0.6576616764068604, acc.: 59.96%] [G loss: 0.7694267630577087]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 66/86 [D loss: 0.6722808480262756, acc.: 56.40%] [G loss: 0.7313677072525024]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 67/86 [D loss: 0.683836042881012, acc.: 53.76%] [G loss: 0.7447605729103088]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 68/86 [D loss: 0.6681816577911377, acc.: 59.23%] [G loss: 0.8133595585823059]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 69/86 [D loss: 0.6493788659572601, acc.: 64.01%] [G loss: 0.890891432762146]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 70/86 [D loss: 0.6348940432071686, acc.: 66.16%] [G loss: 0.9482033252716064]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 71/86 [D loss: 0.6253483295440674, acc.: 70.02%] [G loss: 0.9618172645568848]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 72/86 [D loss: 0.633517324924469, acc.: 70.07%] [G loss: 0.9020047187805176]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 73/86 [D loss: 0.6598571240901947, acc.: 62.89%] [G loss: 0.8277069330215454]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 74/86 [D loss: 0.6871149241924286, acc.: 57.18%] [G loss: 0.7646379470825195]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 75/86 [D loss: 0.6939779222011566, acc.: 53.32%] [G loss: 0.7354106903076172]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 23/200, Batch 76/86 [D loss: 0.6963761150836945, acc.: 52.05%] [G loss: 0.7297638654708862]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 77/86 [D loss: 0.6814554333686829, acc.: 56.64%] [G loss: 0.740264892578125]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 78/86 [D loss: 0.657350093126297, acc.: 65.77%] [G loss: 0.7672119736671448]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 79/86 [D loss: 0.6367069184780121, acc.: 71.04%] [G loss: 0.8069124817848206]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 23/200, Batch 80/86 [D loss: 0.6222595274448395, acc.: 74.22%] [G loss: 0.8358792066574097]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 81/86 [D loss: 0.6126596927642822, acc.: 74.51%] [G loss: 0.8544405698776245]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 82/86 [D loss: 0.6168289184570312, acc.: 71.04%] [G loss: 0.8517171144485474]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 83/86 [D loss: 0.6272159814834595, acc.: 66.31%] [G loss: 0.8169513940811157]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 23/200, Batch 84/86 [D loss: 0.641086220741272, acc.: 62.01%] [G loss: 0.7792653441429138]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 23/200, Batch 85/86 [D loss: 0.6618845760822296, acc.: 57.86%] [G loss: 0.7456439733505249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 23/200, Batch 86/86 [D loss: 0.6719304919242859, acc.: 56.88%] [G loss: 0.7454550266265869]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 1/86 [D loss: 0.6758613288402557, acc.: 57.62%] [G loss: 0.7890805006027222]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 2/86 [D loss: 0.6498987674713135, acc.: 62.65%] [G loss: 0.8634937405586243]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 3/86 [D loss: 0.6255441606044769, acc.: 70.80%] [G loss: 0.9327530264854431]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 4/86 [D loss: 0.614000141620636, acc.: 72.61%] [G loss: 0.9732261896133423]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 5/86 [D loss: 0.6119924485683441, acc.: 72.22%] [G loss: 0.9734512567520142]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 6/86 [D loss: 0.6326594650745392, acc.: 69.19%] [G loss: 0.9018017053604126]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 7/86 [D loss: 0.6558567881584167, acc.: 64.84%] [G loss: 0.8333665132522583]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 8/86 [D loss: 0.6720512509346008, acc.: 61.77%] [G loss: 0.7662592530250549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 9/86 [D loss: 0.6906684935092926, acc.: 54.79%] [G loss: 0.7292841672897339]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 10/86 [D loss: 0.6834701895713806, acc.: 55.18%] [G loss: 0.7241685390472412]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 11/86 [D loss: 0.6714018285274506, acc.: 58.30%] [G loss: 0.7426932454109192]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 12/86 [D loss: 0.6529050171375275, acc.: 63.13%] [G loss: 0.7646517753601074]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 13/86 [D loss: 0.64226233959198, acc.: 68.16%] [G loss: 0.7948917150497437]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 14/86 [D loss: 0.6277246475219727, acc.: 71.00%] [G loss: 0.8203445672988892]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 15/86 [D loss: 0.6187954843044281, acc.: 72.07%] [G loss: 0.8380293846130371]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 16/86 [D loss: 0.6185607016086578, acc.: 70.90%] [G loss: 0.8271864056587219]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 17/86 [D loss: 0.6356970369815826, acc.: 65.53%] [G loss: 0.8012586832046509]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 18/86 [D loss: 0.6465592980384827, acc.: 61.47%] [G loss: 0.7672342658042908]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 19/86 [D loss: 0.6671707034111023, acc.: 57.86%] [G loss: 0.7614699602127075]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 20/86 [D loss: 0.6733105778694153, acc.: 56.64%] [G loss: 0.7564981579780579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 21/86 [D loss: 0.6710608303546906, acc.: 57.23%] [G loss: 0.8048853278160095]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 22/86 [D loss: 0.6501226425170898, acc.: 65.23%] [G loss: 0.8723475933074951]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 23/86 [D loss: 0.6234475672245026, acc.: 68.90%] [G loss: 0.9601226449012756]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 24/86 [D loss: 0.6092771589756012, acc.: 73.19%] [G loss: 0.9957399368286133]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 25/86 [D loss: 0.605320394039154, acc.: 75.05%] [G loss: 0.9930345416069031]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 26/86 [D loss: 0.617141455411911, acc.: 72.46%] [G loss: 0.9364769458770752]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 27/86 [D loss: 0.638310432434082, acc.: 69.38%] [G loss: 0.8628261089324951]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 28/86 [D loss: 0.6591928005218506, acc.: 64.55%] [G loss: 0.7945061922073364]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 29/86 [D loss: 0.6699714362621307, acc.: 58.94%] [G loss: 0.7535549998283386]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 30/86 [D loss: 0.6772201061248779, acc.: 58.11%] [G loss: 0.7370172739028931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 31/86 [D loss: 0.665747195482254, acc.: 60.30%] [G loss: 0.744329571723938]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 32/86 [D loss: 0.6494340598583221, acc.: 65.87%] [G loss: 0.7779240012168884]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 33/86 [D loss: 0.6317878663539886, acc.: 71.63%] [G loss: 0.8076905608177185]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 34/86 [D loss: 0.6180820763111115, acc.: 74.12%] [G loss: 0.8260718584060669]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 35/86 [D loss: 0.608955591917038, acc.: 73.05%] [G loss: 0.8489325046539307]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 36/86 [D loss: 0.616689532995224, acc.: 69.43%] [G loss: 0.8210054636001587]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 37/86 [D loss: 0.638220489025116, acc.: 65.04%] [G loss: 0.8011434078216553]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 38/86 [D loss: 0.6524045467376709, acc.: 61.38%] [G loss: 0.7623962163925171]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 39/86 [D loss: 0.6827214062213898, acc.: 54.20%] [G loss: 0.718306839466095]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 40/86 [D loss: 0.6917542517185211, acc.: 54.05%] [G loss: 0.7255073189735413]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 41/86 [D loss: 0.6804417371749878, acc.: 56.84%] [G loss: 0.7809077501296997]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 42/86 [D loss: 0.6598071157932281, acc.: 61.18%] [G loss: 0.8518549203872681]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 43/86 [D loss: 0.6222677528858185, acc.: 69.73%] [G loss: 0.9385717511177063]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 24/200, Batch 44/86 [D loss: 0.6074742376804352, acc.: 72.71%] [G loss: 0.9947695732116699]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 45/86 [D loss: 0.6011926531791687, acc.: 74.02%] [G loss: 0.9905445575714111]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 46/86 [D loss: 0.6136402189731598, acc.: 71.97%] [G loss: 0.9584282040596008]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 47/86 [D loss: 0.6402725279331207, acc.: 68.51%] [G loss: 0.876538872718811]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 48/86 [D loss: 0.6481021642684937, acc.: 67.33%] [G loss: 0.8064352869987488]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 49/86 [D loss: 0.6685878038406372, acc.: 60.45%] [G loss: 0.7732385396957397]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 50/86 [D loss: 0.668150782585144, acc.: 60.94%] [G loss: 0.7568973898887634]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 51/86 [D loss: 0.6611452400684357, acc.: 60.35%] [G loss: 0.7651451826095581]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 52/86 [D loss: 0.6431635320186615, acc.: 66.50%] [G loss: 0.7865870594978333]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 53/86 [D loss: 0.6223464608192444, acc.: 71.19%] [G loss: 0.8286001086235046]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 54/86 [D loss: 0.6078186333179474, acc.: 73.14%] [G loss: 0.8484808802604675]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 55/86 [D loss: 0.6062511801719666, acc.: 73.24%] [G loss: 0.8517525792121887]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 56/86 [D loss: 0.6184436082839966, acc.: 70.12%] [G loss: 0.8280849456787109]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 57/86 [D loss: 0.6310079395771027, acc.: 64.60%] [G loss: 0.7979265451431274]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 58/86 [D loss: 0.6545976400375366, acc.: 60.21%] [G loss: 0.7521175742149353]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 59/86 [D loss: 0.6801660358905792, acc.: 53.66%] [G loss: 0.7310926914215088]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 60/86 [D loss: 0.6860377192497253, acc.: 54.35%] [G loss: 0.7156610488891602]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 61/86 [D loss: 0.6929189264774323, acc.: 52.64%] [G loss: 0.769293487071991]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 62/86 [D loss: 0.6692399680614471, acc.: 58.69%] [G loss: 0.8450464606285095]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 63/86 [D loss: 0.6325893998146057, acc.: 67.72%] [G loss: 0.9231405258178711]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 64/86 [D loss: 0.6161830425262451, acc.: 70.80%] [G loss: 0.9708815217018127]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 65/86 [D loss: 0.6114197075366974, acc.: 72.22%] [G loss: 0.9780187606811523]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 66/86 [D loss: 0.627934068441391, acc.: 69.14%] [G loss: 0.9340053796768188]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 67/86 [D loss: 0.6413810551166534, acc.: 69.58%] [G loss: 0.8720533847808838]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 68/86 [D loss: 0.6539962291717529, acc.: 65.82%] [G loss: 0.8094702959060669]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 69/86 [D loss: 0.6678627729415894, acc.: 61.38%] [G loss: 0.7785440683364868]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 70/86 [D loss: 0.6696447134017944, acc.: 61.62%] [G loss: 0.7685399651527405]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 71/86 [D loss: 0.6568225622177124, acc.: 62.94%] [G loss: 0.7782663702964783]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 72/86 [D loss: 0.6365860998630524, acc.: 68.21%] [G loss: 0.8018575310707092]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 73/86 [D loss: 0.625828742980957, acc.: 71.04%] [G loss: 0.8431118726730347]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 74/86 [D loss: 0.6142932772636414, acc.: 74.22%] [G loss: 0.8582072257995605]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 75/86 [D loss: 0.6079602539539337, acc.: 73.93%] [G loss: 0.8473398089408875]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 76/86 [D loss: 0.6134365797042847, acc.: 71.48%] [G loss: 0.8427338600158691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 77/86 [D loss: 0.6297898292541504, acc.: 66.16%] [G loss: 0.7993503212928772]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 78/86 [D loss: 0.6602605879306793, acc.: 60.21%] [G loss: 0.7510896921157837]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 79/86 [D loss: 0.6819749176502228, acc.: 55.08%] [G loss: 0.7221225500106812]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 80/86 [D loss: 0.7004909813404083, acc.: 53.08%] [G loss: 0.7335753440856934]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 81/86 [D loss: 0.6909061372280121, acc.: 53.66%] [G loss: 0.7684808969497681]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 82/86 [D loss: 0.6687393188476562, acc.: 59.03%] [G loss: 0.8352180123329163]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 24/200, Batch 83/86 [D loss: 0.6424957513809204, acc.: 65.72%] [G loss: 0.9149608612060547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 84/86 [D loss: 0.6220881342887878, acc.: 71.58%] [G loss: 0.9679827690124512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 24/200, Batch 85/86 [D loss: 0.6255028247833252, acc.: 71.68%] [G loss: 0.9392639994621277]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 24/200, Batch 86/86 [D loss: 0.650568276643753, acc.: 66.99%] [G loss: 0.8632078170776367]\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 1/86 [D loss: 0.6680383086204529, acc.: 62.06%] [G loss: 0.8068763613700867]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 2/86 [D loss: 0.6796992123126984, acc.: 59.23%] [G loss: 0.758794903755188]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 3/86 [D loss: 0.691761314868927, acc.: 52.00%] [G loss: 0.7307301163673401]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 4/86 [D loss: 0.6876164376735687, acc.: 54.59%] [G loss: 0.7456946969032288]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 5/86 [D loss: 0.6690337657928467, acc.: 59.57%] [G loss: 0.772984504699707]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 6/86 [D loss: 0.6478864252567291, acc.: 64.16%] [G loss: 0.8040831685066223]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 7/86 [D loss: 0.6303249895572662, acc.: 69.92%] [G loss: 0.8367191553115845]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 8/86 [D loss: 0.6163275241851807, acc.: 73.83%] [G loss: 0.8479461669921875]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 9/86 [D loss: 0.6219721734523773, acc.: 70.75%] [G loss: 0.8338627219200134]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 10/86 [D loss: 0.6453143060207367, acc.: 63.67%] [G loss: 0.7920969724655151]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 11/86 [D loss: 0.6637609601020813, acc.: 58.98%] [G loss: 0.7438740730285645]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 12/86 [D loss: 0.7061249911785126, acc.: 51.17%] [G loss: 0.6941388249397278]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 13/86 [D loss: 0.7208985090255737, acc.: 48.88%] [G loss: 0.6990906596183777]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 14/86 [D loss: 0.7065238654613495, acc.: 51.66%] [G loss: 0.7467356324195862]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 15/86 [D loss: 0.6889881789684296, acc.: 54.93%] [G loss: 0.8168990015983582]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 16/86 [D loss: 0.6622777879238129, acc.: 62.74%] [G loss: 0.8973760008811951]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 17/86 [D loss: 0.6494158208370209, acc.: 66.41%] [G loss: 0.9235209226608276]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 18/86 [D loss: 0.6608149409294128, acc.: 61.77%] [G loss: 0.8792919516563416]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 19/86 [D loss: 0.6799224317073822, acc.: 59.28%] [G loss: 0.8151563405990601]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 20/86 [D loss: 0.7027731537818909, acc.: 52.44%] [G loss: 0.7596771121025085]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 21/86 [D loss: 0.7197307646274567, acc.: 46.97%] [G loss: 0.7208049297332764]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 22/86 [D loss: 0.7207343280315399, acc.: 44.92%] [G loss: 0.7111238241195679]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 23/86 [D loss: 0.6999553143978119, acc.: 50.54%] [G loss: 0.7326452732086182]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 24/86 [D loss: 0.6831883788108826, acc.: 55.42%] [G loss: 0.7548402547836304]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 25/86 [D loss: 0.6628449261188507, acc.: 62.16%] [G loss: 0.7868515253067017]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 26/86 [D loss: 0.6528972089290619, acc.: 66.89%] [G loss: 0.8056169748306274]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 27/86 [D loss: 0.6526326239109039, acc.: 64.31%] [G loss: 0.8122496604919434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 28/86 [D loss: 0.6597962081432343, acc.: 62.89%] [G loss: 0.7894469499588013]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 29/86 [D loss: 0.6747200191020966, acc.: 58.54%] [G loss: 0.750080943107605]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 30/86 [D loss: 0.6942495107650757, acc.: 54.10%] [G loss: 0.7218998670578003]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 31/86 [D loss: 0.707127571105957, acc.: 50.34%] [G loss: 0.7244358658790588]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 32/86 [D loss: 0.6956349611282349, acc.: 52.93%] [G loss: 0.7410399913787842]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 33/86 [D loss: 0.6842998564243317, acc.: 56.01%] [G loss: 0.80023193359375]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 34/86 [D loss: 0.664102703332901, acc.: 62.65%] [G loss: 0.8601515293121338]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 35/86 [D loss: 0.6527022421360016, acc.: 65.09%] [G loss: 0.8795604705810547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 36/86 [D loss: 0.6669130027294159, acc.: 60.30%] [G loss: 0.8704368472099304]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 37/86 [D loss: 0.6854987740516663, acc.: 56.84%] [G loss: 0.8103280067443848]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 38/86 [D loss: 0.6973325908184052, acc.: 51.90%] [G loss: 0.7532036304473877]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 39/86 [D loss: 0.7051232755184174, acc.: 48.73%] [G loss: 0.7229079008102417]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 40/86 [D loss: 0.7121754586696625, acc.: 45.65%] [G loss: 0.7108230590820312]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 41/86 [D loss: 0.7014322578907013, acc.: 49.07%] [G loss: 0.7244536876678467]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 42/86 [D loss: 0.6834395527839661, acc.: 54.74%] [G loss: 0.7381836175918579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 43/86 [D loss: 0.6789758503437042, acc.: 56.69%] [G loss: 0.7572762370109558]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 44/86 [D loss: 0.6610794961452484, acc.: 62.35%] [G loss: 0.7685941457748413]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 45/86 [D loss: 0.665362536907196, acc.: 62.45%] [G loss: 0.771938681602478]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 46/86 [D loss: 0.6780068874359131, acc.: 57.13%] [G loss: 0.761440634727478]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 47/86 [D loss: 0.6846016347408295, acc.: 56.30%] [G loss: 0.7313560247421265]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 48/86 [D loss: 0.6934018135070801, acc.: 53.03%] [G loss: 0.7221786975860596]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 49/86 [D loss: 0.6943399608135223, acc.: 52.59%] [G loss: 0.7262291312217712]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 50/86 [D loss: 0.6922986507415771, acc.: 52.44%] [G loss: 0.7666947841644287]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 51/86 [D loss: 0.6804113984107971, acc.: 56.35%] [G loss: 0.8070802688598633]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 52/86 [D loss: 0.6631214320659637, acc.: 61.57%] [G loss: 0.8590461015701294]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 53/86 [D loss: 0.654107540845871, acc.: 63.13%] [G loss: 0.8692534565925598]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 54/86 [D loss: 0.6655271053314209, acc.: 61.87%] [G loss: 0.8502131104469299]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 55/86 [D loss: 0.664547860622406, acc.: 62.21%] [G loss: 0.8234289884567261]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 56/86 [D loss: 0.6815211176872253, acc.: 57.42%] [G loss: 0.7772989869117737]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 57/86 [D loss: 0.6925890147686005, acc.: 54.30%] [G loss: 0.7567420601844788]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 58/86 [D loss: 0.6967810690402985, acc.: 52.05%] [G loss: 0.747809112071991]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 59/86 [D loss: 0.6857974827289581, acc.: 55.03%] [G loss: 0.741311252117157]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 60/86 [D loss: 0.6770549416542053, acc.: 56.79%] [G loss: 0.7489967942237854]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 61/86 [D loss: 0.6706049740314484, acc.: 61.13%] [G loss: 0.7585799694061279]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 62/86 [D loss: 0.6648860573768616, acc.: 61.57%] [G loss: 0.7694590091705322]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 25/200, Batch 63/86 [D loss: 0.6662927269935608, acc.: 60.69%] [G loss: 0.7679778933525085]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 64/86 [D loss: 0.6787409484386444, acc.: 56.54%] [G loss: 0.7486982345581055]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 65/86 [D loss: 0.6816762387752533, acc.: 55.91%] [G loss: 0.7357499599456787]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 66/86 [D loss: 0.6923770308494568, acc.: 53.03%] [G loss: 0.7235678434371948]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 67/86 [D loss: 0.6963764429092407, acc.: 51.56%] [G loss: 0.7396641969680786]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 68/86 [D loss: 0.6914857029914856, acc.: 53.17%] [G loss: 0.7582659721374512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 69/86 [D loss: 0.6860821843147278, acc.: 56.69%] [G loss: 0.7933286428451538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 70/86 [D loss: 0.6718834340572357, acc.: 59.03%] [G loss: 0.8542025089263916]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 71/86 [D loss: 0.669492781162262, acc.: 59.08%] [G loss: 0.8535382747650146]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 72/86 [D loss: 0.6633127331733704, acc.: 60.89%] [G loss: 0.8517066240310669]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 73/86 [D loss: 0.6703867614269257, acc.: 61.28%] [G loss: 0.8165163397789001]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 74/86 [D loss: 0.6783859133720398, acc.: 59.08%] [G loss: 0.8012778162956238]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 75/86 [D loss: 0.6803255379199982, acc.: 59.38%] [G loss: 0.7763202786445618]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 76/86 [D loss: 0.6891137063503265, acc.: 55.08%] [G loss: 0.7637774348258972]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 77/86 [D loss: 0.6777811944484711, acc.: 59.23%] [G loss: 0.7506148219108582]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 78/86 [D loss: 0.6733362078666687, acc.: 60.40%] [G loss: 0.7614350914955139]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 79/86 [D loss: 0.6713859438896179, acc.: 61.38%] [G loss: 0.7684916257858276]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 80/86 [D loss: 0.6570932865142822, acc.: 66.41%] [G loss: 0.7730072736740112]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 81/86 [D loss: 0.6602892577648163, acc.: 63.92%] [G loss: 0.7738467454910278]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 25/200, Batch 82/86 [D loss: 0.6695976853370667, acc.: 59.08%] [G loss: 0.7543845176696777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 83/86 [D loss: 0.6789596974849701, acc.: 56.69%] [G loss: 0.7442622184753418]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 25/200, Batch 84/86 [D loss: 0.6867043077945709, acc.: 55.32%] [G loss: 0.7435010671615601]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 85/86 [D loss: 0.6910725235939026, acc.: 52.34%] [G loss: 0.7306029200553894]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 25/200, Batch 86/86 [D loss: 0.6905712187290192, acc.: 53.22%] [G loss: 0.7415566444396973]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 1/86 [D loss: 0.685989111661911, acc.: 55.18%] [G loss: 0.7681095600128174]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 2/86 [D loss: 0.675929844379425, acc.: 57.81%] [G loss: 0.8051339387893677]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 3/86 [D loss: 0.6677528619766235, acc.: 61.47%] [G loss: 0.8137330412864685]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 4/86 [D loss: 0.6698574423789978, acc.: 59.72%] [G loss: 0.8227259516716003]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 5/86 [D loss: 0.6659194231033325, acc.: 61.77%] [G loss: 0.8167442679405212]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 6/86 [D loss: 0.6769354343414307, acc.: 59.18%] [G loss: 0.8034257292747498]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 7/86 [D loss: 0.6770932078361511, acc.: 59.13%] [G loss: 0.7719082832336426]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 8/86 [D loss: 0.6759446561336517, acc.: 59.33%] [G loss: 0.7665286660194397]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 9/86 [D loss: 0.6766879856586456, acc.: 60.30%] [G loss: 0.7579038739204407]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 10/86 [D loss: 0.6715663373470306, acc.: 60.50%] [G loss: 0.7589927315711975]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 11/86 [D loss: 0.6676450669765472, acc.: 62.45%] [G loss: 0.767654538154602]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 12/86 [D loss: 0.6611234247684479, acc.: 64.21%] [G loss: 0.7725106477737427]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 13/86 [D loss: 0.655512809753418, acc.: 65.04%] [G loss: 0.7785319089889526]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 14/86 [D loss: 0.6569892466068268, acc.: 65.53%] [G loss: 0.7797090411186218]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 15/86 [D loss: 0.6663409769535065, acc.: 61.38%] [G loss: 0.7605984210968018]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 16/86 [D loss: 0.6667002141475677, acc.: 60.94%] [G loss: 0.7418453097343445]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 17/86 [D loss: 0.6806546449661255, acc.: 56.59%] [G loss: 0.7361038327217102]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 18/86 [D loss: 0.6830808818340302, acc.: 54.79%] [G loss: 0.7315350770950317]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 19/86 [D loss: 0.6876352727413177, acc.: 53.71%] [G loss: 0.7498915791511536]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 20/86 [D loss: 0.6715043187141418, acc.: 60.01%] [G loss: 0.7709524035453796]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 21/86 [D loss: 0.678333580493927, acc.: 58.50%] [G loss: 0.8006730079650879]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 22/86 [D loss: 0.6630993485450745, acc.: 63.13%] [G loss: 0.8194990754127502]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 23/86 [D loss: 0.6691411435604095, acc.: 60.35%] [G loss: 0.8251330852508545]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 24/86 [D loss: 0.6672963798046112, acc.: 62.55%] [G loss: 0.8143120408058167]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 25/86 [D loss: 0.6728850603103638, acc.: 61.33%] [G loss: 0.7945491075515747]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 26/86 [D loss: 0.6806100010871887, acc.: 58.06%] [G loss: 0.7777761816978455]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 27/86 [D loss: 0.6839956641197205, acc.: 55.91%] [G loss: 0.7659810781478882]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 28/86 [D loss: 0.6811191439628601, acc.: 57.86%] [G loss: 0.7601938843727112]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 29/86 [D loss: 0.6740953028202057, acc.: 59.81%] [G loss: 0.7618158459663391]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 30/86 [D loss: 0.6713935732841492, acc.: 60.79%] [G loss: 0.7760676741600037]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 31/86 [D loss: 0.6574046015739441, acc.: 65.67%] [G loss: 0.7779913544654846]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 32/86 [D loss: 0.6554085314273834, acc.: 66.75%] [G loss: 0.7808991074562073]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 33/86 [D loss: 0.6533817648887634, acc.: 66.50%] [G loss: 0.7821037173271179]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 34/86 [D loss: 0.6582154035568237, acc.: 63.33%] [G loss: 0.7664111852645874]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 35/86 [D loss: 0.6668421030044556, acc.: 59.77%] [G loss: 0.7528905868530273]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 36/86 [D loss: 0.6813026368618011, acc.: 55.13%] [G loss: 0.7305157780647278]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 37/86 [D loss: 0.6895104646682739, acc.: 52.78%] [G loss: 0.7344858646392822]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 38/86 [D loss: 0.6855634152889252, acc.: 55.32%] [G loss: 0.752048134803772]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 39/86 [D loss: 0.6755070090293884, acc.: 59.81%] [G loss: 0.7834886312484741]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 40/86 [D loss: 0.6657993793487549, acc.: 61.82%] [G loss: 0.8230406641960144]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 41/86 [D loss: 0.6554758846759796, acc.: 65.53%] [G loss: 0.8443864583969116]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 42/86 [D loss: 0.6635638475418091, acc.: 64.50%] [G loss: 0.8229174017906189]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 43/86 [D loss: 0.6673905253410339, acc.: 61.82%] [G loss: 0.807892382144928]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 44/86 [D loss: 0.6804769039154053, acc.: 59.52%] [G loss: 0.7742360234260559]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 45/86 [D loss: 0.689028799533844, acc.: 54.74%] [G loss: 0.7478629946708679]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 46/86 [D loss: 0.692871630191803, acc.: 52.59%] [G loss: 0.7372180223464966]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 47/86 [D loss: 0.6856541633605957, acc.: 56.25%] [G loss: 0.7304773926734924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 48/86 [D loss: 0.6751716434955597, acc.: 59.47%] [G loss: 0.7487672567367554]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 49/86 [D loss: 0.6673860549926758, acc.: 62.99%] [G loss: 0.759301483631134]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 50/86 [D loss: 0.6506012380123138, acc.: 67.92%] [G loss: 0.7787861824035645]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 51/86 [D loss: 0.6452250778675079, acc.: 67.68%] [G loss: 0.7866767048835754]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 52/86 [D loss: 0.6524916291236877, acc.: 63.72%] [G loss: 0.7768536806106567]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 53/86 [D loss: 0.6681851744651794, acc.: 58.94%] [G loss: 0.7527506351470947]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 54/86 [D loss: 0.6745733916759491, acc.: 56.84%] [G loss: 0.7371454834938049]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 55/86 [D loss: 0.6862218976020813, acc.: 54.69%] [G loss: 0.7267370223999023]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 56/86 [D loss: 0.689476728439331, acc.: 53.66%] [G loss: 0.7408695220947266]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 57/86 [D loss: 0.680202841758728, acc.: 56.15%] [G loss: 0.7744492292404175]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 58/86 [D loss: 0.6653592586517334, acc.: 61.96%] [G loss: 0.8195095658302307]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 59/86 [D loss: 0.6540354490280151, acc.: 64.36%] [G loss: 0.8555595874786377]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 60/86 [D loss: 0.6565649807453156, acc.: 64.21%] [G loss: 0.8557165265083313]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 61/86 [D loss: 0.6557648181915283, acc.: 65.33%] [G loss: 0.8378148078918457]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 62/86 [D loss: 0.6721235811710358, acc.: 60.74%] [G loss: 0.8073769211769104]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 63/86 [D loss: 0.6822661757469177, acc.: 58.25%] [G loss: 0.7669932842254639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 64/86 [D loss: 0.6882731318473816, acc.: 55.81%] [G loss: 0.7442941069602966]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 65/86 [D loss: 0.6898036599159241, acc.: 53.76%] [G loss: 0.7387446761131287]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 66/86 [D loss: 0.682790219783783, acc.: 55.86%] [G loss: 0.7472582459449768]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 67/86 [D loss: 0.6707312762737274, acc.: 61.04%] [G loss: 0.7630403637886047]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 68/86 [D loss: 0.6545832753181458, acc.: 66.60%] [G loss: 0.7765588760375977]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 69/86 [D loss: 0.6479942500591278, acc.: 67.04%] [G loss: 0.7859423160552979]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 70/86 [D loss: 0.6513507068157196, acc.: 65.14%] [G loss: 0.7874671220779419]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 71/86 [D loss: 0.6628849506378174, acc.: 60.40%] [G loss: 0.7722381949424744]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 72/86 [D loss: 0.6653555631637573, acc.: 60.40%] [G loss: 0.7527039051055908]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 73/86 [D loss: 0.6820340156555176, acc.: 55.32%] [G loss: 0.7299216985702515]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 74/86 [D loss: 0.6903802156448364, acc.: 53.71%] [G loss: 0.7349269390106201]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 75/86 [D loss: 0.6853867769241333, acc.: 55.37%] [G loss: 0.7551307678222656]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 76/86 [D loss: 0.6761006712913513, acc.: 58.06%] [G loss: 0.7912741899490356]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 77/86 [D loss: 0.6584875285625458, acc.: 64.16%] [G loss: 0.8419620990753174]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 78/86 [D loss: 0.6524902880191803, acc.: 66.55%] [G loss: 0.8622410297393799]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 79/86 [D loss: 0.6524485945701599, acc.: 67.53%] [G loss: 0.8595697283744812]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 80/86 [D loss: 0.6571578681468964, acc.: 66.21%] [G loss: 0.8291219472885132]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 81/86 [D loss: 0.6703523397445679, acc.: 63.23%] [G loss: 0.8046953678131104]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 82/86 [D loss: 0.6792130768299103, acc.: 59.47%] [G loss: 0.7738102078437805]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 26/200, Batch 83/86 [D loss: 0.6869308650493622, acc.: 55.18%] [G loss: 0.7474175691604614]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 84/86 [D loss: 0.6874809563159943, acc.: 55.62%] [G loss: 0.7332794666290283]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 26/200, Batch 85/86 [D loss: 0.6800171732902527, acc.: 58.15%] [G loss: 0.7453358769416809]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 26/200, Batch 86/86 [D loss: 0.6631366312503815, acc.: 64.70%] [G loss: 0.7649831175804138]\n",
      "4/4 [==============================] - 0s 12ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 1/86 [D loss: 0.6564132571220398, acc.: 67.14%] [G loss: 0.7798850536346436]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 2/86 [D loss: 0.6490872502326965, acc.: 68.12%] [G loss: 0.7873491048812866]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 3/86 [D loss: 0.6428142786026001, acc.: 68.99%] [G loss: 0.7951905131340027]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 4/86 [D loss: 0.6535899341106415, acc.: 63.96%] [G loss: 0.7718960046768188]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 5/86 [D loss: 0.6652467250823975, acc.: 58.50%] [G loss: 0.7464326620101929]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 6/86 [D loss: 0.6808033287525177, acc.: 56.59%] [G loss: 0.7308940291404724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 7/86 [D loss: 0.6885481178760529, acc.: 53.96%] [G loss: 0.7236990332603455]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 8/86 [D loss: 0.6874593198299408, acc.: 54.74%] [G loss: 0.7455742955207825]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 9/86 [D loss: 0.674931526184082, acc.: 58.30%] [G loss: 0.7965262532234192]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 27/200, Batch 10/86 [D loss: 0.6596500873565674, acc.: 63.53%] [G loss: 0.8410835862159729]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 11/86 [D loss: 0.6486534476280212, acc.: 67.29%] [G loss: 0.8662247657775879]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 12/86 [D loss: 0.6542706489562988, acc.: 65.23%] [G loss: 0.8661062121391296]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 13/86 [D loss: 0.6608002185821533, acc.: 63.48%] [G loss: 0.8327348828315735]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 14/86 [D loss: 0.6677069962024689, acc.: 63.82%] [G loss: 0.7994794249534607]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 15/86 [D loss: 0.6811394095420837, acc.: 59.08%] [G loss: 0.7690386772155762]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 16/86 [D loss: 0.6860419511795044, acc.: 55.76%] [G loss: 0.7397995591163635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 17/86 [D loss: 0.6822983026504517, acc.: 57.62%] [G loss: 0.7452763319015503]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 18/86 [D loss: 0.6769689619541168, acc.: 58.79%] [G loss: 0.7514382600784302]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 19/86 [D loss: 0.6651620864868164, acc.: 64.75%] [G loss: 0.7705657482147217]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 20/86 [D loss: 0.6477531492710114, acc.: 69.87%] [G loss: 0.7915929555892944]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 21/86 [D loss: 0.6390275359153748, acc.: 70.70%] [G loss: 0.8011255860328674]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 22/86 [D loss: 0.6447197496891022, acc.: 66.36%] [G loss: 0.8004326820373535]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 23/86 [D loss: 0.650423139333725, acc.: 64.45%] [G loss: 0.7798522710800171]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 24/86 [D loss: 0.6710776388645172, acc.: 58.45%] [G loss: 0.7435700297355652]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 25/86 [D loss: 0.682438462972641, acc.: 56.01%] [G loss: 0.7295815348625183]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 26/86 [D loss: 0.691321611404419, acc.: 53.32%] [G loss: 0.7256335020065308]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 27/86 [D loss: 0.6918889284133911, acc.: 52.64%] [G loss: 0.759340226650238]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 28/86 [D loss: 0.6743146479129791, acc.: 58.30%] [G loss: 0.8051836490631104]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 29/86 [D loss: 0.6518606841564178, acc.: 66.36%] [G loss: 0.8604365587234497]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 30/86 [D loss: 0.6504133939743042, acc.: 66.46%] [G loss: 0.8835325837135315]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 31/86 [D loss: 0.6489868462085724, acc.: 67.38%] [G loss: 0.8658139109611511]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 32/86 [D loss: 0.658200204372406, acc.: 66.06%] [G loss: 0.8382879495620728]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 33/86 [D loss: 0.6682205200195312, acc.: 62.30%] [G loss: 0.7958083152770996]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 34/86 [D loss: 0.6800305247306824, acc.: 59.23%] [G loss: 0.7583231329917908]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 35/86 [D loss: 0.6878442466259003, acc.: 54.98%] [G loss: 0.7388081550598145]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 36/86 [D loss: 0.6804540157318115, acc.: 57.28%] [G loss: 0.7402946949005127]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 37/86 [D loss: 0.6716042757034302, acc.: 60.45%] [G loss: 0.7525416016578674]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 38/86 [D loss: 0.6614416241645813, acc.: 64.89%] [G loss: 0.7767108678817749]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 39/86 [D loss: 0.6467706859111786, acc.: 70.51%] [G loss: 0.7949535250663757]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 40/86 [D loss: 0.6406853497028351, acc.: 70.65%] [G loss: 0.8110947608947754]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 41/86 [D loss: 0.6374886333942413, acc.: 68.80%] [G loss: 0.7989071607589722]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 42/86 [D loss: 0.650702714920044, acc.: 63.13%] [G loss: 0.7813503742218018]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 43/86 [D loss: 0.6645441949367523, acc.: 59.33%] [G loss: 0.7442664504051208]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 44/86 [D loss: 0.6844359338283539, acc.: 54.88%] [G loss: 0.7244219183921814]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 45/86 [D loss: 0.6939376592636108, acc.: 52.69%] [G loss: 0.7207939028739929]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 46/86 [D loss: 0.6884357929229736, acc.: 53.22%] [G loss: 0.753396213054657]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 47/86 [D loss: 0.6737681925296783, acc.: 59.62%] [G loss: 0.8084859251976013]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 48/86 [D loss: 0.6496032178401947, acc.: 67.19%] [G loss: 0.8700915575027466]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 27/200, Batch 49/86 [D loss: 0.6415247023105621, acc.: 69.34%] [G loss: 0.9027818441390991]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 50/86 [D loss: 0.6403619945049286, acc.: 70.51%] [G loss: 0.8879269361495972]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 51/86 [D loss: 0.6572113931179047, acc.: 66.60%] [G loss: 0.8387335538864136]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 52/86 [D loss: 0.6722023189067841, acc.: 62.65%] [G loss: 0.7884228825569153]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 53/86 [D loss: 0.6910038590431213, acc.: 56.40%] [G loss: 0.7488158345222473]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 27/200, Batch 54/86 [D loss: 0.6954257190227509, acc.: 52.10%] [G loss: 0.7339481711387634]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 55/86 [D loss: 0.6883137822151184, acc.: 54.54%] [G loss: 0.7384929060935974]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 56/86 [D loss: 0.6688413321971893, acc.: 62.89%] [G loss: 0.770443856716156]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 57/86 [D loss: 0.6518602073192596, acc.: 68.16%] [G loss: 0.7890845537185669]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 27/200, Batch 58/86 [D loss: 0.6378623247146606, acc.: 72.75%] [G loss: 0.8142241835594177]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 59/86 [D loss: 0.6351606547832489, acc.: 71.48%] [G loss: 0.8184621334075928]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 60/86 [D loss: 0.6376418173313141, acc.: 66.50%] [G loss: 0.8091890811920166]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 61/86 [D loss: 0.6529347002506256, acc.: 63.23%] [G loss: 0.7735011577606201]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 62/86 [D loss: 0.6752815842628479, acc.: 55.86%] [G loss: 0.7334575653076172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 63/86 [D loss: 0.6957355439662933, acc.: 51.27%] [G loss: 0.721359372138977]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 64/86 [D loss: 0.6973744928836823, acc.: 51.12%] [G loss: 0.7323468923568726]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 65/86 [D loss: 0.6843402087688446, acc.: 56.20%] [G loss: 0.7833856344223022]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 66/86 [D loss: 0.6642428636550903, acc.: 62.26%] [G loss: 0.8445234298706055]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 67/86 [D loss: 0.6429301500320435, acc.: 67.53%] [G loss: 0.8861855864524841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 68/86 [D loss: 0.6385945379734039, acc.: 69.48%] [G loss: 0.9081668853759766]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 69/86 [D loss: 0.6476443707942963, acc.: 69.73%] [G loss: 0.8801923990249634]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 70/86 [D loss: 0.6585389375686646, acc.: 66.75%] [G loss: 0.8327752351760864]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 71/86 [D loss: 0.675369143486023, acc.: 60.25%] [G loss: 0.7849424481391907]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 72/86 [D loss: 0.6869584918022156, acc.: 55.71%] [G loss: 0.7441956996917725]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 73/86 [D loss: 0.6869289577007294, acc.: 56.01%] [G loss: 0.7402112483978271]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 74/86 [D loss: 0.6800715029239655, acc.: 56.74%] [G loss: 0.7506998777389526]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 75/86 [D loss: 0.6639571785926819, acc.: 63.43%] [G loss: 0.7678257822990417]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 76/86 [D loss: 0.6492655873298645, acc.: 70.61%] [G loss: 0.7929596304893494]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 77/86 [D loss: 0.6412296891212463, acc.: 71.48%] [G loss: 0.8138325810432434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 78/86 [D loss: 0.6318454146385193, acc.: 72.07%] [G loss: 0.8234948515892029]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 79/86 [D loss: 0.6375480890274048, acc.: 69.19%] [G loss: 0.8159782886505127]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 80/86 [D loss: 0.6442794799804688, acc.: 64.75%] [G loss: 0.7853062152862549]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 27/200, Batch 81/86 [D loss: 0.6698628067970276, acc.: 57.62%] [G loss: 0.7473886013031006]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 82/86 [D loss: 0.6865229904651642, acc.: 54.10%] [G loss: 0.737088143825531]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 83/86 [D loss: 0.6864653825759888, acc.: 53.96%] [G loss: 0.7334365844726562]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 27/200, Batch 84/86 [D loss: 0.6761013269424438, acc.: 57.81%] [G loss: 0.7679105997085571]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 85/86 [D loss: 0.6570055186748505, acc.: 64.60%] [G loss: 0.8120838403701782]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 27/200, Batch 86/86 [D loss: 0.6473098397254944, acc.: 67.97%] [G loss: 0.8672304153442383]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 1/86 [D loss: 0.6373538374900818, acc.: 69.43%] [G loss: 0.903028666973114]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 2/86 [D loss: 0.6417621970176697, acc.: 68.60%] [G loss: 0.8837478756904602]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 3/86 [D loss: 0.6485467851161957, acc.: 68.36%] [G loss: 0.8484626412391663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 4/86 [D loss: 0.6661503612995148, acc.: 62.89%] [G loss: 0.7922465205192566]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 5/86 [D loss: 0.6800774335861206, acc.: 57.91%] [G loss: 0.7591913342475891]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 6/86 [D loss: 0.6833197474479675, acc.: 57.13%] [G loss: 0.7434176802635193]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 7/86 [D loss: 0.6841990351676941, acc.: 56.01%] [G loss: 0.7453989386558533]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 8/86 [D loss: 0.671856015920639, acc.: 60.50%] [G loss: 0.7611778974533081]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 9/86 [D loss: 0.6514107882976532, acc.: 68.60%] [G loss: 0.7947613000869751]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 10/86 [D loss: 0.6407164335250854, acc.: 72.07%] [G loss: 0.8216783404350281]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 11/86 [D loss: 0.6297616064548492, acc.: 72.95%] [G loss: 0.8292481303215027]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 12/86 [D loss: 0.6308487057685852, acc.: 70.36%] [G loss: 0.8261646628379822]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 13/86 [D loss: 0.6599584817886353, acc.: 61.57%] [G loss: 0.795417308807373]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 14/86 [D loss: 0.665188729763031, acc.: 58.30%] [G loss: 0.7649723291397095]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 15/86 [D loss: 0.6730509698390961, acc.: 57.91%] [G loss: 0.7365992665290833]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 16/86 [D loss: 0.6820603311061859, acc.: 55.32%] [G loss: 0.7305068373680115]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 17/86 [D loss: 0.6792051196098328, acc.: 56.49%] [G loss: 0.7718064188957214]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 18/86 [D loss: 0.6637645959854126, acc.: 62.06%] [G loss: 0.8206831812858582]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 19/86 [D loss: 0.6478431820869446, acc.: 66.89%] [G loss: 0.8736859560012817]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 20/86 [D loss: 0.6327501535415649, acc.: 72.12%] [G loss: 0.9031339883804321]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 21/86 [D loss: 0.6405949294567108, acc.: 69.53%] [G loss: 0.8922978639602661]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 22/86 [D loss: 0.6496505439281464, acc.: 67.29%] [G loss: 0.8546318411827087]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 23/86 [D loss: 0.6645459830760956, acc.: 64.11%] [G loss: 0.7948670983314514]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 24/86 [D loss: 0.6812823414802551, acc.: 58.94%] [G loss: 0.7631407380104065]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 25/86 [D loss: 0.6823225915431976, acc.: 58.06%] [G loss: 0.7434682846069336]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 26/86 [D loss: 0.6795874238014221, acc.: 57.81%] [G loss: 0.741861879825592]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 27/86 [D loss: 0.6658746004104614, acc.: 62.35%] [G loss: 0.7577473521232605]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 28/86 [D loss: 0.6523562371730804, acc.: 67.19%] [G loss: 0.7824654579162598]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 29/86 [D loss: 0.6410926580429077, acc.: 71.58%] [G loss: 0.8026351928710938]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 30/86 [D loss: 0.6365330517292023, acc.: 70.80%] [G loss: 0.8177140951156616]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 31/86 [D loss: 0.629467785358429, acc.: 70.65%] [G loss: 0.8124853372573853]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 28/200, Batch 32/86 [D loss: 0.6451709270477295, acc.: 64.79%] [G loss: 0.7957524657249451]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 33/86 [D loss: 0.6633974313735962, acc.: 61.43%] [G loss: 0.7653760313987732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 34/86 [D loss: 0.6730494797229767, acc.: 58.11%] [G loss: 0.7449281215667725]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 35/86 [D loss: 0.6773135960102081, acc.: 56.88%] [G loss: 0.7468178868293762]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 36/86 [D loss: 0.6810336112976074, acc.: 57.13%] [G loss: 0.7712507247924805]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 37/86 [D loss: 0.6600192487239838, acc.: 63.43%] [G loss: 0.8148915767669678]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 38/86 [D loss: 0.6462152898311615, acc.: 67.63%] [G loss: 0.8569146394729614]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 39/86 [D loss: 0.6341387927532196, acc.: 72.07%] [G loss: 0.8952485918998718]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 40/86 [D loss: 0.6339824795722961, acc.: 70.31%] [G loss: 0.8951757550239563]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 41/86 [D loss: 0.6470493972301483, acc.: 67.77%] [G loss: 0.8466863632202148]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 42/86 [D loss: 0.6568780839443207, acc.: 66.26%] [G loss: 0.8023233413696289]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 43/86 [D loss: 0.6788238883018494, acc.: 59.52%] [G loss: 0.7668895721435547]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 44/86 [D loss: 0.6764586567878723, acc.: 58.45%] [G loss: 0.7440415620803833]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 45/86 [D loss: 0.674361377954483, acc.: 58.98%] [G loss: 0.7436343431472778]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 46/86 [D loss: 0.6709854602813721, acc.: 61.57%] [G loss: 0.7691060304641724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 47/86 [D loss: 0.6503068804740906, acc.: 68.70%] [G loss: 0.7879527807235718]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 48/86 [D loss: 0.6369677186012268, acc.: 71.24%] [G loss: 0.8100711703300476]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 49/86 [D loss: 0.6359603106975555, acc.: 70.36%] [G loss: 0.8287448883056641]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 50/86 [D loss: 0.6339868605136871, acc.: 68.65%] [G loss: 0.818604052066803]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 51/86 [D loss: 0.6481200158596039, acc.: 64.70%] [G loss: 0.796125054359436]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 52/86 [D loss: 0.6647835671901703, acc.: 60.16%] [G loss: 0.7658751606941223]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 53/86 [D loss: 0.6747758090496063, acc.: 56.88%] [G loss: 0.7429982423782349]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 54/86 [D loss: 0.6808739304542542, acc.: 57.37%] [G loss: 0.7587133049964905]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 55/86 [D loss: 0.6729051172733307, acc.: 60.16%] [G loss: 0.7888346314430237]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 56/86 [D loss: 0.6560373902320862, acc.: 65.62%] [G loss: 0.843209981918335]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 57/86 [D loss: 0.645801454782486, acc.: 68.31%] [G loss: 0.8765155673027039]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 58/86 [D loss: 0.6310664117336273, acc.: 70.65%] [G loss: 0.9013124704360962]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 59/86 [D loss: 0.6426981091499329, acc.: 68.99%] [G loss: 0.8795040249824524]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 60/86 [D loss: 0.6504256725311279, acc.: 67.29%] [G loss: 0.8290435671806335]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 61/86 [D loss: 0.6593838930130005, acc.: 65.43%] [G loss: 0.8026253581047058]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 62/86 [D loss: 0.6785100400447845, acc.: 59.52%] [G loss: 0.7633753418922424]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 28/200, Batch 63/86 [D loss: 0.6694356501102448, acc.: 61.08%] [G loss: 0.7551764249801636]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 64/86 [D loss: 0.6701522171497345, acc.: 61.43%] [G loss: 0.7562239170074463]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 65/86 [D loss: 0.6540790796279907, acc.: 68.36%] [G loss: 0.7758197784423828]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 66/86 [D loss: 0.6425624787807465, acc.: 69.19%] [G loss: 0.7994931936264038]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 67/86 [D loss: 0.6370127201080322, acc.: 70.41%] [G loss: 0.8106829524040222]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 28/200, Batch 68/86 [D loss: 0.6293735206127167, acc.: 70.80%] [G loss: 0.8250258564949036]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 69/86 [D loss: 0.6403507590293884, acc.: 66.50%] [G loss: 0.8059549927711487]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 70/86 [D loss: 0.6434801816940308, acc.: 64.45%] [G loss: 0.7782955765724182]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 71/86 [D loss: 0.6652548611164093, acc.: 59.52%] [G loss: 0.7611461877822876]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 72/86 [D loss: 0.6790033578872681, acc.: 56.15%] [G loss: 0.7438223361968994]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 73/86 [D loss: 0.6756914258003235, acc.: 59.72%] [G loss: 0.7590572834014893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 74/86 [D loss: 0.6667611598968506, acc.: 60.99%] [G loss: 0.8044068217277527]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 75/86 [D loss: 0.6487618386745453, acc.: 66.85%] [G loss: 0.8581466674804688]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 76/86 [D loss: 0.6311801373958588, acc.: 71.88%] [G loss: 0.9031954407691956]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 77/86 [D loss: 0.631678581237793, acc.: 71.34%] [G loss: 0.8967697620391846]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 78/86 [D loss: 0.6460572183132172, acc.: 67.43%] [G loss: 0.8618004322052002]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 79/86 [D loss: 0.6622772812843323, acc.: 62.99%] [G loss: 0.8051174879074097]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 80/86 [D loss: 0.6663293540477753, acc.: 63.23%] [G loss: 0.7703004479408264]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 81/86 [D loss: 0.674141526222229, acc.: 61.77%] [G loss: 0.7498651742935181]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 82/86 [D loss: 0.6708378791809082, acc.: 60.94%] [G loss: 0.7603723406791687]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 83/86 [D loss: 0.6640986204147339, acc.: 64.36%] [G loss: 0.7822659015655518]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 28/200, Batch 84/86 [D loss: 0.6455137729644775, acc.: 67.53%] [G loss: 0.8043432235717773]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 28/200, Batch 85/86 [D loss: 0.6325133144855499, acc.: 72.41%] [G loss: 0.8238544464111328]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 28/200, Batch 86/86 [D loss: 0.6324465870857239, acc.: 70.51%] [G loss: 0.8301181793212891]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 1/86 [D loss: 0.6315922141075134, acc.: 69.97%] [G loss: 0.8160749077796936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 2/86 [D loss: 0.6511684060096741, acc.: 64.26%] [G loss: 0.7898048162460327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 3/86 [D loss: 0.6726821064949036, acc.: 55.96%] [G loss: 0.7540642023086548]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 4/86 [D loss: 0.681880533695221, acc.: 55.62%] [G loss: 0.7287662029266357]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 5/86 [D loss: 0.6812179684638977, acc.: 57.28%] [G loss: 0.7557954788208008]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 6/86 [D loss: 0.6668467223644257, acc.: 61.38%] [G loss: 0.8022516369819641]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 7/86 [D loss: 0.6515789031982422, acc.: 66.11%] [G loss: 0.8719708919525146]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 8/86 [D loss: 0.633043110370636, acc.: 70.36%] [G loss: 0.923044741153717]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 9/86 [D loss: 0.630661129951477, acc.: 70.85%] [G loss: 0.9103899002075195]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 29/200, Batch 10/86 [D loss: 0.6368347406387329, acc.: 70.41%] [G loss: 0.8787152171134949]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 11/86 [D loss: 0.6596199870109558, acc.: 64.40%] [G loss: 0.8242684006690979]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 12/86 [D loss: 0.6709454357624054, acc.: 61.18%] [G loss: 0.770072877407074]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 13/86 [D loss: 0.6817784607410431, acc.: 58.30%] [G loss: 0.7406993508338928]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 14/86 [D loss: 0.6759523451328278, acc.: 58.74%] [G loss: 0.7508411407470703]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 15/86 [D loss: 0.6637648940086365, acc.: 64.31%] [G loss: 0.7720105648040771]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 16/86 [D loss: 0.6498558223247528, acc.: 66.99%] [G loss: 0.803961992263794]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 17/86 [D loss: 0.629106879234314, acc.: 73.54%] [G loss: 0.8306238651275635]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 18/86 [D loss: 0.6277948915958405, acc.: 71.53%] [G loss: 0.8366042375564575]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 19/86 [D loss: 0.6314339339733124, acc.: 69.87%] [G loss: 0.8203253746032715]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 20/86 [D loss: 0.6449074745178223, acc.: 64.16%] [G loss: 0.7955746650695801]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 21/86 [D loss: 0.664464145898819, acc.: 58.84%] [G loss: 0.7433796525001526]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 22/86 [D loss: 0.6847815811634064, acc.: 55.13%] [G loss: 0.7364240884780884]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 23/86 [D loss: 0.679977297782898, acc.: 55.76%] [G loss: 0.7519675493240356]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 24/86 [D loss: 0.670588493347168, acc.: 58.74%] [G loss: 0.789919912815094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 25/86 [D loss: 0.6519977450370789, acc.: 66.02%] [G loss: 0.847428023815155]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 26/86 [D loss: 0.6399720907211304, acc.: 69.24%] [G loss: 0.901830792427063]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 27/86 [D loss: 0.6285118758678436, acc.: 72.51%] [G loss: 0.898039698600769]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 28/86 [D loss: 0.6384405195713043, acc.: 70.02%] [G loss: 0.8692226409912109]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 29/86 [D loss: 0.6491694450378418, acc.: 66.75%] [G loss: 0.839291512966156]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 30/86 [D loss: 0.6724304556846619, acc.: 60.25%] [G loss: 0.7959780097007751]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 31/86 [D loss: 0.6813156008720398, acc.: 58.94%] [G loss: 0.7674293518066406]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 32/86 [D loss: 0.6752768158912659, acc.: 58.50%] [G loss: 0.7674438953399658]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 33/86 [D loss: 0.6611529886722565, acc.: 63.18%] [G loss: 0.7750570774078369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 34/86 [D loss: 0.6485591530799866, acc.: 69.09%] [G loss: 0.80026775598526]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 35/86 [D loss: 0.633001297712326, acc.: 73.00%] [G loss: 0.8213871717453003]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 36/86 [D loss: 0.6335876286029816, acc.: 69.53%] [G loss: 0.8354213237762451]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 37/86 [D loss: 0.631012499332428, acc.: 70.36%] [G loss: 0.8249157667160034]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 38/86 [D loss: 0.6418816149234772, acc.: 65.28%] [G loss: 0.8033649921417236]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 39/86 [D loss: 0.6583964824676514, acc.: 60.69%] [G loss: 0.7778944373130798]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 40/86 [D loss: 0.6718899011611938, acc.: 57.96%] [G loss: 0.7450745701789856]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 41/86 [D loss: 0.6812105774879456, acc.: 55.42%] [G loss: 0.7532986402511597]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 42/86 [D loss: 0.675959974527359, acc.: 57.86%] [G loss: 0.7847549915313721]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 43/86 [D loss: 0.6595911383628845, acc.: 63.38%] [G loss: 0.8322712182998657]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 44/86 [D loss: 0.6452638804912567, acc.: 67.68%] [G loss: 0.8726415038108826]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 45/86 [D loss: 0.6354737877845764, acc.: 70.02%] [G loss: 0.9035201072692871]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 46/86 [D loss: 0.6412864327430725, acc.: 68.07%] [G loss: 0.8796234130859375]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 47/86 [D loss: 0.647190123796463, acc.: 67.29%] [G loss: 0.852918267250061]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 48/86 [D loss: 0.6632852554321289, acc.: 63.57%] [G loss: 0.7940696477890015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 49/86 [D loss: 0.6791668236255646, acc.: 58.89%] [G loss: 0.7755547165870667]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 50/86 [D loss: 0.6700662076473236, acc.: 60.01%] [G loss: 0.7628939747810364]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 51/86 [D loss: 0.6618617177009583, acc.: 63.57%] [G loss: 0.7740041613578796]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 52/86 [D loss: 0.6519255340099335, acc.: 65.97%] [G loss: 0.7943870425224304]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 53/86 [D loss: 0.63411545753479, acc.: 72.17%] [G loss: 0.8318174481391907]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 54/86 [D loss: 0.6266147494316101, acc.: 72.66%] [G loss: 0.8393146991729736]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 55/86 [D loss: 0.6299979090690613, acc.: 71.68%] [G loss: 0.8364482522010803]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 56/86 [D loss: 0.6474328637123108, acc.: 64.65%] [G loss: 0.8075213432312012]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 57/86 [D loss: 0.6586417257785797, acc.: 60.55%] [G loss: 0.763396143913269]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 58/86 [D loss: 0.676479697227478, acc.: 58.11%] [G loss: 0.7455442547798157]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 59/86 [D loss: 0.6829065382480621, acc.: 56.15%] [G loss: 0.7472273111343384]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 60/86 [D loss: 0.674433171749115, acc.: 57.86%] [G loss: 0.7742639780044556]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 61/86 [D loss: 0.6607711911201477, acc.: 64.06%] [G loss: 0.8318840861320496]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 62/86 [D loss: 0.6416638791561127, acc.: 69.34%] [G loss: 0.8951117992401123]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 63/86 [D loss: 0.6422854959964752, acc.: 66.75%] [G loss: 0.903906524181366]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 64/86 [D loss: 0.6483440399169922, acc.: 66.75%] [G loss: 0.873448371887207]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 65/86 [D loss: 0.6503930687904358, acc.: 66.02%] [G loss: 0.830877423286438]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 66/86 [D loss: 0.6649069786071777, acc.: 63.77%] [G loss: 0.7927780747413635]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 67/86 [D loss: 0.6745445132255554, acc.: 60.11%] [G loss: 0.7697407007217407]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 68/86 [D loss: 0.6738838255405426, acc.: 59.13%] [G loss: 0.7544423341751099]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 69/86 [D loss: 0.6650665104389191, acc.: 61.52%] [G loss: 0.7660360336303711]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 70/86 [D loss: 0.6518990099430084, acc.: 66.46%] [G loss: 0.7922827005386353]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 71/86 [D loss: 0.6392059028148651, acc.: 70.17%] [G loss: 0.813230037689209]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 72/86 [D loss: 0.6270597875118256, acc.: 73.49%] [G loss: 0.8392974138259888]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 73/86 [D loss: 0.6321763396263123, acc.: 70.17%] [G loss: 0.8280327916145325]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 74/86 [D loss: 0.6382259130477905, acc.: 65.82%] [G loss: 0.7871948480606079]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 75/86 [D loss: 0.6689529120922089, acc.: 58.98%] [G loss: 0.7624866962432861]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 76/86 [D loss: 0.6879933774471283, acc.: 54.15%] [G loss: 0.7377409934997559]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 77/86 [D loss: 0.6897054314613342, acc.: 53.42%] [G loss: 0.745092511177063]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 78/86 [D loss: 0.6782395839691162, acc.: 57.96%] [G loss: 0.7850369811058044]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 79/86 [D loss: 0.653603732585907, acc.: 65.62%] [G loss: 0.8602079749107361]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 80/86 [D loss: 0.637279748916626, acc.: 70.12%] [G loss: 0.905828058719635]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 81/86 [D loss: 0.6386595368385315, acc.: 67.53%] [G loss: 0.9058895707130432]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 82/86 [D loss: 0.6497003436088562, acc.: 66.16%] [G loss: 0.8611031174659729]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 29/200, Batch 83/86 [D loss: 0.6589241921901703, acc.: 63.48%] [G loss: 0.8192136287689209]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 84/86 [D loss: 0.6790306270122528, acc.: 58.89%] [G loss: 0.7725507616996765]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 29/200, Batch 85/86 [D loss: 0.6810914576053619, acc.: 57.08%] [G loss: 0.7493590712547302]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 29/200, Batch 86/86 [D loss: 0.6700553297996521, acc.: 59.47%] [G loss: 0.755378007888794]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 1/86 [D loss: 0.6599906980991364, acc.: 63.87%] [G loss: 0.7759462594985962]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 2/86 [D loss: 0.6408382952213287, acc.: 70.02%] [G loss: 0.8069444298744202]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 3/86 [D loss: 0.6342765092849731, acc.: 71.34%] [G loss: 0.8282901644706726]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 4/86 [D loss: 0.6333118081092834, acc.: 68.60%] [G loss: 0.822087824344635]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 5/86 [D loss: 0.6438997387886047, acc.: 64.65%] [G loss: 0.8162431716918945]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 6/86 [D loss: 0.6634582877159119, acc.: 59.96%] [G loss: 0.7686893939971924]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 7/86 [D loss: 0.6800661683082581, acc.: 55.57%] [G loss: 0.7316641807556152]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 8/86 [D loss: 0.6863366365432739, acc.: 54.74%] [G loss: 0.7492142915725708]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 9/86 [D loss: 0.6766367852687836, acc.: 58.20%] [G loss: 0.7834770083427429]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 10/86 [D loss: 0.6636768877506256, acc.: 62.45%] [G loss: 0.8390905261039734]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 11/86 [D loss: 0.6389658153057098, acc.: 68.65%] [G loss: 0.8888040781021118]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 12/86 [D loss: 0.6369208693504333, acc.: 68.75%] [G loss: 0.8947531580924988]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 13/86 [D loss: 0.6394419968128204, acc.: 68.46%] [G loss: 0.8736773133277893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 14/86 [D loss: 0.6635133326053619, acc.: 61.91%] [G loss: 0.8263508677482605]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 15/86 [D loss: 0.6732434034347534, acc.: 59.96%] [G loss: 0.7906961441040039]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 16/86 [D loss: 0.6793657839298248, acc.: 57.52%] [G loss: 0.7590243816375732]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 17/86 [D loss: 0.677965372800827, acc.: 57.52%] [G loss: 0.7561532258987427]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 18/86 [D loss: 0.6704486310482025, acc.: 60.50%] [G loss: 0.7710625529289246]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 19/86 [D loss: 0.6495673358440399, acc.: 67.19%] [G loss: 0.7937818765640259]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 20/86 [D loss: 0.6433079838752747, acc.: 68.99%] [G loss: 0.8061349987983704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 21/86 [D loss: 0.6355721950531006, acc.: 69.58%] [G loss: 0.8132614493370056]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 22/86 [D loss: 0.6487181186676025, acc.: 65.67%] [G loss: 0.8063064813613892]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 23/86 [D loss: 0.6513008773326874, acc.: 63.04%] [G loss: 0.7828754186630249]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 24/86 [D loss: 0.6620808839797974, acc.: 59.96%] [G loss: 0.7505936622619629]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 25/86 [D loss: 0.6866444945335388, acc.: 52.78%] [G loss: 0.7369493246078491]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 26/86 [D loss: 0.6812325716018677, acc.: 55.91%] [G loss: 0.7646926045417786]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 27/86 [D loss: 0.6662116348743439, acc.: 60.89%] [G loss: 0.8074113726615906]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 28/86 [D loss: 0.6536416411399841, acc.: 65.09%] [G loss: 0.8478711843490601]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 29/86 [D loss: 0.6399128437042236, acc.: 68.75%] [G loss: 0.8829094171524048]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 30/86 [D loss: 0.6400094032287598, acc.: 68.90%] [G loss: 0.8772677183151245]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 31/86 [D loss: 0.6481893956661224, acc.: 65.67%] [G loss: 0.8473820090293884]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 32/86 [D loss: 0.6678069829940796, acc.: 60.45%] [G loss: 0.7989709973335266]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 33/86 [D loss: 0.6741171181201935, acc.: 58.30%] [G loss: 0.7688670754432678]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 34/86 [D loss: 0.6738738417625427, acc.: 59.57%] [G loss: 0.7624111175537109]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 35/86 [D loss: 0.672557920217514, acc.: 58.79%] [G loss: 0.7560577988624573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 36/86 [D loss: 0.6641547381877899, acc.: 61.82%] [G loss: 0.7701578140258789]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 37/86 [D loss: 0.6470467448234558, acc.: 68.21%] [G loss: 0.796649158000946]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 38/86 [D loss: 0.6462824642658234, acc.: 69.29%] [G loss: 0.8064199686050415]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 39/86 [D loss: 0.6501924395561218, acc.: 65.33%] [G loss: 0.8002170920372009]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 40/86 [D loss: 0.6571848392486572, acc.: 62.50%] [G loss: 0.7836090326309204]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 41/86 [D loss: 0.6704528331756592, acc.: 58.50%] [G loss: 0.7678627371788025]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 42/86 [D loss: 0.677974283695221, acc.: 58.06%] [G loss: 0.7562735080718994]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 43/86 [D loss: 0.685194343328476, acc.: 55.37%] [G loss: 0.7675735354423523]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 44/86 [D loss: 0.6700079143047333, acc.: 60.74%] [G loss: 0.7957789301872253]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 45/86 [D loss: 0.6516133546829224, acc.: 65.58%] [G loss: 0.8450887203216553]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 46/86 [D loss: 0.6455390751361847, acc.: 67.29%] [G loss: 0.8734429478645325]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 47/86 [D loss: 0.6392965614795685, acc.: 68.12%] [G loss: 0.8915954828262329]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 48/86 [D loss: 0.6487570405006409, acc.: 66.94%] [G loss: 0.8595327734947205]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 49/86 [D loss: 0.659324049949646, acc.: 63.48%] [G loss: 0.8202119469642639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 50/86 [D loss: 0.6702826023101807, acc.: 60.40%] [G loss: 0.7818437814712524]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 51/86 [D loss: 0.6738113462924957, acc.: 59.28%] [G loss: 0.7632834911346436]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 52/86 [D loss: 0.6778576672077179, acc.: 57.32%] [G loss: 0.7690320014953613]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 53/86 [D loss: 0.6617924273014069, acc.: 64.11%] [G loss: 0.7772522568702698]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 54/86 [D loss: 0.6556210517883301, acc.: 65.09%] [G loss: 0.7846441268920898]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 55/86 [D loss: 0.6424661874771118, acc.: 68.65%] [G loss: 0.8008573055267334]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 56/86 [D loss: 0.6402243673801422, acc.: 68.31%] [G loss: 0.8019842505455017]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 57/86 [D loss: 0.6489875018596649, acc.: 64.50%] [G loss: 0.7845004200935364]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 58/86 [D loss: 0.6694860458374023, acc.: 58.74%] [G loss: 0.760564386844635]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 59/86 [D loss: 0.6770192086696625, acc.: 58.40%] [G loss: 0.745540976524353]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 60/86 [D loss: 0.6821756958961487, acc.: 56.25%] [G loss: 0.7514094710350037]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 61/86 [D loss: 0.66532301902771, acc.: 62.40%] [G loss: 0.7969149947166443]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 62/86 [D loss: 0.6559712588787079, acc.: 65.23%] [G loss: 0.8343527317047119]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 63/86 [D loss: 0.638741672039032, acc.: 69.19%] [G loss: 0.8678455352783203]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 64/86 [D loss: 0.6436019837856293, acc.: 68.51%] [G loss: 0.8691310286521912]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 65/86 [D loss: 0.6486424207687378, acc.: 67.38%] [G loss: 0.8520938158035278]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 66/86 [D loss: 0.6601559817790985, acc.: 63.92%] [G loss: 0.8123438954353333]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 67/86 [D loss: 0.665430873632431, acc.: 63.04%] [G loss: 0.7866040468215942]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 68/86 [D loss: 0.6750034391880035, acc.: 58.45%] [G loss: 0.7548485398292542]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 69/86 [D loss: 0.6763414740562439, acc.: 58.89%] [G loss: 0.7603123188018799]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 70/86 [D loss: 0.6629103720188141, acc.: 62.11%] [G loss: 0.7703446745872498]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 71/86 [D loss: 0.6624282002449036, acc.: 63.13%] [G loss: 0.7859453558921814]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 72/86 [D loss: 0.6517417728900909, acc.: 65.77%] [G loss: 0.8001160621643066]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 73/86 [D loss: 0.6468641757965088, acc.: 66.02%] [G loss: 0.8059920072555542]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 74/86 [D loss: 0.6518658399581909, acc.: 64.11%] [G loss: 0.7857203483581543]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 75/86 [D loss: 0.6631969511508942, acc.: 60.01%] [G loss: 0.7659645080566406]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 76/86 [D loss: 0.6780780553817749, acc.: 57.18%] [G loss: 0.757366955280304]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 77/86 [D loss: 0.6819051504135132, acc.: 55.86%] [G loss: 0.7603493332862854]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 78/86 [D loss: 0.6692473590373993, acc.: 60.35%] [G loss: 0.7961911559104919]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 79/86 [D loss: 0.66282719373703, acc.: 63.13%] [G loss: 0.8355932831764221]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 80/86 [D loss: 0.6500042676925659, acc.: 66.75%] [G loss: 0.8584156632423401]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 81/86 [D loss: 0.6422153115272522, acc.: 68.51%] [G loss: 0.8736554980278015]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 82/86 [D loss: 0.6565613448619843, acc.: 63.87%] [G loss: 0.8590818643569946]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 30/200, Batch 83/86 [D loss: 0.6641974747180939, acc.: 61.77%] [G loss: 0.8127332329750061]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 84/86 [D loss: 0.679919958114624, acc.: 58.35%] [G loss: 0.7806831002235413]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 30/200, Batch 85/86 [D loss: 0.6796608567237854, acc.: 57.91%] [G loss: 0.7590109705924988]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 30/200, Batch 86/86 [D loss: 0.6698570549488068, acc.: 60.30%] [G loss: 0.7717464566230774]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 1/86 [D loss: 0.6665790379047394, acc.: 61.72%] [G loss: 0.7778334617614746]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 2/86 [D loss: 0.6574963331222534, acc.: 64.40%] [G loss: 0.782717227935791]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 3/86 [D loss: 0.6481106877326965, acc.: 67.72%] [G loss: 0.8022472858428955]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 4/86 [D loss: 0.6499903202056885, acc.: 66.70%] [G loss: 0.7893787026405334]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 5/86 [D loss: 0.6550052165985107, acc.: 64.01%] [G loss: 0.7745010256767273]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 6/86 [D loss: 0.6633964776992798, acc.: 61.47%] [G loss: 0.7593115568161011]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 7/86 [D loss: 0.6714552938938141, acc.: 59.08%] [G loss: 0.7624182105064392]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 8/86 [D loss: 0.6685484349727631, acc.: 60.21%] [G loss: 0.7566785216331482]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 9/86 [D loss: 0.6722642183303833, acc.: 58.30%] [G loss: 0.7805612087249756]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 10/86 [D loss: 0.6636461019515991, acc.: 62.94%] [G loss: 0.8082758188247681]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 11/86 [D loss: 0.6541511416435242, acc.: 64.94%] [G loss: 0.8323973417282104]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 12/86 [D loss: 0.6507324278354645, acc.: 64.84%] [G loss: 0.8420131206512451]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 13/86 [D loss: 0.6584250032901764, acc.: 63.96%] [G loss: 0.8269984126091003]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 14/86 [D loss: 0.6641700267791748, acc.: 62.21%] [G loss: 0.8184068202972412]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 15/86 [D loss: 0.6745980381965637, acc.: 59.03%] [G loss: 0.8029738068580627]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 16/86 [D loss: 0.6759570837020874, acc.: 58.69%] [G loss: 0.776068389415741]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 17/86 [D loss: 0.6734531819820404, acc.: 59.42%] [G loss: 0.7736600041389465]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 18/86 [D loss: 0.6675086319446564, acc.: 61.82%] [G loss: 0.7819806933403015]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 19/86 [D loss: 0.656914234161377, acc.: 65.04%] [G loss: 0.7871936559677124]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 20/86 [D loss: 0.6547741889953613, acc.: 65.58%] [G loss: 0.8002576231956482]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 21/86 [D loss: 0.6478018462657928, acc.: 67.68%] [G loss: 0.8054107427597046]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 22/86 [D loss: 0.655790239572525, acc.: 63.72%] [G loss: 0.7957472801208496]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 23/86 [D loss: 0.6609422862529755, acc.: 61.87%] [G loss: 0.7679963111877441]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 24/86 [D loss: 0.6722565293312073, acc.: 58.69%] [G loss: 0.7636697292327881]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 25/86 [D loss: 0.6735521852970123, acc.: 59.08%] [G loss: 0.7596127390861511]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 26/86 [D loss: 0.6677223742008209, acc.: 59.77%] [G loss: 0.7725254893302917]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 27/86 [D loss: 0.6715861260890961, acc.: 60.64%] [G loss: 0.8003724813461304]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 28/86 [D loss: 0.6568564772605896, acc.: 63.09%] [G loss: 0.8343636989593506]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 29/86 [D loss: 0.6576682031154633, acc.: 64.06%] [G loss: 0.8362689018249512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 30/86 [D loss: 0.6557457149028778, acc.: 64.31%] [G loss: 0.8284537196159363]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 31/86 [D loss: 0.6630257070064545, acc.: 62.55%] [G loss: 0.8074147701263428]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 32/86 [D loss: 0.6718865036964417, acc.: 61.62%] [G loss: 0.7871896028518677]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 33/86 [D loss: 0.6691571772098541, acc.: 61.38%] [G loss: 0.7797627449035645]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 34/86 [D loss: 0.6714922487735748, acc.: 60.21%] [G loss: 0.7779343128204346]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 35/86 [D loss: 0.6597810983657837, acc.: 63.67%] [G loss: 0.7765090465545654]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 36/86 [D loss: 0.6543864607810974, acc.: 66.94%] [G loss: 0.7928720116615295]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 37/86 [D loss: 0.6504282653331757, acc.: 66.60%] [G loss: 0.7956508994102478]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 38/86 [D loss: 0.652467280626297, acc.: 64.89%] [G loss: 0.7907023429870605]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 39/86 [D loss: 0.6513891220092773, acc.: 65.62%] [G loss: 0.778164803981781]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 40/86 [D loss: 0.6653758883476257, acc.: 61.33%] [G loss: 0.7612385749816895]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 41/86 [D loss: 0.6671251654624939, acc.: 60.45%] [G loss: 0.7483747601509094]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 42/86 [D loss: 0.6769992411136627, acc.: 58.06%] [G loss: 0.7612817287445068]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 43/86 [D loss: 0.6703284680843353, acc.: 60.16%] [G loss: 0.7707381844520569]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 44/86 [D loss: 0.6660857200622559, acc.: 62.55%] [G loss: 0.8061559200286865]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 45/86 [D loss: 0.6573984920978546, acc.: 64.40%] [G loss: 0.8347508311271667]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 46/86 [D loss: 0.6562162041664124, acc.: 65.04%] [G loss: 0.8431487083435059]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 47/86 [D loss: 0.6652567386627197, acc.: 62.35%] [G loss: 0.8291100263595581]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 48/86 [D loss: 0.6678366363048553, acc.: 62.16%] [G loss: 0.8123522400856018]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 49/86 [D loss: 0.6713276505470276, acc.: 62.30%] [G loss: 0.7882089018821716]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 50/86 [D loss: 0.6746708750724792, acc.: 58.01%] [G loss: 0.7716017365455627]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 51/86 [D loss: 0.6707930266857147, acc.: 60.30%] [G loss: 0.773119330406189]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 52/86 [D loss: 0.6672937273979187, acc.: 61.87%] [G loss: 0.7823526263237]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 53/86 [D loss: 0.6537160873413086, acc.: 65.33%] [G loss: 0.790195643901825]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 54/86 [D loss: 0.6454965770244598, acc.: 68.65%] [G loss: 0.7917342782020569]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 55/86 [D loss: 0.6521733701229095, acc.: 67.19%] [G loss: 0.7934361696243286]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 56/86 [D loss: 0.6566178500652313, acc.: 62.74%] [G loss: 0.7759339809417725]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 31/200, Batch 57/86 [D loss: 0.6664018034934998, acc.: 61.57%] [G loss: 0.7574462890625]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 58/86 [D loss: 0.6758155226707458, acc.: 59.38%] [G loss: 0.7559368014335632]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 59/86 [D loss: 0.6763592958450317, acc.: 58.69%] [G loss: 0.7605247497558594]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 60/86 [D loss: 0.6714208424091339, acc.: 59.38%] [G loss: 0.7803971767425537]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 61/86 [D loss: 0.6622834503650665, acc.: 63.87%] [G loss: 0.8088461756706238]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 62/86 [D loss: 0.6573434472084045, acc.: 65.09%] [G loss: 0.8326661586761475]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 63/86 [D loss: 0.6503172218799591, acc.: 66.02%] [G loss: 0.832170844078064]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 64/86 [D loss: 0.6608540415763855, acc.: 63.23%] [G loss: 0.8286291360855103]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 65/86 [D loss: 0.6701652109622955, acc.: 61.18%] [G loss: 0.804630696773529]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 66/86 [D loss: 0.6739287972450256, acc.: 61.04%] [G loss: 0.7788656949996948]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 67/86 [D loss: 0.6728204190731049, acc.: 61.33%] [G loss: 0.7631688714027405]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 68/86 [D loss: 0.6656879782676697, acc.: 62.35%] [G loss: 0.7638912796974182]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 69/86 [D loss: 0.662835568189621, acc.: 63.04%] [G loss: 0.7755000591278076]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 70/86 [D loss: 0.661036491394043, acc.: 63.23%] [G loss: 0.7930302619934082]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 71/86 [D loss: 0.6515500843524933, acc.: 65.87%] [G loss: 0.7958227396011353]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 72/86 [D loss: 0.6491694748401642, acc.: 66.50%] [G loss: 0.7871538400650024]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 73/86 [D loss: 0.6578515768051147, acc.: 63.23%] [G loss: 0.7829762697219849]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 74/86 [D loss: 0.6639007329940796, acc.: 60.55%] [G loss: 0.7692058682441711]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 75/86 [D loss: 0.677887499332428, acc.: 57.08%] [G loss: 0.7549867033958435]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 76/86 [D loss: 0.6720734536647797, acc.: 59.42%] [G loss: 0.7744467258453369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 77/86 [D loss: 0.6657324731349945, acc.: 60.99%] [G loss: 0.7953934669494629]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 78/86 [D loss: 0.6530217230319977, acc.: 65.28%] [G loss: 0.8298372030258179]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 79/86 [D loss: 0.6611887514591217, acc.: 62.99%] [G loss: 0.8525304794311523]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 80/86 [D loss: 0.6570216119289398, acc.: 65.97%] [G loss: 0.8359811305999756]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 81/86 [D loss: 0.6584816873073578, acc.: 64.36%] [G loss: 0.828075110912323]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 31/200, Batch 82/86 [D loss: 0.6688496172428131, acc.: 62.65%] [G loss: 0.800678014755249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 83/86 [D loss: 0.6761863827705383, acc.: 59.62%] [G loss: 0.7817562818527222]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 84/86 [D loss: 0.676098644733429, acc.: 58.40%] [G loss: 0.7665036916732788]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 31/200, Batch 85/86 [D loss: 0.6735356450080872, acc.: 60.16%] [G loss: 0.7734564542770386]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 31/200, Batch 86/86 [D loss: 0.6660738587379456, acc.: 63.04%] [G loss: 0.7848450541496277]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 1/86 [D loss: 0.6592029333114624, acc.: 64.45%] [G loss: 0.792829692363739]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 2/86 [D loss: 0.6502512097358704, acc.: 65.67%] [G loss: 0.7859893441200256]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 3/86 [D loss: 0.6543752253055573, acc.: 64.45%] [G loss: 0.7857199311256409]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 4/86 [D loss: 0.6652014851570129, acc.: 61.43%] [G loss: 0.772061288356781]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 5/86 [D loss: 0.6669901311397552, acc.: 59.33%] [G loss: 0.7694159150123596]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 6/86 [D loss: 0.677886575460434, acc.: 57.13%] [G loss: 0.7609052658081055]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 7/86 [D loss: 0.6687958836555481, acc.: 60.50%] [G loss: 0.7782465219497681]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 8/86 [D loss: 0.6570288240909576, acc.: 63.48%] [G loss: 0.7988519668579102]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 9/86 [D loss: 0.6611297428607941, acc.: 64.16%] [G loss: 0.8288983106613159]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 10/86 [D loss: 0.6517064869403839, acc.: 65.77%] [G loss: 0.8429050445556641]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 11/86 [D loss: 0.6580633223056793, acc.: 63.96%] [G loss: 0.8298596739768982]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 12/86 [D loss: 0.6655181348323822, acc.: 61.87%] [G loss: 0.8137494325637817]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 13/86 [D loss: 0.6669146716594696, acc.: 62.79%] [G loss: 0.783332347869873]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 14/86 [D loss: 0.6754834949970245, acc.: 59.47%] [G loss: 0.7688090801239014]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 15/86 [D loss: 0.6700844168663025, acc.: 60.79%] [G loss: 0.7657488584518433]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 16/86 [D loss: 0.6614998281002045, acc.: 63.04%] [G loss: 0.773760974407196]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 17/86 [D loss: 0.6573297679424286, acc.: 65.28%] [G loss: 0.7798404693603516]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 18/86 [D loss: 0.6568509638309479, acc.: 64.26%] [G loss: 0.7846922278404236]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 32/200, Batch 19/86 [D loss: 0.6563428044319153, acc.: 63.72%] [G loss: 0.7875058650970459]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 20/86 [D loss: 0.6582692861557007, acc.: 64.65%] [G loss: 0.7801129817962646]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 21/86 [D loss: 0.6616950035095215, acc.: 61.67%] [G loss: 0.7750977277755737]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 22/86 [D loss: 0.6718002557754517, acc.: 58.79%] [G loss: 0.7643056511878967]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 32/200, Batch 23/86 [D loss: 0.6680720448493958, acc.: 60.35%] [G loss: 0.7728815674781799]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 24/86 [D loss: 0.6721112132072449, acc.: 59.33%] [G loss: 0.7921179533004761]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 25/86 [D loss: 0.6580438911914825, acc.: 64.01%] [G loss: 0.8151071071624756]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 26/86 [D loss: 0.6590251922607422, acc.: 63.67%] [G loss: 0.828193187713623]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 27/86 [D loss: 0.6592564284801483, acc.: 65.14%] [G loss: 0.8419464230537415]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 28/86 [D loss: 0.6637250483036041, acc.: 62.60%] [G loss: 0.8214505910873413]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 29/86 [D loss: 0.66515052318573, acc.: 63.96%] [G loss: 0.8094556927680969]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 30/86 [D loss: 0.6683142781257629, acc.: 62.01%] [G loss: 0.7805069088935852]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 31/86 [D loss: 0.6761350929737091, acc.: 58.69%] [G loss: 0.768510103225708]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 32/86 [D loss: 0.6692509651184082, acc.: 61.33%] [G loss: 0.7674368023872375]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 33/86 [D loss: 0.6654463708400726, acc.: 62.45%] [G loss: 0.7713026404380798]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 34/86 [D loss: 0.6648171544075012, acc.: 61.87%] [G loss: 0.775295615196228]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 35/86 [D loss: 0.6514021754264832, acc.: 66.50%] [G loss: 0.7841764092445374]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 36/86 [D loss: 0.6597247123718262, acc.: 63.48%] [G loss: 0.7807283997535706]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 37/86 [D loss: 0.6576580703258514, acc.: 64.99%] [G loss: 0.779374361038208]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 38/86 [D loss: 0.6646315157413483, acc.: 61.08%] [G loss: 0.7630987167358398]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 39/86 [D loss: 0.6701174080371857, acc.: 59.03%] [G loss: 0.7559448480606079]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 40/86 [D loss: 0.667310357093811, acc.: 61.23%] [G loss: 0.7636553049087524]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 41/86 [D loss: 0.6694373190402985, acc.: 61.96%] [G loss: 0.7820430994033813]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 42/86 [D loss: 0.6584444642066956, acc.: 63.87%] [G loss: 0.809059739112854]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 43/86 [D loss: 0.6495150327682495, acc.: 66.55%] [G loss: 0.8353344202041626]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 44/86 [D loss: 0.6520388722419739, acc.: 65.38%] [G loss: 0.8284707069396973]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 45/86 [D loss: 0.6557437181472778, acc.: 66.06%] [G loss: 0.8159551024436951]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 32/200, Batch 46/86 [D loss: 0.6708680987358093, acc.: 61.77%] [G loss: 0.7973254919052124]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 47/86 [D loss: 0.675398051738739, acc.: 58.11%] [G loss: 0.7704986333847046]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 48/86 [D loss: 0.6701026558876038, acc.: 60.74%] [G loss: 0.7624224424362183]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 49/86 [D loss: 0.6628877818584442, acc.: 63.04%] [G loss: 0.7768926620483398]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 50/86 [D loss: 0.6590347290039062, acc.: 64.70%] [G loss: 0.7853208780288696]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 51/86 [D loss: 0.6614525020122528, acc.: 64.50%] [G loss: 0.7879795432090759]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 52/86 [D loss: 0.6574864387512207, acc.: 65.58%] [G loss: 0.8015801310539246]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 53/86 [D loss: 0.6571069955825806, acc.: 64.21%] [G loss: 0.7865796685218811]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 54/86 [D loss: 0.667487621307373, acc.: 60.69%] [G loss: 0.7768985033035278]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 55/86 [D loss: 0.6660608053207397, acc.: 61.23%] [G loss: 0.7638610601425171]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 56/86 [D loss: 0.6717295050621033, acc.: 58.69%] [G loss: 0.7691238522529602]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 57/86 [D loss: 0.6672760546207428, acc.: 61.72%] [G loss: 0.7914713025093079]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 58/86 [D loss: 0.6620419323444366, acc.: 64.26%] [G loss: 0.8055441379547119]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 59/86 [D loss: 0.6571689248085022, acc.: 65.58%] [G loss: 0.8287758231163025]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 32/200, Batch 60/86 [D loss: 0.650974303483963, acc.: 65.72%] [G loss: 0.8316109776496887]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 61/86 [D loss: 0.6528365015983582, acc.: 66.31%] [G loss: 0.8334501385688782]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 62/86 [D loss: 0.659448653459549, acc.: 64.84%] [G loss: 0.8190821409225464]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 63/86 [D loss: 0.6692972183227539, acc.: 60.35%] [G loss: 0.7800023555755615]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 32/200, Batch 64/86 [D loss: 0.6706397831439972, acc.: 60.69%] [G loss: 0.7755573987960815]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 65/86 [D loss: 0.6710104048252106, acc.: 60.30%] [G loss: 0.762097954750061]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 66/86 [D loss: 0.6612153947353363, acc.: 64.01%] [G loss: 0.7737509608268738]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 67/86 [D loss: 0.6575579047203064, acc.: 65.14%] [G loss: 0.7846417427062988]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 68/86 [D loss: 0.6470047831535339, acc.: 67.53%] [G loss: 0.8030252456665039]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 69/86 [D loss: 0.647999495267868, acc.: 66.94%] [G loss: 0.7920314073562622]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 70/86 [D loss: 0.6590625643730164, acc.: 62.60%] [G loss: 0.787561297416687]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 71/86 [D loss: 0.6652653515338898, acc.: 61.28%] [G loss: 0.7673563361167908]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 72/86 [D loss: 0.6654512286186218, acc.: 61.33%] [G loss: 0.7549266815185547]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 73/86 [D loss: 0.6722058355808258, acc.: 59.03%] [G loss: 0.7688907384872437]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 74/86 [D loss: 0.6716711819171906, acc.: 59.52%] [G loss: 0.7903426885604858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 75/86 [D loss: 0.6646340191364288, acc.: 62.50%] [G loss: 0.8143439292907715]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 76/86 [D loss: 0.6493872106075287, acc.: 68.02%] [G loss: 0.839745044708252]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 77/86 [D loss: 0.6483680307865143, acc.: 67.58%] [G loss: 0.8505863547325134]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 78/86 [D loss: 0.6525374054908752, acc.: 66.16%] [G loss: 0.8370071053504944]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 79/86 [D loss: 0.6632075011730194, acc.: 62.55%] [G loss: 0.8060851693153381]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 80/86 [D loss: 0.6714885234832764, acc.: 60.25%] [G loss: 0.7888998985290527]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 81/86 [D loss: 0.6781133413314819, acc.: 58.59%] [G loss: 0.7671920657157898]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 82/86 [D loss: 0.6713854670524597, acc.: 58.35%] [G loss: 0.7749555110931396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 83/86 [D loss: 0.661575049161911, acc.: 64.55%] [G loss: 0.7801999449729919]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 32/200, Batch 84/86 [D loss: 0.6566492915153503, acc.: 64.45%] [G loss: 0.7940152883529663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 32/200, Batch 85/86 [D loss: 0.645661860704422, acc.: 67.97%] [G loss: 0.801891565322876]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 32/200, Batch 86/86 [D loss: 0.6493312418460846, acc.: 66.11%] [G loss: 0.8036195635795593]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 1/86 [D loss: 0.6502908170223236, acc.: 64.99%] [G loss: 0.7879999876022339]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 2/86 [D loss: 0.6643950045108795, acc.: 61.82%] [G loss: 0.7663607001304626]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 3/86 [D loss: 0.6693713366985321, acc.: 58.84%] [G loss: 0.7577081322669983]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 4/86 [D loss: 0.6774140000343323, acc.: 57.52%] [G loss: 0.7773213386535645]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 5/86 [D loss: 0.6637819111347198, acc.: 63.18%] [G loss: 0.800390362739563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 6/86 [D loss: 0.6533723175525665, acc.: 65.43%] [G loss: 0.8373772501945496]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 7/86 [D loss: 0.6497905850410461, acc.: 66.75%] [G loss: 0.8541864156723022]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 8/86 [D loss: 0.6450015306472778, acc.: 67.92%] [G loss: 0.8613958358764648]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 9/86 [D loss: 0.6547791957855225, acc.: 67.33%] [G loss: 0.8360064029693604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 10/86 [D loss: 0.6697023808956146, acc.: 61.18%] [G loss: 0.7933284044265747]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 11/86 [D loss: 0.6744405925273895, acc.: 58.15%] [G loss: 0.7740311622619629]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 33/200, Batch 12/86 [D loss: 0.6705001294612885, acc.: 60.99%] [G loss: 0.774946391582489]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 13/86 [D loss: 0.667848140001297, acc.: 61.87%] [G loss: 0.7725576758384705]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 14/86 [D loss: 0.6571349799633026, acc.: 65.14%] [G loss: 0.7912302017211914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 15/86 [D loss: 0.6513376832008362, acc.: 67.82%] [G loss: 0.7993079423904419]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 33/200, Batch 16/86 [D loss: 0.6534272730350494, acc.: 66.36%] [G loss: 0.796113908290863]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 17/86 [D loss: 0.6497593820095062, acc.: 65.38%] [G loss: 0.7932067513465881]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 18/86 [D loss: 0.6590849459171295, acc.: 63.04%] [G loss: 0.7816300392150879]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 19/86 [D loss: 0.6737414300441742, acc.: 57.71%] [G loss: 0.7647933959960938]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 20/86 [D loss: 0.6749889552593231, acc.: 58.40%] [G loss: 0.7642984390258789]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 21/86 [D loss: 0.6690913736820221, acc.: 60.16%] [G loss: 0.7802797555923462]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 22/86 [D loss: 0.6595746278762817, acc.: 64.26%] [G loss: 0.8143236637115479]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 23/86 [D loss: 0.6475703120231628, acc.: 67.09%] [G loss: 0.8603340983390808]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 24/86 [D loss: 0.651086837053299, acc.: 66.36%] [G loss: 0.8659826517105103]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 33/200, Batch 25/86 [D loss: 0.6507037580013275, acc.: 66.31%] [G loss: 0.8559883832931519]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 26/86 [D loss: 0.6608582139015198, acc.: 63.87%] [G loss: 0.8181973099708557]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 27/86 [D loss: 0.6714059114456177, acc.: 61.04%] [G loss: 0.78868567943573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 28/86 [D loss: 0.6837169528007507, acc.: 57.18%] [G loss: 0.7585093975067139]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 29/86 [D loss: 0.6730870306491852, acc.: 59.62%] [G loss: 0.7705913186073303]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 30/86 [D loss: 0.6672005653381348, acc.: 60.35%] [G loss: 0.7870898842811584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 31/86 [D loss: 0.653618186712265, acc.: 65.87%] [G loss: 0.7977820038795471]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 32/86 [D loss: 0.651287168264389, acc.: 65.92%] [G loss: 0.809044361114502]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 33/86 [D loss: 0.6467836499214172, acc.: 67.58%] [G loss: 0.813298761844635]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 34/86 [D loss: 0.6582389771938324, acc.: 62.45%] [G loss: 0.7863898277282715]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 35/86 [D loss: 0.6603751182556152, acc.: 62.79%] [G loss: 0.7712766528129578]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 36/86 [D loss: 0.6716624200344086, acc.: 57.86%] [G loss: 0.7638073563575745]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 37/86 [D loss: 0.6707939505577087, acc.: 58.64%] [G loss: 0.766510009765625]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 33/200, Batch 38/86 [D loss: 0.6650317311286926, acc.: 63.13%] [G loss: 0.7935488224029541]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 39/86 [D loss: 0.6531588137149811, acc.: 66.11%] [G loss: 0.840919017791748]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 40/86 [D loss: 0.6508660912513733, acc.: 66.26%] [G loss: 0.8630511164665222]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 41/86 [D loss: 0.6518206000328064, acc.: 66.65%] [G loss: 0.8551909923553467]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 33/200, Batch 42/86 [D loss: 0.6561870276927948, acc.: 65.67%] [G loss: 0.8368133306503296]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 43/86 [D loss: 0.6635558009147644, acc.: 63.96%] [G loss: 0.7973446249961853]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 44/86 [D loss: 0.6737463474273682, acc.: 59.86%] [G loss: 0.7767231464385986]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 45/86 [D loss: 0.6733073890209198, acc.: 61.04%] [G loss: 0.7684741020202637]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 46/86 [D loss: 0.6691295206546783, acc.: 61.67%] [G loss: 0.7690683603286743]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 47/86 [D loss: 0.6537067890167236, acc.: 67.38%] [G loss: 0.7878466844558716]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 48/86 [D loss: 0.6501473784446716, acc.: 66.02%] [G loss: 0.793795645236969]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 49/86 [D loss: 0.6432509422302246, acc.: 67.87%] [G loss: 0.8000137805938721]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 33/200, Batch 50/86 [D loss: 0.6392051577568054, acc.: 69.29%] [G loss: 0.7967349290847778]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 51/86 [D loss: 0.6507231891155243, acc.: 64.21%] [G loss: 0.7901860475540161]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 52/86 [D loss: 0.6624265611171722, acc.: 60.16%] [G loss: 0.761390209197998]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 53/86 [D loss: 0.673202782869339, acc.: 57.62%] [G loss: 0.759330153465271]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 54/86 [D loss: 0.6768240928649902, acc.: 56.74%] [G loss: 0.7783628702163696]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 55/86 [D loss: 0.6586533188819885, acc.: 63.82%] [G loss: 0.804370641708374]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 56/86 [D loss: 0.6490826606750488, acc.: 67.24%] [G loss: 0.8471357822418213]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 57/86 [D loss: 0.6457584798336029, acc.: 68.02%] [G loss: 0.8662835955619812]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 58/86 [D loss: 0.6518966555595398, acc.: 65.97%] [G loss: 0.84998619556427]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 33/200, Batch 59/86 [D loss: 0.6563008725643158, acc.: 64.75%] [G loss: 0.8344379663467407]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 60/86 [D loss: 0.666027843952179, acc.: 62.11%] [G loss: 0.7949507236480713]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 61/86 [D loss: 0.6755838096141815, acc.: 60.16%] [G loss: 0.7716602087020874]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 62/86 [D loss: 0.6765850782394409, acc.: 58.50%] [G loss: 0.7595841288566589]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 33/200, Batch 63/86 [D loss: 0.6629213690757751, acc.: 62.70%] [G loss: 0.7803495526313782]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 64/86 [D loss: 0.6547599136829376, acc.: 66.80%] [G loss: 0.8006329536437988]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 65/86 [D loss: 0.6431219279766083, acc.: 69.87%] [G loss: 0.8176605701446533]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 66/86 [D loss: 0.6384954154491425, acc.: 68.80%] [G loss: 0.8183724880218506]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 67/86 [D loss: 0.6477463841438293, acc.: 66.50%] [G loss: 0.8043727278709412]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 68/86 [D loss: 0.6630556881427765, acc.: 61.57%] [G loss: 0.7656432390213013]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 69/86 [D loss: 0.6699488162994385, acc.: 58.45%] [G loss: 0.759268581867218]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 70/86 [D loss: 0.6698587834835052, acc.: 58.06%] [G loss: 0.7692546248435974]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 71/86 [D loss: 0.6653147637844086, acc.: 61.52%] [G loss: 0.779598593711853]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 72/86 [D loss: 0.6627992987632751, acc.: 62.26%] [G loss: 0.8228395581245422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 73/86 [D loss: 0.6481101214885712, acc.: 66.85%] [G loss: 0.8577550649642944]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 74/86 [D loss: 0.6438937485218048, acc.: 67.33%] [G loss: 0.8627725839614868]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 75/86 [D loss: 0.6454864144325256, acc.: 67.72%] [G loss: 0.845427393913269]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 33/200, Batch 76/86 [D loss: 0.6596993505954742, acc.: 63.67%] [G loss: 0.8153342008590698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 77/86 [D loss: 0.6665166914463043, acc.: 61.28%] [G loss: 0.7825623154640198]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 78/86 [D loss: 0.6696629524230957, acc.: 60.60%] [G loss: 0.777033269405365]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 79/86 [D loss: 0.6693681478500366, acc.: 60.60%] [G loss: 0.7704331278800964]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 33/200, Batch 80/86 [D loss: 0.6635546088218689, acc.: 62.65%] [G loss: 0.784699559211731]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 81/86 [D loss: 0.6486465334892273, acc.: 68.55%] [G loss: 0.7999782562255859]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 82/86 [D loss: 0.6437558829784393, acc.: 69.14%] [G loss: 0.8136680126190186]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 83/86 [D loss: 0.6372334063053131, acc.: 69.92%] [G loss: 0.8114816546440125]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 33/200, Batch 84/86 [D loss: 0.6445222795009613, acc.: 65.62%] [G loss: 0.8004365563392639]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 33/200, Batch 85/86 [D loss: 0.6576296091079712, acc.: 62.21%] [G loss: 0.7742792367935181]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 33/200, Batch 86/86 [D loss: 0.6677124798297882, acc.: 59.86%] [G loss: 0.75226891040802]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 1/86 [D loss: 0.675798624753952, acc.: 56.25%] [G loss: 0.7666894197463989]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 2/86 [D loss: 0.670197606086731, acc.: 60.16%] [G loss: 0.7943216562271118]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 3/86 [D loss: 0.6576522588729858, acc.: 63.57%] [G loss: 0.8221766948699951]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 4/86 [D loss: 0.6484189629554749, acc.: 67.43%] [G loss: 0.859429657459259]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 5/86 [D loss: 0.6439254581928253, acc.: 67.82%] [G loss: 0.8699860572814941]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 6/86 [D loss: 0.64994877576828, acc.: 65.97%] [G loss: 0.8540297746658325]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 7/86 [D loss: 0.6655334234237671, acc.: 63.38%] [G loss: 0.8134362101554871]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 8/86 [D loss: 0.6717703342437744, acc.: 62.11%] [G loss: 0.7847112417221069]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 9/86 [D loss: 0.6739082336425781, acc.: 60.01%] [G loss: 0.7729476690292358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 10/86 [D loss: 0.6682592332363129, acc.: 60.94%] [G loss: 0.7754867076873779]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 34/200, Batch 11/86 [D loss: 0.6587142050266266, acc.: 64.16%] [G loss: 0.7933160662651062]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 12/86 [D loss: 0.6445611417293549, acc.: 69.92%] [G loss: 0.8126002550125122]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 13/86 [D loss: 0.6362334489822388, acc.: 70.85%] [G loss: 0.828931450843811]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 14/86 [D loss: 0.6391243040561676, acc.: 67.97%] [G loss: 0.8261266946792603]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 15/86 [D loss: 0.6414199471473694, acc.: 66.21%] [G loss: 0.8096373677253723]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 16/86 [D loss: 0.6634883284568787, acc.: 62.60%] [G loss: 0.7735874652862549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 17/86 [D loss: 0.6777496337890625, acc.: 56.10%] [G loss: 0.7598081231117249]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 18/86 [D loss: 0.6742176711559296, acc.: 56.84%] [G loss: 0.766941487789154]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 19/86 [D loss: 0.6696138978004456, acc.: 60.94%] [G loss: 0.8026004433631897]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 20/86 [D loss: 0.6537428498268127, acc.: 65.09%] [G loss: 0.8579065799713135]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 21/86 [D loss: 0.6374743282794952, acc.: 70.51%] [G loss: 0.8844296932220459]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 22/86 [D loss: 0.638456404209137, acc.: 69.38%] [G loss: 0.8864708542823792]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 23/86 [D loss: 0.6495989859104156, acc.: 67.33%] [G loss: 0.8622735738754272]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 34/200, Batch 24/86 [D loss: 0.6611437201499939, acc.: 63.62%] [G loss: 0.8036317825317383]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 25/86 [D loss: 0.675385594367981, acc.: 59.81%] [G loss: 0.7824944257736206]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 26/86 [D loss: 0.673475593328476, acc.: 60.74%] [G loss: 0.7631548643112183]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 27/86 [D loss: 0.6664153933525085, acc.: 62.16%] [G loss: 0.7737168669700623]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 28/86 [D loss: 0.649138331413269, acc.: 65.62%] [G loss: 0.7830849289894104]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 29/86 [D loss: 0.6333024203777313, acc.: 71.09%] [G loss: 0.8161974549293518]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 30/86 [D loss: 0.6303859949111938, acc.: 71.78%] [G loss: 0.8281218409538269]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 31/86 [D loss: 0.63413405418396, acc.: 68.16%] [G loss: 0.825711727142334]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 32/86 [D loss: 0.6412725448608398, acc.: 65.48%] [G loss: 0.794819176197052]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 33/86 [D loss: 0.6677877902984619, acc.: 58.20%] [G loss: 0.7635926008224487]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 34/86 [D loss: 0.6769072115421295, acc.: 54.98%] [G loss: 0.7442317008972168]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 35/86 [D loss: 0.6842535734176636, acc.: 54.54%] [G loss: 0.761426568031311]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 36/86 [D loss: 0.6680525243282318, acc.: 60.64%] [G loss: 0.8131040334701538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 37/86 [D loss: 0.6459893882274628, acc.: 66.60%] [G loss: 0.8757764101028442]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 38/86 [D loss: 0.6411302387714386, acc.: 68.85%] [G loss: 0.8910971283912659]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 39/86 [D loss: 0.6406318545341492, acc.: 68.80%] [G loss: 0.8854222297668457]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 34/200, Batch 40/86 [D loss: 0.6512966454029083, acc.: 66.50%] [G loss: 0.8365922570228577]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 41/86 [D loss: 0.6736561059951782, acc.: 59.62%] [G loss: 0.8004555702209473]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 42/86 [D loss: 0.6754790842533112, acc.: 57.71%] [G loss: 0.7686046957969666]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 43/86 [D loss: 0.6762107312679291, acc.: 57.96%] [G loss: 0.7760233283042908]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 44/86 [D loss: 0.6619727611541748, acc.: 62.60%] [G loss: 0.7962398529052734]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 45/86 [D loss: 0.6451666951179504, acc.: 67.63%] [G loss: 0.8251549005508423]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 46/86 [D loss: 0.6389152407646179, acc.: 69.97%] [G loss: 0.8440011739730835]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 47/86 [D loss: 0.6337270140647888, acc.: 70.21%] [G loss: 0.8267564177513123]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 48/86 [D loss: 0.643803060054779, acc.: 66.26%] [G loss: 0.8084090352058411]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 49/86 [D loss: 0.6640591323375702, acc.: 59.62%] [G loss: 0.7849564552307129]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 50/86 [D loss: 0.6734234690666199, acc.: 57.52%] [G loss: 0.7590955495834351]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 51/86 [D loss: 0.6832852065563202, acc.: 56.10%] [G loss: 0.7531282901763916]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 52/86 [D loss: 0.6727206408977509, acc.: 58.54%] [G loss: 0.7792215943336487]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 34/200, Batch 53/86 [D loss: 0.6564083099365234, acc.: 64.06%] [G loss: 0.8325599431991577]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 54/86 [D loss: 0.6464252471923828, acc.: 67.87%] [G loss: 0.8715607523918152]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 55/86 [D loss: 0.6346654295921326, acc.: 69.82%] [G loss: 0.8906891942024231]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 56/86 [D loss: 0.6407193243503571, acc.: 68.85%] [G loss: 0.8666505813598633]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 57/86 [D loss: 0.6568126678466797, acc.: 64.79%] [G loss: 0.8253720998764038]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 58/86 [D loss: 0.6696732640266418, acc.: 62.01%] [G loss: 0.7930670380592346]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 59/86 [D loss: 0.6810108423233032, acc.: 58.64%] [G loss: 0.7691383361816406]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 60/86 [D loss: 0.6716841161251068, acc.: 59.13%] [G loss: 0.7715675830841064]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 61/86 [D loss: 0.6620953977108002, acc.: 61.91%] [G loss: 0.7893056273460388]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 62/86 [D loss: 0.6468305289745331, acc.: 68.85%] [G loss: 0.8141266703605652]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 63/86 [D loss: 0.6384971737861633, acc.: 69.53%] [G loss: 0.825631320476532]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 64/86 [D loss: 0.6279098093509674, acc.: 71.48%] [G loss: 0.8345174193382263]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 65/86 [D loss: 0.6425057649612427, acc.: 65.92%] [G loss: 0.8054649829864502]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 66/86 [D loss: 0.658389151096344, acc.: 61.52%] [G loss: 0.7714539170265198]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 67/86 [D loss: 0.6812939345836639, acc.: 55.22%] [G loss: 0.7446163892745972]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 68/86 [D loss: 0.6814084053039551, acc.: 56.10%] [G loss: 0.7541053295135498]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 69/86 [D loss: 0.6762929260730743, acc.: 57.67%] [G loss: 0.786933958530426]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 70/86 [D loss: 0.6540221273899078, acc.: 63.62%] [G loss: 0.8499963283538818]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 71/86 [D loss: 0.6422056555747986, acc.: 67.68%] [G loss: 0.9006854891777039]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 72/86 [D loss: 0.634132593870163, acc.: 70.75%] [G loss: 0.8966224193572998]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 73/86 [D loss: 0.6405359506607056, acc.: 68.90%] [G loss: 0.8705260157585144]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 74/86 [D loss: 0.6582865417003632, acc.: 64.26%] [G loss: 0.8259690999984741]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 75/86 [D loss: 0.672365128993988, acc.: 59.96%] [G loss: 0.7847692966461182]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 76/86 [D loss: 0.6760720610618591, acc.: 58.74%] [G loss: 0.771740198135376]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 77/86 [D loss: 0.6710687279701233, acc.: 59.77%] [G loss: 0.7698294520378113]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 78/86 [D loss: 0.6545762717723846, acc.: 65.14%] [G loss: 0.792973518371582]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 79/86 [D loss: 0.6488547325134277, acc.: 66.80%] [G loss: 0.8228483200073242]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 34/200, Batch 80/86 [D loss: 0.625927209854126, acc.: 72.27%] [G loss: 0.8393936157226562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 81/86 [D loss: 0.6292461454868317, acc.: 70.65%] [G loss: 0.8301153779029846]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 82/86 [D loss: 0.6478819251060486, acc.: 65.09%] [G loss: 0.8040565252304077]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 83/86 [D loss: 0.6617588102817535, acc.: 58.45%] [G loss: 0.7697961926460266]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 34/200, Batch 84/86 [D loss: 0.6766907274723053, acc.: 56.74%] [G loss: 0.757777988910675]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 34/200, Batch 85/86 [D loss: 0.6806216537952423, acc.: 56.05%] [G loss: 0.7558556199073792]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 34/200, Batch 86/86 [D loss: 0.6693050265312195, acc.: 59.33%] [G loss: 0.7948824167251587]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 1/86 [D loss: 0.6524877846240997, acc.: 65.87%] [G loss: 0.8532044291496277]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 2/86 [D loss: 0.6394522786140442, acc.: 67.58%] [G loss: 0.8844646215438843]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 3/86 [D loss: 0.6350306570529938, acc.: 68.80%] [G loss: 0.9035012722015381]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 4/86 [D loss: 0.6439678072929382, acc.: 67.87%] [G loss: 0.8841577768325806]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 5/86 [D loss: 0.654793918132782, acc.: 65.04%] [G loss: 0.840360701084137]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 6/86 [D loss: 0.6725912392139435, acc.: 59.96%] [G loss: 0.794785737991333]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 7/86 [D loss: 0.6711690127849579, acc.: 59.67%] [G loss: 0.7678650617599487]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 8/86 [D loss: 0.673887699842453, acc.: 57.47%] [G loss: 0.7644425630569458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 9/86 [D loss: 0.6618690192699432, acc.: 62.50%] [G loss: 0.78737473487854]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 10/86 [D loss: 0.6388510167598724, acc.: 69.34%] [G loss: 0.8163791298866272]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 11/86 [D loss: 0.6342021524906158, acc.: 70.51%] [G loss: 0.8337256908416748]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 12/86 [D loss: 0.6318488717079163, acc.: 69.68%] [G loss: 0.8272165060043335]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 13/86 [D loss: 0.6399355828762054, acc.: 66.31%] [G loss: 0.8139148950576782]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 14/86 [D loss: 0.6562313139438629, acc.: 62.01%] [G loss: 0.7822909355163574]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 15/86 [D loss: 0.6692012548446655, acc.: 58.15%] [G loss: 0.7507646083831787]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 16/86 [D loss: 0.678462952375412, acc.: 56.88%] [G loss: 0.7420012354850769]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 17/86 [D loss: 0.6819992959499359, acc.: 56.49%] [G loss: 0.7787926197052002]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 18/86 [D loss: 0.6630890965461731, acc.: 61.38%] [G loss: 0.8321182131767273]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 19/86 [D loss: 0.6413592100143433, acc.: 66.65%] [G loss: 0.8884707689285278]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 20/86 [D loss: 0.630810409784317, acc.: 70.61%] [G loss: 0.92686927318573]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 35/200, Batch 21/86 [D loss: 0.6390734016895294, acc.: 68.75%] [G loss: 0.9054001569747925]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 22/86 [D loss: 0.6554861962795258, acc.: 64.55%] [G loss: 0.8541530966758728]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 23/86 [D loss: 0.6654763221740723, acc.: 62.55%] [G loss: 0.8005374670028687]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 24/86 [D loss: 0.6810051500797272, acc.: 58.06%] [G loss: 0.7713662981987]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 25/86 [D loss: 0.6768183708190918, acc.: 59.08%] [G loss: 0.7586351633071899]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 26/86 [D loss: 0.6624186336994171, acc.: 61.91%] [G loss: 0.7802518606185913]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 27/86 [D loss: 0.6541610360145569, acc.: 65.72%] [G loss: 0.8113124370574951]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 28/86 [D loss: 0.631685733795166, acc.: 70.95%] [G loss: 0.8438124656677246]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 29/86 [D loss: 0.625260978937149, acc.: 72.31%] [G loss: 0.8374239802360535]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 35/200, Batch 30/86 [D loss: 0.6319992244243622, acc.: 69.14%] [G loss: 0.8243695497512817]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 31/86 [D loss: 0.6516843438148499, acc.: 63.48%] [G loss: 0.7901092171669006]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 32/86 [D loss: 0.677914172410965, acc.: 56.01%] [G loss: 0.7556357383728027]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 33/86 [D loss: 0.6833841800689697, acc.: 54.74%] [G loss: 0.7506774663925171]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 35/200, Batch 34/86 [D loss: 0.6763116121292114, acc.: 56.64%] [G loss: 0.7691437602043152]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 35/86 [D loss: 0.662738710641861, acc.: 61.18%] [G loss: 0.819316565990448]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 36/86 [D loss: 0.6486726701259613, acc.: 68.12%] [G loss: 0.8744181394577026]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 37/86 [D loss: 0.6335723698139191, acc.: 69.19%] [G loss: 0.9240567684173584]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 38/86 [D loss: 0.6326613426208496, acc.: 69.29%] [G loss: 0.91025710105896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 39/86 [D loss: 0.6472333371639252, acc.: 66.31%] [G loss: 0.8683989644050598]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 40/86 [D loss: 0.6569421589374542, acc.: 64.31%] [G loss: 0.8109091520309448]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 41/86 [D loss: 0.679463654756546, acc.: 57.57%] [G loss: 0.775631308555603]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 42/86 [D loss: 0.6765431761741638, acc.: 58.45%] [G loss: 0.7571123838424683]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 43/86 [D loss: 0.6674996614456177, acc.: 60.64%] [G loss: 0.7692843675613403]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 44/86 [D loss: 0.6539103388786316, acc.: 64.75%] [G loss: 0.790253221988678]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 45/86 [D loss: 0.6414135098457336, acc.: 67.72%] [G loss: 0.8150961995124817]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 46/86 [D loss: 0.633976936340332, acc.: 68.75%] [G loss: 0.8325378894805908]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 47/86 [D loss: 0.6325576603412628, acc.: 68.80%] [G loss: 0.8327434659004211]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 35/200, Batch 48/86 [D loss: 0.6497368514537811, acc.: 62.94%] [G loss: 0.8055233359336853]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 49/86 [D loss: 0.6594400703907013, acc.: 60.79%] [G loss: 0.7717399597167969]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 50/86 [D loss: 0.6709627509117126, acc.: 57.52%] [G loss: 0.756971001625061]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 51/86 [D loss: 0.6843819320201874, acc.: 54.49%] [G loss: 0.7499094605445862]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 52/86 [D loss: 0.6671765148639679, acc.: 59.91%] [G loss: 0.7992805242538452]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 53/86 [D loss: 0.6534045040607452, acc.: 65.04%] [G loss: 0.8523963093757629]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 54/86 [D loss: 0.6332979202270508, acc.: 69.73%] [G loss: 0.9047224521636963]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 55/86 [D loss: 0.6285158395767212, acc.: 70.70%] [G loss: 0.9168285131454468]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 56/86 [D loss: 0.6347517371177673, acc.: 69.48%] [G loss: 0.8978337049484253]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 57/86 [D loss: 0.64702108502388, acc.: 68.02%] [G loss: 0.849381685256958]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 58/86 [D loss: 0.6657519042491913, acc.: 62.99%] [G loss: 0.797173023223877]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 59/86 [D loss: 0.6789710819721222, acc.: 56.88%] [G loss: 0.7624398469924927]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 60/86 [D loss: 0.6783289313316345, acc.: 57.76%] [G loss: 0.7528613805770874]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 61/86 [D loss: 0.6629005074501038, acc.: 64.16%] [G loss: 0.7836916446685791]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 62/86 [D loss: 0.6510671377182007, acc.: 65.33%] [G loss: 0.8091763854026794]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 63/86 [D loss: 0.6347742676734924, acc.: 70.51%] [G loss: 0.8266458511352539]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 64/86 [D loss: 0.6281774044036865, acc.: 72.07%] [G loss: 0.832240104675293]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 65/86 [D loss: 0.634756326675415, acc.: 68.12%] [G loss: 0.8295097351074219]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 66/86 [D loss: 0.6560738682746887, acc.: 61.77%] [G loss: 0.7855175733566284]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 67/86 [D loss: 0.6659993827342987, acc.: 59.03%] [G loss: 0.7631262540817261]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 68/86 [D loss: 0.6754291653633118, acc.: 56.79%] [G loss: 0.7448225021362305]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 69/86 [D loss: 0.6789624691009521, acc.: 56.30%] [G loss: 0.7625042796134949]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 70/86 [D loss: 0.6657072603702545, acc.: 59.72%] [G loss: 0.8079347014427185]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 71/86 [D loss: 0.6480587422847748, acc.: 66.55%] [G loss: 0.8659369945526123]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 72/86 [D loss: 0.6382202208042145, acc.: 69.38%] [G loss: 0.9132276773452759]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 73/86 [D loss: 0.6294759809970856, acc.: 70.12%] [G loss: 0.9217886328697205]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 74/86 [D loss: 0.6407385468482971, acc.: 68.16%] [G loss: 0.8908600211143494]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 75/86 [D loss: 0.6526708602905273, acc.: 65.48%] [G loss: 0.8275406360626221]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 76/86 [D loss: 0.6734820306301117, acc.: 60.64%] [G loss: 0.7809798717498779]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 77/86 [D loss: 0.6861787736415863, acc.: 56.54%] [G loss: 0.748978853225708]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 78/86 [D loss: 0.6805424988269806, acc.: 56.45%] [G loss: 0.758472740650177]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 79/86 [D loss: 0.6663478910923004, acc.: 61.28%] [G loss: 0.7743682265281677]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 80/86 [D loss: 0.6481727361679077, acc.: 66.02%] [G loss: 0.802336573600769]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 35/200, Batch 81/86 [D loss: 0.631992518901825, acc.: 70.51%] [G loss: 0.833010196685791]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 82/86 [D loss: 0.6320590078830719, acc.: 69.58%] [G loss: 0.8400275707244873]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 83/86 [D loss: 0.6406063735485077, acc.: 66.99%] [G loss: 0.8213454484939575]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 84/86 [D loss: 0.6578137874603271, acc.: 60.01%] [G loss: 0.7921386361122131]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 35/200, Batch 85/86 [D loss: 0.6701624691486359, acc.: 56.88%] [G loss: 0.7573742866516113]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 35/200, Batch 86/86 [D loss: 0.6795549690723419, acc.: 55.76%] [G loss: 0.7532289624214172]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 1/86 [D loss: 0.6715857982635498, acc.: 57.52%] [G loss: 0.7825873494148254]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 2/86 [D loss: 0.6641651690006256, acc.: 61.57%] [G loss: 0.8404792547225952]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 3/86 [D loss: 0.6468485593795776, acc.: 65.28%] [G loss: 0.8843482732772827]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 4/86 [D loss: 0.6329028308391571, acc.: 69.09%] [G loss: 0.9241712093353271]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 5/86 [D loss: 0.633863627910614, acc.: 69.92%] [G loss: 0.9093391299247742]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 6/86 [D loss: 0.6426523327827454, acc.: 67.63%] [G loss: 0.8600995540618896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 7/86 [D loss: 0.6687321066856384, acc.: 61.18%] [G loss: 0.8075287342071533]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 8/86 [D loss: 0.6807473599910736, acc.: 57.62%] [G loss: 0.7681208848953247]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 9/86 [D loss: 0.6777560412883759, acc.: 57.42%] [G loss: 0.753192126750946]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 10/86 [D loss: 0.6733117401599884, acc.: 58.64%] [G loss: 0.7650744318962097]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 11/86 [D loss: 0.6514127850532532, acc.: 66.31%] [G loss: 0.7904894351959229]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 12/86 [D loss: 0.6423474848270416, acc.: 67.58%] [G loss: 0.8120583891868591]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 13/86 [D loss: 0.6303851306438446, acc.: 70.61%] [G loss: 0.8406654000282288]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 14/86 [D loss: 0.6366019546985626, acc.: 66.70%] [G loss: 0.8235552310943604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 15/86 [D loss: 0.6456619203090668, acc.: 63.87%] [G loss: 0.815631628036499]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 16/86 [D loss: 0.6560746431350708, acc.: 61.87%] [G loss: 0.7813040614128113]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 17/86 [D loss: 0.6760079860687256, acc.: 57.13%] [G loss: 0.7475757598876953]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 18/86 [D loss: 0.679078221321106, acc.: 55.32%] [G loss: 0.7587034702301025]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 19/86 [D loss: 0.6707673966884613, acc.: 59.28%] [G loss: 0.7880347967147827]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 20/86 [D loss: 0.6564397513866425, acc.: 64.01%] [G loss: 0.8573199510574341]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 21/86 [D loss: 0.6383309662342072, acc.: 68.90%] [G loss: 0.895565390586853]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 22/86 [D loss: 0.6305014193058014, acc.: 69.87%] [G loss: 0.9090585708618164]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 23/86 [D loss: 0.6328299045562744, acc.: 69.63%] [G loss: 0.9046932458877563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 24/86 [D loss: 0.6515135169029236, acc.: 66.65%] [G loss: 0.849102258682251]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 25/86 [D loss: 0.6757881045341492, acc.: 58.79%] [G loss: 0.7946271896362305]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 26/86 [D loss: 0.6789575219154358, acc.: 59.57%] [G loss: 0.7540479898452759]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 27/86 [D loss: 0.6829696297645569, acc.: 56.40%] [G loss: 0.7604675889015198]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 28/86 [D loss: 0.6730909943580627, acc.: 59.38%] [G loss: 0.7740848064422607]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 29/86 [D loss: 0.6542246639728546, acc.: 65.53%] [G loss: 0.8162344098091125]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 30/86 [D loss: 0.6339968740940094, acc.: 69.63%] [G loss: 0.8464744687080383]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 31/86 [D loss: 0.6259274780750275, acc.: 71.14%] [G loss: 0.8630523681640625]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 32/86 [D loss: 0.6368719637393951, acc.: 67.77%] [G loss: 0.8241757750511169]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 33/86 [D loss: 0.650993674993515, acc.: 61.57%] [G loss: 0.7984501123428345]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 34/86 [D loss: 0.6693273484706879, acc.: 59.03%] [G loss: 0.7619662284851074]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 35/86 [D loss: 0.677024781703949, acc.: 56.93%] [G loss: 0.7446070909500122]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 36/86 [D loss: 0.6870269179344177, acc.: 55.08%] [G loss: 0.7664274573326111]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 37/86 [D loss: 0.6661921143531799, acc.: 60.21%] [G loss: 0.8077316880226135]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 38/86 [D loss: 0.6501069962978363, acc.: 65.53%] [G loss: 0.8682500123977661]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 39/86 [D loss: 0.6339602470397949, acc.: 69.04%] [G loss: 0.9217146039009094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 40/86 [D loss: 0.6380646824836731, acc.: 68.26%] [G loss: 0.9108424782752991]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 41/86 [D loss: 0.6376415193080902, acc.: 68.75%] [G loss: 0.8892191052436829]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 42/86 [D loss: 0.6587112545967102, acc.: 62.01%] [G loss: 0.8194325566291809]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 43/86 [D loss: 0.6775150299072266, acc.: 57.76%] [G loss: 0.7787227034568787]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 36/200, Batch 44/86 [D loss: 0.6846842169761658, acc.: 56.35%] [G loss: 0.7504753470420837]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 45/86 [D loss: 0.6740770936012268, acc.: 59.47%] [G loss: 0.7562379240989685]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 46/86 [D loss: 0.6601240336894989, acc.: 62.30%] [G loss: 0.783292293548584]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 47/86 [D loss: 0.6520548164844513, acc.: 64.45%] [G loss: 0.806809663772583]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 48/86 [D loss: 0.6301902532577515, acc.: 70.26%] [G loss: 0.8340895771980286]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 49/86 [D loss: 0.6295602321624756, acc.: 69.97%] [G loss: 0.8432250618934631]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 50/86 [D loss: 0.6404695808887482, acc.: 65.97%] [G loss: 0.8241494297981262]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 51/86 [D loss: 0.6480247378349304, acc.: 63.38%] [G loss: 0.783663272857666]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 52/86 [D loss: 0.6714953780174255, acc.: 57.03%] [G loss: 0.7579284906387329]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 53/86 [D loss: 0.6838020384311676, acc.: 55.52%] [G loss: 0.7456282377243042]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 54/86 [D loss: 0.6853339672088623, acc.: 54.69%] [G loss: 0.768968939781189]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 55/86 [D loss: 0.6694199442863464, acc.: 60.06%] [G loss: 0.8098772764205933]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 56/86 [D loss: 0.64993816614151, acc.: 65.48%] [G loss: 0.8665269613265991]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 57/86 [D loss: 0.6381341516971588, acc.: 67.14%] [G loss: 0.9152963161468506]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 58/86 [D loss: 0.6354808211326599, acc.: 69.29%] [G loss: 0.914852499961853]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 59/86 [D loss: 0.6479041874408722, acc.: 66.60%] [G loss: 0.8759520053863525]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 60/86 [D loss: 0.6596179008483887, acc.: 64.94%] [G loss: 0.8353283405303955]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 61/86 [D loss: 0.6719151139259338, acc.: 59.38%] [G loss: 0.7961498498916626]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 62/86 [D loss: 0.6786576211452484, acc.: 57.86%] [G loss: 0.7732709050178528]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 63/86 [D loss: 0.67450150847435, acc.: 60.60%] [G loss: 0.776570200920105]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 64/86 [D loss: 0.6683694422245026, acc.: 61.08%] [G loss: 0.7855328917503357]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 65/86 [D loss: 0.6526788771152496, acc.: 65.67%] [G loss: 0.8159079551696777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 66/86 [D loss: 0.6383413374423981, acc.: 68.65%] [G loss: 0.8413522243499756]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 67/86 [D loss: 0.6339143216609955, acc.: 68.99%] [G loss: 0.8386037945747375]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 68/86 [D loss: 0.6401247382164001, acc.: 65.62%] [G loss: 0.8162007331848145]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 69/86 [D loss: 0.6602724194526672, acc.: 61.57%] [G loss: 0.7875794172286987]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 70/86 [D loss: 0.6649001240730286, acc.: 59.47%] [G loss: 0.7614381313323975]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 71/86 [D loss: 0.6782755255699158, acc.: 54.35%] [G loss: 0.7582080364227295]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 72/86 [D loss: 0.6774166226387024, acc.: 57.52%] [G loss: 0.7705152034759521]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 73/86 [D loss: 0.6665497422218323, acc.: 60.89%] [G loss: 0.8192064762115479]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 74/86 [D loss: 0.6534131467342377, acc.: 64.16%] [G loss: 0.8516747951507568]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 75/86 [D loss: 0.6386604607105255, acc.: 68.12%] [G loss: 0.889413058757782]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 76/86 [D loss: 0.6347933709621429, acc.: 68.85%] [G loss: 0.8924841284751892]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 77/86 [D loss: 0.6386213600635529, acc.: 68.41%] [G loss: 0.8631101250648499]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 78/86 [D loss: 0.6530172824859619, acc.: 64.60%] [G loss: 0.8198673725128174]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 79/86 [D loss: 0.6685073375701904, acc.: 60.99%] [G loss: 0.7964418530464172]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 36/200, Batch 80/86 [D loss: 0.6725584864616394, acc.: 58.30%] [G loss: 0.7729928493499756]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 36/200, Batch 81/86 [D loss: 0.6777242422103882, acc.: 57.86%] [G loss: 0.7630782127380371]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 82/86 [D loss: 0.6690177023410797, acc.: 61.08%] [G loss: 0.7702456116676331]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 83/86 [D loss: 0.6557806730270386, acc.: 64.55%] [G loss: 0.8010243773460388]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 84/86 [D loss: 0.6362380087375641, acc.: 70.36%] [G loss: 0.817133903503418]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 85/86 [D loss: 0.6345982849597931, acc.: 68.90%] [G loss: 0.8149098753929138]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 36/200, Batch 86/86 [D loss: 0.6377098262310028, acc.: 66.41%] [G loss: 0.8286714553833008]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 1/86 [D loss: 0.6566620767116547, acc.: 61.38%] [G loss: 0.7921106815338135]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 2/86 [D loss: 0.6695155203342438, acc.: 57.91%] [G loss: 0.7680314183235168]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 3/86 [D loss: 0.6803860366344452, acc.: 54.98%] [G loss: 0.7556703090667725]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 4/86 [D loss: 0.6752939820289612, acc.: 57.62%] [G loss: 0.7792779803276062]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 5/86 [D loss: 0.6695989966392517, acc.: 59.86%] [G loss: 0.8211684823036194]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 6/86 [D loss: 0.6456438302993774, acc.: 66.41%] [G loss: 0.8774703741073608]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 7/86 [D loss: 0.6355364322662354, acc.: 69.04%] [G loss: 0.9091684222221375]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 8/86 [D loss: 0.6342180371284485, acc.: 68.99%] [G loss: 0.9218859672546387]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 9/86 [D loss: 0.6441697776317596, acc.: 66.80%] [G loss: 0.8823078274726868]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 10/86 [D loss: 0.6602820754051208, acc.: 61.87%] [G loss: 0.8269979953765869]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 11/86 [D loss: 0.6680654883384705, acc.: 59.81%] [G loss: 0.7872946858406067]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 12/86 [D loss: 0.6787023842334747, acc.: 58.54%] [G loss: 0.7503523826599121]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 13/86 [D loss: 0.6795696914196014, acc.: 57.47%] [G loss: 0.7574774026870728]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 14/86 [D loss: 0.6678017973899841, acc.: 60.74%] [G loss: 0.7722976207733154]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 15/86 [D loss: 0.645756334066391, acc.: 66.02%] [G loss: 0.8031383156776428]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 16/86 [D loss: 0.6360226273536682, acc.: 68.02%] [G loss: 0.8242663145065308]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 17/86 [D loss: 0.6322118043899536, acc.: 69.58%] [G loss: 0.8339795470237732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 18/86 [D loss: 0.6365588307380676, acc.: 66.99%] [G loss: 0.8205338716506958]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 19/86 [D loss: 0.6516494750976562, acc.: 60.60%] [G loss: 0.7895014882087708]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 20/86 [D loss: 0.6762259006500244, acc.: 55.91%] [G loss: 0.755459189414978]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 37/200, Batch 21/86 [D loss: 0.6802931129932404, acc.: 55.81%] [G loss: 0.749857485294342]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 22/86 [D loss: 0.6809507608413696, acc.: 54.98%] [G loss: 0.7718626856803894]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 23/86 [D loss: 0.6627530455589294, acc.: 61.13%] [G loss: 0.8202528953552246]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 24/86 [D loss: 0.6489731669425964, acc.: 66.55%] [G loss: 0.8729318976402283]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 25/86 [D loss: 0.6399217247962952, acc.: 67.19%] [G loss: 0.9096218347549438]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 26/86 [D loss: 0.6361635625362396, acc.: 69.14%] [G loss: 0.9093654751777649]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 27/86 [D loss: 0.6453636884689331, acc.: 65.92%] [G loss: 0.8711655139923096]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 28/86 [D loss: 0.6616213321685791, acc.: 62.74%] [G loss: 0.8200681209564209]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 29/86 [D loss: 0.6723545789718628, acc.: 60.30%] [G loss: 0.7784810662269592]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 30/86 [D loss: 0.6906769871711731, acc.: 53.91%] [G loss: 0.7585145235061646]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 31/86 [D loss: 0.6814007461071014, acc.: 56.05%] [G loss: 0.7659409046173096]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 32/86 [D loss: 0.667548269033432, acc.: 60.35%] [G loss: 0.7792640924453735]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 33/86 [D loss: 0.6486602127552032, acc.: 65.23%] [G loss: 0.8105026483535767]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 34/86 [D loss: 0.640328049659729, acc.: 68.07%] [G loss: 0.8242698907852173]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 35/86 [D loss: 0.6350145041942596, acc.: 68.31%] [G loss: 0.8355988264083862]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 36/86 [D loss: 0.6444416642189026, acc.: 65.77%] [G loss: 0.814603328704834]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 37/200, Batch 37/86 [D loss: 0.6519663035869598, acc.: 62.35%] [G loss: 0.7932367324829102]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 38/86 [D loss: 0.6698645055294037, acc.: 57.57%] [G loss: 0.7686161994934082]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 39/86 [D loss: 0.6760115325450897, acc.: 55.57%] [G loss: 0.7526332139968872]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 40/86 [D loss: 0.6774218678474426, acc.: 56.35%] [G loss: 0.7777741551399231]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 41/86 [D loss: 0.6679666042327881, acc.: 60.55%] [G loss: 0.8068772554397583]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 42/86 [D loss: 0.6482419669628143, acc.: 66.46%] [G loss: 0.8648430109024048]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 43/86 [D loss: 0.6425909101963043, acc.: 67.48%] [G loss: 0.8960818648338318]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 44/86 [D loss: 0.6359926164150238, acc.: 68.65%] [G loss: 0.8971197605133057]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 45/86 [D loss: 0.6467286050319672, acc.: 66.60%] [G loss: 0.8683237433433533]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 37/200, Batch 46/86 [D loss: 0.6614997386932373, acc.: 62.70%] [G loss: 0.8258297443389893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 47/86 [D loss: 0.6679149866104126, acc.: 61.47%] [G loss: 0.795505166053772]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 48/86 [D loss: 0.6821082830429077, acc.: 56.15%] [G loss: 0.7752388715744019]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 49/86 [D loss: 0.6800205707550049, acc.: 56.54%] [G loss: 0.7677026391029358]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 50/86 [D loss: 0.6671767830848694, acc.: 60.84%] [G loss: 0.78200763463974]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 51/86 [D loss: 0.6499187350273132, acc.: 66.21%] [G loss: 0.812902569770813]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 52/86 [D loss: 0.6380893588066101, acc.: 68.85%] [G loss: 0.8372603058815002]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 53/86 [D loss: 0.6374210715293884, acc.: 67.09%] [G loss: 0.8396524786949158]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 54/86 [D loss: 0.6317835748195648, acc.: 68.31%] [G loss: 0.8308688402175903]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 37/200, Batch 55/86 [D loss: 0.6534876525402069, acc.: 62.99%] [G loss: 0.792832612991333]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 56/86 [D loss: 0.6662015914916992, acc.: 59.03%] [G loss: 0.7582848072052002]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 57/86 [D loss: 0.6816019117832184, acc.: 55.71%] [G loss: 0.7538624405860901]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 58/86 [D loss: 0.6763646006584167, acc.: 57.81%] [G loss: 0.7697354555130005]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 59/86 [D loss: 0.6704423427581787, acc.: 58.69%] [G loss: 0.8030180335044861]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 60/86 [D loss: 0.6523931324481964, acc.: 64.36%] [G loss: 0.8483012318611145]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 61/86 [D loss: 0.6357004046440125, acc.: 69.38%] [G loss: 0.9000651836395264]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 62/86 [D loss: 0.6385497152805328, acc.: 66.89%] [G loss: 0.9080224633216858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 63/86 [D loss: 0.6384193003177643, acc.: 68.51%] [G loss: 0.8843362927436829]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 64/86 [D loss: 0.6454681158065796, acc.: 66.50%] [G loss: 0.8403884172439575]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 65/86 [D loss: 0.6673507690429688, acc.: 62.40%] [G loss: 0.7866845726966858]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 66/86 [D loss: 0.6761276721954346, acc.: 58.84%] [G loss: 0.7696025371551514]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 67/86 [D loss: 0.6790085732936859, acc.: 57.23%] [G loss: 0.7606177926063538]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 68/86 [D loss: 0.6666626334190369, acc.: 61.62%] [G loss: 0.7757057547569275]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 69/86 [D loss: 0.6504441201686859, acc.: 65.67%] [G loss: 0.802355945110321]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 70/86 [D loss: 0.6469560265541077, acc.: 65.72%] [G loss: 0.8298091292381287]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 71/86 [D loss: 0.6393541097640991, acc.: 68.41%] [G loss: 0.8419598340988159]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 72/86 [D loss: 0.6380495727062225, acc.: 67.04%] [G loss: 0.8206726908683777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 73/86 [D loss: 0.6527357697486877, acc.: 61.33%] [G loss: 0.8028472661972046]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 74/86 [D loss: 0.6657346785068512, acc.: 59.62%] [G loss: 0.7628412842750549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 75/86 [D loss: 0.6743716299533844, acc.: 56.35%] [G loss: 0.7592717409133911]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 76/86 [D loss: 0.6815528273582458, acc.: 56.01%] [G loss: 0.771428108215332]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 77/86 [D loss: 0.6684979498386383, acc.: 59.28%] [G loss: 0.8163622617721558]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 78/86 [D loss: 0.655683696269989, acc.: 64.06%] [G loss: 0.857434868812561]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 79/86 [D loss: 0.6389916241168976, acc.: 67.48%] [G loss: 0.8975157737731934]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 80/86 [D loss: 0.6321908533573151, acc.: 68.60%] [G loss: 0.9086676836013794]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 81/86 [D loss: 0.6396105885505676, acc.: 68.02%] [G loss: 0.8867194652557373]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 82/86 [D loss: 0.6618742346763611, acc.: 63.23%] [G loss: 0.8392450213432312]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 37/200, Batch 83/86 [D loss: 0.6661913394927979, acc.: 59.13%] [G loss: 0.784432590007782]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 84/86 [D loss: 0.6734371483325958, acc.: 59.03%] [G loss: 0.7647232413291931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 37/200, Batch 85/86 [D loss: 0.6729896664619446, acc.: 57.71%] [G loss: 0.7570351362228394]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 37/200, Batch 86/86 [D loss: 0.6740480959415436, acc.: 58.30%] [G loss: 0.768571138381958]\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 1/86 [D loss: 0.6552591621875763, acc.: 63.62%] [G loss: 0.7854263186454773]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 2/86 [D loss: 0.6425018012523651, acc.: 67.92%] [G loss: 0.8190723061561584]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 3/86 [D loss: 0.6354740262031555, acc.: 68.75%] [G loss: 0.8304616212844849]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 4/86 [D loss: 0.6323446929454803, acc.: 67.92%] [G loss: 0.8289375305175781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 5/86 [D loss: 0.6442087292671204, acc.: 65.09%] [G loss: 0.7890522480010986]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 38/200, Batch 6/86 [D loss: 0.6664089560508728, acc.: 58.40%] [G loss: 0.7631787657737732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 7/86 [D loss: 0.6776599586009979, acc.: 55.86%] [G loss: 0.7538120746612549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 8/86 [D loss: 0.6797627508640289, acc.: 55.76%] [G loss: 0.7650534510612488]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 9/86 [D loss: 0.6636381447315216, acc.: 61.52%] [G loss: 0.7987884879112244]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 10/86 [D loss: 0.661820113658905, acc.: 61.13%] [G loss: 0.858222484588623]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 11/86 [D loss: 0.6416164040565491, acc.: 67.33%] [G loss: 0.895171046257019]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 12/86 [D loss: 0.6312334835529327, acc.: 70.36%] [G loss: 0.9099352955818176]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 13/86 [D loss: 0.6414344608783722, acc.: 66.70%] [G loss: 0.8844175338745117]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 14/86 [D loss: 0.6533286869525909, acc.: 64.31%] [G loss: 0.8431813716888428]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 15/86 [D loss: 0.6690174639225006, acc.: 60.25%] [G loss: 0.7967141270637512]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 16/86 [D loss: 0.6864800453186035, acc.: 54.69%] [G loss: 0.762840986251831]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 17/86 [D loss: 0.6755763292312622, acc.: 57.47%] [G loss: 0.762115478515625]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 18/86 [D loss: 0.6705058217048645, acc.: 59.62%] [G loss: 0.7705292701721191]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 19/86 [D loss: 0.6539598107337952, acc.: 65.43%] [G loss: 0.7948082089424133]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 20/86 [D loss: 0.6408348381519318, acc.: 69.04%] [G loss: 0.8194456100463867]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 21/86 [D loss: 0.6310577988624573, acc.: 70.12%] [G loss: 0.8330449461936951]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 22/86 [D loss: 0.6341738998889923, acc.: 67.14%] [G loss: 0.8318057656288147]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 23/86 [D loss: 0.6516610980033875, acc.: 63.04%] [G loss: 0.7971888780593872]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 24/86 [D loss: 0.6712173521518707, acc.: 57.81%] [G loss: 0.7624923586845398]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 25/86 [D loss: 0.6692163348197937, acc.: 56.69%] [G loss: 0.7641640901565552]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 26/86 [D loss: 0.67604860663414, acc.: 56.25%] [G loss: 0.7544286847114563]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 27/86 [D loss: 0.6713801026344299, acc.: 59.33%] [G loss: 0.7937459349632263]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 28/86 [D loss: 0.6550163924694061, acc.: 63.38%] [G loss: 0.8457881212234497]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 29/86 [D loss: 0.637835681438446, acc.: 68.16%] [G loss: 0.8817869424819946]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 30/86 [D loss: 0.6354810893535614, acc.: 69.43%] [G loss: 0.9110004901885986]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 31/86 [D loss: 0.6406376659870148, acc.: 66.70%] [G loss: 0.8968867063522339]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 32/86 [D loss: 0.6589744687080383, acc.: 63.53%] [G loss: 0.8501213192939758]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 33/86 [D loss: 0.6710570752620697, acc.: 60.74%] [G loss: 0.793541431427002]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 34/86 [D loss: 0.6862294375896454, acc.: 56.59%] [G loss: 0.7692669630050659]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 35/86 [D loss: 0.682821124792099, acc.: 56.05%] [G loss: 0.7592760324478149]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 36/86 [D loss: 0.6719088852405548, acc.: 60.35%] [G loss: 0.7801166772842407]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 37/86 [D loss: 0.6542750597000122, acc.: 65.43%] [G loss: 0.8065253496170044]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 38/86 [D loss: 0.6386652290821075, acc.: 68.51%] [G loss: 0.8220205307006836]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 39/86 [D loss: 0.6332603991031647, acc.: 69.97%] [G loss: 0.8338069319725037]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 40/86 [D loss: 0.6303391754627228, acc.: 68.21%] [G loss: 0.8366059064865112]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 41/86 [D loss: 0.6479924023151398, acc.: 63.43%] [G loss: 0.8067721724510193]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 42/86 [D loss: 0.668735921382904, acc.: 58.45%] [G loss: 0.775327742099762]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 43/86 [D loss: 0.6800649166107178, acc.: 55.66%] [G loss: 0.7484563589096069]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 38/200, Batch 44/86 [D loss: 0.6859893202781677, acc.: 53.86%] [G loss: 0.7588537931442261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 45/86 [D loss: 0.6760964393615723, acc.: 56.98%] [G loss: 0.7982154488563538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 46/86 [D loss: 0.6535052061080933, acc.: 64.31%] [G loss: 0.8487470149993896]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 47/86 [D loss: 0.6385157108306885, acc.: 67.82%] [G loss: 0.9029713869094849]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 48/86 [D loss: 0.6370691061019897, acc.: 67.92%] [G loss: 0.9026446342468262]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 49/86 [D loss: 0.6328975856304169, acc.: 69.68%] [G loss: 0.8973480463027954]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 50/86 [D loss: 0.6508170366287231, acc.: 66.11%] [G loss: 0.8426631093025208]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 51/86 [D loss: 0.6691161394119263, acc.: 60.84%] [G loss: 0.7895851731300354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 52/86 [D loss: 0.6789029240608215, acc.: 58.15%] [G loss: 0.7554047703742981]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 53/86 [D loss: 0.6841271817684174, acc.: 55.42%] [G loss: 0.7490604519844055]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 54/86 [D loss: 0.6714212596416473, acc.: 57.03%] [G loss: 0.7674252986907959]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 55/86 [D loss: 0.6582297086715698, acc.: 63.57%] [G loss: 0.7917934656143188]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 56/86 [D loss: 0.6431006193161011, acc.: 67.77%] [G loss: 0.8192375898361206]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 57/86 [D loss: 0.6349494159221649, acc.: 69.38%] [G loss: 0.8393012881278992]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 58/86 [D loss: 0.634627103805542, acc.: 68.26%] [G loss: 0.8322696089744568]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 59/86 [D loss: 0.6454342901706696, acc.: 64.06%] [G loss: 0.8231613636016846]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 60/86 [D loss: 0.6614127457141876, acc.: 59.62%] [G loss: 0.7778554558753967]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 38/200, Batch 61/86 [D loss: 0.6740512549877167, acc.: 56.01%] [G loss: 0.7446969747543335]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 62/86 [D loss: 0.6754915714263916, acc.: 56.64%] [G loss: 0.7593736052513123]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 63/86 [D loss: 0.675349086523056, acc.: 57.91%] [G loss: 0.7953760623931885]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 64/86 [D loss: 0.661593496799469, acc.: 61.96%] [G loss: 0.8515058159828186]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 38/200, Batch 65/86 [D loss: 0.6425816118717194, acc.: 67.63%] [G loss: 0.8965409994125366]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 66/86 [D loss: 0.6300411224365234, acc.: 69.78%] [G loss: 0.9127155542373657]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 67/86 [D loss: 0.6383905708789825, acc.: 68.02%] [G loss: 0.8992454409599304]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 68/86 [D loss: 0.6575941145420074, acc.: 64.99%] [G loss: 0.8523378968238831]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 69/86 [D loss: 0.6646909713745117, acc.: 62.30%] [G loss: 0.7977557182312012]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 70/86 [D loss: 0.6767092049121857, acc.: 57.37%] [G loss: 0.7673015594482422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 71/86 [D loss: 0.6876103579998016, acc.: 55.18%] [G loss: 0.7574329376220703]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 72/86 [D loss: 0.6721836626529694, acc.: 60.06%] [G loss: 0.7631566524505615]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 73/86 [D loss: 0.6630070209503174, acc.: 62.84%] [G loss: 0.7922393083572388]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 38/200, Batch 74/86 [D loss: 0.6409609317779541, acc.: 68.80%] [G loss: 0.8252711296081543]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 75/86 [D loss: 0.6300982236862183, acc.: 69.82%] [G loss: 0.8372173309326172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 76/86 [D loss: 0.6362150609493256, acc.: 67.09%] [G loss: 0.8421863913536072]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 77/86 [D loss: 0.6435783207416534, acc.: 65.53%] [G loss: 0.7941327095031738]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 78/86 [D loss: 0.6690090298652649, acc.: 57.76%] [G loss: 0.765970766544342]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 38/200, Batch 79/86 [D loss: 0.6760948896408081, acc.: 57.08%] [G loss: 0.7418498396873474]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 80/86 [D loss: 0.6781443357467651, acc.: 55.37%] [G loss: 0.7547626495361328]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 81/86 [D loss: 0.6740902662277222, acc.: 57.37%] [G loss: 0.796708345413208]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 82/86 [D loss: 0.6574904322624207, acc.: 63.09%] [G loss: 0.850368320941925]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 38/200, Batch 83/86 [D loss: 0.6381933093070984, acc.: 68.65%] [G loss: 0.9141652584075928]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 84/86 [D loss: 0.6317877173423767, acc.: 69.68%] [G loss: 0.9214461445808411]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 38/200, Batch 85/86 [D loss: 0.6374876499176025, acc.: 67.97%] [G loss: 0.8995035290718079]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 38/200, Batch 86/86 [D loss: 0.646483987569809, acc.: 65.97%] [G loss: 0.840191125869751]\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 1/86 [D loss: 0.6669396460056305, acc.: 60.94%] [G loss: 0.8030170202255249]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 39/200, Batch 2/86 [D loss: 0.6808705031871796, acc.: 56.93%] [G loss: 0.7555266618728638]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 3/86 [D loss: 0.6830516457557678, acc.: 55.57%] [G loss: 0.7426265478134155]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 4/86 [D loss: 0.6733385920524597, acc.: 57.13%] [G loss: 0.757175087928772]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 5/86 [D loss: 0.6578813791275024, acc.: 62.50%] [G loss: 0.8014207482337952]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 6/86 [D loss: 0.6440613865852356, acc.: 66.50%] [G loss: 0.8291540741920471]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 7/86 [D loss: 0.62906613945961, acc.: 70.31%] [G loss: 0.8463521003723145]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 8/86 [D loss: 0.6382880508899689, acc.: 66.50%] [G loss: 0.8296529054641724]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 9/86 [D loss: 0.6453524827957153, acc.: 63.62%] [G loss: 0.8100011348724365]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 39/200, Batch 10/86 [D loss: 0.6629711389541626, acc.: 58.94%] [G loss: 0.7783809304237366]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 11/86 [D loss: 0.6721210181713104, acc.: 56.54%] [G loss: 0.7633426189422607]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 12/86 [D loss: 0.6844178438186646, acc.: 53.71%] [G loss: 0.7496519088745117]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 13/86 [D loss: 0.6759984791278839, acc.: 56.74%] [G loss: 0.7815858721733093]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 14/86 [D loss: 0.6582573652267456, acc.: 63.09%] [G loss: 0.8316649198532104]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 15/86 [D loss: 0.6449609994888306, acc.: 66.36%] [G loss: 0.885408878326416]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 16/86 [D loss: 0.6354317665100098, acc.: 68.80%] [G loss: 0.9089063405990601]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 17/86 [D loss: 0.6404930651187897, acc.: 67.53%] [G loss: 0.8939401507377625]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 18/86 [D loss: 0.6544334590435028, acc.: 63.62%] [G loss: 0.8607228994369507]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 39/200, Batch 19/86 [D loss: 0.662218451499939, acc.: 61.77%] [G loss: 0.8159078359603882]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 20/86 [D loss: 0.6831054389476776, acc.: 56.40%] [G loss: 0.7721835374832153]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 21/86 [D loss: 0.6808792948722839, acc.: 56.88%] [G loss: 0.7534283995628357]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 22/86 [D loss: 0.6733602285385132, acc.: 59.33%] [G loss: 0.768647313117981]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 39/200, Batch 23/86 [D loss: 0.6572683155536652, acc.: 63.92%] [G loss: 0.7936221361160278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 24/86 [D loss: 0.6445223689079285, acc.: 67.53%] [G loss: 0.8194757103919983]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 25/86 [D loss: 0.6388639807701111, acc.: 67.68%] [G loss: 0.8340305089950562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 26/86 [D loss: 0.6325144469738007, acc.: 69.04%] [G loss: 0.8352445363998413]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 39/200, Batch 27/86 [D loss: 0.6463178098201752, acc.: 63.87%] [G loss: 0.8081255555152893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 28/86 [D loss: 0.6595533192157745, acc.: 60.11%] [G loss: 0.7952900528907776]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 29/86 [D loss: 0.6686101853847504, acc.: 57.86%] [G loss: 0.766947329044342]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 30/86 [D loss: 0.680576354265213, acc.: 56.30%] [G loss: 0.7596365809440613]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 39/200, Batch 31/86 [D loss: 0.6775766015052795, acc.: 57.18%] [G loss: 0.7810268402099609]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 32/86 [D loss: 0.6653746366500854, acc.: 61.82%] [G loss: 0.8259966969490051]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 33/86 [D loss: 0.6489376723766327, acc.: 64.60%] [G loss: 0.8634945750236511]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 34/86 [D loss: 0.6398920118808746, acc.: 67.58%] [G loss: 0.8880407214164734]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 39/200, Batch 35/86 [D loss: 0.6317042708396912, acc.: 68.90%] [G loss: 0.8940671682357788]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 36/86 [D loss: 0.6448480784893036, acc.: 65.87%] [G loss: 0.8674843907356262]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 37/86 [D loss: 0.6571866571903229, acc.: 63.43%] [G loss: 0.8281296491622925]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 38/86 [D loss: 0.678239494562149, acc.: 58.69%] [G loss: 0.7903083562850952]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 39/86 [D loss: 0.6800362467765808, acc.: 56.35%] [G loss: 0.7637657523155212]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 40/86 [D loss: 0.6759835183620453, acc.: 57.37%] [G loss: 0.7706003189086914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 41/86 [D loss: 0.6636894941329956, acc.: 61.23%] [G loss: 0.79059237241745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 42/86 [D loss: 0.6519319713115692, acc.: 65.19%] [G loss: 0.8152452707290649]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 43/86 [D loss: 0.6383555233478546, acc.: 68.75%] [G loss: 0.847934365272522]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 44/86 [D loss: 0.6335695385932922, acc.: 66.75%] [G loss: 0.8497744202613831]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 45/86 [D loss: 0.6451272964477539, acc.: 64.60%] [G loss: 0.8309167623519897]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 46/86 [D loss: 0.6536246836185455, acc.: 61.43%] [G loss: 0.8061708211898804]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 47/86 [D loss: 0.6715382933616638, acc.: 55.81%] [G loss: 0.7642437219619751]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 48/86 [D loss: 0.669716864824295, acc.: 58.98%] [G loss: 0.7614114880561829]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 49/86 [D loss: 0.6778225302696228, acc.: 56.69%] [G loss: 0.7683150768280029]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 50/86 [D loss: 0.6680598556995392, acc.: 59.77%] [G loss: 0.8105786442756653]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 51/86 [D loss: 0.655164897441864, acc.: 63.57%] [G loss: 0.8580499291419983]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 52/86 [D loss: 0.6424841284751892, acc.: 67.92%] [G loss: 0.9044506549835205]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 53/86 [D loss: 0.6367011666297913, acc.: 67.38%] [G loss: 0.9065402746200562]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 54/86 [D loss: 0.6374025940895081, acc.: 68.36%] [G loss: 0.8847918510437012]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 55/86 [D loss: 0.6553182005882263, acc.: 63.82%] [G loss: 0.8497362732887268]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 56/86 [D loss: 0.6678954362869263, acc.: 59.67%] [G loss: 0.7975894212722778]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 57/86 [D loss: 0.6771892607212067, acc.: 56.69%] [G loss: 0.7540112733840942]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 58/86 [D loss: 0.6817344427108765, acc.: 56.20%] [G loss: 0.7539259791374207]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 59/86 [D loss: 0.6752415001392365, acc.: 58.54%] [G loss: 0.7656702399253845]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 60/86 [D loss: 0.6622639894485474, acc.: 62.11%] [G loss: 0.7951076626777649]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 61/86 [D loss: 0.6372652649879456, acc.: 68.70%] [G loss: 0.8360527753829956]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 62/86 [D loss: 0.6315577328205109, acc.: 69.53%] [G loss: 0.8456723093986511]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 63/86 [D loss: 0.6349924802780151, acc.: 68.80%] [G loss: 0.8405476212501526]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 64/86 [D loss: 0.6479769647121429, acc.: 63.43%] [G loss: 0.8052918314933777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 65/86 [D loss: 0.6576588451862335, acc.: 62.01%] [G loss: 0.7721439599990845]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 66/86 [D loss: 0.6753445267677307, acc.: 57.13%] [G loss: 0.7530969977378845]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 67/86 [D loss: 0.6828587055206299, acc.: 54.88%] [G loss: 0.7567485570907593]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 39/200, Batch 68/86 [D loss: 0.6787439584732056, acc.: 56.35%] [G loss: 0.7921547293663025]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 69/86 [D loss: 0.6612252593040466, acc.: 62.01%] [G loss: 0.8386139273643494]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 70/86 [D loss: 0.6448214948177338, acc.: 66.99%] [G loss: 0.8948342800140381]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 71/86 [D loss: 0.6335560083389282, acc.: 67.87%] [G loss: 0.9199274778366089]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 72/86 [D loss: 0.6338926553726196, acc.: 68.41%] [G loss: 0.9041128754615784]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 73/86 [D loss: 0.6514126062393188, acc.: 64.55%] [G loss: 0.8758319020271301]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 74/86 [D loss: 0.6646203994750977, acc.: 62.16%] [G loss: 0.801477313041687]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 75/86 [D loss: 0.6810141205787659, acc.: 57.42%] [G loss: 0.7654099464416504]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 76/86 [D loss: 0.68072909116745, acc.: 56.69%] [G loss: 0.7561757564544678]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 77/86 [D loss: 0.6770675778388977, acc.: 57.57%] [G loss: 0.7595771551132202]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 78/86 [D loss: 0.6629030108451843, acc.: 60.84%] [G loss: 0.788947343826294]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 79/86 [D loss: 0.6428674459457397, acc.: 67.58%] [G loss: 0.8375065922737122]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 80/86 [D loss: 0.6268597543239594, acc.: 70.36%] [G loss: 0.8391706347465515]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 39/200, Batch 81/86 [D loss: 0.6317191421985626, acc.: 68.80%] [G loss: 0.8410090208053589]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 82/86 [D loss: 0.6387147307395935, acc.: 66.50%] [G loss: 0.8217025399208069]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 39/200, Batch 83/86 [D loss: 0.6513862013816833, acc.: 61.72%] [G loss: 0.782890796661377]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 84/86 [D loss: 0.6685750484466553, acc.: 57.62%] [G loss: 0.7536881566047668]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 85/86 [D loss: 0.686195433139801, acc.: 52.59%] [G loss: 0.7594037055969238]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 39/200, Batch 86/86 [D loss: 0.6745828986167908, acc.: 58.06%] [G loss: 0.7872395515441895]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 1/86 [D loss: 0.6630856990814209, acc.: 60.74%] [G loss: 0.8313354253768921]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 2/86 [D loss: 0.6475658416748047, acc.: 65.58%] [G loss: 0.876660943031311]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 3/86 [D loss: 0.6368278563022614, acc.: 67.48%] [G loss: 0.9054083824157715]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 4/86 [D loss: 0.633562445640564, acc.: 68.70%] [G loss: 0.9182065725326538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 5/86 [D loss: 0.6377333998680115, acc.: 67.77%] [G loss: 0.8810199499130249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 6/86 [D loss: 0.6537476778030396, acc.: 64.55%] [G loss: 0.8368363380432129]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 7/86 [D loss: 0.665682464838028, acc.: 61.57%] [G loss: 0.7963113784790039]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 8/86 [D loss: 0.6781403720378876, acc.: 58.01%] [G loss: 0.7649052143096924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 9/86 [D loss: 0.6820935606956482, acc.: 56.20%] [G loss: 0.7536411285400391]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 10/86 [D loss: 0.6721658408641815, acc.: 58.35%] [G loss: 0.7729353904724121]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 11/86 [D loss: 0.6498650014400482, acc.: 64.79%] [G loss: 0.8001680970191956]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 12/86 [D loss: 0.6404432058334351, acc.: 67.72%] [G loss: 0.8302910327911377]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 13/86 [D loss: 0.6313822865486145, acc.: 69.87%] [G loss: 0.8358940482139587]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 14/86 [D loss: 0.6359574496746063, acc.: 66.36%] [G loss: 0.8397307395935059]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 15/86 [D loss: 0.6445859372615814, acc.: 65.38%] [G loss: 0.8022176027297974]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 40/200, Batch 16/86 [D loss: 0.6679730415344238, acc.: 58.89%] [G loss: 0.7781241536140442]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 17/86 [D loss: 0.6812247037887573, acc.: 55.57%] [G loss: 0.7593456506729126]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 18/86 [D loss: 0.685497373342514, acc.: 53.81%] [G loss: 0.7623768448829651]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 19/86 [D loss: 0.6676991283893585, acc.: 60.11%] [G loss: 0.7944263219833374]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 20/86 [D loss: 0.6567887663841248, acc.: 62.06%] [G loss: 0.8530265092849731]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 21/86 [D loss: 0.6367032527923584, acc.: 68.12%] [G loss: 0.879111111164093]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 22/86 [D loss: 0.636400431394577, acc.: 67.43%] [G loss: 0.9068170189857483]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 23/86 [D loss: 0.6373714208602905, acc.: 67.43%] [G loss: 0.8962611556053162]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 24/86 [D loss: 0.6399071216583252, acc.: 68.65%] [G loss: 0.8648555278778076]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 25/86 [D loss: 0.6665571630001068, acc.: 61.87%] [G loss: 0.8111476898193359]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 26/86 [D loss: 0.6784927845001221, acc.: 57.13%] [G loss: 0.7700402140617371]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 27/86 [D loss: 0.6811195015907288, acc.: 56.45%] [G loss: 0.7539180517196655]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 28/86 [D loss: 0.677735447883606, acc.: 57.18%] [G loss: 0.7552984952926636]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 29/86 [D loss: 0.6601408421993256, acc.: 62.79%] [G loss: 0.790010929107666]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 30/86 [D loss: 0.6491878926753998, acc.: 66.02%] [G loss: 0.8204993009567261]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 31/86 [D loss: 0.6309452652931213, acc.: 70.46%] [G loss: 0.8454943895339966]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 32/86 [D loss: 0.6338546574115753, acc.: 66.21%] [G loss: 0.8458896279335022]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 33/86 [D loss: 0.6374075412750244, acc.: 66.75%] [G loss: 0.8289158344268799]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 34/86 [D loss: 0.6583251953125, acc.: 60.79%] [G loss: 0.7814404368400574]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 35/86 [D loss: 0.6812717020511627, acc.: 54.54%] [G loss: 0.7638084888458252]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 36/86 [D loss: 0.679899662733078, acc.: 56.35%] [G loss: 0.7466959953308105]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 37/86 [D loss: 0.6801939904689789, acc.: 55.27%] [G loss: 0.7868351936340332]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 38/86 [D loss: 0.6625385582447052, acc.: 60.35%] [G loss: 0.8321781754493713]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 39/86 [D loss: 0.6514452993869781, acc.: 64.45%] [G loss: 0.8862437009811401]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 40/86 [D loss: 0.6297585964202881, acc.: 69.14%] [G loss: 0.9179884791374207]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 41/86 [D loss: 0.629064679145813, acc.: 70.61%] [G loss: 0.9129254221916199]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 42/86 [D loss: 0.6434962749481201, acc.: 66.85%] [G loss: 0.8796550035476685]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 43/86 [D loss: 0.6618118286132812, acc.: 62.21%] [G loss: 0.8212290406227112]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 44/86 [D loss: 0.6832051277160645, acc.: 56.30%] [G loss: 0.7893562912940979]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 45/86 [D loss: 0.6877698600292206, acc.: 54.35%] [G loss: 0.7606824636459351]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 46/86 [D loss: 0.6763475239276886, acc.: 57.37%] [G loss: 0.7736193537712097]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 47/86 [D loss: 0.6670609712600708, acc.: 60.74%] [G loss: 0.790195882320404]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 48/86 [D loss: 0.6444882750511169, acc.: 66.11%] [G loss: 0.833202600479126]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 49/86 [D loss: 0.6370213627815247, acc.: 68.46%] [G loss: 0.8632856607437134]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 50/86 [D loss: 0.6252610087394714, acc.: 71.19%] [G loss: 0.8681718111038208]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 51/86 [D loss: 0.6400213539600372, acc.: 66.02%] [G loss: 0.8466248512268066]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 52/86 [D loss: 0.6474815011024475, acc.: 63.13%] [G loss: 0.8025597333908081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 53/86 [D loss: 0.6763874888420105, acc.: 55.32%] [G loss: 0.7727668285369873]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 54/86 [D loss: 0.6839311420917511, acc.: 54.83%] [G loss: 0.7536725401878357]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 40/200, Batch 55/86 [D loss: 0.681321769952774, acc.: 54.98%] [G loss: 0.7713128924369812]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 56/86 [D loss: 0.6705207526683807, acc.: 58.89%] [G loss: 0.8148397207260132]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 57/86 [D loss: 0.6509301364421844, acc.: 64.79%] [G loss: 0.8683332800865173]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 58/86 [D loss: 0.6347532272338867, acc.: 69.04%] [G loss: 0.9138888120651245]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 59/86 [D loss: 0.6297785341739655, acc.: 69.04%] [G loss: 0.9148604869842529]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 40/200, Batch 60/86 [D loss: 0.6403999626636505, acc.: 67.24%] [G loss: 0.8918697834014893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 61/86 [D loss: 0.6475233733654022, acc.: 65.92%] [G loss: 0.8595107793807983]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 62/86 [D loss: 0.665620893239975, acc.: 60.94%] [G loss: 0.8150383234024048]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 63/86 [D loss: 0.6826289594173431, acc.: 57.76%] [G loss: 0.7760847210884094]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 64/86 [D loss: 0.6814547777175903, acc.: 56.64%] [G loss: 0.7591632008552551]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 65/86 [D loss: 0.6752521097660065, acc.: 57.96%] [G loss: 0.768869936466217]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 66/86 [D loss: 0.6581392884254456, acc.: 63.13%] [G loss: 0.7933600544929504]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 67/86 [D loss: 0.645638108253479, acc.: 67.33%] [G loss: 0.8340946435928345]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 68/86 [D loss: 0.6303085386753082, acc.: 69.53%] [G loss: 0.8488321900367737]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 69/86 [D loss: 0.6228574216365814, acc.: 71.04%] [G loss: 0.8670784831047058]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 70/86 [D loss: 0.6401242911815643, acc.: 65.38%] [G loss: 0.8301213979721069]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 71/86 [D loss: 0.6566632390022278, acc.: 61.47%] [G loss: 0.7885318994522095]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 72/86 [D loss: 0.6757268011569977, acc.: 56.20%] [G loss: 0.7636495232582092]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 73/86 [D loss: 0.6848435997962952, acc.: 54.25%] [G loss: 0.7483946084976196]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 74/86 [D loss: 0.6717706918716431, acc.: 57.62%] [G loss: 0.779115617275238]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 75/86 [D loss: 0.6649419665336609, acc.: 60.50%] [G loss: 0.8262378573417664]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 76/86 [D loss: 0.6425991058349609, acc.: 67.24%] [G loss: 0.8971735239028931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 77/86 [D loss: 0.6333355605602264, acc.: 68.99%] [G loss: 0.9287484884262085]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 78/86 [D loss: 0.6297350823879242, acc.: 68.65%] [G loss: 0.9372706413269043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 79/86 [D loss: 0.6376808285713196, acc.: 67.53%] [G loss: 0.9016525149345398]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 80/86 [D loss: 0.6576861143112183, acc.: 64.89%] [G loss: 0.8336783647537231]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 81/86 [D loss: 0.6721040606498718, acc.: 59.08%] [G loss: 0.7873053550720215]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 82/86 [D loss: 0.6789282560348511, acc.: 57.42%] [G loss: 0.7570212483406067]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 40/200, Batch 83/86 [D loss: 0.6872211396694183, acc.: 54.59%] [G loss: 0.7531024813652039]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 84/86 [D loss: 0.6727912724018097, acc.: 57.42%] [G loss: 0.7751060724258423]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 40/200, Batch 85/86 [D loss: 0.648466944694519, acc.: 66.11%] [G loss: 0.8059408664703369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 40/200, Batch 86/86 [D loss: 0.6297190189361572, acc.: 71.19%] [G loss: 0.8453975319862366]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 1/86 [D loss: 0.6273652017116547, acc.: 70.12%] [G loss: 0.8676432371139526]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 2/86 [D loss: 0.6258420348167419, acc.: 68.85%] [G loss: 0.8555030226707458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 3/86 [D loss: 0.6463615596294403, acc.: 63.48%] [G loss: 0.8107736706733704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 4/86 [D loss: 0.6520495712757111, acc.: 61.62%] [G loss: 0.7839762568473816]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 5/86 [D loss: 0.679360955953598, acc.: 54.88%] [G loss: 0.7628259062767029]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 6/86 [D loss: 0.6825496554374695, acc.: 53.32%] [G loss: 0.7477456331253052]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 7/86 [D loss: 0.675125926733017, acc.: 56.05%] [G loss: 0.7914614081382751]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 8/86 [D loss: 0.6595845818519592, acc.: 62.40%] [G loss: 0.8479058146476746]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 9/86 [D loss: 0.6400675475597382, acc.: 67.68%] [G loss: 0.8980302810668945]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 10/86 [D loss: 0.6319776177406311, acc.: 67.92%] [G loss: 0.9470979571342468]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 11/86 [D loss: 0.629893571138382, acc.: 67.53%] [G loss: 0.9543962478637695]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 12/86 [D loss: 0.6368140578269958, acc.: 68.41%] [G loss: 0.9089770317077637]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 13/86 [D loss: 0.6538571417331696, acc.: 64.89%] [G loss: 0.8389687538146973]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 14/86 [D loss: 0.6721746325492859, acc.: 58.98%] [G loss: 0.7797403335571289]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 15/86 [D loss: 0.6910127103328705, acc.: 53.47%] [G loss: 0.7526097297668457]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 16/86 [D loss: 0.6775383949279785, acc.: 55.13%] [G loss: 0.7477024793624878]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 17/86 [D loss: 0.6671566665172577, acc.: 60.89%] [G loss: 0.7763516902923584]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 18/86 [D loss: 0.6520692706108093, acc.: 65.53%] [G loss: 0.8134922981262207]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 19/86 [D loss: 0.6393851041793823, acc.: 68.75%] [G loss: 0.852135181427002]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 20/86 [D loss: 0.6184099614620209, acc.: 72.36%] [G loss: 0.8761218786239624]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 21/86 [D loss: 0.6316399574279785, acc.: 68.07%] [G loss: 0.8640297055244446]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 22/86 [D loss: 0.6422746479511261, acc.: 64.16%] [G loss: 0.8211829662322998]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 41/200, Batch 23/86 [D loss: 0.6528920531272888, acc.: 62.16%] [G loss: 0.7789438366889954]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 24/86 [D loss: 0.6773989200592041, acc.: 55.22%] [G loss: 0.7558004260063171]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 25/86 [D loss: 0.6817276775836945, acc.: 55.71%] [G loss: 0.7505322098731995]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 26/86 [D loss: 0.6746580898761749, acc.: 57.13%] [G loss: 0.7843682169914246]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 27/86 [D loss: 0.6611802577972412, acc.: 60.64%] [G loss: 0.8391771912574768]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 28/86 [D loss: 0.6433991193771362, acc.: 66.94%] [G loss: 0.8998129367828369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 29/86 [D loss: 0.6265340745449066, acc.: 70.17%] [G loss: 0.9424256682395935]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 30/86 [D loss: 0.6289513409137726, acc.: 69.29%] [G loss: 0.9469338059425354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 31/86 [D loss: 0.6265437304973602, acc.: 70.07%] [G loss: 0.9187585115432739]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 41/200, Batch 32/86 [D loss: 0.6436032950878143, acc.: 66.70%] [G loss: 0.8663296699523926]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 33/86 [D loss: 0.6691037714481354, acc.: 59.96%] [G loss: 0.7960188984870911]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 34/86 [D loss: 0.6827101707458496, acc.: 56.01%] [G loss: 0.7526878118515015]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 35/86 [D loss: 0.6834436058998108, acc.: 54.44%] [G loss: 0.7449082732200623]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 36/86 [D loss: 0.6824929118156433, acc.: 55.52%] [G loss: 0.7599486112594604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 37/86 [D loss: 0.6611677408218384, acc.: 62.26%] [G loss: 0.8018679022789001]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 38/86 [D loss: 0.6386143565177917, acc.: 69.19%] [G loss: 0.8363906145095825]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 39/86 [D loss: 0.6243076622486115, acc.: 71.39%] [G loss: 0.8600835204124451]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 40/86 [D loss: 0.6173343658447266, acc.: 71.48%] [G loss: 0.8656761646270752]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 41/86 [D loss: 0.6341974139213562, acc.: 66.36%] [G loss: 0.8455430865287781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 42/86 [D loss: 0.6552526354789734, acc.: 60.84%] [G loss: 0.7934197187423706]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 43/86 [D loss: 0.6690084636211395, acc.: 57.23%] [G loss: 0.7614097595214844]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 44/86 [D loss: 0.6789855659008026, acc.: 54.83%] [G loss: 0.754351019859314]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 45/86 [D loss: 0.6762609481811523, acc.: 55.62%] [G loss: 0.7630313038825989]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 46/86 [D loss: 0.6679073870182037, acc.: 59.67%] [G loss: 0.8147215247154236]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 47/86 [D loss: 0.6505832970142365, acc.: 64.50%] [G loss: 0.875708818435669]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 48/86 [D loss: 0.6408148109912872, acc.: 66.46%] [G loss: 0.9297669529914856]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 41/200, Batch 49/86 [D loss: 0.6219253838062286, acc.: 70.46%] [G loss: 0.9488613605499268]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 50/86 [D loss: 0.6277263462543488, acc.: 70.26%] [G loss: 0.9402316808700562]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 51/86 [D loss: 0.6392783224582672, acc.: 66.55%] [G loss: 0.8799757361412048]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 52/86 [D loss: 0.6620834171772003, acc.: 62.35%] [G loss: 0.8052208423614502]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 53/86 [D loss: 0.6838403046131134, acc.: 55.47%] [G loss: 0.765315055847168]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 54/86 [D loss: 0.6911784410476685, acc.: 54.00%] [G loss: 0.7383729219436646]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 55/86 [D loss: 0.6844774484634399, acc.: 54.59%] [G loss: 0.7483375072479248]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 56/86 [D loss: 0.6667395830154419, acc.: 60.25%] [G loss: 0.7871582508087158]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 57/86 [D loss: 0.6423958837985992, acc.: 67.09%] [G loss: 0.8147834539413452]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 58/86 [D loss: 0.6325969099998474, acc.: 68.90%] [G loss: 0.8511420488357544]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 59/86 [D loss: 0.6236726641654968, acc.: 70.07%] [G loss: 0.8658667802810669]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 60/86 [D loss: 0.6272462606430054, acc.: 69.29%] [G loss: 0.8502846360206604]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 61/86 [D loss: 0.6388732194900513, acc.: 65.04%] [G loss: 0.8231523633003235]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 62/86 [D loss: 0.6581089496612549, acc.: 59.91%] [G loss: 0.790235698223114]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 63/86 [D loss: 0.6797228753566742, acc.: 55.47%] [G loss: 0.7569887638092041]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 64/86 [D loss: 0.679828405380249, acc.: 55.76%] [G loss: 0.7510557174682617]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 65/86 [D loss: 0.6826463043689728, acc.: 55.18%] [G loss: 0.7833651900291443]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 66/86 [D loss: 0.6667933762073517, acc.: 60.25%] [G loss: 0.8400999903678894]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 67/86 [D loss: 0.6451613903045654, acc.: 65.48%] [G loss: 0.8921894431114197]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 68/86 [D loss: 0.6302611827850342, acc.: 68.90%] [G loss: 0.9364919662475586]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 69/86 [D loss: 0.6289059519767761, acc.: 68.75%] [G loss: 0.9381983280181885]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 70/86 [D loss: 0.6301850080490112, acc.: 69.78%] [G loss: 0.9128991961479187]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 71/86 [D loss: 0.6442972719669342, acc.: 66.41%] [G loss: 0.8720074892044067]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 72/86 [D loss: 0.6752482354640961, acc.: 59.03%] [G loss: 0.8041670322418213]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 73/86 [D loss: 0.6843777298927307, acc.: 55.91%] [G loss: 0.7690490484237671]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 74/86 [D loss: 0.6933933198451996, acc.: 53.42%] [G loss: 0.7565949559211731]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 75/86 [D loss: 0.6796812415122986, acc.: 57.18%] [G loss: 0.7620338201522827]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 76/86 [D loss: 0.6605936884880066, acc.: 62.89%] [G loss: 0.7958957552909851]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 77/86 [D loss: 0.6447393894195557, acc.: 66.65%] [G loss: 0.8386117815971375]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 78/86 [D loss: 0.6306968927383423, acc.: 69.43%] [G loss: 0.8734179735183716]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 79/86 [D loss: 0.6272079348564148, acc.: 68.75%] [G loss: 0.8733362555503845]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 80/86 [D loss: 0.6260766685009003, acc.: 68.75%] [G loss: 0.8490949869155884]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 81/86 [D loss: 0.6433316171169281, acc.: 64.89%] [G loss: 0.8069256544113159]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 82/86 [D loss: 0.6708464622497559, acc.: 56.88%] [G loss: 0.7652670741081238]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 41/200, Batch 83/86 [D loss: 0.6748087704181671, acc.: 56.25%] [G loss: 0.7501410245895386]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 41/200, Batch 84/86 [D loss: 0.6779299080371857, acc.: 55.57%] [G loss: 0.7586321830749512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 85/86 [D loss: 0.6723136901855469, acc.: 58.89%] [G loss: 0.8007355332374573]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 41/200, Batch 86/86 [D loss: 0.6528621315956116, acc.: 64.31%] [G loss: 0.8623788356781006]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 42/200, Batch 1/86 [D loss: 0.6365205943584442, acc.: 67.97%] [G loss: 0.9087579846382141]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 2/86 [D loss: 0.6249485313892365, acc.: 70.26%] [G loss: 0.9496607780456543]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 3/86 [D loss: 0.6252498030662537, acc.: 70.70%] [G loss: 0.9540074467658997]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 4/86 [D loss: 0.6388343572616577, acc.: 67.53%] [G loss: 0.9085526466369629]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 5/86 [D loss: 0.6593301296234131, acc.: 63.38%] [G loss: 0.8455145359039307]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 6/86 [D loss: 0.6760283410549164, acc.: 58.40%] [G loss: 0.7882919311523438]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 7/86 [D loss: 0.6859975755214691, acc.: 55.03%] [G loss: 0.7626937627792358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 8/86 [D loss: 0.6893346607685089, acc.: 53.42%] [G loss: 0.7637093663215637]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 9/86 [D loss: 0.6766500771045685, acc.: 57.52%] [G loss: 0.7824557423591614]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 10/86 [D loss: 0.6514376401901245, acc.: 64.89%] [G loss: 0.8155673146247864]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 11/86 [D loss: 0.6290983855724335, acc.: 69.97%] [G loss: 0.840916097164154]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 12/86 [D loss: 0.6205680072307587, acc.: 70.80%] [G loss: 0.8750078082084656]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 13/86 [D loss: 0.6257326602935791, acc.: 68.36%] [G loss: 0.8692663908004761]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 14/86 [D loss: 0.6367382109165192, acc.: 64.65%] [G loss: 0.8344590663909912]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 15/86 [D loss: 0.6582157611846924, acc.: 60.01%] [G loss: 0.781160831451416]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 16/86 [D loss: 0.6717498302459717, acc.: 56.93%] [G loss: 0.7460924983024597]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 17/86 [D loss: 0.680102527141571, acc.: 55.81%] [G loss: 0.7524080276489258]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 18/86 [D loss: 0.6799275279045105, acc.: 55.57%] [G loss: 0.7776168584823608]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 19/86 [D loss: 0.6670370697975159, acc.: 59.72%] [G loss: 0.826145589351654]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 20/86 [D loss: 0.6505098342895508, acc.: 64.60%] [G loss: 0.9013498425483704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 21/86 [D loss: 0.6292499005794525, acc.: 69.58%] [G loss: 0.946074903011322]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 22/86 [D loss: 0.6253372430801392, acc.: 70.12%] [G loss: 0.963073194026947]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 23/86 [D loss: 0.6312607228755951, acc.: 68.21%] [G loss: 0.927101731300354]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 24/86 [D loss: 0.6440738141536713, acc.: 65.97%] [G loss: 0.8551108837127686]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 25/86 [D loss: 0.6629751324653625, acc.: 62.11%] [G loss: 0.8059608340263367]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 26/86 [D loss: 0.6790450513362885, acc.: 57.13%] [G loss: 0.7669360637664795]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 27/86 [D loss: 0.6858606338500977, acc.: 54.64%] [G loss: 0.7567744851112366]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 28/86 [D loss: 0.6778181195259094, acc.: 56.93%] [G loss: 0.7592871189117432]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 29/86 [D loss: 0.6593405604362488, acc.: 62.89%] [G loss: 0.8095848560333252]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 30/86 [D loss: 0.6446260213851929, acc.: 65.87%] [G loss: 0.8408441543579102]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 31/86 [D loss: 0.6263779699802399, acc.: 71.19%] [G loss: 0.8551124334335327]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 32/86 [D loss: 0.623386412858963, acc.: 70.12%] [G loss: 0.8681020736694336]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 33/86 [D loss: 0.6330945193767548, acc.: 67.77%] [G loss: 0.8389896154403687]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 34/86 [D loss: 0.6500347256660461, acc.: 62.94%] [G loss: 0.808976411819458]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 35/86 [D loss: 0.6648073196411133, acc.: 58.30%] [G loss: 0.7705938816070557]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 36/86 [D loss: 0.6816640496253967, acc.: 55.22%] [G loss: 0.761542797088623]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 37/86 [D loss: 0.6791348159313202, acc.: 55.27%] [G loss: 0.758844256401062]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 38/86 [D loss: 0.6759753227233887, acc.: 56.98%] [G loss: 0.8039280772209167]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 39/86 [D loss: 0.6525943279266357, acc.: 63.72%] [G loss: 0.8576030135154724]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 40/86 [D loss: 0.6416019797325134, acc.: 66.26%] [G loss: 0.9058222770690918]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 41/86 [D loss: 0.6271377205848694, acc.: 70.02%] [G loss: 0.9489467740058899]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 42/86 [D loss: 0.6226840019226074, acc.: 70.07%] [G loss: 0.9497794508934021]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 43/86 [D loss: 0.6281423568725586, acc.: 70.12%] [G loss: 0.9100006222724915]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 44/86 [D loss: 0.6597874164581299, acc.: 62.11%] [G loss: 0.8428747653961182]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 42/200, Batch 45/86 [D loss: 0.6709584593772888, acc.: 58.30%] [G loss: 0.788384199142456]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 46/86 [D loss: 0.6896384656429291, acc.: 54.69%] [G loss: 0.7548602819442749]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 47/86 [D loss: 0.6865554451942444, acc.: 55.52%] [G loss: 0.7567318081855774]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 48/86 [D loss: 0.6723140776157379, acc.: 58.25%] [G loss: 0.7729092836380005]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 49/86 [D loss: 0.6612057685852051, acc.: 62.65%] [G loss: 0.8025580644607544]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 50/86 [D loss: 0.6388656795024872, acc.: 68.85%] [G loss: 0.845661461353302]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 51/86 [D loss: 0.623346358537674, acc.: 71.92%] [G loss: 0.8815415501594543]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 52/86 [D loss: 0.6198521852493286, acc.: 71.39%] [G loss: 0.8801710605621338]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 53/86 [D loss: 0.6358004212379456, acc.: 66.41%] [G loss: 0.8459450006484985]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 54/86 [D loss: 0.6439823806285858, acc.: 63.96%] [G loss: 0.8089613914489746]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 55/86 [D loss: 0.6684606075286865, acc.: 57.03%] [G loss: 0.7768304944038391]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 56/86 [D loss: 0.6958974599838257, acc.: 50.88%] [G loss: 0.7452028393745422]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 57/86 [D loss: 0.6909994781017303, acc.: 51.71%] [G loss: 0.7725354433059692]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 42/200, Batch 58/86 [D loss: 0.6740622222423553, acc.: 57.42%] [G loss: 0.8174235820770264]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 59/86 [D loss: 0.6565041840076447, acc.: 62.60%] [G loss: 0.8638195991516113]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 60/86 [D loss: 0.6390221118927002, acc.: 67.87%] [G loss: 0.9270630478858948]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 61/86 [D loss: 0.6203995645046234, acc.: 71.73%] [G loss: 0.9475091695785522]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 62/86 [D loss: 0.6233327984809875, acc.: 70.80%] [G loss: 0.941077709197998]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 63/86 [D loss: 0.6391381025314331, acc.: 67.24%] [G loss: 0.8928446173667908]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 64/86 [D loss: 0.6494581997394562, acc.: 64.11%] [G loss: 0.8441033959388733]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 65/86 [D loss: 0.6773142218589783, acc.: 57.57%] [G loss: 0.7784453630447388]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 66/86 [D loss: 0.6862421929836273, acc.: 55.96%] [G loss: 0.7553658485412598]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 67/86 [D loss: 0.6928353011608124, acc.: 53.91%] [G loss: 0.7488534450531006]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 68/86 [D loss: 0.669971764087677, acc.: 59.81%] [G loss: 0.7768341302871704]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 69/86 [D loss: 0.6553912460803986, acc.: 63.33%] [G loss: 0.8094496130943298]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 70/86 [D loss: 0.6330636441707611, acc.: 69.58%] [G loss: 0.8469096422195435]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 71/86 [D loss: 0.6220310926437378, acc.: 71.48%] [G loss: 0.8748778104782104]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 72/86 [D loss: 0.6222254633903503, acc.: 70.21%] [G loss: 0.8766744136810303]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 73/86 [D loss: 0.6290777027606964, acc.: 67.53%] [G loss: 0.8508921265602112]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 74/86 [D loss: 0.6393242180347443, acc.: 64.06%] [G loss: 0.8089849948883057]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 75/86 [D loss: 0.6725283563137054, acc.: 56.74%] [G loss: 0.7746260762214661]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 76/86 [D loss: 0.6805846989154816, acc.: 54.49%] [G loss: 0.7423695921897888]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 77/86 [D loss: 0.6884809732437134, acc.: 54.15%] [G loss: 0.767697811126709]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 78/86 [D loss: 0.6810988187789917, acc.: 55.47%] [G loss: 0.8029265403747559]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 79/86 [D loss: 0.6505925357341766, acc.: 64.60%] [G loss: 0.8727245926856995]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 80/86 [D loss: 0.6326737999916077, acc.: 69.92%] [G loss: 0.9245688915252686]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 81/86 [D loss: 0.6241706311702728, acc.: 70.46%] [G loss: 0.9514063596725464]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 82/86 [D loss: 0.6206712424755096, acc.: 70.46%] [G loss: 0.9480005502700806]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 83/86 [D loss: 0.6348603665828705, acc.: 68.02%] [G loss: 0.9171162843704224]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 42/200, Batch 84/86 [D loss: 0.6614810824394226, acc.: 62.11%] [G loss: 0.8376339673995972]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 42/200, Batch 85/86 [D loss: 0.6782518327236176, acc.: 57.28%] [G loss: 0.7733142971992493]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 42/200, Batch 86/86 [D loss: 0.6890110075473785, acc.: 54.30%] [G loss: 0.7391146421432495]\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 1/86 [D loss: 0.6971983909606934, acc.: 50.83%] [G loss: 0.7410133481025696]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 2/86 [D loss: 0.6754665970802307, acc.: 58.30%] [G loss: 0.7745578289031982]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 3/86 [D loss: 0.6538970172405243, acc.: 64.06%] [G loss: 0.8150100708007812]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 4/86 [D loss: 0.6290785372257233, acc.: 71.48%] [G loss: 0.8541249632835388]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 5/86 [D loss: 0.6188656389713287, acc.: 70.56%] [G loss: 0.8720483779907227]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 6/86 [D loss: 0.624820351600647, acc.: 68.75%] [G loss: 0.8762044906616211]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 7/86 [D loss: 0.6290536522865295, acc.: 66.70%] [G loss: 0.8444764018058777]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 8/86 [D loss: 0.6530580222606659, acc.: 60.64%] [G loss: 0.8001655340194702]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 9/86 [D loss: 0.6738273799419403, acc.: 56.54%] [G loss: 0.7617783546447754]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 10/86 [D loss: 0.6729730367660522, acc.: 56.15%] [G loss: 0.7629762887954712]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 11/86 [D loss: 0.6828051507472992, acc.: 55.32%] [G loss: 0.7659637331962585]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 12/86 [D loss: 0.6757600903511047, acc.: 57.23%] [G loss: 0.8155596256256104]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 13/86 [D loss: 0.6498774886131287, acc.: 64.75%] [G loss: 0.8841577172279358]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 43/200, Batch 14/86 [D loss: 0.6310819983482361, acc.: 68.95%] [G loss: 0.9227734208106995]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 15/86 [D loss: 0.6218748092651367, acc.: 71.39%] [G loss: 0.9492111206054688]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 16/86 [D loss: 0.621629536151886, acc.: 70.61%] [G loss: 0.9443740248680115]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 17/86 [D loss: 0.6371308863162994, acc.: 69.43%] [G loss: 0.89601731300354]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 18/86 [D loss: 0.6434475183486938, acc.: 66.02%] [G loss: 0.8530755043029785]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 19/86 [D loss: 0.666929692029953, acc.: 60.16%] [G loss: 0.7929502725601196]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 20/86 [D loss: 0.6890886425971985, acc.: 55.27%] [G loss: 0.7570898532867432]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 21/86 [D loss: 0.6829261183738708, acc.: 55.32%] [G loss: 0.7467747926712036]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 22/86 [D loss: 0.6821305751800537, acc.: 56.25%] [G loss: 0.7650136947631836]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 23/86 [D loss: 0.6594667136669159, acc.: 62.01%] [G loss: 0.8045783638954163]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 24/86 [D loss: 0.6472999453544617, acc.: 66.85%] [G loss: 0.8405697345733643]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 25/86 [D loss: 0.6217516362667084, acc.: 72.17%] [G loss: 0.8826242089271545]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 26/86 [D loss: 0.6210049986839294, acc.: 71.24%] [G loss: 0.8913561701774597]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 43/200, Batch 27/86 [D loss: 0.6223118603229523, acc.: 69.04%] [G loss: 0.8646140694618225]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 28/86 [D loss: 0.6366720497608185, acc.: 64.70%] [G loss: 0.8283629417419434]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 29/86 [D loss: 0.663409024477005, acc.: 57.23%] [G loss: 0.778894305229187]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 30/86 [D loss: 0.6811912655830383, acc.: 54.25%] [G loss: 0.7452137470245361]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 31/86 [D loss: 0.6792793273925781, acc.: 56.49%] [G loss: 0.7500736713409424]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 32/86 [D loss: 0.6746408939361572, acc.: 57.57%] [G loss: 0.7759629487991333]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 33/86 [D loss: 0.66024249792099, acc.: 62.45%] [G loss: 0.8251727819442749]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 34/86 [D loss: 0.6479698419570923, acc.: 65.33%] [G loss: 0.8973363041877747]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 35/86 [D loss: 0.6351594924926758, acc.: 67.43%] [G loss: 0.9362574815750122]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 36/86 [D loss: 0.6255120933055878, acc.: 69.63%] [G loss: 0.9532554149627686]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 37/86 [D loss: 0.6180720031261444, acc.: 70.61%] [G loss: 0.9330960512161255]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 38/86 [D loss: 0.6386593878269196, acc.: 67.63%] [G loss: 0.8845723867416382]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 39/86 [D loss: 0.6558153927326202, acc.: 63.23%] [G loss: 0.8248829245567322]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 40/86 [D loss: 0.67648646235466, acc.: 58.11%] [G loss: 0.7686190009117126]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 41/86 [D loss: 0.6948331892490387, acc.: 53.56%] [G loss: 0.7548756003379822]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 42/86 [D loss: 0.6842462718486786, acc.: 54.83%] [G loss: 0.7427588701248169]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 43/86 [D loss: 0.6717172265052795, acc.: 59.18%] [G loss: 0.7742828130722046]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 44/86 [D loss: 0.6509402096271515, acc.: 64.70%] [G loss: 0.8111385107040405]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 45/86 [D loss: 0.6316283941268921, acc.: 68.65%] [G loss: 0.8562097549438477]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 46/86 [D loss: 0.6177196800708771, acc.: 71.63%] [G loss: 0.8749955892562866]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 47/86 [D loss: 0.6147831678390503, acc.: 70.26%] [G loss: 0.8723335862159729]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 48/86 [D loss: 0.6341929733753204, acc.: 65.82%] [G loss: 0.8383525609970093]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 49/86 [D loss: 0.6559283435344696, acc.: 60.89%] [G loss: 0.7839956283569336]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 50/86 [D loss: 0.6713327765464783, acc.: 56.01%] [G loss: 0.7637007832527161]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 51/86 [D loss: 0.6854651570320129, acc.: 53.81%] [G loss: 0.7484021782875061]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 52/86 [D loss: 0.6820606589317322, acc.: 55.08%] [G loss: 0.7734731435775757]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 53/86 [D loss: 0.6691324710845947, acc.: 58.35%] [G loss: 0.8235369920730591]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 54/86 [D loss: 0.6494109332561493, acc.: 64.11%] [G loss: 0.8745619654655457]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 55/86 [D loss: 0.6322485506534576, acc.: 69.48%] [G loss: 0.9305031895637512]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 56/86 [D loss: 0.6311399042606354, acc.: 67.58%] [G loss: 0.9544916152954102]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 57/86 [D loss: 0.6220448613166809, acc.: 69.58%] [G loss: 0.9365339875221252]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 58/86 [D loss: 0.6348077952861786, acc.: 68.31%] [G loss: 0.8869653344154358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 59/86 [D loss: 0.6550218462944031, acc.: 62.94%] [G loss: 0.8321460485458374]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 60/86 [D loss: 0.6732833683490753, acc.: 58.69%] [G loss: 0.77760249376297]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 61/86 [D loss: 0.6860525608062744, acc.: 54.88%] [G loss: 0.7484855055809021]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 62/86 [D loss: 0.6872270405292511, acc.: 55.32%] [G loss: 0.7527536749839783]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 63/86 [D loss: 0.669100433588028, acc.: 59.52%] [G loss: 0.7754791975021362]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 64/86 [D loss: 0.6529100239276886, acc.: 63.04%] [G loss: 0.8188220262527466]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 65/86 [D loss: 0.6345683932304382, acc.: 67.38%] [G loss: 0.8476160168647766]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 66/86 [D loss: 0.6267628073692322, acc.: 70.12%] [G loss: 0.8801664113998413]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 67/86 [D loss: 0.6225822865962982, acc.: 70.61%] [G loss: 0.8714562654495239]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 68/86 [D loss: 0.6235580146312714, acc.: 68.12%] [G loss: 0.8595209121704102]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 69/86 [D loss: 0.6467999219894409, acc.: 62.50%] [G loss: 0.8138898611068726]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 70/86 [D loss: 0.6647680699825287, acc.: 58.35%] [G loss: 0.7636971473693848]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 71/86 [D loss: 0.6781301498413086, acc.: 55.96%] [G loss: 0.7675116658210754]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 72/86 [D loss: 0.6781843602657318, acc.: 55.71%] [G loss: 0.7599655389785767]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 73/86 [D loss: 0.6748050451278687, acc.: 58.11%] [G loss: 0.7957524061203003]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 74/86 [D loss: 0.6593390703201294, acc.: 62.16%] [G loss: 0.8472874164581299]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 75/86 [D loss: 0.6416527926921844, acc.: 66.60%] [G loss: 0.8981156349182129]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 76/86 [D loss: 0.623614490032196, acc.: 70.41%] [G loss: 0.9362895488739014]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 77/86 [D loss: 0.625208705663681, acc.: 68.95%] [G loss: 0.9427787065505981]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 78/86 [D loss: 0.6288090348243713, acc.: 68.80%] [G loss: 0.9147363305091858]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 79/86 [D loss: 0.6535674333572388, acc.: 63.57%] [G loss: 0.8503426909446716]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 80/86 [D loss: 0.6713818609714508, acc.: 58.50%] [G loss: 0.7924757599830627]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 81/86 [D loss: 0.6866593658924103, acc.: 54.59%] [G loss: 0.7540512681007385]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 82/86 [D loss: 0.6875663995742798, acc.: 54.15%] [G loss: 0.7484297752380371]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 43/200, Batch 83/86 [D loss: 0.6817168295383453, acc.: 54.79%] [G loss: 0.7642192840576172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 84/86 [D loss: 0.6571285128593445, acc.: 62.74%] [G loss: 0.7942620515823364]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 43/200, Batch 85/86 [D loss: 0.6353641748428345, acc.: 69.29%] [G loss: 0.843916654586792]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 43/200, Batch 86/86 [D loss: 0.6281291842460632, acc.: 69.97%] [G loss: 0.87315833568573]\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 1/86 [D loss: 0.6140437126159668, acc.: 70.80%] [G loss: 0.8851550817489624]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 2/86 [D loss: 0.6309960782527924, acc.: 66.50%] [G loss: 0.8693540096282959]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 3/86 [D loss: 0.64924356341362, acc.: 62.94%] [G loss: 0.8265432119369507]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 4/86 [D loss: 0.6598691344261169, acc.: 59.52%] [G loss: 0.777928352355957]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 5/86 [D loss: 0.6809563338756561, acc.: 54.35%] [G loss: 0.7471334934234619]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 6/86 [D loss: 0.6879295706748962, acc.: 54.88%] [G loss: 0.7515568137168884]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 7/86 [D loss: 0.6789548099040985, acc.: 55.86%] [G loss: 0.7971280813217163]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 8/86 [D loss: 0.660446435213089, acc.: 60.84%] [G loss: 0.8587245345115662]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 9/86 [D loss: 0.6388733983039856, acc.: 68.12%] [G loss: 0.9310144782066345]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 10/86 [D loss: 0.623831182718277, acc.: 69.73%] [G loss: 0.9679787755012512]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 11/86 [D loss: 0.6187441349029541, acc.: 69.68%] [G loss: 0.9723202586174011]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 12/86 [D loss: 0.6271490156650543, acc.: 68.85%] [G loss: 0.9271055459976196]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 13/86 [D loss: 0.644379734992981, acc.: 65.72%] [G loss: 0.8559495806694031]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 14/86 [D loss: 0.6711976230144501, acc.: 58.59%] [G loss: 0.7946581244468689]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 15/86 [D loss: 0.6862200200557709, acc.: 55.57%] [G loss: 0.7613343596458435]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 16/86 [D loss: 0.7029851973056793, acc.: 49.80%] [G loss: 0.7398284673690796]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 17/86 [D loss: 0.6808350384235382, acc.: 56.25%] [G loss: 0.7572674751281738]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 18/86 [D loss: 0.6586500704288483, acc.: 61.62%] [G loss: 0.7996411323547363]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 19/86 [D loss: 0.6372638046741486, acc.: 67.92%] [G loss: 0.847594141960144]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 20/86 [D loss: 0.6240433752536774, acc.: 70.17%] [G loss: 0.8845067024230957]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 21/86 [D loss: 0.6121153831481934, acc.: 72.90%] [G loss: 0.9052516222000122]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 22/86 [D loss: 0.6214325129985809, acc.: 68.41%] [G loss: 0.8910122513771057]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 23/86 [D loss: 0.6490467488765717, acc.: 62.45%] [G loss: 0.8354339599609375]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 24/86 [D loss: 0.6598174571990967, acc.: 59.96%] [G loss: 0.78826504945755]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 25/86 [D loss: 0.6767617762088776, acc.: 55.91%] [G loss: 0.7631666660308838]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 26/86 [D loss: 0.6852171719074249, acc.: 54.05%] [G loss: 0.7534386515617371]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 27/86 [D loss: 0.683586061000824, acc.: 54.69%] [G loss: 0.783693253993988]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 28/86 [D loss: 0.6680382192134857, acc.: 59.91%] [G loss: 0.8578749299049377]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 29/86 [D loss: 0.6484089195728302, acc.: 65.28%] [G loss: 0.9010071158409119]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 30/86 [D loss: 0.6316029727458954, acc.: 67.58%] [G loss: 0.9565625786781311]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 44/200, Batch 31/86 [D loss: 0.6167325675487518, acc.: 69.68%] [G loss: 0.9755770564079285]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 32/86 [D loss: 0.6298612058162689, acc.: 68.26%] [G loss: 0.9480764865875244]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 33/86 [D loss: 0.6454377770423889, acc.: 65.62%] [G loss: 0.8908820152282715]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 34/86 [D loss: 0.6600188612937927, acc.: 62.50%] [G loss: 0.8266277313232422]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 35/86 [D loss: 0.6773616969585419, acc.: 57.57%] [G loss: 0.774488091468811]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 36/86 [D loss: 0.6954215168952942, acc.: 52.44%] [G loss: 0.7564343214035034]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 37/86 [D loss: 0.6876489520072937, acc.: 53.56%] [G loss: 0.7539561986923218]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 38/86 [D loss: 0.6732460856437683, acc.: 57.71%] [G loss: 0.773797869682312]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 39/86 [D loss: 0.6487341821193695, acc.: 65.14%] [G loss: 0.8212127685546875]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 40/86 [D loss: 0.6328364908695221, acc.: 69.53%] [G loss: 0.8532657623291016]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 41/86 [D loss: 0.6233506500720978, acc.: 70.07%] [G loss: 0.8699362874031067]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 42/86 [D loss: 0.6220783293247223, acc.: 69.09%] [G loss: 0.8826518058776855]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 43/86 [D loss: 0.6264499723911285, acc.: 67.14%] [G loss: 0.8450478315353394]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 44/86 [D loss: 0.6485305428504944, acc.: 61.77%] [G loss: 0.7997412085533142]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 45/86 [D loss: 0.6692367494106293, acc.: 57.23%] [G loss: 0.7671830058097839]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 46/86 [D loss: 0.6959848701953888, acc.: 52.83%] [G loss: 0.7613832950592041]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 47/86 [D loss: 0.6801919937133789, acc.: 54.59%] [G loss: 0.7662671208381653]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 48/86 [D loss: 0.6705868244171143, acc.: 57.81%] [G loss: 0.8165846467018127]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 49/86 [D loss: 0.6492912471294403, acc.: 66.06%] [G loss: 0.8625575304031372]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 50/86 [D loss: 0.6385293006896973, acc.: 67.09%] [G loss: 0.9169746041297913]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 51/86 [D loss: 0.6233848333358765, acc.: 70.70%] [G loss: 0.9577248096466064]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 52/86 [D loss: 0.6219203174114227, acc.: 71.24%] [G loss: 0.9480783939361572]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 53/86 [D loss: 0.6343512535095215, acc.: 66.85%] [G loss: 0.9060966372489929]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 54/86 [D loss: 0.6567424833774567, acc.: 63.04%] [G loss: 0.8406493663787842]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 44/200, Batch 55/86 [D loss: 0.6640268862247467, acc.: 60.30%] [G loss: 0.7879405617713928]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 56/86 [D loss: 0.6866505742073059, acc.: 54.49%] [G loss: 0.7534732818603516]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 57/86 [D loss: 0.6814447939395905, acc.: 56.49%] [G loss: 0.7521408200263977]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 58/86 [D loss: 0.6847308874130249, acc.: 55.18%] [G loss: 0.7590854167938232]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 59/86 [D loss: 0.6712672412395477, acc.: 58.59%] [G loss: 0.7843520045280457]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 44/200, Batch 60/86 [D loss: 0.6437368988990784, acc.: 66.16%] [G loss: 0.8256876468658447]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 61/86 [D loss: 0.6329092383384705, acc.: 68.99%] [G loss: 0.8532553911209106]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 62/86 [D loss: 0.6202150285243988, acc.: 70.61%] [G loss: 0.8671854138374329]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 63/86 [D loss: 0.6246843039989471, acc.: 69.68%] [G loss: 0.8555648922920227]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 64/86 [D loss: 0.6406261920928955, acc.: 64.31%] [G loss: 0.8396442532539368]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 65/86 [D loss: 0.6583857536315918, acc.: 60.89%] [G loss: 0.7874555587768555]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 66/86 [D loss: 0.6789047122001648, acc.: 55.37%] [G loss: 0.7499354481697083]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 67/86 [D loss: 0.6801695823669434, acc.: 55.18%] [G loss: 0.7600639462471008]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 68/86 [D loss: 0.6803503632545471, acc.: 56.20%] [G loss: 0.7957732677459717]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 69/86 [D loss: 0.6586084067821503, acc.: 62.06%] [G loss: 0.8381639719009399]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 70/86 [D loss: 0.647593766450882, acc.: 65.23%] [G loss: 0.8759749531745911]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 71/86 [D loss: 0.6302479803562164, acc.: 68.70%] [G loss: 0.9315220713615417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 72/86 [D loss: 0.6273221373558044, acc.: 69.09%] [G loss: 0.9424214363098145]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 73/86 [D loss: 0.6302819550037384, acc.: 69.29%] [G loss: 0.9203996658325195]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 74/86 [D loss: 0.6411323249340057, acc.: 66.46%] [G loss: 0.8835594058036804]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 75/86 [D loss: 0.6498005092144012, acc.: 64.84%] [G loss: 0.8327153325080872]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 76/86 [D loss: 0.6695646643638611, acc.: 59.13%] [G loss: 0.791060209274292]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 77/86 [D loss: 0.677026778459549, acc.: 57.23%] [G loss: 0.7726175785064697]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 78/86 [D loss: 0.6793031692504883, acc.: 57.37%] [G loss: 0.7637166976928711]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 79/86 [D loss: 0.6695707738399506, acc.: 57.81%] [G loss: 0.7832282781600952]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 80/86 [D loss: 0.6519931554794312, acc.: 64.50%] [G loss: 0.8120419979095459]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 81/86 [D loss: 0.6364964544773102, acc.: 67.77%] [G loss: 0.8525945544242859]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 82/86 [D loss: 0.6278107762336731, acc.: 68.80%] [G loss: 0.8804377317428589]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 83/86 [D loss: 0.6227929890155792, acc.: 69.87%] [G loss: 0.8816109299659729]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 44/200, Batch 84/86 [D loss: 0.6258968114852905, acc.: 66.89%] [G loss: 0.8654691576957703]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 44/200, Batch 85/86 [D loss: 0.6429939568042755, acc.: 63.09%] [G loss: 0.8143925666809082]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 44/200, Batch 86/86 [D loss: 0.6568889915943146, acc.: 59.18%] [G loss: 0.7718846797943115]\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 1/86 [D loss: 0.6788501739501953, acc.: 53.81%] [G loss: 0.7557580471038818]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 2/86 [D loss: 0.6779270768165588, acc.: 56.30%] [G loss: 0.7532928586006165]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 3/86 [D loss: 0.6788783073425293, acc.: 55.08%] [G loss: 0.7979174256324768]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 4/86 [D loss: 0.6567594707012177, acc.: 61.38%] [G loss: 0.8469452857971191]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 5/86 [D loss: 0.6403926610946655, acc.: 67.09%] [G loss: 0.9064977765083313]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 6/86 [D loss: 0.6221203804016113, acc.: 70.41%] [G loss: 0.9418323040008545]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 7/86 [D loss: 0.620773196220398, acc.: 71.24%] [G loss: 0.9487859010696411]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 8/86 [D loss: 0.6375828981399536, acc.: 67.77%] [G loss: 0.9070402979850769]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 9/86 [D loss: 0.6473493576049805, acc.: 64.99%] [G loss: 0.8673967719078064]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 10/86 [D loss: 0.6614152193069458, acc.: 60.84%] [G loss: 0.8114951848983765]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 11/86 [D loss: 0.6828626990318298, acc.: 55.81%] [G loss: 0.7684004902839661]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 12/86 [D loss: 0.6889695525169373, acc.: 53.81%] [G loss: 0.7436401844024658]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 13/86 [D loss: 0.6800375580787659, acc.: 54.59%] [G loss: 0.7597802877426147]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 14/86 [D loss: 0.6647179126739502, acc.: 59.33%] [G loss: 0.7889220118522644]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 15/86 [D loss: 0.6474286317825317, acc.: 66.26%] [G loss: 0.8267838358879089]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 16/86 [D loss: 0.6331823170185089, acc.: 68.85%] [G loss: 0.8683117032051086]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 17/86 [D loss: 0.6181193292140961, acc.: 71.00%] [G loss: 0.8954911231994629]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 18/86 [D loss: 0.6240313947200775, acc.: 68.46%] [G loss: 0.8789108991622925]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 19/86 [D loss: 0.6399776935577393, acc.: 64.21%] [G loss: 0.8366917371749878]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 20/86 [D loss: 0.6521518528461456, acc.: 60.64%] [G loss: 0.8042082786560059]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 21/86 [D loss: 0.6688394248485565, acc.: 57.23%] [G loss: 0.7636604905128479]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 22/86 [D loss: 0.6785676777362823, acc.: 56.10%] [G loss: 0.7487418055534363]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 23/86 [D loss: 0.6843562126159668, acc.: 55.76%] [G loss: 0.7683438062667847]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 24/86 [D loss: 0.6753444075584412, acc.: 58.74%] [G loss: 0.8204025030136108]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 25/86 [D loss: 0.6505565345287323, acc.: 65.14%] [G loss: 0.8759458065032959]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 26/86 [D loss: 0.6359744668006897, acc.: 67.63%] [G loss: 0.9286600351333618]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 27/86 [D loss: 0.6204841434955597, acc.: 71.09%] [G loss: 0.9580097198486328]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 28/86 [D loss: 0.6248698830604553, acc.: 69.68%] [G loss: 0.9383928179740906]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 29/86 [D loss: 0.6376533806324005, acc.: 66.46%] [G loss: 0.9006717205047607]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 30/86 [D loss: 0.6622364521026611, acc.: 60.06%] [G loss: 0.842710018157959]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 31/86 [D loss: 0.678877055644989, acc.: 57.57%] [G loss: 0.7813100814819336]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 32/86 [D loss: 0.6909528076648712, acc.: 54.05%] [G loss: 0.7551461458206177]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 33/86 [D loss: 0.6942103803157806, acc.: 52.25%] [G loss: 0.7391211986541748]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 34/86 [D loss: 0.671937495470047, acc.: 57.96%] [G loss: 0.7810441255569458]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 35/86 [D loss: 0.650750458240509, acc.: 66.16%] [G loss: 0.8172133564949036]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 45/200, Batch 36/86 [D loss: 0.6391970813274384, acc.: 67.63%] [G loss: 0.8537446856498718]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 37/86 [D loss: 0.6224740743637085, acc.: 70.36%] [G loss: 0.8852044343948364]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 38/86 [D loss: 0.6200320422649384, acc.: 69.92%] [G loss: 0.8834462761878967]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 39/86 [D loss: 0.6274315714836121, acc.: 67.04%] [G loss: 0.8555940389633179]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 40/86 [D loss: 0.647860586643219, acc.: 61.91%] [G loss: 0.8284899592399597]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 41/86 [D loss: 0.6661595702171326, acc.: 58.45%] [G loss: 0.7858446836471558]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 42/86 [D loss: 0.676832526922226, acc.: 56.45%] [G loss: 0.7672945857048035]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 43/86 [D loss: 0.6810186505317688, acc.: 56.49%] [G loss: 0.7656705379486084]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 44/86 [D loss: 0.6725876331329346, acc.: 57.23%] [G loss: 0.8035155534744263]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 45/86 [D loss: 0.6583423912525177, acc.: 62.40%] [G loss: 0.8492024540901184]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 46/86 [D loss: 0.6392937302589417, acc.: 66.21%] [G loss: 0.900527834892273]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 47/86 [D loss: 0.6284257173538208, acc.: 69.38%] [G loss: 0.94056236743927]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 48/86 [D loss: 0.6291126310825348, acc.: 67.97%] [G loss: 0.9391771554946899]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 49/86 [D loss: 0.6274541318416595, acc.: 68.26%] [G loss: 0.9149112105369568]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 50/86 [D loss: 0.647864818572998, acc.: 65.04%] [G loss: 0.8766882419586182]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 51/86 [D loss: 0.6557050049304962, acc.: 62.65%] [G loss: 0.8155619502067566]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 52/86 [D loss: 0.6874919533729553, acc.: 55.42%] [G loss: 0.7733379006385803]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 53/86 [D loss: 0.6916412115097046, acc.: 52.78%] [G loss: 0.7487069368362427]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 54/86 [D loss: 0.6793321669101715, acc.: 56.88%] [G loss: 0.7663096785545349]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 55/86 [D loss: 0.6627216339111328, acc.: 61.91%] [G loss: 0.7856647968292236]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 56/86 [D loss: 0.6402431726455688, acc.: 67.29%] [G loss: 0.8357694745063782]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 57/86 [D loss: 0.6320735812187195, acc.: 68.99%] [G loss: 0.869702935218811]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 58/86 [D loss: 0.620238721370697, acc.: 70.61%] [G loss: 0.8823888897895813]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 59/86 [D loss: 0.6193258166313171, acc.: 68.36%] [G loss: 0.8815380930900574]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 60/86 [D loss: 0.6317825615406036, acc.: 64.89%] [G loss: 0.8417301177978516]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 61/86 [D loss: 0.652724951505661, acc.: 62.55%] [G loss: 0.8161436319351196]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 62/86 [D loss: 0.6737561523914337, acc.: 56.98%] [G loss: 0.7535067796707153]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 63/86 [D loss: 0.6831755638122559, acc.: 54.49%] [G loss: 0.7554386854171753]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 64/86 [D loss: 0.6896630227565765, acc.: 54.64%] [G loss: 0.7752542495727539]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 65/86 [D loss: 0.6671918034553528, acc.: 58.84%] [G loss: 0.8125039935112]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 66/86 [D loss: 0.651342898607254, acc.: 62.84%] [G loss: 0.8843518495559692]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 67/86 [D loss: 0.6306601166725159, acc.: 68.02%] [G loss: 0.9263423681259155]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 68/86 [D loss: 0.6306699216365814, acc.: 68.75%] [G loss: 0.9471661448478699]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 69/86 [D loss: 0.6202960014343262, acc.: 71.39%] [G loss: 0.956111490726471]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 70/86 [D loss: 0.6270639598369598, acc.: 69.19%] [G loss: 0.9126410484313965]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 71/86 [D loss: 0.645172119140625, acc.: 65.43%] [G loss: 0.8437538743019104]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 72/86 [D loss: 0.6811105012893677, acc.: 56.30%] [G loss: 0.7949900031089783]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 73/86 [D loss: 0.688858687877655, acc.: 53.86%] [G loss: 0.7635985016822815]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 74/86 [D loss: 0.6925104856491089, acc.: 52.39%] [G loss: 0.7492380142211914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 75/86 [D loss: 0.6802958548069, acc.: 56.01%] [G loss: 0.7753138542175293]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 76/86 [D loss: 0.657864511013031, acc.: 62.50%] [G loss: 0.8022927045822144]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 77/86 [D loss: 0.6390909850597382, acc.: 66.70%] [G loss: 0.8421716094017029]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 45/200, Batch 78/86 [D loss: 0.6301093101501465, acc.: 68.75%] [G loss: 0.8738521337509155]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 79/86 [D loss: 0.619778037071228, acc.: 68.75%] [G loss: 0.8755121827125549]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 80/86 [D loss: 0.6242403090000153, acc.: 68.02%] [G loss: 0.8670980334281921]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 81/86 [D loss: 0.6391542851924896, acc.: 63.53%] [G loss: 0.8320655822753906]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 82/86 [D loss: 0.6557743549346924, acc.: 60.11%] [G loss: 0.7956136465072632]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 83/86 [D loss: 0.6837854981422424, acc.: 53.61%] [G loss: 0.7752905488014221]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 84/86 [D loss: 0.6836086213588715, acc.: 54.54%] [G loss: 0.7687650918960571]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 45/200, Batch 85/86 [D loss: 0.6753027141094208, acc.: 57.37%] [G loss: 0.789698600769043]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 45/200, Batch 86/86 [D loss: 0.6586413681507111, acc.: 61.52%] [G loss: 0.830422043800354]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 1/86 [D loss: 0.6554232239723206, acc.: 63.23%] [G loss: 0.889282763004303]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 2/86 [D loss: 0.6346544325351715, acc.: 67.43%] [G loss: 0.9382962584495544]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 3/86 [D loss: 0.6136683225631714, acc.: 71.58%] [G loss: 0.9541239142417908]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 4/86 [D loss: 0.6228471398353577, acc.: 70.70%] [G loss: 0.9492355585098267]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 5/86 [D loss: 0.6416341066360474, acc.: 65.87%] [G loss: 0.9077605605125427]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 6/86 [D loss: 0.6532615721225739, acc.: 63.09%] [G loss: 0.8429936170578003]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 7/86 [D loss: 0.6764758229255676, acc.: 57.13%] [G loss: 0.7873246669769287]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 8/86 [D loss: 0.6840563416481018, acc.: 55.08%] [G loss: 0.7535816431045532]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 9/86 [D loss: 0.6854925155639648, acc.: 54.44%] [G loss: 0.7540027499198914]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 10/86 [D loss: 0.6802255809307098, acc.: 56.20%] [G loss: 0.7731415629386902]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 11/86 [D loss: 0.6521157920360565, acc.: 63.77%] [G loss: 0.811644971370697]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 12/86 [D loss: 0.6365605890750885, acc.: 68.31%] [G loss: 0.8495904803276062]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 13/86 [D loss: 0.6246998906135559, acc.: 70.17%] [G loss: 0.8754751086235046]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 14/86 [D loss: 0.6261451840400696, acc.: 67.63%] [G loss: 0.8818822503089905]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 15/86 [D loss: 0.6303311288356781, acc.: 67.63%] [G loss: 0.8555377721786499]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 16/86 [D loss: 0.6428931057453156, acc.: 62.65%] [G loss: 0.8219225406646729]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 17/86 [D loss: 0.6610864698886871, acc.: 59.52%] [G loss: 0.7855958342552185]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 18/86 [D loss: 0.6705729961395264, acc.: 57.03%] [G loss: 0.7717581987380981]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 19/86 [D loss: 0.679272323846817, acc.: 55.52%] [G loss: 0.7635505199432373]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 20/86 [D loss: 0.677678644657135, acc.: 56.69%] [G loss: 0.8003160357475281]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 21/86 [D loss: 0.660714715719223, acc.: 61.33%] [G loss: 0.8462914228439331]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 22/86 [D loss: 0.6444441378116608, acc.: 65.28%] [G loss: 0.8973603248596191]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 23/86 [D loss: 0.6363983154296875, acc.: 67.29%] [G loss: 0.9345456957817078]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 24/86 [D loss: 0.622795820236206, acc.: 69.73%] [G loss: 0.9546878933906555]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 25/86 [D loss: 0.6271798610687256, acc.: 68.80%] [G loss: 0.9307116866111755]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 26/86 [D loss: 0.6332022845745087, acc.: 68.21%] [G loss: 0.884147047996521]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 27/86 [D loss: 0.6591853499412537, acc.: 60.99%] [G loss: 0.8191120028495789]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 28/86 [D loss: 0.68585005402565, acc.: 56.10%] [G loss: 0.7701871991157532]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 29/86 [D loss: 0.6879242360591888, acc.: 54.64%] [G loss: 0.7502696514129639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 30/86 [D loss: 0.6800694763660431, acc.: 56.49%] [G loss: 0.760463535785675]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 31/86 [D loss: 0.6624663174152374, acc.: 60.16%] [G loss: 0.787786066532135]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 32/86 [D loss: 0.6524278819561005, acc.: 64.31%] [G loss: 0.8277972936630249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 33/86 [D loss: 0.6268675625324249, acc.: 71.00%] [G loss: 0.8610291481018066]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 34/86 [D loss: 0.623010903596878, acc.: 69.48%] [G loss: 0.8811447620391846]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 35/86 [D loss: 0.627693235874176, acc.: 66.75%] [G loss: 0.8642997145652771]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 36/86 [D loss: 0.6433553993701935, acc.: 63.38%] [G loss: 0.8388590812683105]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 37/86 [D loss: 0.6557571887969971, acc.: 61.28%] [G loss: 0.8036222457885742]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 38/86 [D loss: 0.6794261634349823, acc.: 56.15%] [G loss: 0.7708855271339417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 39/86 [D loss: 0.6799146831035614, acc.: 55.86%] [G loss: 0.7672005295753479]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 40/86 [D loss: 0.6781837642192841, acc.: 55.57%] [G loss: 0.7763729095458984]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 41/86 [D loss: 0.662975937128067, acc.: 60.30%] [G loss: 0.8033105134963989]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 42/86 [D loss: 0.6444826424121857, acc.: 66.21%] [G loss: 0.8715179562568665]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 43/86 [D loss: 0.6382431089878082, acc.: 67.58%] [G loss: 0.9080438613891602]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 44/86 [D loss: 0.6313197612762451, acc.: 67.87%] [G loss: 0.9330580234527588]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 45/86 [D loss: 0.6270459890365601, acc.: 68.46%] [G loss: 0.9304205179214478]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 46/86 [D loss: 0.6310221552848816, acc.: 67.97%] [G loss: 0.9009354114532471]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 47/86 [D loss: 0.6454872488975525, acc.: 64.75%] [G loss: 0.8393585085868835]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 48/86 [D loss: 0.6742814183235168, acc.: 57.91%] [G loss: 0.8063487410545349]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 49/86 [D loss: 0.6811711192131042, acc.: 56.10%] [G loss: 0.7749423980712891]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 50/86 [D loss: 0.6838639676570892, acc.: 54.79%] [G loss: 0.7597709894180298]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 51/86 [D loss: 0.6772187948226929, acc.: 56.93%] [G loss: 0.7787855863571167]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 52/86 [D loss: 0.6560940146446228, acc.: 62.94%] [G loss: 0.7988792657852173]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 53/86 [D loss: 0.6444849073886871, acc.: 65.62%] [G loss: 0.8320655226707458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 54/86 [D loss: 0.6284095644950867, acc.: 69.43%] [G loss: 0.8569508790969849]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 55/86 [D loss: 0.6280101537704468, acc.: 66.80%] [G loss: 0.8693851232528687]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 56/86 [D loss: 0.6277720034122467, acc.: 66.31%] [G loss: 0.8581992387771606]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 57/86 [D loss: 0.64286008477211, acc.: 64.55%] [G loss: 0.8227274417877197]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 58/86 [D loss: 0.6581293940544128, acc.: 60.55%] [G loss: 0.8044830560684204]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 59/86 [D loss: 0.6738211512565613, acc.: 58.11%] [G loss: 0.775597095489502]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 60/86 [D loss: 0.6848142147064209, acc.: 55.66%] [G loss: 0.7737017273902893]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 61/86 [D loss: 0.675557404756546, acc.: 57.62%] [G loss: 0.7959303855895996]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 62/86 [D loss: 0.6676627397537231, acc.: 59.96%] [G loss: 0.8429524898529053]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 63/86 [D loss: 0.6455722749233246, acc.: 65.72%] [G loss: 0.8879456520080566]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 64/86 [D loss: 0.6374209225177765, acc.: 66.50%] [G loss: 0.9241605401039124]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 65/86 [D loss: 0.6265802085399628, acc.: 69.53%] [G loss: 0.9389073252677917]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 66/86 [D loss: 0.6284219920635223, acc.: 68.16%] [G loss: 0.923464298248291]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 67/86 [D loss: 0.6443039774894714, acc.: 65.09%] [G loss: 0.8872299790382385]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 68/86 [D loss: 0.663040816783905, acc.: 60.30%] [G loss: 0.8113843202590942]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 69/86 [D loss: 0.6767206192016602, acc.: 56.79%] [G loss: 0.7871564030647278]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 70/86 [D loss: 0.6801250278949738, acc.: 56.05%] [G loss: 0.7578178644180298]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 71/86 [D loss: 0.6740323305130005, acc.: 58.20%] [G loss: 0.7725661396980286]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 72/86 [D loss: 0.6635453402996063, acc.: 60.89%] [G loss: 0.798391580581665]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 73/86 [D loss: 0.6403069496154785, acc.: 67.72%] [G loss: 0.8380267024040222]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 74/86 [D loss: 0.6275890171527863, acc.: 68.80%] [G loss: 0.8487967848777771]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 46/200, Batch 75/86 [D loss: 0.6181477904319763, acc.: 70.90%] [G loss: 0.873297393321991]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 76/86 [D loss: 0.6288710832595825, acc.: 66.94%] [G loss: 0.8506590127944946]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 77/86 [D loss: 0.6365036070346832, acc.: 66.31%] [G loss: 0.8331587314605713]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 78/86 [D loss: 0.6617380082607269, acc.: 59.13%] [G loss: 0.803950309753418]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 79/86 [D loss: 0.6693838834762573, acc.: 57.57%] [G loss: 0.7685543298721313]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 80/86 [D loss: 0.672129899263382, acc.: 57.57%] [G loss: 0.7628190517425537]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 81/86 [D loss: 0.6787129342556, acc.: 56.69%] [G loss: 0.7887538075447083]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 82/86 [D loss: 0.6654953956604004, acc.: 60.25%] [G loss: 0.8210591077804565]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 46/200, Batch 83/86 [D loss: 0.6488277018070221, acc.: 63.96%] [G loss: 0.8691044449806213]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 84/86 [D loss: 0.6377040445804596, acc.: 67.24%] [G loss: 0.9096930623054504]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 46/200, Batch 85/86 [D loss: 0.6298300623893738, acc.: 68.31%] [G loss: 0.9366511702537537]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 46/200, Batch 86/86 [D loss: 0.6283870041370392, acc.: 68.21%] [G loss: 0.9208007454872131]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 1/86 [D loss: 0.6291833221912384, acc.: 68.41%] [G loss: 0.8878046870231628]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 2/86 [D loss: 0.6595253050327301, acc.: 61.77%] [G loss: 0.82408207654953]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 3/86 [D loss: 0.6781555414199829, acc.: 57.32%] [G loss: 0.7845087051391602]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 4/86 [D loss: 0.6822578608989716, acc.: 54.05%] [G loss: 0.7639836072921753]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 5/86 [D loss: 0.6882604658603668, acc.: 54.69%] [G loss: 0.7645176649093628]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 6/86 [D loss: 0.6658778190612793, acc.: 60.16%] [G loss: 0.789641261100769]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 7/86 [D loss: 0.6548062264919281, acc.: 63.09%] [G loss: 0.8301844000816345]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 8/86 [D loss: 0.6280125677585602, acc.: 70.17%] [G loss: 0.8479728102684021]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 9/86 [D loss: 0.6298509538173676, acc.: 69.58%] [G loss: 0.8707689046859741]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 10/86 [D loss: 0.6223319172859192, acc.: 69.04%] [G loss: 0.8764538764953613]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 11/86 [D loss: 0.6364529728889465, acc.: 66.41%] [G loss: 0.8491830825805664]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 12/86 [D loss: 0.6451293528079987, acc.: 62.74%] [G loss: 0.8138242363929749]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 13/86 [D loss: 0.6661886274814606, acc.: 57.91%] [G loss: 0.7745609283447266]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 14/86 [D loss: 0.6807615458965302, acc.: 55.71%] [G loss: 0.7718150019645691]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 15/86 [D loss: 0.6757059693336487, acc.: 57.08%] [G loss: 0.7740962505340576]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 16/86 [D loss: 0.6786375939846039, acc.: 57.81%] [G loss: 0.8051735758781433]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 17/86 [D loss: 0.6555811762809753, acc.: 63.43%] [G loss: 0.8538246750831604]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 18/86 [D loss: 0.6441629528999329, acc.: 66.26%] [G loss: 0.8986246585845947]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 19/86 [D loss: 0.6314243078231812, acc.: 68.16%] [G loss: 0.9254123568534851]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 20/86 [D loss: 0.6295048296451569, acc.: 68.31%] [G loss: 0.92131507396698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 21/86 [D loss: 0.636118471622467, acc.: 66.80%] [G loss: 0.89751797914505]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 22/86 [D loss: 0.6516812145709991, acc.: 63.53%] [G loss: 0.8484508395195007]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 23/86 [D loss: 0.6657166182994843, acc.: 60.30%] [G loss: 0.780942976474762]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 24/86 [D loss: 0.683691143989563, acc.: 56.25%] [G loss: 0.7573068141937256]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 25/86 [D loss: 0.6862063705921173, acc.: 55.47%] [G loss: 0.7617994546890259]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 26/86 [D loss: 0.6802051961421967, acc.: 57.42%] [G loss: 0.7726708054542542]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 27/86 [D loss: 0.6610563099384308, acc.: 60.74%] [G loss: 0.8087425231933594]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 28/86 [D loss: 0.6402095258235931, acc.: 67.33%] [G loss: 0.8542434573173523]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 29/86 [D loss: 0.6213986575603485, acc.: 70.51%] [G loss: 0.8746938705444336]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 30/86 [D loss: 0.6293639242649078, acc.: 68.99%] [G loss: 0.8680371046066284]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 31/86 [D loss: 0.645762026309967, acc.: 63.57%] [G loss: 0.8512622714042664]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 32/86 [D loss: 0.6504994034767151, acc.: 62.16%] [G loss: 0.8235673904418945]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 33/86 [D loss: 0.6605252623558044, acc.: 59.38%] [G loss: 0.7887206673622131]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 34/86 [D loss: 0.6697429120540619, acc.: 58.06%] [G loss: 0.7817363739013672]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 35/86 [D loss: 0.6740350425243378, acc.: 58.50%] [G loss: 0.7813335657119751]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 36/86 [D loss: 0.6798758208751678, acc.: 57.08%] [G loss: 0.8071633577346802]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 37/86 [D loss: 0.6542573571205139, acc.: 62.70%] [G loss: 0.8697115182876587]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 38/86 [D loss: 0.6486849784851074, acc.: 64.45%] [G loss: 0.8990009427070618]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 39/86 [D loss: 0.6326274871826172, acc.: 67.24%] [G loss: 0.9304256439208984]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 40/86 [D loss: 0.6297228038311005, acc.: 68.12%] [G loss: 0.9255694150924683]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 41/86 [D loss: 0.6397704780101776, acc.: 65.82%] [G loss: 0.8904664516448975]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 42/86 [D loss: 0.6517423391342163, acc.: 63.53%] [G loss: 0.8494495749473572]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 43/86 [D loss: 0.6638836562633514, acc.: 60.01%] [G loss: 0.8007861375808716]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 44/86 [D loss: 0.6741145849227905, acc.: 57.96%] [G loss: 0.7773643732070923]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 45/86 [D loss: 0.6790716350078583, acc.: 57.57%] [G loss: 0.764374315738678]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 47/200, Batch 46/86 [D loss: 0.6688000857830048, acc.: 58.11%] [G loss: 0.7807873487472534]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 47/86 [D loss: 0.6490189135074615, acc.: 64.45%] [G loss: 0.8103431463241577]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 48/86 [D loss: 0.640578418970108, acc.: 67.97%] [G loss: 0.832493245601654]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 49/86 [D loss: 0.6370764374732971, acc.: 66.06%] [G loss: 0.8562871813774109]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 50/86 [D loss: 0.6260430812835693, acc.: 68.95%] [G loss: 0.852767288684845]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 51/86 [D loss: 0.6415293514728546, acc.: 64.36%] [G loss: 0.8516640067100525]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 52/86 [D loss: 0.6413167119026184, acc.: 63.23%] [G loss: 0.8148178458213806]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 53/86 [D loss: 0.6586925983428955, acc.: 59.33%] [G loss: 0.7890152931213379]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 54/86 [D loss: 0.674718976020813, acc.: 57.76%] [G loss: 0.7764816284179688]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 55/86 [D loss: 0.6730609238147736, acc.: 56.40%] [G loss: 0.7756180763244629]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 56/86 [D loss: 0.676074743270874, acc.: 55.66%] [G loss: 0.8078548908233643]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 57/86 [D loss: 0.6577764451503754, acc.: 62.21%] [G loss: 0.8426554799079895]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 58/86 [D loss: 0.6452464461326599, acc.: 65.23%] [G loss: 0.8898716568946838]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 59/86 [D loss: 0.6319285929203033, acc.: 67.38%] [G loss: 0.9203897714614868]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 60/86 [D loss: 0.6321602761745453, acc.: 68.85%] [G loss: 0.9182640314102173]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 61/86 [D loss: 0.6351684927940369, acc.: 67.48%] [G loss: 0.8882558345794678]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 62/86 [D loss: 0.654077559709549, acc.: 62.74%] [G loss: 0.8493407964706421]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 63/86 [D loss: 0.6683240532875061, acc.: 58.89%] [G loss: 0.8005565404891968]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 64/86 [D loss: 0.6741312742233276, acc.: 56.49%] [G loss: 0.7761802673339844]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 65/86 [D loss: 0.6831755042076111, acc.: 55.86%] [G loss: 0.7671985030174255]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 66/86 [D loss: 0.6721264719963074, acc.: 57.32%] [G loss: 0.7778772115707397]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 67/86 [D loss: 0.6533420979976654, acc.: 63.43%] [G loss: 0.8127325773239136]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 68/86 [D loss: 0.6419937312602997, acc.: 65.92%] [G loss: 0.839213490486145]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 69/86 [D loss: 0.6354636251926422, acc.: 67.09%] [G loss: 0.8584372997283936]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 70/86 [D loss: 0.6358645558357239, acc.: 65.62%] [G loss: 0.8632025718688965]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 71/86 [D loss: 0.6399511098861694, acc.: 66.55%] [G loss: 0.8447617888450623]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 72/86 [D loss: 0.649823784828186, acc.: 63.23%] [G loss: 0.8097474575042725]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 73/86 [D loss: 0.6583712697029114, acc.: 60.99%] [G loss: 0.7856154441833496]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 74/86 [D loss: 0.6730200946331024, acc.: 57.71%] [G loss: 0.7796689867973328]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 75/86 [D loss: 0.6782768666744232, acc.: 57.23%] [G loss: 0.7842969298362732]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 76/86 [D loss: 0.6708590686321259, acc.: 58.30%] [G loss: 0.8039618730545044]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 77/86 [D loss: 0.6546778082847595, acc.: 63.04%] [G loss: 0.8460419178009033]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 47/200, Batch 78/86 [D loss: 0.6455460488796234, acc.: 65.43%] [G loss: 0.8816380500793457]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 79/86 [D loss: 0.6310986578464508, acc.: 68.36%] [G loss: 0.9168505072593689]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 80/86 [D loss: 0.6404361128807068, acc.: 66.16%] [G loss: 0.9229008555412292]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 81/86 [D loss: 0.6395127475261688, acc.: 66.75%] [G loss: 0.8860374689102173]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 47/200, Batch 82/86 [D loss: 0.6499941051006317, acc.: 63.13%] [G loss: 0.8401618599891663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 83/86 [D loss: 0.6659341156482697, acc.: 58.15%] [G loss: 0.7973783016204834]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 47/200, Batch 84/86 [D loss: 0.6835042238235474, acc.: 55.52%] [G loss: 0.7811578512191772]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 85/86 [D loss: 0.6794447004795074, acc.: 56.01%] [G loss: 0.7780097126960754]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 47/200, Batch 86/86 [D loss: 0.6631618738174438, acc.: 59.86%] [G loss: 0.7927895784378052]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 1/86 [D loss: 0.6486904919147491, acc.: 64.01%] [G loss: 0.8144318461418152]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 2/86 [D loss: 0.6418515741825104, acc.: 66.55%] [G loss: 0.8452373743057251]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 3/86 [D loss: 0.6315419375896454, acc.: 68.55%] [G loss: 0.8626108765602112]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 4/86 [D loss: 0.637843906879425, acc.: 66.36%] [G loss: 0.8514427542686462]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 48/200, Batch 5/86 [D loss: 0.6386586129665375, acc.: 64.16%] [G loss: 0.8231291770935059]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 6/86 [D loss: 0.6565897464752197, acc.: 60.79%] [G loss: 0.7903266549110413]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 7/86 [D loss: 0.6707471609115601, acc.: 58.40%] [G loss: 0.7812737226486206]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 8/86 [D loss: 0.6754471659660339, acc.: 57.81%] [G loss: 0.7787677645683289]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 9/86 [D loss: 0.6720943450927734, acc.: 57.32%] [G loss: 0.7915766835212708]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 10/86 [D loss: 0.663489431142807, acc.: 59.96%] [G loss: 0.8166017532348633]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 11/86 [D loss: 0.6520358920097351, acc.: 63.72%] [G loss: 0.8547887802124023]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 12/86 [D loss: 0.6486942768096924, acc.: 65.43%] [G loss: 0.8982354402542114]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 13/86 [D loss: 0.6354774534702301, acc.: 67.14%] [G loss: 0.898110568523407]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 14/86 [D loss: 0.6386871039867401, acc.: 66.41%] [G loss: 0.8985798358917236]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 15/86 [D loss: 0.6494049727916718, acc.: 64.50%] [G loss: 0.8709421157836914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 16/86 [D loss: 0.665111780166626, acc.: 60.30%] [G loss: 0.8279589414596558]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 17/86 [D loss: 0.6687478125095367, acc.: 58.45%] [G loss: 0.7912265658378601]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 18/86 [D loss: 0.6755472421646118, acc.: 57.71%] [G loss: 0.7723076939582825]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 48/200, Batch 19/86 [D loss: 0.6761707067489624, acc.: 57.57%] [G loss: 0.7765154838562012]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 20/86 [D loss: 0.665651947259903, acc.: 60.16%] [G loss: 0.784500241279602]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 21/86 [D loss: 0.650193989276886, acc.: 65.82%] [G loss: 0.8181495070457458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 22/86 [D loss: 0.6417976021766663, acc.: 66.06%] [G loss: 0.8461893200874329]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 48/200, Batch 23/86 [D loss: 0.6321012377738953, acc.: 67.82%] [G loss: 0.8490890264511108]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 24/86 [D loss: 0.6398902535438538, acc.: 67.19%] [G loss: 0.8366740345954895]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 25/86 [D loss: 0.6510948538780212, acc.: 62.11%] [G loss: 0.8179559707641602]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 26/86 [D loss: 0.6564240455627441, acc.: 62.01%] [G loss: 0.8017427325248718]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 27/86 [D loss: 0.6659656167030334, acc.: 58.40%] [G loss: 0.7899230718612671]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 28/86 [D loss: 0.6744070649147034, acc.: 57.28%] [G loss: 0.7806239724159241]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 29/86 [D loss: 0.6696797013282776, acc.: 57.71%] [G loss: 0.797157347202301]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 30/86 [D loss: 0.6617555916309357, acc.: 60.89%] [G loss: 0.838483989238739]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 31/86 [D loss: 0.6489066779613495, acc.: 64.21%] [G loss: 0.8624735474586487]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 32/86 [D loss: 0.6366285979747772, acc.: 67.58%] [G loss: 0.9002763032913208]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 33/86 [D loss: 0.6313113570213318, acc.: 68.36%] [G loss: 0.920419454574585]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 34/86 [D loss: 0.642680287361145, acc.: 65.53%] [G loss: 0.8900302052497864]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 35/86 [D loss: 0.6463388800621033, acc.: 63.13%] [G loss: 0.8603072166442871]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 36/86 [D loss: 0.6532571017742157, acc.: 62.99%] [G loss: 0.8143014907836914]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 37/86 [D loss: 0.6759174764156342, acc.: 59.13%] [G loss: 0.7756834626197815]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 38/86 [D loss: 0.6800048053264618, acc.: 56.20%] [G loss: 0.7739791870117188]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 39/86 [D loss: 0.6727302372455597, acc.: 59.62%] [G loss: 0.7785807847976685]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 40/86 [D loss: 0.6589003801345825, acc.: 62.21%] [G loss: 0.8082959651947021]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 41/86 [D loss: 0.6444575488567352, acc.: 65.48%] [G loss: 0.8206498026847839]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 42/86 [D loss: 0.6328655183315277, acc.: 67.92%] [G loss: 0.8523380756378174]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 43/86 [D loss: 0.6283553242683411, acc.: 67.48%] [G loss: 0.8627082109451294]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 44/86 [D loss: 0.6340451538562775, acc.: 66.75%] [G loss: 0.8538747429847717]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 45/86 [D loss: 0.6462172865867615, acc.: 64.06%] [G loss: 0.8025661706924438]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 46/86 [D loss: 0.65785813331604, acc.: 62.16%] [G loss: 0.7916780710220337]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 47/86 [D loss: 0.671360045671463, acc.: 58.06%] [G loss: 0.7757761478424072]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 48/86 [D loss: 0.6802715063095093, acc.: 56.93%] [G loss: 0.7837131023406982]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 49/86 [D loss: 0.668992280960083, acc.: 59.57%] [G loss: 0.8052140474319458]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 50/86 [D loss: 0.6510220170021057, acc.: 64.70%] [G loss: 0.8499557971954346]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 51/86 [D loss: 0.6478402316570282, acc.: 66.36%] [G loss: 0.8867665529251099]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 52/86 [D loss: 0.6431919634342194, acc.: 66.11%] [G loss: 0.9120384454727173]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 53/86 [D loss: 0.6263105869293213, acc.: 68.95%] [G loss: 0.915077269077301]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 54/86 [D loss: 0.6387813687324524, acc.: 65.58%] [G loss: 0.8873915672302246]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 55/86 [D loss: 0.6545845568180084, acc.: 63.04%] [G loss: 0.8364654779434204]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 56/86 [D loss: 0.6715520620346069, acc.: 56.93%] [G loss: 0.7981568574905396]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 57/86 [D loss: 0.6817678809165955, acc.: 56.05%] [G loss: 0.77045738697052]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 58/86 [D loss: 0.6727484464645386, acc.: 58.50%] [G loss: 0.773968517780304]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 59/86 [D loss: 0.6646411418914795, acc.: 59.33%] [G loss: 0.7885544300079346]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 60/86 [D loss: 0.6482913196086884, acc.: 64.75%] [G loss: 0.8342559933662415]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 61/86 [D loss: 0.6296462714672089, acc.: 68.70%] [G loss: 0.8671354055404663]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 62/86 [D loss: 0.6259861290454865, acc.: 70.36%] [G loss: 0.8650721311569214]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 63/86 [D loss: 0.6362559795379639, acc.: 65.82%] [G loss: 0.8609914183616638]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 64/86 [D loss: 0.6475272178649902, acc.: 62.26%] [G loss: 0.8378400802612305]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 65/86 [D loss: 0.6625270545482635, acc.: 59.77%] [G loss: 0.799011766910553]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 66/86 [D loss: 0.6681655049324036, acc.: 58.54%] [G loss: 0.7925335168838501]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 67/86 [D loss: 0.6717174351215363, acc.: 56.59%] [G loss: 0.7754801511764526]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 68/86 [D loss: 0.6726910173892975, acc.: 58.50%] [G loss: 0.7945107817649841]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 69/86 [D loss: 0.6686873137950897, acc.: 58.94%] [G loss: 0.8307018876075745]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 70/86 [D loss: 0.6508094370365143, acc.: 62.35%] [G loss: 0.8756884336471558]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 71/86 [D loss: 0.6335389614105225, acc.: 67.53%] [G loss: 0.9099070429801941]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 72/86 [D loss: 0.6315844655036926, acc.: 68.21%] [G loss: 0.9278002381324768]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 73/86 [D loss: 0.6354916393756866, acc.: 67.43%] [G loss: 0.908265233039856]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 74/86 [D loss: 0.6494815051555634, acc.: 63.43%] [G loss: 0.8691263198852539]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 75/86 [D loss: 0.6688650250434875, acc.: 59.96%] [G loss: 0.8133741021156311]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 76/86 [D loss: 0.6864161789417267, acc.: 55.03%] [G loss: 0.7713193297386169]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 77/86 [D loss: 0.6760532259941101, acc.: 57.32%] [G loss: 0.7679362297058105]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 78/86 [D loss: 0.6731216311454773, acc.: 57.08%] [G loss: 0.7877075672149658]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 79/86 [D loss: 0.6568692922592163, acc.: 62.35%] [G loss: 0.819162905216217]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 80/86 [D loss: 0.6418967843055725, acc.: 65.87%] [G loss: 0.8388450741767883]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 81/86 [D loss: 0.633482813835144, acc.: 67.72%] [G loss: 0.8605161309242249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 82/86 [D loss: 0.6379315853118896, acc.: 66.06%] [G loss: 0.8537665605545044]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 48/200, Batch 83/86 [D loss: 0.642026960849762, acc.: 64.06%] [G loss: 0.8289451599121094]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 84/86 [D loss: 0.6601689457893372, acc.: 60.55%] [G loss: 0.8108516931533813]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 48/200, Batch 85/86 [D loss: 0.6672134399414062, acc.: 59.52%] [G loss: 0.7814193964004517]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 48/200, Batch 86/86 [D loss: 0.6738702058792114, acc.: 57.76%] [G loss: 0.7690560221672058]\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 1/86 [D loss: 0.6738380193710327, acc.: 56.98%] [G loss: 0.7925312519073486]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 49/200, Batch 2/86 [D loss: 0.6620096862316132, acc.: 60.55%] [G loss: 0.8305917978286743]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 3/86 [D loss: 0.6550952792167664, acc.: 61.96%] [G loss: 0.8781899809837341]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 4/86 [D loss: 0.6416913568973541, acc.: 65.82%] [G loss: 0.9103196263313293]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 5/86 [D loss: 0.6379479169845581, acc.: 66.65%] [G loss: 0.9183619022369385]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 6/86 [D loss: 0.6394159495830536, acc.: 65.82%] [G loss: 0.8983238339424133]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 7/86 [D loss: 0.6572394669055939, acc.: 62.35%] [G loss: 0.8671573996543884]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 8/86 [D loss: 0.6682614386081696, acc.: 59.18%] [G loss: 0.8056423664093018]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 9/86 [D loss: 0.682290256023407, acc.: 55.42%] [G loss: 0.7732495069503784]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 10/86 [D loss: 0.6806018650531769, acc.: 55.42%] [G loss: 0.7619876265525818]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 49/200, Batch 11/86 [D loss: 0.6688742935657501, acc.: 59.91%] [G loss: 0.7886489629745483]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 12/86 [D loss: 0.6529565453529358, acc.: 64.16%] [G loss: 0.8195950984954834]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 13/86 [D loss: 0.6423161327838898, acc.: 67.14%] [G loss: 0.8486877083778381]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 14/86 [D loss: 0.6336218416690826, acc.: 67.87%] [G loss: 0.8523253798484802]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 15/86 [D loss: 0.6354951560497284, acc.: 65.58%] [G loss: 0.8610734939575195]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 16/86 [D loss: 0.6407487988471985, acc.: 65.82%] [G loss: 0.8389902114868164]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 17/86 [D loss: 0.6529229879379272, acc.: 62.50%] [G loss: 0.8189783692359924]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 18/86 [D loss: 0.6617621183395386, acc.: 58.79%] [G loss: 0.7842153310775757]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 19/86 [D loss: 0.6673359274864197, acc.: 59.03%] [G loss: 0.7732051014900208]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 49/200, Batch 20/86 [D loss: 0.6697530746459961, acc.: 59.18%] [G loss: 0.7853928804397583]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 21/86 [D loss: 0.6666958630084991, acc.: 59.42%] [G loss: 0.8179804682731628]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 22/86 [D loss: 0.6544891893863678, acc.: 62.45%] [G loss: 0.8704022169113159]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 23/86 [D loss: 0.6376058161258698, acc.: 65.67%] [G loss: 0.9158291816711426]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 24/86 [D loss: 0.6289741396903992, acc.: 68.55%] [G loss: 0.9336085319519043]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 49/200, Batch 25/86 [D loss: 0.6338203847408295, acc.: 67.29%] [G loss: 0.8983615636825562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 26/86 [D loss: 0.6521025598049164, acc.: 63.62%] [G loss: 0.8634318113327026]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 27/86 [D loss: 0.6664073169231415, acc.: 58.79%] [G loss: 0.8117005228996277]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 28/86 [D loss: 0.6751753091812134, acc.: 56.54%] [G loss: 0.7699710130691528]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 29/86 [D loss: 0.6792783141136169, acc.: 55.22%] [G loss: 0.7593641877174377]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 49/200, Batch 30/86 [D loss: 0.6683106422424316, acc.: 59.38%] [G loss: 0.779157817363739]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 31/86 [D loss: 0.6521244049072266, acc.: 63.43%] [G loss: 0.8124433755874634]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 32/86 [D loss: 0.6386186480522156, acc.: 66.26%] [G loss: 0.8367994427680969]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 33/86 [D loss: 0.6366544961929321, acc.: 68.21%] [G loss: 0.8536611199378967]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 34/86 [D loss: 0.6379859447479248, acc.: 64.99%] [G loss: 0.8549274802207947]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 35/86 [D loss: 0.6377564072608948, acc.: 65.62%] [G loss: 0.8472769260406494]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 36/86 [D loss: 0.6452849507331848, acc.: 64.40%] [G loss: 0.8267773389816284]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 37/86 [D loss: 0.6603065133094788, acc.: 60.69%] [G loss: 0.789206326007843]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 38/86 [D loss: 0.6710498332977295, acc.: 56.35%] [G loss: 0.7745060920715332]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 39/86 [D loss: 0.6762576401233673, acc.: 57.47%] [G loss: 0.7758174538612366]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 40/86 [D loss: 0.6684934794902802, acc.: 60.50%] [G loss: 0.8124729990959167]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 41/86 [D loss: 0.6551154255867004, acc.: 62.89%] [G loss: 0.8666672706604004]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 42/86 [D loss: 0.6414980888366699, acc.: 65.58%] [G loss: 0.9065654873847961]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 43/86 [D loss: 0.6328118741512299, acc.: 67.72%] [G loss: 0.9314455986022949]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 44/86 [D loss: 0.6413127481937408, acc.: 65.97%] [G loss: 0.9060156345367432]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 45/86 [D loss: 0.6483857333660126, acc.: 63.48%] [G loss: 0.8643398284912109]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 46/86 [D loss: 0.6566477119922638, acc.: 62.26%] [G loss: 0.8169745802879333]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 47/86 [D loss: 0.6780092716217041, acc.: 56.79%] [G loss: 0.7746970653533936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 48/86 [D loss: 0.6811484098434448, acc.: 56.05%] [G loss: 0.7713125944137573]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 49/86 [D loss: 0.6743279099464417, acc.: 57.23%] [G loss: 0.7780811786651611]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 50/86 [D loss: 0.653698056936264, acc.: 61.96%] [G loss: 0.8019117712974548]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 51/86 [D loss: 0.6379059851169586, acc.: 67.38%] [G loss: 0.8497673273086548]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 52/86 [D loss: 0.6290337145328522, acc.: 68.51%] [G loss: 0.873518705368042]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 53/86 [D loss: 0.6258149743080139, acc.: 67.82%] [G loss: 0.875074565410614]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 54/86 [D loss: 0.6503272354602814, acc.: 62.79%] [G loss: 0.8316566944122314]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 55/86 [D loss: 0.6634366810321808, acc.: 60.11%] [G loss: 0.8034180402755737]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 56/86 [D loss: 0.6699586510658264, acc.: 59.13%] [G loss: 0.7751964330673218]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 57/86 [D loss: 0.6834096610546112, acc.: 55.71%] [G loss: 0.7745338082313538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 58/86 [D loss: 0.6788877844810486, acc.: 56.35%] [G loss: 0.788557767868042]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 59/86 [D loss: 0.6685116291046143, acc.: 60.11%] [G loss: 0.8416411280632019]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 60/86 [D loss: 0.6508322358131409, acc.: 63.04%] [G loss: 0.8681468963623047]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 61/86 [D loss: 0.6379477977752686, acc.: 66.41%] [G loss: 0.9156911373138428]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 62/86 [D loss: 0.6369074583053589, acc.: 65.87%] [G loss: 0.9219508767127991]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 63/86 [D loss: 0.6386629641056061, acc.: 65.87%] [G loss: 0.8855819702148438]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 64/86 [D loss: 0.6562028527259827, acc.: 61.91%] [G loss: 0.8504219055175781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 65/86 [D loss: 0.674396276473999, acc.: 57.91%] [G loss: 0.8021711111068726]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 66/86 [D loss: 0.6797162890434265, acc.: 55.37%] [G loss: 0.7617085576057434]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 67/86 [D loss: 0.6794628500938416, acc.: 56.25%] [G loss: 0.7716394662857056]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 68/86 [D loss: 0.6583669185638428, acc.: 60.99%] [G loss: 0.800666093826294]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 69/86 [D loss: 0.6505506038665771, acc.: 63.72%] [G loss: 0.8341041207313538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 70/86 [D loss: 0.6405853033065796, acc.: 66.21%] [G loss: 0.8538833856582642]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 71/86 [D loss: 0.6359511911869049, acc.: 66.06%] [G loss: 0.8635244369506836]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 72/86 [D loss: 0.6344413459300995, acc.: 66.31%] [G loss: 0.8604311943054199]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 73/86 [D loss: 0.656651496887207, acc.: 61.04%] [G loss: 0.8198537230491638]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 74/86 [D loss: 0.6548893451690674, acc.: 60.16%] [G loss: 0.8046253323554993]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 75/86 [D loss: 0.6689874529838562, acc.: 57.67%] [G loss: 0.7909660935401917]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 76/86 [D loss: 0.6809715330600739, acc.: 54.54%] [G loss: 0.7797571420669556]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 77/86 [D loss: 0.6740396022796631, acc.: 57.57%] [G loss: 0.8110594153404236]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 78/86 [D loss: 0.6645748913288116, acc.: 61.13%] [G loss: 0.8542068600654602]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 79/86 [D loss: 0.6500275433063507, acc.: 63.13%] [G loss: 0.9000653028488159]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 80/86 [D loss: 0.6411377191543579, acc.: 65.92%] [G loss: 0.9211374521255493]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 81/86 [D loss: 0.6320902407169342, acc.: 66.50%] [G loss: 0.9120150804519653]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 82/86 [D loss: 0.6423000693321228, acc.: 65.43%] [G loss: 0.8830410838127136]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 83/86 [D loss: 0.6575096249580383, acc.: 61.47%] [G loss: 0.834109902381897]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 49/200, Batch 84/86 [D loss: 0.6790914833545685, acc.: 56.01%] [G loss: 0.789432942867279]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 49/200, Batch 85/86 [D loss: 0.684303492307663, acc.: 56.45%] [G loss: 0.7822202444076538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 49/200, Batch 86/86 [D loss: 0.6761189103126526, acc.: 57.67%] [G loss: 0.7731223106384277]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 1/86 [D loss: 0.6648423373699188, acc.: 59.28%] [G loss: 0.8020492196083069]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 2/86 [D loss: 0.651055097579956, acc.: 64.16%] [G loss: 0.8168052434921265]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 3/86 [D loss: 0.6376714408397675, acc.: 67.53%] [G loss: 0.8520249128341675]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 4/86 [D loss: 0.6364388465881348, acc.: 67.48%] [G loss: 0.8567531704902649]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 5/86 [D loss: 0.634323924779892, acc.: 66.06%] [G loss: 0.847632884979248]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 6/86 [D loss: 0.6477550566196442, acc.: 62.99%] [G loss: 0.833077609539032]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 7/86 [D loss: 0.6576225459575653, acc.: 61.08%] [G loss: 0.7964962124824524]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 8/86 [D loss: 0.666758120059967, acc.: 58.64%] [G loss: 0.788190484046936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 9/86 [D loss: 0.6665078997612, acc.: 59.52%] [G loss: 0.7948751449584961]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 10/86 [D loss: 0.6792933344841003, acc.: 57.13%] [G loss: 0.8160345554351807]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 11/86 [D loss: 0.6601789593696594, acc.: 62.16%] [G loss: 0.8450095653533936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 12/86 [D loss: 0.6473971605300903, acc.: 65.72%] [G loss: 0.9038616418838501]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 13/86 [D loss: 0.6306332945823669, acc.: 68.60%] [G loss: 0.9146709442138672]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 14/86 [D loss: 0.6374879777431488, acc.: 65.72%] [G loss: 0.9171217083930969]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 15/86 [D loss: 0.6486887037754059, acc.: 63.92%] [G loss: 0.8758037090301514]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 16/86 [D loss: 0.656805008649826, acc.: 61.91%] [G loss: 0.8294099569320679]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 17/86 [D loss: 0.6774547398090363, acc.: 55.42%] [G loss: 0.7947075366973877]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 18/86 [D loss: 0.674800843000412, acc.: 58.30%] [G loss: 0.773844838142395]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 19/86 [D loss: 0.6760172247886658, acc.: 57.28%] [G loss: 0.7761549353599548]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 20/86 [D loss: 0.6624665260314941, acc.: 61.13%] [G loss: 0.804419755935669]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 21/86 [D loss: 0.6431155204772949, acc.: 66.21%] [G loss: 0.8411771059036255]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 22/86 [D loss: 0.6380375623703003, acc.: 65.43%] [G loss: 0.8604907989501953]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 23/86 [D loss: 0.6408199667930603, acc.: 64.70%] [G loss: 0.860184907913208]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 24/86 [D loss: 0.6452094316482544, acc.: 64.79%] [G loss: 0.8516215682029724]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 25/86 [D loss: 0.6476508975028992, acc.: 63.23%] [G loss: 0.822976291179657]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 26/86 [D loss: 0.6710830628871918, acc.: 58.69%] [G loss: 0.792799711227417]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 27/86 [D loss: 0.671376496553421, acc.: 57.71%] [G loss: 0.7798205614089966]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 28/86 [D loss: 0.6728967726230621, acc.: 57.32%] [G loss: 0.7860683798789978]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 29/86 [D loss: 0.6665028631687164, acc.: 59.67%] [G loss: 0.812788724899292]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 30/86 [D loss: 0.6617737412452698, acc.: 61.33%] [G loss: 0.8423518538475037]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 31/86 [D loss: 0.6416046917438507, acc.: 65.58%] [G loss: 0.8904483318328857]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 32/86 [D loss: 0.6338896751403809, acc.: 68.65%] [G loss: 0.9032580256462097]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 33/86 [D loss: 0.6394030749797821, acc.: 66.41%] [G loss: 0.8943400382995605]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 34/86 [D loss: 0.6582987308502197, acc.: 61.91%] [G loss: 0.8631032705307007]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 35/86 [D loss: 0.6598455905914307, acc.: 59.28%] [G loss: 0.8100155591964722]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 36/86 [D loss: 0.6751413941383362, acc.: 56.54%] [G loss: 0.7781438231468201]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 37/86 [D loss: 0.6727220416069031, acc.: 57.13%] [G loss: 0.7768171429634094]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 38/86 [D loss: 0.66957888007164, acc.: 58.94%] [G loss: 0.7998402118682861]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 39/86 [D loss: 0.6557536721229553, acc.: 62.21%] [G loss: 0.8090089559555054]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 40/86 [D loss: 0.6465279459953308, acc.: 64.60%] [G loss: 0.8367307186126709]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 41/86 [D loss: 0.6309302449226379, acc.: 69.29%] [G loss: 0.8514413237571716]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 42/86 [D loss: 0.6337663233280182, acc.: 66.80%] [G loss: 0.8585953712463379]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 43/86 [D loss: 0.6419050097465515, acc.: 64.84%] [G loss: 0.8291456699371338]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 44/86 [D loss: 0.6626039743423462, acc.: 60.89%] [G loss: 0.8080066442489624]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 45/86 [D loss: 0.6667826771736145, acc.: 59.18%] [G loss: 0.7869003415107727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 46/86 [D loss: 0.6607148349285126, acc.: 60.21%] [G loss: 0.7856830954551697]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 47/86 [D loss: 0.6791537702083588, acc.: 55.76%] [G loss: 0.7841014862060547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 48/86 [D loss: 0.6662416458129883, acc.: 59.13%] [G loss: 0.8192980289459229]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 49/86 [D loss: 0.6523866057395935, acc.: 62.50%] [G loss: 0.8664381504058838]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 50/86 [D loss: 0.633195161819458, acc.: 68.26%] [G loss: 0.9046307802200317]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 51/86 [D loss: 0.6365484297275543, acc.: 66.26%] [G loss: 0.9302946329116821]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 52/86 [D loss: 0.6521441340446472, acc.: 64.60%] [G loss: 0.8822306990623474]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 53/86 [D loss: 0.6478736996650696, acc.: 63.87%] [G loss: 0.849766731262207]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 54/86 [D loss: 0.6701084673404694, acc.: 59.96%] [G loss: 0.8130846619606018]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 55/86 [D loss: 0.6860807538032532, acc.: 55.81%] [G loss: 0.7975958585739136]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 56/86 [D loss: 0.6691431105136871, acc.: 58.01%] [G loss: 0.7762631177902222]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 57/86 [D loss: 0.6691605746746063, acc.: 58.98%] [G loss: 0.7952941060066223]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 58/86 [D loss: 0.6532762944698334, acc.: 62.55%] [G loss: 0.818753719329834]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 59/86 [D loss: 0.6347343325614929, acc.: 66.16%] [G loss: 0.8345310688018799]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 60/86 [D loss: 0.6285380721092224, acc.: 68.65%] [G loss: 0.8602169752120972]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 61/86 [D loss: 0.650327056646347, acc.: 64.36%] [G loss: 0.8383063673973083]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 62/86 [D loss: 0.6448965668678284, acc.: 64.01%] [G loss: 0.818108856678009]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 63/86 [D loss: 0.6635570526123047, acc.: 59.52%] [G loss: 0.8004074096679688]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 64/86 [D loss: 0.6703827381134033, acc.: 58.54%] [G loss: 0.7875794768333435]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 65/86 [D loss: 0.6742688119411469, acc.: 56.69%] [G loss: 0.7918294668197632]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 66/86 [D loss: 0.6630374193191528, acc.: 60.16%] [G loss: 0.8128828406333923]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 67/86 [D loss: 0.6525484919548035, acc.: 63.38%] [G loss: 0.8465498685836792]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 68/86 [D loss: 0.6492007672786713, acc.: 63.57%] [G loss: 0.8908672332763672]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 69/86 [D loss: 0.6385842561721802, acc.: 66.80%] [G loss: 0.8974253535270691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 70/86 [D loss: 0.6389580965042114, acc.: 66.21%] [G loss: 0.886583685874939]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 71/86 [D loss: 0.6561683714389801, acc.: 60.79%] [G loss: 0.8442697525024414]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 72/86 [D loss: 0.6633292734622955, acc.: 60.64%] [G loss: 0.8120357990264893]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 73/86 [D loss: 0.6664296984672546, acc.: 58.84%] [G loss: 0.7916515469551086]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 74/86 [D loss: 0.6675733923912048, acc.: 58.30%] [G loss: 0.7793986797332764]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 75/86 [D loss: 0.661496102809906, acc.: 60.40%] [G loss: 0.7950698733329773]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 76/86 [D loss: 0.6533530652523041, acc.: 63.04%] [G loss: 0.8180923461914062]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 77/86 [D loss: 0.6490365862846375, acc.: 64.11%] [G loss: 0.8355851769447327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 78/86 [D loss: 0.6343876123428345, acc.: 67.63%] [G loss: 0.8475741744041443]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 79/86 [D loss: 0.63863605260849, acc.: 65.82%] [G loss: 0.8477058410644531]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 50/200, Batch 80/86 [D loss: 0.6523250341415405, acc.: 61.87%] [G loss: 0.8470648527145386]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 81/86 [D loss: 0.6533252000808716, acc.: 62.70%] [G loss: 0.8154493570327759]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 82/86 [D loss: 0.6653804183006287, acc.: 57.86%] [G loss: 0.7976308465003967]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 83/86 [D loss: 0.6776171326637268, acc.: 56.59%] [G loss: 0.7906001210212708]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 84/86 [D loss: 0.6683224141597748, acc.: 59.86%] [G loss: 0.8080706000328064]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 50/200, Batch 85/86 [D loss: 0.6550599932670593, acc.: 62.70%] [G loss: 0.845003604888916]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 50/200, Batch 86/86 [D loss: 0.6420215666294098, acc.: 65.58%] [G loss: 0.8723673224449158]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 1/86 [D loss: 0.6418858170509338, acc.: 66.41%] [G loss: 0.8920454978942871]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 2/86 [D loss: 0.6428926885128021, acc.: 64.89%] [G loss: 0.8897324204444885]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 3/86 [D loss: 0.6495848000049591, acc.: 62.65%] [G loss: 0.8575467467308044]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 4/86 [D loss: 0.6566190719604492, acc.: 60.60%] [G loss: 0.8289477825164795]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 5/86 [D loss: 0.6720525622367859, acc.: 57.13%] [G loss: 0.7867956161499023]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 6/86 [D loss: 0.6753053069114685, acc.: 57.23%] [G loss: 0.7854284048080444]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 7/86 [D loss: 0.6654089391231537, acc.: 59.28%] [G loss: 0.787920355796814]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 8/86 [D loss: 0.6566303968429565, acc.: 62.30%] [G loss: 0.8122836947441101]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 9/86 [D loss: 0.6458396911621094, acc.: 65.09%] [G loss: 0.8308910131454468]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 10/86 [D loss: 0.6348042488098145, acc.: 67.63%] [G loss: 0.844437301158905]\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "Epoch 51/200, Batch 11/86 [D loss: 0.6376239955425262, acc.: 66.65%] [G loss: 0.8395258188247681]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 12/86 [D loss: 0.6498048305511475, acc.: 62.94%] [G loss: 0.8277088403701782]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 13/86 [D loss: 0.6515886783599854, acc.: 62.70%] [G loss: 0.8050162196159363]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 14/86 [D loss: 0.6633428931236267, acc.: 59.67%] [G loss: 0.7977626919746399]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 15/86 [D loss: 0.6661194562911987, acc.: 59.23%] [G loss: 0.7983818650245667]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 16/86 [D loss: 0.6635813117027283, acc.: 59.28%] [G loss: 0.8141579627990723]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 17/86 [D loss: 0.653053879737854, acc.: 63.13%] [G loss: 0.841313898563385]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 18/86 [D loss: 0.6427157819271088, acc.: 66.02%] [G loss: 0.8778618574142456]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 19/86 [D loss: 0.6397993266582489, acc.: 64.65%] [G loss: 0.8944880962371826]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 20/86 [D loss: 0.6364401578903198, acc.: 66.70%] [G loss: 0.8631927371025085]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 21/86 [D loss: 0.6542980968952179, acc.: 61.47%] [G loss: 0.8551965951919556]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 22/86 [D loss: 0.664556622505188, acc.: 57.62%] [G loss: 0.834923505783081]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 23/86 [D loss: 0.671583890914917, acc.: 57.42%] [G loss: 0.7840173840522766]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 24/86 [D loss: 0.6678424775600433, acc.: 59.08%] [G loss: 0.7860549092292786]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 25/86 [D loss: 0.6682493686676025, acc.: 59.03%] [G loss: 0.7892306447029114]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 26/86 [D loss: 0.6599477231502533, acc.: 61.38%] [G loss: 0.8240043520927429]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 27/86 [D loss: 0.6470268964767456, acc.: 65.48%] [G loss: 0.8428530693054199]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 28/86 [D loss: 0.6374407410621643, acc.: 67.53%] [G loss: 0.8515332937240601]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 29/86 [D loss: 0.6431511342525482, acc.: 65.58%] [G loss: 0.83949214220047]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 30/86 [D loss: 0.6498673558235168, acc.: 62.94%] [G loss: 0.8264282941818237]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 31/86 [D loss: 0.6574239134788513, acc.: 61.62%] [G loss: 0.8093924522399902]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 32/86 [D loss: 0.6726262867450714, acc.: 58.15%] [G loss: 0.7881289124488831]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 33/86 [D loss: 0.6713300943374634, acc.: 57.91%] [G loss: 0.8025330305099487]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 34/86 [D loss: 0.6633280813694, acc.: 60.35%] [G loss: 0.8193396925926208]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 35/86 [D loss: 0.6581476032733917, acc.: 60.99%] [G loss: 0.8480183482170105]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 36/86 [D loss: 0.6532963812351227, acc.: 63.48%] [G loss: 0.8735373020172119]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 37/86 [D loss: 0.6403844952583313, acc.: 66.50%] [G loss: 0.8791853785514832]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 38/86 [D loss: 0.6383296847343445, acc.: 65.62%] [G loss: 0.8783541917800903]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 39/86 [D loss: 0.6444073617458344, acc.: 63.53%] [G loss: 0.8441488742828369]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 40/86 [D loss: 0.658684641122818, acc.: 61.18%] [G loss: 0.823921799659729]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 41/86 [D loss: 0.6696495413780212, acc.: 57.47%] [G loss: 0.7975313067436218]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 42/86 [D loss: 0.6664432883262634, acc.: 59.13%] [G loss: 0.7807648777961731]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 43/86 [D loss: 0.6628267168998718, acc.: 58.98%] [G loss: 0.8054591417312622]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 44/86 [D loss: 0.6510675847530365, acc.: 64.45%] [G loss: 0.8066516518592834]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 45/86 [D loss: 0.6504276692867279, acc.: 63.57%] [G loss: 0.8292098641395569]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 46/86 [D loss: 0.6443474888801575, acc.: 65.14%] [G loss: 0.8398793935775757]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 47/86 [D loss: 0.6387530863285065, acc.: 66.75%] [G loss: 0.8417681455612183]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 48/86 [D loss: 0.6497679650783539, acc.: 62.89%] [G loss: 0.8201853036880493]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 49/86 [D loss: 0.6592306196689606, acc.: 60.89%] [G loss: 0.8000738620758057]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 50/86 [D loss: 0.6639093160629272, acc.: 58.94%] [G loss: 0.7922210693359375]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 51/86 [D loss: 0.6703902781009674, acc.: 58.35%] [G loss: 0.7904108762741089]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 52/86 [D loss: 0.66386878490448, acc.: 60.06%] [G loss: 0.8249469995498657]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 53/86 [D loss: 0.651487797498703, acc.: 63.67%] [G loss: 0.8687845468521118]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 54/86 [D loss: 0.6451163589954376, acc.: 64.16%] [G loss: 0.8836627006530762]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 55/86 [D loss: 0.6438073515892029, acc.: 63.72%] [G loss: 0.8848331570625305]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 56/86 [D loss: 0.6463584005832672, acc.: 63.92%] [G loss: 0.8776707053184509]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 57/86 [D loss: 0.6556924283504486, acc.: 62.06%] [G loss: 0.8411452770233154]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 58/86 [D loss: 0.6660823822021484, acc.: 58.79%] [G loss: 0.7983868718147278]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 59/86 [D loss: 0.665920078754425, acc.: 60.11%] [G loss: 0.7861427068710327]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 60/86 [D loss: 0.6594094038009644, acc.: 61.28%] [G loss: 0.7889626622200012]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 61/86 [D loss: 0.661169707775116, acc.: 61.13%] [G loss: 0.810113787651062]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 62/86 [D loss: 0.6544361114501953, acc.: 61.77%] [G loss: 0.8267923593521118]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 63/86 [D loss: 0.6469705402851105, acc.: 63.82%] [G loss: 0.8397564888000488]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 64/86 [D loss: 0.6437832117080688, acc.: 64.40%] [G loss: 0.8305115699768066]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 65/86 [D loss: 0.6506239175796509, acc.: 63.33%] [G loss: 0.8178581595420837]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 66/86 [D loss: 0.6558001041412354, acc.: 62.01%] [G loss: 0.8024938106536865]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 67/86 [D loss: 0.6706812977790833, acc.: 58.59%] [G loss: 0.7989181876182556]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 68/86 [D loss: 0.6620798110961914, acc.: 61.13%] [G loss: 0.8006380200386047]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 69/86 [D loss: 0.6613629758358002, acc.: 60.21%] [G loss: 0.8117813467979431]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 70/86 [D loss: 0.657090425491333, acc.: 61.87%] [G loss: 0.8291033506393433]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 71/86 [D loss: 0.6506501734256744, acc.: 62.65%] [G loss: 0.8685988187789917]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 72/86 [D loss: 0.6445489227771759, acc.: 64.50%] [G loss: 0.8770285844802856]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 73/86 [D loss: 0.6518823206424713, acc.: 63.09%] [G loss: 0.8744984269142151]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 74/86 [D loss: 0.6460568606853485, acc.: 64.11%] [G loss: 0.8423847556114197]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 75/86 [D loss: 0.6670823991298676, acc.: 58.35%] [G loss: 0.8220559358596802]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 76/86 [D loss: 0.6656994819641113, acc.: 59.72%] [G loss: 0.7999534606933594]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 77/86 [D loss: 0.6720850765705109, acc.: 56.93%] [G loss: 0.7943249344825745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 78/86 [D loss: 0.6616275012493134, acc.: 61.43%] [G loss: 0.7995243072509766]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 79/86 [D loss: 0.6563449800014496, acc.: 62.94%] [G loss: 0.8227142095565796]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 51/200, Batch 80/86 [D loss: 0.6451165676116943, acc.: 65.19%] [G loss: 0.8390965461730957]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 81/86 [D loss: 0.6462389826774597, acc.: 63.72%] [G loss: 0.8453740477561951]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 82/86 [D loss: 0.6519200503826141, acc.: 62.35%] [G loss: 0.8267857432365417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 83/86 [D loss: 0.6523231863975525, acc.: 62.79%] [G loss: 0.8226822018623352]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 84/86 [D loss: 0.6754019558429718, acc.: 56.10%] [G loss: 0.8050351738929749]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 51/200, Batch 85/86 [D loss: 0.6709141731262207, acc.: 58.11%] [G loss: 0.8033496141433716]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 51/200, Batch 86/86 [D loss: 0.6653460264205933, acc.: 59.77%] [G loss: 0.8141793608665466]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 1/86 [D loss: 0.6582350432872772, acc.: 61.77%] [G loss: 0.8395403623580933]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 2/86 [D loss: 0.6470683217048645, acc.: 64.75%] [G loss: 0.8454585075378418]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 3/86 [D loss: 0.6524747908115387, acc.: 63.09%] [G loss: 0.876599907875061]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 4/86 [D loss: 0.6517368257045746, acc.: 62.30%] [G loss: 0.8609894514083862]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 5/86 [D loss: 0.6422003209590912, acc.: 65.43%] [G loss: 0.8433399796485901]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 6/86 [D loss: 0.656913697719574, acc.: 61.04%] [G loss: 0.83101886510849]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 7/86 [D loss: 0.6690708100795746, acc.: 58.59%] [G loss: 0.8005679845809937]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 8/86 [D loss: 0.6699776351451874, acc.: 57.81%] [G loss: 0.8047094345092773]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 9/86 [D loss: 0.6624909043312073, acc.: 59.42%] [G loss: 0.8007751703262329]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 10/86 [D loss: 0.6578800976276398, acc.: 60.30%] [G loss: 0.8102996945381165]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 11/86 [D loss: 0.6517046391963959, acc.: 63.13%] [G loss: 0.8289331197738647]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 12/86 [D loss: 0.6421322524547577, acc.: 65.87%] [G loss: 0.8243002891540527]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 13/86 [D loss: 0.6432916522026062, acc.: 65.92%] [G loss: 0.831627368927002]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 14/86 [D loss: 0.6491632759571075, acc.: 63.67%] [G loss: 0.8248031139373779]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 15/86 [D loss: 0.6582210958003998, acc.: 62.30%] [G loss: 0.8145285844802856]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 16/86 [D loss: 0.6614145040512085, acc.: 60.64%] [G loss: 0.7961943745613098]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 17/86 [D loss: 0.6585786044597626, acc.: 60.55%] [G loss: 0.8022838234901428]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 18/86 [D loss: 0.6578971445560455, acc.: 61.82%] [G loss: 0.8114948868751526]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 19/86 [D loss: 0.6570130586624146, acc.: 61.87%] [G loss: 0.8502134680747986]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 20/86 [D loss: 0.644368976354599, acc.: 63.77%] [G loss: 0.8599432706832886]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 52/200, Batch 21/86 [D loss: 0.6401471495628357, acc.: 65.82%] [G loss: 0.8811154365539551]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 22/86 [D loss: 0.6466761231422424, acc.: 64.01%] [G loss: 0.8593206405639648]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 23/86 [D loss: 0.6494260728359222, acc.: 62.74%] [G loss: 0.8460586667060852]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 24/86 [D loss: 0.6667873561382294, acc.: 58.69%] [G loss: 0.8049452304840088]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 25/86 [D loss: 0.6621174812316895, acc.: 61.33%] [G loss: 0.7918664216995239]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 26/86 [D loss: 0.6767993271350861, acc.: 57.18%] [G loss: 0.7869199514389038]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 27/86 [D loss: 0.6553337574005127, acc.: 63.09%] [G loss: 0.8109373450279236]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 28/86 [D loss: 0.6447390019893646, acc.: 65.72%] [G loss: 0.8314846754074097]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 29/86 [D loss: 0.6472549140453339, acc.: 65.09%] [G loss: 0.8242706060409546]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 30/86 [D loss: 0.6446292996406555, acc.: 64.16%] [G loss: 0.8353214263916016]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 31/86 [D loss: 0.6482520401477814, acc.: 63.92%] [G loss: 0.8254764080047607]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 32/86 [D loss: 0.6535763740539551, acc.: 62.11%] [G loss: 0.8015425205230713]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 33/86 [D loss: 0.6667908430099487, acc.: 58.50%] [G loss: 0.786087155342102]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 34/86 [D loss: 0.6648769080638885, acc.: 59.57%] [G loss: 0.8070728778839111]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 35/86 [D loss: 0.6637909412384033, acc.: 60.69%] [G loss: 0.8332526087760925]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 36/86 [D loss: 0.6531803905963898, acc.: 62.30%] [G loss: 0.8557305335998535]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 37/86 [D loss: 0.6480010151863098, acc.: 64.36%] [G loss: 0.8746429085731506]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 38/86 [D loss: 0.6435839533805847, acc.: 64.31%] [G loss: 0.8878745436668396]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 39/86 [D loss: 0.6538713574409485, acc.: 61.72%] [G loss: 0.8611752986907959]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 40/86 [D loss: 0.6542672514915466, acc.: 62.21%] [G loss: 0.8239930272102356]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 41/86 [D loss: 0.6634408831596375, acc.: 59.72%] [G loss: 0.793254554271698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 42/86 [D loss: 0.6696231365203857, acc.: 59.67%] [G loss: 0.7890615463256836]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 43/86 [D loss: 0.6669233739376068, acc.: 58.45%] [G loss: 0.8022899627685547]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 44/86 [D loss: 0.6506658792495728, acc.: 63.57%] [G loss: 0.8171775341033936]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 45/86 [D loss: 0.6441443264484406, acc.: 64.31%] [G loss: 0.8260138630867004]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 46/86 [D loss: 0.6350862681865692, acc.: 66.85%] [G loss: 0.833930492401123]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 47/86 [D loss: 0.640603631734848, acc.: 65.23%] [G loss: 0.8276214599609375]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 48/86 [D loss: 0.6514492332935333, acc.: 62.70%] [G loss: 0.8138063549995422]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 52/200, Batch 49/86 [D loss: 0.6670055985450745, acc.: 59.38%] [G loss: 0.8057138323783875]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 50/86 [D loss: 0.6747992038726807, acc.: 57.03%] [G loss: 0.7938845157623291]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 51/86 [D loss: 0.6600091457366943, acc.: 61.52%] [G loss: 0.7970952987670898]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 52/86 [D loss: 0.6604238152503967, acc.: 61.87%] [G loss: 0.8439547419548035]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 53/86 [D loss: 0.6452794075012207, acc.: 65.67%] [G loss: 0.883322536945343]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 54/86 [D loss: 0.6437181830406189, acc.: 64.16%] [G loss: 0.8934259414672852]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 55/86 [D loss: 0.6448096930980682, acc.: 63.77%] [G loss: 0.8714296817779541]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 56/86 [D loss: 0.6502931714057922, acc.: 62.01%] [G loss: 0.8426108360290527]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 52/200, Batch 57/86 [D loss: 0.6750299036502838, acc.: 56.98%] [G loss: 0.8012189865112305]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 58/86 [D loss: 0.667478621006012, acc.: 58.30%] [G loss: 0.7914823293685913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 59/86 [D loss: 0.6735562086105347, acc.: 57.47%] [G loss: 0.7880681753158569]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 60/86 [D loss: 0.6597742736339569, acc.: 60.55%] [G loss: 0.799437940120697]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 61/86 [D loss: 0.647852748632431, acc.: 63.43%] [G loss: 0.8318239450454712]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 62/86 [D loss: 0.6408997178077698, acc.: 65.62%] [G loss: 0.8556140661239624]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 63/86 [D loss: 0.6453101336956024, acc.: 65.09%] [G loss: 0.830914318561554]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 64/86 [D loss: 0.6502832472324371, acc.: 62.45%] [G loss: 0.8305636644363403]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 65/86 [D loss: 0.655048131942749, acc.: 62.21%] [G loss: 0.8011634349822998]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 66/86 [D loss: 0.6691279709339142, acc.: 57.76%] [G loss: 0.7823007106781006]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 67/86 [D loss: 0.6749312877655029, acc.: 58.98%] [G loss: 0.8064857721328735]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 68/86 [D loss: 0.6595154404640198, acc.: 61.82%] [G loss: 0.822432279586792]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 69/86 [D loss: 0.6510747373104095, acc.: 63.57%] [G loss: 0.8558681011199951]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 70/86 [D loss: 0.639892190694809, acc.: 65.53%] [G loss: 0.8880592584609985]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 71/86 [D loss: 0.6410681903362274, acc.: 64.01%] [G loss: 0.8669993877410889]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 72/86 [D loss: 0.6468871831893921, acc.: 63.57%] [G loss: 0.8531545996665955]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 73/86 [D loss: 0.663767546415329, acc.: 60.30%] [G loss: 0.8252121210098267]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 74/86 [D loss: 0.6636877357959747, acc.: 59.23%] [G loss: 0.7933368682861328]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 75/86 [D loss: 0.6742346882820129, acc.: 58.59%] [G loss: 0.7979208827018738]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 76/86 [D loss: 0.6585541069507599, acc.: 62.30%] [G loss: 0.802262008190155]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 77/86 [D loss: 0.6571834981441498, acc.: 62.50%] [G loss: 0.8331121206283569]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 78/86 [D loss: 0.6407075226306915, acc.: 65.72%] [G loss: 0.8548071384429932]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 79/86 [D loss: 0.6382980644702911, acc.: 65.72%] [G loss: 0.8510023355484009]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 80/86 [D loss: 0.64006507396698, acc.: 65.14%] [G loss: 0.8373414278030396]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 81/86 [D loss: 0.6627532243728638, acc.: 59.81%] [G loss: 0.8139286637306213]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 82/86 [D loss: 0.6713752448558807, acc.: 58.69%] [G loss: 0.8015028834342957]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 52/200, Batch 83/86 [D loss: 0.6742669641971588, acc.: 56.45%] [G loss: 0.8026435971260071]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 84/86 [D loss: 0.6601771414279938, acc.: 61.38%] [G loss: 0.8212575316429138]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 52/200, Batch 85/86 [D loss: 0.651860386133194, acc.: 63.04%] [G loss: 0.8465876579284668]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 52/200, Batch 86/86 [D loss: 0.6483651697635651, acc.: 63.62%] [G loss: 0.8767154812812805]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 1/86 [D loss: 0.6450636386871338, acc.: 64.75%] [G loss: 0.8810544013977051]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 2/86 [D loss: 0.6492302715778351, acc.: 63.92%] [G loss: 0.8726652264595032]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 3/86 [D loss: 0.6580218374729156, acc.: 60.74%] [G loss: 0.8497985601425171]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 4/86 [D loss: 0.6679461598396301, acc.: 58.74%] [G loss: 0.8060728311538696]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 5/86 [D loss: 0.6696350276470184, acc.: 58.89%] [G loss: 0.798565149307251]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 6/86 [D loss: 0.6681088805198669, acc.: 59.18%] [G loss: 0.8000073432922363]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 7/86 [D loss: 0.6617429852485657, acc.: 60.06%] [G loss: 0.817717432975769]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 8/86 [D loss: 0.6437914967536926, acc.: 65.14%] [G loss: 0.8442417979240417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 9/86 [D loss: 0.6411476135253906, acc.: 65.53%] [G loss: 0.8546762466430664]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 10/86 [D loss: 0.6384044587612152, acc.: 64.75%] [G loss: 0.8496726155281067]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 11/86 [D loss: 0.650407612323761, acc.: 62.89%] [G loss: 0.8222973346710205]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 12/86 [D loss: 0.6627536416053772, acc.: 58.74%] [G loss: 0.8033449053764343]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 13/86 [D loss: 0.6763877868652344, acc.: 56.69%] [G loss: 0.787350594997406]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 14/86 [D loss: 0.6696645319461823, acc.: 58.89%] [G loss: 0.8040412068367004]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 15/86 [D loss: 0.6602329015731812, acc.: 61.91%] [G loss: 0.8306965231895447]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 16/86 [D loss: 0.6473483145236969, acc.: 64.65%] [G loss: 0.8667712211608887]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 17/86 [D loss: 0.6387270390987396, acc.: 65.23%] [G loss: 0.8987317085266113]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 18/86 [D loss: 0.6404965817928314, acc.: 66.21%] [G loss: 0.8879016041755676]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 19/86 [D loss: 0.6523064970970154, acc.: 62.16%] [G loss: 0.852630615234375]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 20/86 [D loss: 0.6581786274909973, acc.: 60.55%] [G loss: 0.8081828951835632]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 21/86 [D loss: 0.6731780767440796, acc.: 56.69%] [G loss: 0.8002932667732239]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 22/86 [D loss: 0.6700473427772522, acc.: 58.25%] [G loss: 0.7831597328186035]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 23/86 [D loss: 0.6597501933574677, acc.: 61.13%] [G loss: 0.814902663230896]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 24/86 [D loss: 0.6455718874931335, acc.: 64.94%] [G loss: 0.8462616801261902]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 25/86 [D loss: 0.6374236047267914, acc.: 66.75%] [G loss: 0.8482780456542969]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 26/86 [D loss: 0.6369480490684509, acc.: 66.36%] [G loss: 0.8437705636024475]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 27/86 [D loss: 0.6526724100112915, acc.: 61.52%] [G loss: 0.8244550228118896]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 28/86 [D loss: 0.6645548641681671, acc.: 59.47%] [G loss: 0.805112361907959]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 29/86 [D loss: 0.6633943915367126, acc.: 59.42%] [G loss: 0.8095186352729797]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 30/86 [D loss: 0.6680891811847687, acc.: 58.89%] [G loss: 0.7994404435157776]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 31/86 [D loss: 0.6637489795684814, acc.: 60.45%] [G loss: 0.8309754133224487]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 32/86 [D loss: 0.6561896502971649, acc.: 63.13%] [G loss: 0.8544325232505798]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 33/86 [D loss: 0.6446016132831573, acc.: 64.70%] [G loss: 0.8949402570724487]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 34/86 [D loss: 0.6408774852752686, acc.: 66.16%] [G loss: 0.87791907787323]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 35/86 [D loss: 0.6539011597633362, acc.: 62.65%] [G loss: 0.8554519414901733]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 36/86 [D loss: 0.6493218541145325, acc.: 62.16%] [G loss: 0.8302184343338013]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 37/86 [D loss: 0.6701572239398956, acc.: 58.11%] [G loss: 0.7963719964027405]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 38/86 [D loss: 0.6774297952651978, acc.: 56.35%] [G loss: 0.7875164151191711]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 39/86 [D loss: 0.6606208086013794, acc.: 61.04%] [G loss: 0.7996731996536255]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 40/86 [D loss: 0.6486614048480988, acc.: 62.21%] [G loss: 0.8276916742324829]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 41/86 [D loss: 0.6417565643787384, acc.: 65.04%] [G loss: 0.8420912623405457]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 42/86 [D loss: 0.6423898041248322, acc.: 64.36%] [G loss: 0.8430979251861572]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 43/86 [D loss: 0.649339884519577, acc.: 62.50%] [G loss: 0.8344359397888184]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 44/86 [D loss: 0.648768275976181, acc.: 63.82%] [G loss: 0.8144102096557617]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 45/86 [D loss: 0.6654317677021027, acc.: 60.16%] [G loss: 0.7980783581733704]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 46/86 [D loss: 0.669636994600296, acc.: 58.84%] [G loss: 0.8018115162849426]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 47/86 [D loss: 0.6636139154434204, acc.: 60.11%] [G loss: 0.8177727460861206]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 48/86 [D loss: 0.6548992991447449, acc.: 61.77%] [G loss: 0.8395012021064758]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 49/86 [D loss: 0.6389476954936981, acc.: 65.82%] [G loss: 0.8677300214767456]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 50/86 [D loss: 0.6446889340877533, acc.: 63.77%] [G loss: 0.8766435980796814]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 51/86 [D loss: 0.6433078348636627, acc.: 65.04%] [G loss: 0.8676621317863464]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 52/86 [D loss: 0.6650758385658264, acc.: 61.04%] [G loss: 0.8329665660858154]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 53/86 [D loss: 0.6635052561759949, acc.: 60.89%] [G loss: 0.8056609630584717]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 54/86 [D loss: 0.664626270532608, acc.: 60.64%] [G loss: 0.8121991157531738]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 55/86 [D loss: 0.6637339890003204, acc.: 58.45%] [G loss: 0.7996812462806702]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 56/86 [D loss: 0.6552239954471588, acc.: 61.33%] [G loss: 0.8112056255340576]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 57/86 [D loss: 0.6443740725517273, acc.: 64.55%] [G loss: 0.8253864049911499]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 58/86 [D loss: 0.6398614943027496, acc.: 65.04%] [G loss: 0.8470360636711121]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 59/86 [D loss: 0.6485249102115631, acc.: 64.60%] [G loss: 0.8429014682769775]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 60/86 [D loss: 0.656693696975708, acc.: 61.38%] [G loss: 0.8166891932487488]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 61/86 [D loss: 0.6563095152378082, acc.: 61.77%] [G loss: 0.8063245415687561]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 62/86 [D loss: 0.6599064469337463, acc.: 61.33%] [G loss: 0.7986732721328735]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 63/86 [D loss: 0.6608420312404633, acc.: 60.06%] [G loss: 0.8045351505279541]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 64/86 [D loss: 0.6643036603927612, acc.: 61.43%] [G loss: 0.8156476616859436]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 65/86 [D loss: 0.6488624811172485, acc.: 64.45%] [G loss: 0.8479126691818237]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 66/86 [D loss: 0.6449395716190338, acc.: 64.94%] [G loss: 0.8740096092224121]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 67/86 [D loss: 0.643884927034378, acc.: 63.87%] [G loss: 0.8696625828742981]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 68/86 [D loss: 0.6519171297550201, acc.: 61.52%] [G loss: 0.8310602903366089]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 69/86 [D loss: 0.6582690477371216, acc.: 61.28%] [G loss: 0.8222671151161194]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 70/86 [D loss: 0.6630781590938568, acc.: 60.01%] [G loss: 0.8008143901824951]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 71/86 [D loss: 0.6673084497451782, acc.: 59.08%] [G loss: 0.800141453742981]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 72/86 [D loss: 0.6633932888507843, acc.: 60.06%] [G loss: 0.8094591498374939]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 73/86 [D loss: 0.650938481092453, acc.: 64.40%] [G loss: 0.8228439688682556]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 74/86 [D loss: 0.6440999507904053, acc.: 64.84%] [G loss: 0.8312479257583618]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 75/86 [D loss: 0.6479538083076477, acc.: 63.13%] [G loss: 0.8345412015914917]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 76/86 [D loss: 0.6481139361858368, acc.: 63.33%] [G loss: 0.8283083438873291]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 77/86 [D loss: 0.6555128395557404, acc.: 62.94%] [G loss: 0.8278179168701172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 78/86 [D loss: 0.6662804782390594, acc.: 59.62%] [G loss: 0.8145575523376465]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 79/86 [D loss: 0.6513856053352356, acc.: 62.74%] [G loss: 0.8107255697250366]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 80/86 [D loss: 0.6604130864143372, acc.: 63.28%] [G loss: 0.8290828466415405]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 53/200, Batch 81/86 [D loss: 0.651818722486496, acc.: 63.43%] [G loss: 0.8466960191726685]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 82/86 [D loss: 0.6455027163028717, acc.: 65.04%] [G loss: 0.8538826107978821]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 83/86 [D loss: 0.6500323116779327, acc.: 63.67%] [G loss: 0.8549100160598755]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 53/200, Batch 84/86 [D loss: 0.6508051455020905, acc.: 62.79%] [G loss: 0.8516324758529663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 85/86 [D loss: 0.6623671352863312, acc.: 59.67%] [G loss: 0.818996787071228]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 53/200, Batch 86/86 [D loss: 0.6600695252418518, acc.: 61.91%] [G loss: 0.8124844431877136]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 1/86 [D loss: 0.658821702003479, acc.: 61.72%] [G loss: 0.8163500428199768]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 2/86 [D loss: 0.6501439809799194, acc.: 63.43%] [G loss: 0.8131572008132935]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 3/86 [D loss: 0.65468430519104, acc.: 61.82%] [G loss: 0.8297159671783447]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 4/86 [D loss: 0.6437640190124512, acc.: 65.67%] [G loss: 0.8427242636680603]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 5/86 [D loss: 0.6461257338523865, acc.: 64.40%] [G loss: 0.8336044549942017]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 6/86 [D loss: 0.6486276388168335, acc.: 63.87%] [G loss: 0.8244790434837341]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 7/86 [D loss: 0.661406010389328, acc.: 59.18%] [G loss: 0.8091090321540833]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 8/86 [D loss: 0.662140816450119, acc.: 60.69%] [G loss: 0.8026750087738037]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 9/86 [D loss: 0.6616996228694916, acc.: 60.11%] [G loss: 0.8102400302886963]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 10/86 [D loss: 0.6572310030460358, acc.: 61.38%] [G loss: 0.8389655947685242]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 54/200, Batch 11/86 [D loss: 0.6461862325668335, acc.: 64.65%] [G loss: 0.8508336544036865]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 12/86 [D loss: 0.6500626802444458, acc.: 63.92%] [G loss: 0.8569812178611755]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 13/86 [D loss: 0.6550936996936798, acc.: 60.35%] [G loss: 0.8538065552711487]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 14/86 [D loss: 0.6584780216217041, acc.: 61.96%] [G loss: 0.8313792943954468]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 15/86 [D loss: 0.663602203130722, acc.: 59.47%] [G loss: 0.8183963894844055]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 16/86 [D loss: 0.6612715125083923, acc.: 60.50%] [G loss: 0.8105465173721313]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 17/86 [D loss: 0.6601117253303528, acc.: 61.72%] [G loss: 0.8100367784500122]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 18/86 [D loss: 0.6503901779651642, acc.: 63.18%] [G loss: 0.8277406096458435]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 19/86 [D loss: 0.6472792327404022, acc.: 64.84%] [G loss: 0.8237408399581909]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 20/86 [D loss: 0.6530131697654724, acc.: 62.99%] [G loss: 0.8317655324935913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 21/86 [D loss: 0.6506071984767914, acc.: 63.77%] [G loss: 0.8163504004478455]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 22/86 [D loss: 0.6518966555595398, acc.: 62.30%] [G loss: 0.8141554594039917]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 23/86 [D loss: 0.6520906388759613, acc.: 63.13%] [G loss: 0.807274341583252]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 24/86 [D loss: 0.6660509407520294, acc.: 59.42%] [G loss: 0.8004440069198608]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 25/86 [D loss: 0.6593961715698242, acc.: 61.33%] [G loss: 0.8066468834877014]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 26/86 [D loss: 0.6547127962112427, acc.: 61.91%] [G loss: 0.847598135471344]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 27/86 [D loss: 0.6489230394363403, acc.: 63.92%] [G loss: 0.8568676114082336]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 28/86 [D loss: 0.6490658819675446, acc.: 63.92%] [G loss: 0.8589027523994446]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 29/86 [D loss: 0.6469210982322693, acc.: 63.13%] [G loss: 0.854803204536438]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 30/86 [D loss: 0.6564740538597107, acc.: 62.89%] [G loss: 0.8353260159492493]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 31/86 [D loss: 0.6656984686851501, acc.: 60.45%] [G loss: 0.8136839866638184]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 32/86 [D loss: 0.663304328918457, acc.: 60.45%] [G loss: 0.8171151280403137]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 33/86 [D loss: 0.6560303270816803, acc.: 63.23%] [G loss: 0.804904580116272]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 34/86 [D loss: 0.6508794128894806, acc.: 62.50%] [G loss: 0.8086525797843933]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 35/86 [D loss: 0.6468475461006165, acc.: 64.40%] [G loss: 0.8295252919197083]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 36/86 [D loss: 0.6489370167255402, acc.: 63.77%] [G loss: 0.8354892730712891]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 37/86 [D loss: 0.6552956402301788, acc.: 62.21%] [G loss: 0.8297720551490784]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 38/86 [D loss: 0.6564670503139496, acc.: 62.50%] [G loss: 0.8101351261138916]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 39/86 [D loss: 0.65714430809021, acc.: 61.04%] [G loss: 0.8068326711654663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 40/86 [D loss: 0.6625125706195831, acc.: 59.91%] [G loss: 0.8270308375358582]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 41/86 [D loss: 0.650301069021225, acc.: 64.16%] [G loss: 0.8349401950836182]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 42/86 [D loss: 0.6556267738342285, acc.: 60.30%] [G loss: 0.852132260799408]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 43/86 [D loss: 0.6463109850883484, acc.: 63.38%] [G loss: 0.8692178130149841]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 44/86 [D loss: 0.6529966294765472, acc.: 61.28%] [G loss: 0.8434253931045532]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 45/86 [D loss: 0.6672249436378479, acc.: 59.62%] [G loss: 0.8229811787605286]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 46/86 [D loss: 0.6614147424697876, acc.: 60.40%] [G loss: 0.807099461555481]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 47/86 [D loss: 0.6599562168121338, acc.: 59.67%] [G loss: 0.8042321801185608]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 48/86 [D loss: 0.6593860983848572, acc.: 61.77%] [G loss: 0.815578281879425]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 49/86 [D loss: 0.655208945274353, acc.: 61.18%] [G loss: 0.8292465209960938]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 50/86 [D loss: 0.6452534794807434, acc.: 64.70%] [G loss: 0.8430453538894653]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 51/86 [D loss: 0.6444702744483948, acc.: 65.72%] [G loss: 0.8266252279281616]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 52/86 [D loss: 0.6450105011463165, acc.: 64.79%] [G loss: 0.8376460075378418]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 53/86 [D loss: 0.6603957414627075, acc.: 60.35%] [G loss: 0.8131160736083984]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 54/86 [D loss: 0.6682137250900269, acc.: 59.03%] [G loss: 0.7914953231811523]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 55/86 [D loss: 0.6670435070991516, acc.: 58.45%] [G loss: 0.8103426098823547]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 56/86 [D loss: 0.6520412266254425, acc.: 63.96%] [G loss: 0.8362383842468262]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 57/86 [D loss: 0.653496116399765, acc.: 62.74%] [G loss: 0.8546730875968933]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 58/86 [D loss: 0.6435683369636536, acc.: 64.75%] [G loss: 0.8781834840774536]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 59/86 [D loss: 0.6596390008926392, acc.: 61.52%] [G loss: 0.8574850559234619]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 60/86 [D loss: 0.6620158553123474, acc.: 60.60%] [G loss: 0.8336440920829773]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 61/86 [D loss: 0.6646205186843872, acc.: 59.91%] [G loss: 0.812458336353302]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 62/86 [D loss: 0.6624884009361267, acc.: 60.60%] [G loss: 0.7972696423530579]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 63/86 [D loss: 0.659756064414978, acc.: 62.01%] [G loss: 0.8200995326042175]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 64/86 [D loss: 0.6532460749149323, acc.: 63.67%] [G loss: 0.8298050761222839]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 65/86 [D loss: 0.6408616602420807, acc.: 65.77%] [G loss: 0.833832859992981]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 66/86 [D loss: 0.6495032608509064, acc.: 63.48%] [G loss: 0.8243480920791626]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 67/86 [D loss: 0.6532619893550873, acc.: 62.55%] [G loss: 0.8275320529937744]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 68/86 [D loss: 0.6582911610603333, acc.: 60.55%] [G loss: 0.8035223484039307]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 69/86 [D loss: 0.6612830758094788, acc.: 61.08%] [G loss: 0.8060032725334167]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 70/86 [D loss: 0.6613557636737823, acc.: 60.01%] [G loss: 0.8125519156455994]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 71/86 [D loss: 0.6576092541217804, acc.: 61.67%] [G loss: 0.833876371383667]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 72/86 [D loss: 0.6541397869586945, acc.: 63.13%] [G loss: 0.8472868800163269]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 73/86 [D loss: 0.6448340713977814, acc.: 64.16%] [G loss: 0.8580306768417358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 74/86 [D loss: 0.6424106061458588, acc.: 64.01%] [G loss: 0.8550451397895813]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 75/86 [D loss: 0.6612461507320404, acc.: 60.11%] [G loss: 0.8335821032524109]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 76/86 [D loss: 0.6579355001449585, acc.: 60.99%] [G loss: 0.8026382327079773]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 77/86 [D loss: 0.6691443920135498, acc.: 57.86%] [G loss: 0.795987606048584]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 78/86 [D loss: 0.6626476049423218, acc.: 60.99%] [G loss: 0.8178315758705139]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 79/86 [D loss: 0.6464537978172302, acc.: 63.43%] [G loss: 0.8260470032691956]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 80/86 [D loss: 0.6447446346282959, acc.: 64.94%] [G loss: 0.8424837589263916]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 81/86 [D loss: 0.6487014591693878, acc.: 62.45%] [G loss: 0.8296535015106201]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 82/86 [D loss: 0.6515316665172577, acc.: 62.79%] [G loss: 0.8176233768463135]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 54/200, Batch 83/86 [D loss: 0.6681210100650787, acc.: 58.89%] [G loss: 0.7953449487686157]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 84/86 [D loss: 0.6598442196846008, acc.: 59.38%] [G loss: 0.799009382724762]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 54/200, Batch 85/86 [D loss: 0.6614174544811249, acc.: 59.86%] [G loss: 0.8137837648391724]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 54/200, Batch 86/86 [D loss: 0.6565865576267242, acc.: 62.11%] [G loss: 0.8375659584999084]\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 1/86 [D loss: 0.6456455886363983, acc.: 64.50%] [G loss: 0.8645479083061218]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 2/86 [D loss: 0.6438007652759552, acc.: 63.96%] [G loss: 0.8719285130500793]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 3/86 [D loss: 0.6457743942737579, acc.: 63.67%] [G loss: 0.8634393215179443]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 4/86 [D loss: 0.652515172958374, acc.: 61.77%] [G loss: 0.8289396166801453]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 5/86 [D loss: 0.6620902717113495, acc.: 60.30%] [G loss: 0.8019901514053345]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 6/86 [D loss: 0.6701032221317291, acc.: 57.86%] [G loss: 0.8128511905670166]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 7/86 [D loss: 0.6604825854301453, acc.: 61.18%] [G loss: 0.818949282169342]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 8/86 [D loss: 0.6494172215461731, acc.: 64.21%] [G loss: 0.8350218534469604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 9/86 [D loss: 0.6441388428211212, acc.: 64.40%] [G loss: 0.8449631929397583]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 10/86 [D loss: 0.6432588696479797, acc.: 64.89%] [G loss: 0.8466137647628784]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 11/86 [D loss: 0.6483174264431, acc.: 62.65%] [G loss: 0.8064476251602173]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 12/86 [D loss: 0.6648178100585938, acc.: 59.72%] [G loss: 0.8040447235107422]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 13/86 [D loss: 0.6663166582584381, acc.: 58.45%] [G loss: 0.7971007823944092]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 14/86 [D loss: 0.656289666891098, acc.: 61.28%] [G loss: 0.8147896528244019]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 15/86 [D loss: 0.6533932685852051, acc.: 64.01%] [G loss: 0.8520359992980957]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 16/86 [D loss: 0.6431850790977478, acc.: 64.26%] [G loss: 0.8774225115776062]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 17/86 [D loss: 0.6504064202308655, acc.: 62.70%] [G loss: 0.864782989025116]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 18/86 [D loss: 0.651982456445694, acc.: 61.96%] [G loss: 0.8502975702285767]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 19/86 [D loss: 0.6664101481437683, acc.: 58.89%] [G loss: 0.8221228122711182]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 55/200, Batch 20/86 [D loss: 0.6545795202255249, acc.: 61.38%] [G loss: 0.8153277635574341]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 21/86 [D loss: 0.6590385138988495, acc.: 60.79%] [G loss: 0.8220824003219604]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 22/86 [D loss: 0.6599812209606171, acc.: 59.86%] [G loss: 0.8297926783561707]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 23/86 [D loss: 0.6458797454833984, acc.: 64.94%] [G loss: 0.8243717551231384]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 55/200, Batch 24/86 [D loss: 0.6472643613815308, acc.: 63.57%] [G loss: 0.8309730887413025]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 25/86 [D loss: 0.647166520357132, acc.: 64.70%] [G loss: 0.8263723254203796]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 26/86 [D loss: 0.653560996055603, acc.: 63.62%] [G loss: 0.8125613927841187]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 27/86 [D loss: 0.6575149595737457, acc.: 60.50%] [G loss: 0.7927765250205994]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 28/86 [D loss: 0.6656955778598785, acc.: 59.28%] [G loss: 0.8028960227966309]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 29/86 [D loss: 0.656349778175354, acc.: 62.65%] [G loss: 0.8297648429870605]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 30/86 [D loss: 0.6570520401000977, acc.: 61.67%] [G loss: 0.8487001657485962]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 31/86 [D loss: 0.6391661167144775, acc.: 67.09%] [G loss: 0.8713868856430054]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 32/86 [D loss: 0.6496331095695496, acc.: 62.60%] [G loss: 0.8591291904449463]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 33/86 [D loss: 0.6580570936203003, acc.: 60.64%] [G loss: 0.8377296924591064]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 34/86 [D loss: 0.6665724515914917, acc.: 59.72%] [G loss: 0.8130130767822266]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 35/86 [D loss: 0.6696301400661469, acc.: 58.45%] [G loss: 0.8139762878417969]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 36/86 [D loss: 0.655268669128418, acc.: 62.45%] [G loss: 0.8128729462623596]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 37/86 [D loss: 0.6508715450763702, acc.: 63.18%] [G loss: 0.8192849159240723]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 38/86 [D loss: 0.6450594961643219, acc.: 64.11%] [G loss: 0.8333995938301086]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 39/86 [D loss: 0.647861123085022, acc.: 63.57%] [G loss: 0.8311973810195923]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 40/86 [D loss: 0.6566490530967712, acc.: 61.67%] [G loss: 0.8158765435218811]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 41/86 [D loss: 0.6554815173149109, acc.: 60.74%] [G loss: 0.809240460395813]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 42/86 [D loss: 0.6664654314517975, acc.: 58.45%] [G loss: 0.8093883991241455]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 43/86 [D loss: 0.6621839702129364, acc.: 61.52%] [G loss: 0.8112924695014954]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 44/86 [D loss: 0.6540402472019196, acc.: 62.50%] [G loss: 0.8421024084091187]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 45/86 [D loss: 0.6463569402694702, acc.: 65.19%] [G loss: 0.8628641366958618]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 46/86 [D loss: 0.6543495953083038, acc.: 61.67%] [G loss: 0.8680500984191895]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 47/86 [D loss: 0.65951207280159, acc.: 59.81%] [G loss: 0.8561919927597046]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 55/200, Batch 48/86 [D loss: 0.6528596580028534, acc.: 61.77%] [G loss: 0.8267145752906799]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 49/86 [D loss: 0.6594763696193695, acc.: 59.77%] [G loss: 0.8037947416305542]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 50/86 [D loss: 0.6623237133026123, acc.: 60.79%] [G loss: 0.818517804145813]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 51/86 [D loss: 0.6536871492862701, acc.: 62.11%] [G loss: 0.8263391852378845]\n",
      "32/32 [==============================] - 0s 6ms/step\n",
      "Epoch 55/200, Batch 52/86 [D loss: 0.6476190388202667, acc.: 64.70%] [G loss: 0.8265995979309082]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 53/86 [D loss: 0.651633232831955, acc.: 63.48%] [G loss: 0.8452322483062744]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 54/86 [D loss: 0.6528465747833252, acc.: 62.84%] [G loss: 0.8312378525733948]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 55/86 [D loss: 0.652047723531723, acc.: 61.87%] [G loss: 0.8185902833938599]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 56/86 [D loss: 0.6565776765346527, acc.: 62.11%] [G loss: 0.7995875477790833]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 57/86 [D loss: 0.6695317327976227, acc.: 56.93%] [G loss: 0.8147962093353271]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 58/86 [D loss: 0.6616424918174744, acc.: 60.40%] [G loss: 0.8204655051231384]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 59/86 [D loss: 0.651513934135437, acc.: 62.45%] [G loss: 0.8450796008110046]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 60/86 [D loss: 0.6517433226108551, acc.: 63.43%] [G loss: 0.864662766456604]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 61/86 [D loss: 0.65149986743927, acc.: 62.89%] [G loss: 0.8544386029243469]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 62/86 [D loss: 0.6555230021476746, acc.: 61.18%] [G loss: 0.8408700227737427]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 63/86 [D loss: 0.6531741917133331, acc.: 62.94%] [G loss: 0.8255941867828369]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 64/86 [D loss: 0.6611609160900116, acc.: 60.01%] [G loss: 0.8130082488059998]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 65/86 [D loss: 0.6611827909946442, acc.: 60.84%] [G loss: 0.8132507801055908]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 66/86 [D loss: 0.6522496342658997, acc.: 63.38%] [G loss: 0.8226763606071472]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 67/86 [D loss: 0.6469077169895172, acc.: 63.92%] [G loss: 0.8299204111099243]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 68/86 [D loss: 0.6449006795883179, acc.: 64.40%] [G loss: 0.8286349773406982]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 69/86 [D loss: 0.6475408375263214, acc.: 64.01%] [G loss: 0.8223103284835815]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 70/86 [D loss: 0.6571410000324249, acc.: 60.94%] [G loss: 0.8062859177589417]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 71/86 [D loss: 0.6617270112037659, acc.: 60.30%] [G loss: 0.8087712526321411]\n",
      "32/32 [==============================] - 0s 5ms/step\n",
      "Epoch 55/200, Batch 72/86 [D loss: 0.6550755500793457, acc.: 62.26%] [G loss: 0.823583722114563]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 73/86 [D loss: 0.6528899669647217, acc.: 62.74%] [G loss: 0.8462034463882446]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch 55/200, Batch 74/86 [D loss: 0.6503284573554993, acc.: 63.33%] [G loss: 0.8754491209983826]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch 55/200, Batch 75/86 [D loss: 0.6431199908256531, acc.: 64.21%] [G loss: 0.8541122674942017]\n",
      "32/32 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "cgan = CGAN(img_rows, img_cols, channels)\n",
    "cgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0370418b23d3e6d627974f5b44612aacb169a42c01386bf7ba5dc9099819d8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
