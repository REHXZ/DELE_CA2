{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, Conv2DTranspose, PReLU\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Concatenate\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, HTML\n",
    "import glob\n",
    "from keras.layers import AveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Add\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import Input\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install scikit-image\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Reshape, UpSampling2D, \\\n",
    "    BatchNormalization, Activation, Input, LeakyReLU, ZeroPadding2D, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "#import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('emnist-letters-train.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {1: 0, \n",
    "           2: 1, \n",
    "           3: 2, \n",
    "           4: 3, \n",
    "           5: 4, \n",
    "           6: 5, \n",
    "           7: 6, \n",
    "           8: 7, \n",
    "           9: 8, \n",
    "           10: 9, \n",
    "           11: 10, \n",
    "           12: 11, \n",
    "           13: 12, \n",
    "           14: 13, \n",
    "           15: 14, \n",
    "           16: 15, \n",
    "           17: 16, \n",
    "           18: 17, \n",
    "           19: 18, \n",
    "           20: 19, \n",
    "           21: 20, \n",
    "           22: 21, \n",
    "           23: 22, \n",
    "           24: 23, \n",
    "           25: 24, \n",
    "           26: 25, \n",
    "           27: 26}\n",
    "\n",
    "        # Map the labels column to its corresponding value\n",
    "df[0] = df[0].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = np.array(df.iloc[:,0].values)\n",
    "y_pre = pd.Categorical(y_pre)\n",
    "X = np.array(df.iloc[:,1:].values)\n",
    "X = X.reshape(-1,28,28,1)\n",
    "preprocessed = []\n",
    "for image in X:\n",
    "    rotated_image = rotate(image, 90, reshape=False)\n",
    "    flipped_image = np.flipud(rotated_image)\n",
    "    preprocessed.append(flipped_image)\n",
    "X_pre = np.array(preprocessed)\n",
    "X = X_pre\n",
    "X = X.astype('float32')\n",
    "X_pre = (X - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pre\n",
      "[22, 6, 15, 14, 16, ..., 19, 8, 5, 11, 0]\n",
      "Length: 26\n",
      "Categories (26, int64): [0, 1, 2, 3, ..., 22, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "print(f'y_pre\\n{y_pre.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z = 100):\n",
    "        # Input shape\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        optimizer = Adam(0.00002, 0.6, 0.8)\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'])\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        # The discriminator takes generated images as input and\n",
    "        # determines validity\n",
    "        valid = self.discriminator(img)\n",
    "        # The combined model (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(512 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim, kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(Reshape((7, 7, 512)))\n",
    "        model.add(UpSampling2D())\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\", kernel_initializer=RandomNormal(0, 0.02)))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        \n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        # gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        os.makedirs('generated_mnist', exist_ok=True)\n",
    "        fig.savefig(\"generated_mnist/dcgan_mnist_improved_{:d}.png\".format(epoch))\n",
    "        plt.close()\n",
    "        \n",
    "    def train(self, epochs, batch_size=1024, save_interval=1, gen_steps=2):\n",
    "        # Load the dataset\n",
    "        X_train = X_pre\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "\n",
    "                # ---------------------\n",
    "                # Train Discriminator\n",
    "                # ---------------------\n",
    "                # Select a random half of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "                # Train the discriminator (real classified as ones\n",
    "                # and generated as zeros)\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                # ---------------------\n",
    "                # Train Generator\n",
    "                # ---------------------\n",
    "                # Train the generator (wants discriminator to mistake\n",
    "                # images as real)\n",
    "                # Sample noise and generate a batch of new images\n",
    "\n",
    "                for _ in range(1):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    g_loss = self.combined.train_on_batch(noise, valid)\n",
    "                    \n",
    "                # Plot the progress\n",
    "                print (\"Epoch: %d/%d  Batch Size: %d/%d [loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,epochs,batch,batches_per_epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_40 (Conv2D)          (None, 14, 14, 32)        320       \n",
      "                                                                 \n",
      " leaky_re_lu_36 (LeakyReLU)  (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " leaky_re_lu_37 (LeakyReLU)  (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " zero_padding2d_4 (ZeroPaddi  (None, 5, 5, 128)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 5, 5, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 3, 3, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 3, 3, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_40 (LeakyReLU)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,573,633\n",
      "Trainable params: 1,571,841\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 25088)             2533888   \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " up_sampling2d_8 (UpSampling  (None, 14, 14, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 14, 14, 256)       1179904   \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 14, 14, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_41 (LeakyReLU)  (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_9 (UpSampling  (None, 28, 28, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 28, 28, 128)       295040    \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 28, 28, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 28, 28, 64)        73792     \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 28, 28, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_43 (LeakyReLU)  (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_48 (Conv2D)          (None, 28, 28, 32)        18464     \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 28, 28, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_44 (LeakyReLU)  (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 28, 28, 1)         289       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,103,297\n",
      "Trainable params: 4,102,337\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 0/86 [loss: 1.012409, acc.: 37.16%] [G loss: 0.705170]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 1/86 [loss: 0.844837, acc.: 52.44%] [G loss: 0.721619]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 2/86 [loss: 0.734754, acc.: 57.71%] [G loss: 0.741443]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 3/86 [loss: 0.615121, acc.: 67.09%] [G loss: 0.761927]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 4/86 [loss: 0.540100, acc.: 72.12%] [G loss: 0.784108]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 5/86 [loss: 0.473662, acc.: 77.44%] [G loss: 0.797984]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 6/86 [loss: 0.430975, acc.: 81.54%] [G loss: 0.814969]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 7/86 [loss: 0.419211, acc.: 82.42%] [G loss: 0.819200]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 8/86 [loss: 0.407824, acc.: 83.20%] [G loss: 0.829960]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 9/86 [loss: 0.413104, acc.: 81.54%] [G loss: 0.845798]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 10/86 [loss: 0.443317, acc.: 79.35%] [G loss: 0.867854]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 11/86 [loss: 0.468341, acc.: 77.29%] [G loss: 0.896459]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 12/86 [loss: 0.514359, acc.: 74.41%] [G loss: 0.916639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 13/86 [loss: 0.529048, acc.: 73.05%] [G loss: 0.970812]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 14/86 [loss: 0.547405, acc.: 71.19%] [G loss: 1.026728]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 15/86 [loss: 0.602905, acc.: 69.48%] [G loss: 1.034754]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 16/86 [loss: 0.616294, acc.: 66.99%] [G loss: 1.056702]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 17/86 [loss: 0.646770, acc.: 64.75%] [G loss: 1.062700]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 18/86 [loss: 0.664282, acc.: 63.09%] [G loss: 1.048961]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 19/86 [loss: 0.690819, acc.: 60.35%] [G loss: 1.035940]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 20/86 [loss: 0.661865, acc.: 62.01%] [G loss: 1.040658]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 21/86 [loss: 0.675614, acc.: 60.94%] [G loss: 0.987697]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 22/86 [loss: 0.636274, acc.: 64.55%] [G loss: 0.932935]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 23/86 [loss: 0.617161, acc.: 66.02%] [G loss: 0.897697]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 24/86 [loss: 0.577781, acc.: 69.58%] [G loss: 0.883006]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 25/86 [loss: 0.537030, acc.: 73.05%] [G loss: 0.864405]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 26/86 [loss: 0.493183, acc.: 76.86%] [G loss: 0.848727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 27/86 [loss: 0.459933, acc.: 79.35%] [G loss: 0.841848]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 28/86 [loss: 0.427912, acc.: 82.37%] [G loss: 0.817523]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 29/86 [loss: 0.350779, acc.: 87.35%] [G loss: 0.796841]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 30/86 [loss: 0.308986, acc.: 89.40%] [G loss: 0.802901]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 31/86 [loss: 0.269202, acc.: 92.63%] [G loss: 0.777312]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 32/86 [loss: 0.238257, acc.: 94.92%] [G loss: 0.754638]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 33/86 [loss: 0.215601, acc.: 95.65%] [G loss: 0.767276]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 34/86 [loss: 0.189640, acc.: 97.07%] [G loss: 0.731398]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 35/86 [loss: 0.179185, acc.: 96.58%] [G loss: 0.740557]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 36/86 [loss: 0.167490, acc.: 97.71%] [G loss: 0.746305]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 37/86 [loss: 0.159800, acc.: 97.75%] [G loss: 0.752389]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 38/86 [loss: 0.157311, acc.: 98.10%] [G loss: 0.769489]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 39/86 [loss: 0.157495, acc.: 97.22%] [G loss: 0.766566]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 40/86 [loss: 0.170902, acc.: 96.97%] [G loss: 0.786485]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 41/86 [loss: 0.201593, acc.: 95.26%] [G loss: 0.794979]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 42/86 [loss: 0.205070, acc.: 95.02%] [G loss: 0.814900]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 43/86 [loss: 0.214147, acc.: 95.56%] [G loss: 0.820279]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 44/86 [loss: 0.226998, acc.: 93.60%] [G loss: 0.873360]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 45/86 [loss: 0.259674, acc.: 92.33%] [G loss: 0.843154]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 46/86 [loss: 0.272845, acc.: 91.55%] [G loss: 0.854423]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 47/86 [loss: 0.266067, acc.: 92.43%] [G loss: 0.894232]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 48/86 [loss: 0.252538, acc.: 93.26%] [G loss: 0.871235]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 49/86 [loss: 0.251996, acc.: 93.07%] [G loss: 0.844791]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 50/86 [loss: 0.238211, acc.: 93.31%] [G loss: 0.837302]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 51/86 [loss: 0.216828, acc.: 95.21%] [G loss: 0.799538]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 52/86 [loss: 0.190913, acc.: 96.04%] [G loss: 0.779513]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 53/86 [loss: 0.168448, acc.: 97.07%] [G loss: 0.758567]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 54/86 [loss: 0.160909, acc.: 97.36%] [G loss: 0.722566]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 55/86 [loss: 0.148792, acc.: 97.66%] [G loss: 0.701911]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 56/86 [loss: 0.145607, acc.: 97.66%] [G loss: 0.669938]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 57/86 [loss: 0.159127, acc.: 97.12%] [G loss: 0.646112]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 58/86 [loss: 0.156204, acc.: 96.73%] [G loss: 0.646605]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 59/86 [loss: 0.165445, acc.: 96.39%] [G loss: 0.637417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 60/86 [loss: 0.190450, acc.: 95.21%] [G loss: 0.642141]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 61/86 [loss: 0.209635, acc.: 94.29%] [G loss: 0.645005]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 62/86 [loss: 0.282389, acc.: 89.79%] [G loss: 0.660737]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 63/86 [loss: 0.302914, acc.: 88.48%] [G loss: 0.689279]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 64/86 [loss: 0.342778, acc.: 85.84%] [G loss: 0.703664]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 65/86 [loss: 0.374224, acc.: 83.35%] [G loss: 0.745290]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 66/86 [loss: 0.375678, acc.: 84.38%] [G loss: 0.719375]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 67/86 [loss: 0.396833, acc.: 82.42%] [G loss: 0.717200]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 68/86 [loss: 0.402073, acc.: 82.62%] [G loss: 0.702724]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 69/86 [loss: 0.404275, acc.: 82.76%] [G loss: 0.688741]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 70/86 [loss: 0.416656, acc.: 81.74%] [G loss: 0.670901]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 71/86 [loss: 0.456580, acc.: 78.56%] [G loss: 0.656937]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 72/86 [loss: 0.474678, acc.: 76.32%] [G loss: 0.637788]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 73/86 [loss: 0.482998, acc.: 76.12%] [G loss: 0.653219]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 74/86 [loss: 0.487953, acc.: 76.37%] [G loss: 0.625894]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 75/86 [loss: 0.474672, acc.: 78.32%] [G loss: 0.613422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 76/86 [loss: 0.451211, acc.: 80.08%] [G loss: 0.604185]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 77/86 [loss: 0.486676, acc.: 76.95%] [G loss: 0.594172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 78/86 [loss: 0.459444, acc.: 77.88%] [G loss: 0.559828]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 79/86 [loss: 0.460393, acc.: 78.27%] [G loss: 0.558556]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 80/86 [loss: 0.483234, acc.: 76.61%] [G loss: 0.548475]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 81/86 [loss: 0.480017, acc.: 76.81%] [G loss: 0.531028]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 82/86 [loss: 0.477572, acc.: 77.05%] [G loss: 0.507261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 83/86 [loss: 0.472848, acc.: 77.78%] [G loss: 0.520261]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 0/200  Batch Size: 84/86 [loss: 0.529710, acc.: 73.68%] [G loss: 0.497440]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 0/200  Batch Size: 85/86 [loss: 0.538486, acc.: 72.36%] [G loss: 0.486278]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 0/86 [loss: 0.524577, acc.: 73.24%] [G loss: 0.479857]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 1/86 [loss: 0.548102, acc.: 71.58%] [G loss: 0.471390]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 2/86 [loss: 0.529085, acc.: 72.90%] [G loss: 0.464766]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 3/86 [loss: 0.524591, acc.: 75.05%] [G loss: 0.469735]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 4/86 [loss: 0.526643, acc.: 73.39%] [G loss: 0.457639]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 5/86 [loss: 0.522422, acc.: 74.56%] [G loss: 0.453106]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 6/86 [loss: 0.541711, acc.: 72.31%] [G loss: 0.436079]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 7/86 [loss: 0.559020, acc.: 70.85%] [G loss: 0.423271]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 8/86 [loss: 0.579603, acc.: 69.04%] [G loss: 0.422575]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 9/86 [loss: 0.546174, acc.: 71.63%] [G loss: 0.437093]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 10/86 [loss: 0.587087, acc.: 69.09%] [G loss: 0.435897]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 11/86 [loss: 0.621934, acc.: 65.82%] [G loss: 0.426712]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 12/86 [loss: 0.596209, acc.: 68.26%] [G loss: 0.431163]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 13/86 [loss: 0.605796, acc.: 68.07%] [G loss: 0.428306]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 14/86 [loss: 0.599039, acc.: 67.19%] [G loss: 0.420246]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 15/86 [loss: 0.625451, acc.: 65.38%] [G loss: 0.439172]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 16/86 [loss: 0.612084, acc.: 66.60%] [G loss: 0.419428]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 17/86 [loss: 0.631543, acc.: 66.26%] [G loss: 0.416363]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 18/86 [loss: 0.641518, acc.: 65.33%] [G loss: 0.420705]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 19/86 [loss: 0.636275, acc.: 65.04%] [G loss: 0.407096]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 20/86 [loss: 0.624395, acc.: 65.53%] [G loss: 0.411668]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 21/86 [loss: 0.644078, acc.: 64.55%] [G loss: 0.406562]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 22/86 [loss: 0.632094, acc.: 65.33%] [G loss: 0.412302]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 23/86 [loss: 0.652951, acc.: 64.26%] [G loss: 0.422943]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 24/86 [loss: 0.643217, acc.: 63.87%] [G loss: 0.429764]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 25/86 [loss: 0.619447, acc.: 66.36%] [G loss: 0.411530]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 26/86 [loss: 0.650195, acc.: 64.26%] [G loss: 0.415663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 27/86 [loss: 0.667671, acc.: 61.77%] [G loss: 0.420101]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 28/86 [loss: 0.592471, acc.: 68.36%] [G loss: 0.425133]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 29/86 [loss: 0.599657, acc.: 68.16%] [G loss: 0.428783]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 30/86 [loss: 0.622707, acc.: 65.19%] [G loss: 0.426763]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 31/86 [loss: 0.619012, acc.: 66.26%] [G loss: 0.445074]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 32/86 [loss: 0.610889, acc.: 67.29%] [G loss: 0.451499]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 33/86 [loss: 0.591470, acc.: 68.60%] [G loss: 0.453249]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 34/86 [loss: 0.597615, acc.: 67.77%] [G loss: 0.451358]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 35/86 [loss: 0.600983, acc.: 67.72%] [G loss: 0.446923]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 36/86 [loss: 0.589650, acc.: 68.26%] [G loss: 0.450489]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 37/86 [loss: 0.571858, acc.: 70.75%] [G loss: 0.464630]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 38/86 [loss: 0.573597, acc.: 69.87%] [G loss: 0.476561]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 39/86 [loss: 0.575359, acc.: 68.95%] [G loss: 0.484185]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 40/86 [loss: 0.550838, acc.: 71.04%] [G loss: 0.480579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 41/86 [loss: 0.563800, acc.: 71.00%] [G loss: 0.504360]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 42/86 [loss: 0.551886, acc.: 72.27%] [G loss: 0.491732]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 43/86 [loss: 0.539788, acc.: 71.48%] [G loss: 0.496984]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 44/86 [loss: 0.539395, acc.: 72.12%] [G loss: 0.508285]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 45/86 [loss: 0.527043, acc.: 73.49%] [G loss: 0.506660]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 46/86 [loss: 0.496877, acc.: 75.34%] [G loss: 0.524688]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 47/86 [loss: 0.485973, acc.: 76.51%] [G loss: 0.545227]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 48/86 [loss: 0.487579, acc.: 75.93%] [G loss: 0.543471]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 49/86 [loss: 0.499946, acc.: 76.27%] [G loss: 0.535192]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 50/86 [loss: 0.473414, acc.: 78.03%] [G loss: 0.547286]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 51/86 [loss: 0.473086, acc.: 76.46%] [G loss: 0.547302]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 52/86 [loss: 0.453033, acc.: 80.08%] [G loss: 0.587642]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 53/86 [loss: 0.433523, acc.: 79.83%] [G loss: 0.580181]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 54/86 [loss: 0.433100, acc.: 80.76%] [G loss: 0.573183]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 55/86 [loss: 0.431375, acc.: 80.66%] [G loss: 0.606312]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 56/86 [loss: 0.418630, acc.: 81.49%] [G loss: 0.588623]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 57/86 [loss: 0.409697, acc.: 82.57%] [G loss: 0.608598]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 58/86 [loss: 0.401410, acc.: 83.06%] [G loss: 0.611417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 59/86 [loss: 0.395564, acc.: 84.38%] [G loss: 0.640328]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 60/86 [loss: 0.379911, acc.: 84.47%] [G loss: 0.623949]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 61/86 [loss: 0.376892, acc.: 84.03%] [G loss: 0.629305]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 62/86 [loss: 0.356231, acc.: 85.69%] [G loss: 0.652069]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 63/86 [loss: 0.361535, acc.: 84.77%] [G loss: 0.656577]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 64/86 [loss: 0.336526, acc.: 86.91%] [G loss: 0.676321]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 65/86 [loss: 0.325327, acc.: 88.92%] [G loss: 0.667003]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 66/86 [loss: 0.339447, acc.: 87.65%] [G loss: 0.685937]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 67/86 [loss: 0.338503, acc.: 86.28%] [G loss: 0.700474]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 68/86 [loss: 0.312598, acc.: 88.57%] [G loss: 0.710772]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 69/86 [loss: 0.327873, acc.: 88.48%] [G loss: 0.728332]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 70/86 [loss: 0.303860, acc.: 89.06%] [G loss: 0.732807]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 71/86 [loss: 0.295559, acc.: 89.84%] [G loss: 0.745427]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 72/86 [loss: 0.286752, acc.: 89.99%] [G loss: 0.760319]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 73/86 [loss: 0.285063, acc.: 90.58%] [G loss: 0.761922]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 74/86 [loss: 0.274011, acc.: 90.77%] [G loss: 0.763414]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 75/86 [loss: 0.258537, acc.: 91.55%] [G loss: 0.778963]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 76/86 [loss: 0.262167, acc.: 91.85%] [G loss: 0.777983]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 77/86 [loss: 0.247849, acc.: 92.82%] [G loss: 0.778553]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 78/86 [loss: 0.233550, acc.: 92.72%] [G loss: 0.795170]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 79/86 [loss: 0.227586, acc.: 93.60%] [G loss: 0.801773]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 80/86 [loss: 0.212631, acc.: 93.90%] [G loss: 0.811421]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 1/200  Batch Size: 81/86 [loss: 0.223298, acc.: 93.31%] [G loss: 0.842708]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 82/86 [loss: 0.211737, acc.: 94.09%] [G loss: 0.831984]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 83/86 [loss: 0.202891, acc.: 94.73%] [G loss: 0.825746]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 84/86 [loss: 0.196125, acc.: 95.12%] [G loss: 0.830676]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 1/200  Batch Size: 85/86 [loss: 0.199663, acc.: 94.73%] [G loss: 0.817335]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 0/86 [loss: 0.194362, acc.: 95.26%] [G loss: 0.825630]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 1/86 [loss: 0.181765, acc.: 95.70%] [G loss: 0.844670]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 2/86 [loss: 0.179788, acc.: 95.65%] [G loss: 0.814448]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 3/86 [loss: 0.167799, acc.: 96.00%] [G loss: 0.841341]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 4/86 [loss: 0.170491, acc.: 96.44%] [G loss: 0.839525]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 5/86 [loss: 0.174655, acc.: 95.80%] [G loss: 0.853506]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 6/86 [loss: 0.181118, acc.: 94.73%] [G loss: 0.840012]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 7/86 [loss: 0.177310, acc.: 95.75%] [G loss: 0.839805]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 8/86 [loss: 0.174067, acc.: 95.17%] [G loss: 0.836552]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 9/86 [loss: 0.175892, acc.: 95.51%] [G loss: 0.837603]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 10/86 [loss: 0.186293, acc.: 95.17%] [G loss: 0.863271]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 11/86 [loss: 0.196464, acc.: 94.04%] [G loss: 0.876367]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 12/86 [loss: 0.204200, acc.: 94.14%] [G loss: 0.905992]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 13/86 [loss: 0.235769, acc.: 92.19%] [G loss: 0.882857]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 14/86 [loss: 0.204441, acc.: 93.90%] [G loss: 0.900317]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 15/86 [loss: 0.229715, acc.: 91.80%] [G loss: 0.902873]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 16/86 [loss: 0.217625, acc.: 92.29%] [G loss: 0.899238]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 17/86 [loss: 0.224853, acc.: 92.68%] [G loss: 0.927706]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 18/86 [loss: 0.241486, acc.: 91.65%] [G loss: 0.940811]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 19/86 [loss: 0.218012, acc.: 93.07%] [G loss: 0.917062]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 20/86 [loss: 0.222800, acc.: 93.36%] [G loss: 0.947976]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 21/86 [loss: 0.206009, acc.: 93.99%] [G loss: 0.947678]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 22/86 [loss: 0.200074, acc.: 93.99%] [G loss: 0.937073]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 23/86 [loss: 0.210641, acc.: 93.85%] [G loss: 0.942921]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 24/86 [loss: 0.214459, acc.: 93.16%] [G loss: 0.933174]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 25/86 [loss: 0.221849, acc.: 93.07%] [G loss: 0.963402]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 26/86 [loss: 0.243972, acc.: 92.19%] [G loss: 0.995883]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 27/86 [loss: 0.307640, acc.: 87.84%] [G loss: 1.009745]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 28/86 [loss: 0.301343, acc.: 88.33%] [G loss: 1.037193]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 29/86 [loss: 0.340494, acc.: 85.30%] [G loss: 1.043345]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 30/86 [loss: 0.358234, acc.: 86.28%] [G loss: 1.025187]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 31/86 [loss: 0.376520, acc.: 84.13%] [G loss: 1.002072]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 32/86 [loss: 0.379844, acc.: 83.35%] [G loss: 0.987671]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 33/86 [loss: 0.404004, acc.: 82.28%] [G loss: 0.971893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 34/86 [loss: 0.374112, acc.: 84.57%] [G loss: 0.973487]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 35/86 [loss: 0.393833, acc.: 83.98%] [G loss: 0.922002]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 36/86 [loss: 0.383353, acc.: 83.89%] [G loss: 0.932757]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 37/86 [loss: 0.343707, acc.: 86.47%] [G loss: 0.905655]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 38/86 [loss: 0.329761, acc.: 87.50%] [G loss: 0.858750]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 39/86 [loss: 0.339224, acc.: 86.62%] [G loss: 0.859561]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 40/86 [loss: 0.296035, acc.: 89.45%] [G loss: 0.843470]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 41/86 [loss: 0.289247, acc.: 89.99%] [G loss: 0.808935]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 42/86 [loss: 0.271693, acc.: 90.67%] [G loss: 0.815516]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 43/86 [loss: 0.274656, acc.: 90.43%] [G loss: 0.821282]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 44/86 [loss: 0.251799, acc.: 91.50%] [G loss: 0.809068]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 45/86 [loss: 0.231467, acc.: 92.53%] [G loss: 0.805357]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 46/86 [loss: 0.256048, acc.: 92.19%] [G loss: 0.788987]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 47/86 [loss: 0.231062, acc.: 92.72%] [G loss: 0.842781]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 48/86 [loss: 0.239635, acc.: 91.94%] [G loss: 0.843970]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 49/86 [loss: 0.215835, acc.: 93.80%] [G loss: 0.829815]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 50/86 [loss: 0.229306, acc.: 92.63%] [G loss: 0.777557]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 51/86 [loss: 0.206478, acc.: 94.24%] [G loss: 0.797026]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 52/86 [loss: 0.208358, acc.: 94.19%] [G loss: 0.806290]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 53/86 [loss: 0.189832, acc.: 95.02%] [G loss: 0.797916]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 54/86 [loss: 0.197988, acc.: 94.53%] [G loss: 0.754146]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 55/86 [loss: 0.204881, acc.: 94.73%] [G loss: 0.747916]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 56/86 [loss: 0.180387, acc.: 95.95%] [G loss: 0.742268]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 57/86 [loss: 0.200803, acc.: 94.87%] [G loss: 0.752325]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 58/86 [loss: 0.165388, acc.: 96.00%] [G loss: 0.721865]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 59/86 [loss: 0.172793, acc.: 95.90%] [G loss: 0.730897]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 60/86 [loss: 0.158253, acc.: 96.78%] [G loss: 0.728106]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 61/86 [loss: 0.155008, acc.: 96.92%] [G loss: 0.724974]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 62/86 [loss: 0.139147, acc.: 97.27%] [G loss: 0.671883]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 63/86 [loss: 0.130856, acc.: 97.66%] [G loss: 0.677302]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 64/86 [loss: 0.123118, acc.: 98.10%] [G loss: 0.635229]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 65/86 [loss: 0.111193, acc.: 98.29%] [G loss: 0.612554]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 66/86 [loss: 0.105844, acc.: 98.63%] [G loss: 0.604377]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 67/86 [loss: 0.099871, acc.: 98.73%] [G loss: 0.590706]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 68/86 [loss: 0.100137, acc.: 98.39%] [G loss: 0.569164]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 69/86 [loss: 0.093724, acc.: 98.34%] [G loss: 0.538084]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 70/86 [loss: 0.093913, acc.: 98.78%] [G loss: 0.534451]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 71/86 [loss: 0.093858, acc.: 98.68%] [G loss: 0.507394]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 72/86 [loss: 0.082621, acc.: 99.02%] [G loss: 0.507406]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 73/86 [loss: 0.082315, acc.: 99.12%] [G loss: 0.488086]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 74/86 [loss: 0.088242, acc.: 98.54%] [G loss: 0.480954]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 75/86 [loss: 0.080512, acc.: 98.97%] [G loss: 0.471171]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 76/86 [loss: 0.088337, acc.: 98.49%] [G loss: 0.467924]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 77/86 [loss: 0.083914, acc.: 98.93%] [G loss: 0.419763]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 78/86 [loss: 0.072491, acc.: 99.17%] [G loss: 0.420966]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 79/86 [loss: 0.064754, acc.: 99.32%] [G loss: 0.428222]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 80/86 [loss: 0.064188, acc.: 99.41%] [G loss: 0.426032]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 81/86 [loss: 0.065895, acc.: 99.51%] [G loss: 0.411924]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 82/86 [loss: 0.063549, acc.: 99.22%] [G loss: 0.386767]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 2/200  Batch Size: 83/86 [loss: 0.055564, acc.: 99.80%] [G loss: 0.369417]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 84/86 [loss: 0.052260, acc.: 99.56%] [G loss: 0.356212]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 2/200  Batch Size: 85/86 [loss: 0.046300, acc.: 99.85%] [G loss: 0.360772]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 0/86 [loss: 0.049065, acc.: 99.76%] [G loss: 0.352499]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 1/86 [loss: 0.050929, acc.: 99.71%] [G loss: 0.341755]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 2/86 [loss: 0.046008, acc.: 99.61%] [G loss: 0.342480]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 3/86 [loss: 0.053077, acc.: 99.66%] [G loss: 0.365929]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 4/86 [loss: 0.053378, acc.: 99.46%] [G loss: 0.374893]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 5/86 [loss: 0.055154, acc.: 99.56%] [G loss: 0.362106]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 6/86 [loss: 0.054056, acc.: 99.32%] [G loss: 0.367044]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 7/86 [loss: 0.049904, acc.: 99.61%] [G loss: 0.362981]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 8/86 [loss: 0.050739, acc.: 99.80%] [G loss: 0.351357]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 9/86 [loss: 0.040881, acc.: 99.80%] [G loss: 0.374324]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 10/86 [loss: 0.046676, acc.: 99.66%] [G loss: 0.376841]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 11/86 [loss: 0.046162, acc.: 99.66%] [G loss: 0.366742]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 12/86 [loss: 0.046932, acc.: 99.76%] [G loss: 0.368064]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 13/86 [loss: 0.050482, acc.: 99.51%] [G loss: 0.389246]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 14/86 [loss: 0.044368, acc.: 99.61%] [G loss: 0.381792]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 15/86 [loss: 0.044399, acc.: 99.66%] [G loss: 0.394648]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 16/86 [loss: 0.044843, acc.: 99.61%] [G loss: 0.414017]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 17/86 [loss: 0.043382, acc.: 99.85%] [G loss: 0.407879]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 18/86 [loss: 0.042615, acc.: 99.71%] [G loss: 0.420698]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 19/86 [loss: 0.040049, acc.: 99.76%] [G loss: 0.434191]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 20/86 [loss: 0.046842, acc.: 99.61%] [G loss: 0.428044]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 21/86 [loss: 0.037944, acc.: 99.90%] [G loss: 0.443323]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 22/86 [loss: 0.042387, acc.: 99.51%] [G loss: 0.457476]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 23/86 [loss: 0.039022, acc.: 99.61%] [G loss: 0.461930]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 24/86 [loss: 0.038083, acc.: 99.66%] [G loss: 0.435922]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 25/86 [loss: 0.036133, acc.: 99.66%] [G loss: 0.463931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 26/86 [loss: 0.032405, acc.: 99.90%] [G loss: 0.517263]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 27/86 [loss: 0.031556, acc.: 99.90%] [G loss: 0.527044]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 28/86 [loss: 0.033520, acc.: 99.85%] [G loss: 0.538057]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 29/86 [loss: 0.035748, acc.: 99.71%] [G loss: 0.586189]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 30/86 [loss: 0.037000, acc.: 99.71%] [G loss: 0.571613]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 31/86 [loss: 0.036775, acc.: 99.76%] [G loss: 0.608249]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 32/86 [loss: 0.032672, acc.: 99.80%] [G loss: 0.580714]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 33/86 [loss: 0.035597, acc.: 99.71%] [G loss: 0.592341]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 34/86 [loss: 0.031179, acc.: 99.76%] [G loss: 0.619413]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 35/86 [loss: 0.029233, acc.: 99.85%] [G loss: 0.664608]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 36/86 [loss: 0.029989, acc.: 99.90%] [G loss: 0.660744]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 37/86 [loss: 0.030006, acc.: 99.76%] [G loss: 0.660970]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 38/86 [loss: 0.026791, acc.: 99.85%] [G loss: 0.672845]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 39/86 [loss: 0.024323, acc.: 99.80%] [G loss: 0.716222]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 40/86 [loss: 0.027261, acc.: 99.76%] [G loss: 0.658020]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 41/86 [loss: 0.023117, acc.: 99.95%] [G loss: 0.712883]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 42/86 [loss: 0.022998, acc.: 100.00%] [G loss: 0.713545]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 43/86 [loss: 0.022328, acc.: 99.95%] [G loss: 0.715644]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 44/86 [loss: 0.019799, acc.: 99.85%] [G loss: 0.770401]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 45/86 [loss: 0.021042, acc.: 99.85%] [G loss: 0.731768]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 46/86 [loss: 0.017512, acc.: 99.85%] [G loss: 0.848070]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 47/86 [loss: 0.017758, acc.: 100.00%] [G loss: 0.825932]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 48/86 [loss: 0.020787, acc.: 99.80%] [G loss: 0.838068]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 49/86 [loss: 0.018970, acc.: 99.95%] [G loss: 0.840986]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 50/86 [loss: 0.016502, acc.: 99.95%] [G loss: 0.870340]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 51/86 [loss: 0.016654, acc.: 99.85%] [G loss: 0.872518]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 52/86 [loss: 0.016823, acc.: 99.85%] [G loss: 0.817976]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 53/86 [loss: 0.019076, acc.: 99.95%] [G loss: 0.831166]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 54/86 [loss: 0.018301, acc.: 99.85%] [G loss: 0.831599]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 55/86 [loss: 0.018321, acc.: 99.95%] [G loss: 0.878706]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 56/86 [loss: 0.016352, acc.: 99.80%] [G loss: 0.885198]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 57/86 [loss: 0.018805, acc.: 100.00%] [G loss: 0.876953]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 58/86 [loss: 0.019917, acc.: 99.85%] [G loss: 0.915561]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 59/86 [loss: 0.024352, acc.: 99.56%] [G loss: 0.953131]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 60/86 [loss: 0.027006, acc.: 99.51%] [G loss: 1.000434]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 61/86 [loss: 0.030585, acc.: 99.61%] [G loss: 0.987657]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 62/86 [loss: 0.032482, acc.: 99.41%] [G loss: 0.882796]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 63/86 [loss: 0.041195, acc.: 99.27%] [G loss: 0.887990]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 64/86 [loss: 0.038683, acc.: 99.56%] [G loss: 0.905608]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 65/86 [loss: 0.040387, acc.: 99.51%] [G loss: 0.905314]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 66/86 [loss: 0.041943, acc.: 99.22%] [G loss: 0.949740]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 67/86 [loss: 0.043398, acc.: 99.51%] [G loss: 0.830070]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 68/86 [loss: 0.047947, acc.: 99.27%] [G loss: 0.824522]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 69/86 [loss: 0.046290, acc.: 99.27%] [G loss: 0.787475]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 70/86 [loss: 0.042612, acc.: 99.56%] [G loss: 0.791012]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 71/86 [loss: 0.051855, acc.: 99.46%] [G loss: 0.808761]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 72/86 [loss: 0.054962, acc.: 99.37%] [G loss: 0.855421]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 73/86 [loss: 0.067564, acc.: 98.83%] [G loss: 0.824020]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 74/86 [loss: 0.069004, acc.: 98.93%] [G loss: 0.848515]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 75/86 [loss: 0.077285, acc.: 98.63%] [G loss: 0.857521]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 76/86 [loss: 0.078261, acc.: 98.68%] [G loss: 0.867459]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 77/86 [loss: 0.084333, acc.: 98.49%] [G loss: 0.912170]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 78/86 [loss: 0.093398, acc.: 98.14%] [G loss: 0.894843]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 79/86 [loss: 0.094515, acc.: 98.44%] [G loss: 0.880703]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 80/86 [loss: 0.102247, acc.: 97.95%] [G loss: 0.839321]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 81/86 [loss: 0.107970, acc.: 97.22%] [G loss: 0.841938]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 82/86 [loss: 0.095587, acc.: 97.56%] [G loss: 0.855039]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 3/200  Batch Size: 83/86 [loss: 0.085511, acc.: 98.44%] [G loss: 0.850543]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 84/86 [loss: 0.091512, acc.: 97.61%] [G loss: 0.783548]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 3/200  Batch Size: 85/86 [loss: 0.085773, acc.: 98.29%] [G loss: 0.843577]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 0/86 [loss: 0.094420, acc.: 98.10%] [G loss: 0.836692]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 1/86 [loss: 0.084016, acc.: 98.34%] [G loss: 0.820300]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 2/86 [loss: 0.097621, acc.: 98.14%] [G loss: 0.831359]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 3/86 [loss: 0.087683, acc.: 98.24%] [G loss: 0.808422]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 4/86 [loss: 0.093463, acc.: 97.95%] [G loss: 0.825626]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 5/86 [loss: 0.093018, acc.: 97.80%] [G loss: 0.850869]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 6/86 [loss: 0.098556, acc.: 97.95%] [G loss: 0.909517]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 7/86 [loss: 0.101405, acc.: 97.27%] [G loss: 0.922759]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 8/86 [loss: 0.092878, acc.: 97.95%] [G loss: 0.826157]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 9/86 [loss: 0.100038, acc.: 97.75%] [G loss: 0.854091]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 10/86 [loss: 0.099266, acc.: 97.75%] [G loss: 0.845335]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 11/86 [loss: 0.099933, acc.: 98.00%] [G loss: 0.844233]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 12/86 [loss: 0.103121, acc.: 97.66%] [G loss: 0.865926]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 13/86 [loss: 0.103220, acc.: 97.75%] [G loss: 0.836569]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 14/86 [loss: 0.101213, acc.: 97.56%] [G loss: 0.848581]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 15/86 [loss: 0.107560, acc.: 96.88%] [G loss: 0.837036]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 16/86 [loss: 0.104714, acc.: 97.51%] [G loss: 0.887861]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 17/86 [loss: 0.087614, acc.: 98.39%] [G loss: 0.895631]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 18/86 [loss: 0.099984, acc.: 97.71%] [G loss: 0.832571]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 19/86 [loss: 0.084599, acc.: 98.54%] [G loss: 0.867739]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 20/86 [loss: 0.084084, acc.: 98.68%] [G loss: 0.867087]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 21/86 [loss: 0.088157, acc.: 98.05%] [G loss: 0.854622]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 22/86 [loss: 0.084468, acc.: 98.49%] [G loss: 0.869915]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 23/86 [loss: 0.092711, acc.: 98.10%] [G loss: 0.851040]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 24/86 [loss: 0.083621, acc.: 98.73%] [G loss: 0.847649]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 25/86 [loss: 0.078241, acc.: 98.54%] [G loss: 0.864852]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 26/86 [loss: 0.089251, acc.: 97.80%] [G loss: 0.868103]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 27/86 [loss: 0.079635, acc.: 98.14%] [G loss: 0.854048]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 28/86 [loss: 0.085146, acc.: 98.24%] [G loss: 0.866596]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 29/86 [loss: 0.076725, acc.: 98.58%] [G loss: 0.818941]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 30/86 [loss: 0.089675, acc.: 98.14%] [G loss: 0.893903]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 31/86 [loss: 0.085650, acc.: 98.19%] [G loss: 0.920026]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 32/86 [loss: 0.078931, acc.: 98.63%] [G loss: 0.854895]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 33/86 [loss: 0.080332, acc.: 98.14%] [G loss: 0.860931]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 34/86 [loss: 0.074945, acc.: 98.78%] [G loss: 0.815351]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 35/86 [loss: 0.073255, acc.: 98.29%] [G loss: 0.843044]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 36/86 [loss: 0.070542, acc.: 98.63%] [G loss: 0.816854]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 37/86 [loss: 0.062976, acc.: 99.22%] [G loss: 0.793123]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 38/86 [loss: 0.071192, acc.: 98.78%] [G loss: 0.789348]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 39/86 [loss: 0.063888, acc.: 99.12%] [G loss: 0.838213]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 40/86 [loss: 0.067132, acc.: 98.88%] [G loss: 0.805328]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 41/86 [loss: 0.073996, acc.: 98.88%] [G loss: 0.871612]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 42/86 [loss: 0.080567, acc.: 98.29%] [G loss: 0.796409]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 43/86 [loss: 0.076559, acc.: 98.39%] [G loss: 0.868831]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 44/86 [loss: 0.086473, acc.: 98.05%] [G loss: 0.865269]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 45/86 [loss: 0.062288, acc.: 98.97%] [G loss: 0.900645]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 46/86 [loss: 0.084663, acc.: 98.44%] [G loss: 0.840372]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 47/86 [loss: 0.082198, acc.: 98.24%] [G loss: 0.821350]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 48/86 [loss: 0.071880, acc.: 98.93%] [G loss: 0.853881]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 49/86 [loss: 0.078295, acc.: 98.19%] [G loss: 0.815096]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 50/86 [loss: 0.062961, acc.: 99.12%] [G loss: 0.876239]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 51/86 [loss: 0.063192, acc.: 98.78%] [G loss: 0.860528]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 52/86 [loss: 0.068968, acc.: 98.97%] [G loss: 0.891215]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 53/86 [loss: 0.072376, acc.: 98.73%] [G loss: 0.870928]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 54/86 [loss: 0.063979, acc.: 98.97%] [G loss: 0.902492]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 55/86 [loss: 0.076272, acc.: 98.54%] [G loss: 0.848329]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 56/86 [loss: 0.067820, acc.: 98.63%] [G loss: 0.837643]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 57/86 [loss: 0.064482, acc.: 99.32%] [G loss: 0.872386]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 58/86 [loss: 0.073751, acc.: 98.68%] [G loss: 0.879579]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 59/86 [loss: 0.076275, acc.: 98.78%] [G loss: 0.865080]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 60/86 [loss: 0.080512, acc.: 98.68%] [G loss: 0.902947]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 61/86 [loss: 0.082142, acc.: 98.44%] [G loss: 0.865253]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 62/86 [loss: 0.063375, acc.: 98.88%] [G loss: 0.873304]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 63/86 [loss: 0.066016, acc.: 99.07%] [G loss: 0.878782]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 64/86 [loss: 0.063840, acc.: 99.07%] [G loss: 0.880587]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 65/86 [loss: 0.064974, acc.: 99.41%] [G loss: 0.861044]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 66/86 [loss: 0.071904, acc.: 98.68%] [G loss: 0.851702]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 67/86 [loss: 0.065646, acc.: 99.17%] [G loss: 0.907663]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 68/86 [loss: 0.080906, acc.: 98.68%] [G loss: 0.856289]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 69/86 [loss: 0.061868, acc.: 98.83%] [G loss: 0.906254]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 70/86 [loss: 0.067131, acc.: 98.97%] [G loss: 0.902876]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 71/86 [loss: 0.071614, acc.: 98.63%] [G loss: 0.842495]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 72/86 [loss: 0.069318, acc.: 98.44%] [G loss: 0.830972]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 73/86 [loss: 0.070892, acc.: 98.83%] [G loss: 0.903037]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 74/86 [loss: 0.067617, acc.: 99.17%] [G loss: 0.850641]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 75/86 [loss: 0.066401, acc.: 98.88%] [G loss: 0.881353]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 76/86 [loss: 0.082307, acc.: 98.58%] [G loss: 0.918705]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 77/86 [loss: 0.075353, acc.: 98.49%] [G loss: 0.925628]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 78/86 [loss: 0.077006, acc.: 98.49%] [G loss: 0.907698]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 79/86 [loss: 0.081444, acc.: 98.34%] [G loss: 0.944657]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 80/86 [loss: 0.076442, acc.: 98.83%] [G loss: 0.923212]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 81/86 [loss: 0.081476, acc.: 98.34%] [G loss: 0.932245]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 82/86 [loss: 0.070040, acc.: 98.93%] [G loss: 0.891482]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 4/200  Batch Size: 83/86 [loss: 0.071828, acc.: 98.78%] [G loss: 0.937922]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 84/86 [loss: 0.081109, acc.: 98.68%] [G loss: 0.882390]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 4/200  Batch Size: 85/86 [loss: 0.072431, acc.: 98.78%] [G loss: 0.923586]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 0/86 [loss: 0.064867, acc.: 99.12%] [G loss: 0.917574]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 1/86 [loss: 0.073786, acc.: 98.68%] [G loss: 0.892428]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 2/86 [loss: 0.074375, acc.: 98.58%] [G loss: 0.958994]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 3/86 [loss: 0.071779, acc.: 98.73%] [G loss: 0.990618]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 4/86 [loss: 0.066912, acc.: 99.12%] [G loss: 0.896727]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 5/86 [loss: 0.082397, acc.: 98.88%] [G loss: 0.911119]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 6/86 [loss: 0.073800, acc.: 98.54%] [G loss: 0.942292]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 7/86 [loss: 0.079026, acc.: 98.68%] [G loss: 0.932822]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 8/86 [loss: 0.086682, acc.: 97.85%] [G loss: 0.936556]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 9/86 [loss: 0.078235, acc.: 98.34%] [G loss: 0.969206]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 10/86 [loss: 0.086238, acc.: 98.19%] [G loss: 0.963638]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 11/86 [loss: 0.088529, acc.: 98.19%] [G loss: 0.943127]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 12/86 [loss: 0.089194, acc.: 98.14%] [G loss: 0.862525]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 13/86 [loss: 0.080551, acc.: 98.05%] [G loss: 0.900249]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 14/86 [loss: 0.082093, acc.: 98.29%] [G loss: 0.974022]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 15/86 [loss: 0.077224, acc.: 98.58%] [G loss: 0.961887]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 16/86 [loss: 0.085242, acc.: 98.49%] [G loss: 0.962950]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 17/86 [loss: 0.084900, acc.: 98.24%] [G loss: 0.890913]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 18/86 [loss: 0.079547, acc.: 98.78%] [G loss: 0.896718]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 19/86 [loss: 0.091976, acc.: 98.00%] [G loss: 0.944752]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 20/86 [loss: 0.079134, acc.: 98.78%] [G loss: 0.906896]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 21/86 [loss: 0.086719, acc.: 97.90%] [G loss: 0.888342]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 22/86 [loss: 0.084206, acc.: 98.24%] [G loss: 0.970334]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 23/86 [loss: 0.086704, acc.: 98.24%] [G loss: 0.962048]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 24/86 [loss: 0.092958, acc.: 97.75%] [G loss: 0.944676]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 25/86 [loss: 0.086191, acc.: 98.58%] [G loss: 0.959097]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 26/86 [loss: 0.093692, acc.: 98.19%] [G loss: 0.922722]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 27/86 [loss: 0.076743, acc.: 98.63%] [G loss: 0.956274]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 28/86 [loss: 0.082223, acc.: 98.34%] [G loss: 0.918999]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 29/86 [loss: 0.090125, acc.: 97.90%] [G loss: 0.972812]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 30/86 [loss: 0.077613, acc.: 98.68%] [G loss: 0.966340]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 31/86 [loss: 0.079853, acc.: 98.49%] [G loss: 0.993792]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 32/86 [loss: 0.083507, acc.: 98.34%] [G loss: 0.929178]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 33/86 [loss: 0.089117, acc.: 98.05%] [G loss: 0.882936]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 34/86 [loss: 0.075244, acc.: 98.78%] [G loss: 0.948297]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 35/86 [loss: 0.088929, acc.: 98.34%] [G loss: 0.972791]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 36/86 [loss: 0.091024, acc.: 97.80%] [G loss: 0.914914]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 37/86 [loss: 0.087623, acc.: 98.24%] [G loss: 0.952524]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 38/86 [loss: 0.109385, acc.: 97.31%] [G loss: 0.921301]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 39/86 [loss: 0.084149, acc.: 98.54%] [G loss: 0.949615]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 40/86 [loss: 0.091188, acc.: 97.75%] [G loss: 1.018247]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 41/86 [loss: 0.093406, acc.: 98.05%] [G loss: 0.921777]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 42/86 [loss: 0.087575, acc.: 97.90%] [G loss: 0.940360]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 43/86 [loss: 0.086758, acc.: 98.24%] [G loss: 0.975137]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 44/86 [loss: 0.087688, acc.: 98.00%] [G loss: 1.022824]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 45/86 [loss: 0.094029, acc.: 97.90%] [G loss: 0.978642]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 46/86 [loss: 0.087810, acc.: 97.95%] [G loss: 0.976701]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 47/86 [loss: 0.089217, acc.: 97.95%] [G loss: 0.987190]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 48/86 [loss: 0.084175, acc.: 98.54%] [G loss: 0.978597]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 49/86 [loss: 0.089609, acc.: 98.29%] [G loss: 1.014064]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 50/86 [loss: 0.088622, acc.: 98.78%] [G loss: 1.004472]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 51/86 [loss: 0.080094, acc.: 98.44%] [G loss: 0.969707]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 52/86 [loss: 0.081009, acc.: 98.58%] [G loss: 0.990937]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 53/86 [loss: 0.080418, acc.: 98.44%] [G loss: 0.984349]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 54/86 [loss: 0.091076, acc.: 98.00%] [G loss: 0.935401]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 55/86 [loss: 0.105869, acc.: 97.46%] [G loss: 0.980332]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 56/86 [loss: 0.098217, acc.: 97.71%] [G loss: 1.013028]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 57/86 [loss: 0.077233, acc.: 98.39%] [G loss: 1.027664]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 58/86 [loss: 0.090010, acc.: 97.90%] [G loss: 0.948041]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 59/86 [loss: 0.091998, acc.: 98.24%] [G loss: 0.936725]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 60/86 [loss: 0.096597, acc.: 97.95%] [G loss: 0.932427]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 61/86 [loss: 0.095543, acc.: 97.66%] [G loss: 0.989523]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 62/86 [loss: 0.088701, acc.: 98.14%] [G loss: 0.915990]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 63/86 [loss: 0.093332, acc.: 97.66%] [G loss: 0.999467]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 64/86 [loss: 0.090318, acc.: 98.29%] [G loss: 0.973786]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 65/86 [loss: 0.088399, acc.: 98.34%] [G loss: 0.961290]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 66/86 [loss: 0.117618, acc.: 96.83%] [G loss: 0.942957]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 67/86 [loss: 0.095994, acc.: 97.66%] [G loss: 0.936838]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 68/86 [loss: 0.108289, acc.: 97.61%] [G loss: 0.985577]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 69/86 [loss: 0.099962, acc.: 97.90%] [G loss: 0.986225]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 70/86 [loss: 0.113476, acc.: 97.36%] [G loss: 1.010033]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 71/86 [loss: 0.089624, acc.: 98.19%] [G loss: 1.027973]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 72/86 [loss: 0.125639, acc.: 97.22%] [G loss: 0.933134]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 73/86 [loss: 0.123133, acc.: 96.53%] [G loss: 0.976840]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 74/86 [loss: 0.095961, acc.: 97.71%] [G loss: 1.040865]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 75/86 [loss: 0.085713, acc.: 98.73%] [G loss: 0.985523]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 76/86 [loss: 0.084977, acc.: 98.49%] [G loss: 0.989391]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 77/86 [loss: 0.100488, acc.: 98.00%] [G loss: 0.985986]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 78/86 [loss: 0.110573, acc.: 97.46%] [G loss: 0.953520]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 79/86 [loss: 0.118018, acc.: 96.63%] [G loss: 0.971333]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 80/86 [loss: 0.082440, acc.: 98.54%] [G loss: 0.960476]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 81/86 [loss: 0.116334, acc.: 97.36%] [G loss: 1.039207]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 82/86 [loss: 0.106023, acc.: 97.56%] [G loss: 1.010134]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 5/200  Batch Size: 83/86 [loss: 0.093740, acc.: 97.90%] [G loss: 0.966462]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 84/86 [loss: 0.090075, acc.: 98.05%] [G loss: 0.923111]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 5/200  Batch Size: 85/86 [loss: 0.098255, acc.: 97.80%] [G loss: 0.976393]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 0/86 [loss: 0.094967, acc.: 98.00%] [G loss: 0.980548]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 1/86 [loss: 0.093646, acc.: 97.95%] [G loss: 1.001150]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 2/86 [loss: 0.108764, acc.: 96.88%] [G loss: 0.996586]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 3/86 [loss: 0.094449, acc.: 98.19%] [G loss: 1.038762]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 4/86 [loss: 0.101909, acc.: 97.75%] [G loss: 0.984930]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 5/86 [loss: 0.121971, acc.: 97.07%] [G loss: 0.991088]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 6/86 [loss: 0.113529, acc.: 97.12%] [G loss: 0.975180]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 7/86 [loss: 0.090933, acc.: 98.24%] [G loss: 1.046342]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 8/86 [loss: 0.099005, acc.: 97.85%] [G loss: 1.020664]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 9/86 [loss: 0.096289, acc.: 97.90%] [G loss: 1.029433]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 10/86 [loss: 0.103720, acc.: 98.10%] [G loss: 0.984848]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 11/86 [loss: 0.101710, acc.: 97.22%] [G loss: 1.021958]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 12/86 [loss: 0.102455, acc.: 98.05%] [G loss: 1.009440]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 13/86 [loss: 0.102374, acc.: 97.36%] [G loss: 0.990268]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 14/86 [loss: 0.091435, acc.: 98.58%] [G loss: 0.974427]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 15/86 [loss: 0.103989, acc.: 97.51%] [G loss: 0.998113]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 16/86 [loss: 0.114568, acc.: 97.17%] [G loss: 1.020275]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 17/86 [loss: 0.093279, acc.: 98.29%] [G loss: 1.009220]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 18/86 [loss: 0.115380, acc.: 97.36%] [G loss: 0.935929]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 19/86 [loss: 0.097639, acc.: 97.75%] [G loss: 0.969754]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 20/86 [loss: 0.105137, acc.: 97.71%] [G loss: 0.961514]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 21/86 [loss: 0.110101, acc.: 97.51%] [G loss: 1.018528]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 22/86 [loss: 0.107555, acc.: 96.97%] [G loss: 0.952003]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 23/86 [loss: 0.092282, acc.: 97.80%] [G loss: 0.941518]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 24/86 [loss: 0.106300, acc.: 97.85%] [G loss: 1.005918]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 25/86 [loss: 0.095570, acc.: 98.05%] [G loss: 1.021616]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 26/86 [loss: 0.118810, acc.: 96.83%] [G loss: 0.942482]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 27/86 [loss: 0.097328, acc.: 98.10%] [G loss: 1.041432]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 28/86 [loss: 0.097134, acc.: 98.14%] [G loss: 0.966045]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 29/86 [loss: 0.094236, acc.: 98.24%] [G loss: 0.981029]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 30/86 [loss: 0.088006, acc.: 98.19%] [G loss: 0.991690]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 31/86 [loss: 0.089396, acc.: 97.95%] [G loss: 0.977772]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 32/86 [loss: 0.097728, acc.: 98.00%] [G loss: 1.044572]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 33/86 [loss: 0.097702, acc.: 97.75%] [G loss: 1.002691]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 34/86 [loss: 0.087796, acc.: 98.54%] [G loss: 0.951251]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 35/86 [loss: 0.077885, acc.: 99.07%] [G loss: 0.995089]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 36/86 [loss: 0.113679, acc.: 97.36%] [G loss: 0.976161]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 37/86 [loss: 0.094908, acc.: 98.10%] [G loss: 1.019722]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 38/86 [loss: 0.087819, acc.: 98.24%] [G loss: 0.985129]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 39/86 [loss: 0.090158, acc.: 98.00%] [G loss: 0.987639]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 40/86 [loss: 0.094553, acc.: 98.29%] [G loss: 0.955296]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 41/86 [loss: 0.084366, acc.: 98.63%] [G loss: 0.983878]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 42/86 [loss: 0.110326, acc.: 97.31%] [G loss: 0.991414]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 43/86 [loss: 0.098105, acc.: 98.00%] [G loss: 0.947675]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 44/86 [loss: 0.097585, acc.: 98.10%] [G loss: 0.994449]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 45/86 [loss: 0.114930, acc.: 97.51%] [G loss: 0.968182]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 46/86 [loss: 0.097265, acc.: 97.80%] [G loss: 0.990498]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 47/86 [loss: 0.113428, acc.: 96.97%] [G loss: 0.971917]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 48/86 [loss: 0.097296, acc.: 97.85%] [G loss: 1.009373]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 49/86 [loss: 0.101483, acc.: 98.00%] [G loss: 0.988063]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 50/86 [loss: 0.086192, acc.: 98.29%] [G loss: 1.017787]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 51/86 [loss: 0.104247, acc.: 97.56%] [G loss: 0.951330]\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Epoch: 6/200  Batch Size: 52/86 [loss: 0.100534, acc.: 97.75%] [G loss: 0.973217]\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "Epoch: 6/200  Batch Size: 53/86 [loss: 0.111841, acc.: 97.27%] [G loss: 1.051929]\n",
      "32/32 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dcgan \u001b[38;5;241m=\u001b[39m DCGAN(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 146\u001b[0m, in \u001b[0;36mDCGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval, gen_steps)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    145\u001b[0m     noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim))\n\u001b[1;32m--> 146\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Plot the progress\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m  Batch Size: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m [loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, acc.: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch,epochs,batch,batches_per_epoch, d_loss[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39md_loss[\u001b[38;5;241m1\u001b[39m], g_loss))\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2383\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m   2381\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m-> 2383\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n\u001b[0;32m   2385\u001b[0m     \u001b[39mreturn\u001b[39;00m logs\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN(28,28,1)\n",
    "dcgan.train(epochs=200, batch_size=1024, save_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = Model([z, label], valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = in_shape[0] * in_shape[1]\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        merge = Concatenate()([in_image, li])\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(merge)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Conv2D(128, (3, 3), strides=(2, 2), padding='same')(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        fe = Dropout(0.4)(fe)\n",
    "        out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "        model = Model([in_image, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 128 * 7 * 7\n",
    "        gen = Dense(n_nodes)(in_lat)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Reshape((7, 7, 128))(gen) \n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(merge)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        gen = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same')(gen)\n",
    "        gen = LeakyReLU(alpha=0.2)(gen)\n",
    "        out_layer = Conv2D(1, (7, 7), activation='tanh', padding='same')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes  # Ensure labels are within valid range\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"CGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('CGAN_mnist', exist_ok=True)\n",
    "        fig.savefig(\"CGAN_mnist/CGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=1024, save_interval=1, gen_steps=3):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))  # Real labels\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))  # Fake labels\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, y_train[idx]], labels_real)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, gen_labels], labels_fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                g_loss = None\n",
    "                for _ in range(gen_steps):\n",
    "                    noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                    gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))  # Ensure valid range\n",
    "                    valid_y = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([noise, gen_labels], valid_y)\n",
    "\n",
    "                # Print the progress\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss}]\")\n",
    "\n",
    "            if (epoch) % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set image dimensions\n",
    "# img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# # Instantiate and train the DCGAN\n",
    "# cgan = CGAN(img_rows, img_cols, channels)\n",
    "# cgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN:\n",
    "    def __init__(self, rows, cols, channels, z=100, num_classes=26):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        self.num_classes = num_classes\n",
    "        optimizer = Adam(0.00002, 0.5)\n",
    "        self.discriminator = self.define_discriminator(self.img_shape, self.num_classes)\n",
    "        self.generator = self.define_generator(self.latent_dim, self.num_classes)\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([z, label])\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator([img, label])\n",
    "        self.combined = self.define_gan(self.generator,self.discriminator)\n",
    "\n",
    "        \n",
    "    def define_discriminator(self, in_shape, n_classes):\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        in_image = Input(shape=in_shape)\n",
    "        fe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(64, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Conv2D(256, (3,3), padding='same', kernel_initializer=init)(fe)\n",
    "        fe = BatchNormalization()(fe)\n",
    "        fe = LeakyReLU(alpha=0.2)(fe)\n",
    "        fe = Dropout(0.5)(fe)\n",
    "        fe = Flatten()(fe)\n",
    "        out1 = Dense(1, activation='sigmoid')(fe)\n",
    "        out2 = Dense(n_classes, activation='softmax')(fe)\n",
    "        model = Model(in_image, [out1, out2])\n",
    "        opt = Adam(lr=0.00002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    def define_generator(self, latent_dim, n_classes):\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        in_label = Input(shape=(1,))\n",
    "        li = Embedding(n_classes, 50)(in_label)\n",
    "        n_nodes = 7 * 7\n",
    "        li = Dense(n_nodes, kernel_initializer=init)(li)\n",
    "        li = Reshape((7, 7, 1))(li)\n",
    "        in_lat = Input(shape=(latent_dim,))\n",
    "        n_nodes = 512 * 7 * 7\n",
    "        gen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Reshape((7, 7, 512))(gen)\n",
    "        merge = Concatenate()([gen, li])\n",
    "        gen = Conv2DTranspose(192, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(merge)\n",
    "        gen = BatchNormalization()(gen)\n",
    "        gen = Activation('relu')(gen)\n",
    "        gen = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "        out_layer = Activation('tanh')(gen)\n",
    "        model = Model([in_lat, in_label], out_layer)\n",
    "        return model\n",
    "    \n",
    "        # define the combined generator and discriminator model, for updating the generator\n",
    "    def define_gan(self, g_model, d_model):\n",
    "        # make weights in the discriminator not trainable\n",
    "        for layer in d_model.layers:\n",
    "            if not isinstance(layer, BatchNormalization):\n",
    "                layer.trainable = False\n",
    "        # connect the outputs of the generator to the inputs of the discriminator\n",
    "        gan_output = d_model(g_model.output)\n",
    "        # define gan model as taking noise and label and outputting real/fake and label outputs\n",
    "        model = Model(g_model.input, gan_output)\n",
    "        # compile model\n",
    "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "        model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], optimizer=opt)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.arange(0, r * c).reshape(-1, 1) % self.num_classes\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                # axs[i, j].set_title(chr(sampled_labels[cnt][0] + 65))\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"ACGAN (Epoch {epoch})\", fontsize=16)\n",
    "        os.makedirs('ACGAN_mnist_2', exist_ok=True)\n",
    "        fig.savefig(\"ACGAN_mnist_2/ACGAN_mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def generate_latent_points(self, latent_dim, n_samples, n_classes=26):\n",
    "        # generate points in the latent space\n",
    "        x_input = randn(latent_dim * n_samples)\n",
    "        # reshape into a batch of inputs for the network\n",
    "        z_input = x_input.reshape(n_samples, latent_dim)\n",
    "        # generate labels\n",
    "        labels = randint(0, n_classes, n_samples)\n",
    "        return [z_input, labels]\n",
    "\n",
    "\n",
    "    def train(self, epochs=200, batch_size=2056, save_interval=1, gen_steps=1):\n",
    "        X_train = X_pre\n",
    "        y_train = y_pre\n",
    "\n",
    "        batches_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(batches_per_epoch):\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                labels_real = np.ones((batch_size, 1))\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                gen_labels = np.random.randint(0, self.num_classes, (batch_size, 1))\n",
    "                gen_imgs = self.generator.predict([noise, gen_labels])\n",
    "                labels_fake = np.zeros((batch_size, 1))\n",
    "\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, [labels_real, y_train[idx]])\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [labels_fake, gen_labels])\n",
    "                d_loss_1 = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])\n",
    "                d_loss_2 = 0.5 * np.add(d_loss_real[1], d_loss_fake[1])\n",
    "\n",
    "                for _ in range(gen_steps):\n",
    "                    z_input, z_labels = self.generate_latent_points(self.latent_dim, batch_size)\n",
    "                    y_gan = np.ones((batch_size, 1))\n",
    "                    g_loss = self.combined.train_on_batch([z_input, z_labels], [y_gan, z_labels])\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch + 1}/{batches_per_epoch} [D loss 1: {d_loss_1}, D loss 2: {d_loss_2}, G loss: {g_loss}]\")\n",
    "\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 1/43 [D loss 1: 5.07591700553894, D loss 2: 0.8845160901546478, G loss: [3.950083017349243, 0.6920515894889832, 3.2580313682556152]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 2/43 [D loss 1: 5.070025205612183, D loss 2: 0.9115675687789917, G loss: [3.9487624168395996, 0.6902897357940674, 3.2584726810455322]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 3/43 [D loss 1: 5.0332841873168945, D loss 2: 0.8931187093257904, G loss: [3.9468448162078857, 0.6886608600616455, 3.2581839561462402]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 4/43 [D loss 1: 4.989941120147705, D loss 2: 0.8722847700119019, G loss: [3.9449925422668457, 0.6870461702346802, 3.257946491241455]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 5/43 [D loss 1: 4.971191883087158, D loss 2: 0.8598926067352295, G loss: [3.944643497467041, 0.6860059499740601, 3.2586376667022705]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 6/43 [D loss 1: 4.904329538345337, D loss 2: 0.8344956934452057, G loss: [3.9438390731811523, 0.6852750778198242, 3.258563995361328]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 7/43 [D loss 1: 4.913269758224487, D loss 2: 0.8314395546913147, G loss: [3.9424211978912354, 0.6839473843574524, 3.2584738731384277]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 8/43 [D loss 1: 4.860095739364624, D loss 2: 0.7906611859798431, G loss: [3.942145347595215, 0.682774543762207, 3.259370803833008]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 9/43 [D loss 1: 4.84284234046936, D loss 2: 0.7864927053451538, G loss: [3.9409427642822266, 0.6821892261505127, 3.258753538131714]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 10/43 [D loss 1: 4.830370903015137, D loss 2: 0.7657702565193176, G loss: [3.9419467449188232, 0.6818628907203674, 3.2600839138031006]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 11/43 [D loss 1: 4.75626802444458, D loss 2: 0.7507375776767731, G loss: [3.9409122467041016, 0.6806734204292297, 3.2602388858795166]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 12/43 [D loss 1: 4.819644927978516, D loss 2: 0.7463622093200684, G loss: [3.940704584121704, 0.6791061758995056, 3.2615983486175537]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 13/43 [D loss 1: 4.7850682735443115, D loss 2: 0.7359172403812408, G loss: [3.9358577728271484, 0.6789857149124146, 3.2568719387054443]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 14/43 [D loss 1: 4.818985462188721, D loss 2: 0.7449143528938293, G loss: [3.9369213581085205, 0.6776390075683594, 3.259282350540161]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 15/43 [D loss 1: 4.767596483230591, D loss 2: 0.7248322069644928, G loss: [3.939115524291992, 0.6780299544334412, 3.2610855102539062]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 16/43 [D loss 1: 4.743318796157837, D loss 2: 0.7359158396720886, G loss: [3.937574863433838, 0.6769357919692993, 3.260639190673828]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 17/43 [D loss 1: 4.784788608551025, D loss 2: 0.7724103331565857, G loss: [3.9399352073669434, 0.6767373085021973, 3.263197898864746]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 18/43 [D loss 1: 4.7842116355896, D loss 2: 0.777816891670227, G loss: [3.9400033950805664, 0.6766061782836914, 3.263397216796875]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 19/43 [D loss 1: 4.817892789840698, D loss 2: 0.7937978506088257, G loss: [3.9377219676971436, 0.6756420731544495, 3.262079954147339]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 20/43 [D loss 1: 4.747493743896484, D loss 2: 0.7560080587863922, G loss: [3.939810037612915, 0.6751871705055237, 3.264622926712036]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 21/43 [D loss 1: 4.759363174438477, D loss 2: 0.7320486009120941, G loss: [3.9415171146392822, 0.6764867305755615, 3.2650303840637207]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 22/43 [D loss 1: 4.765618801116943, D loss 2: 0.7428616583347321, G loss: [3.933438777923584, 0.6737281680107117, 3.2597105503082275]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 23/43 [D loss 1: 4.787380933761597, D loss 2: 0.7566502094268799, G loss: [3.9395387172698975, 0.6751462817192078, 3.264392375946045]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 24/43 [D loss 1: 4.8054680824279785, D loss 2: 0.7907784283161163, G loss: [3.938406467437744, 0.6756632924079895, 3.2627432346343994]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 25/43 [D loss 1: 4.807112216949463, D loss 2: 0.7919360399246216, G loss: [3.935399055480957, 0.6732443571090698, 3.2621548175811768]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 26/43 [D loss 1: 4.770796537399292, D loss 2: 0.7652708292007446, G loss: [3.943080186843872, 0.6760971546173096, 3.2669830322265625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 27/43 [D loss 1: 4.686900854110718, D loss 2: 0.7383921444416046, G loss: [3.9395689964294434, 0.6747981309890747, 3.264770984649658]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 28/43 [D loss 1: 4.733527421951294, D loss 2: 0.7249582707881927, G loss: [3.9451186656951904, 0.6737236380577087, 3.271394968032837]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 29/43 [D loss 1: 4.704082250595093, D loss 2: 0.7287471294403076, G loss: [3.9416909217834473, 0.6719011068344116, 3.269789934158325]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 30/43 [D loss 1: 4.699909925460815, D loss 2: 0.7033473551273346, G loss: [3.9370062351226807, 0.6720938086509705, 3.2649123668670654]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 31/43 [D loss 1: 4.665231943130493, D loss 2: 0.6946789622306824, G loss: [3.939164638519287, 0.6712476015090942, 3.2679171562194824]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 32/43 [D loss 1: 4.6573731899261475, D loss 2: 0.7030405402183533, G loss: [3.941307544708252, 0.6716607213020325, 3.2696468830108643]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 33/43 [D loss 1: 4.670935153961182, D loss 2: 0.6853354573249817, G loss: [3.9404187202453613, 0.6711811423301697, 3.269237518310547]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 34/43 [D loss 1: 4.634076833724976, D loss 2: 0.7012009918689728, G loss: [3.9431912899017334, 0.6709563732147217, 3.2722349166870117]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 35/43 [D loss 1: 4.681121110916138, D loss 2: 0.7142938673496246, G loss: [3.937673568725586, 0.6722007989883423, 3.265472650527954]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 36/43 [D loss 1: 4.659386157989502, D loss 2: 0.6957628428936005, G loss: [3.938988208770752, 0.6715925335884094, 3.2673957347869873]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 37/43 [D loss 1: 4.529066324234009, D loss 2: 0.6283451616764069, G loss: [3.943176031112671, 0.6705668568611145, 3.272609233856201]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 38/43 [D loss 1: 4.590491056442261, D loss 2: 0.6540825963020325, G loss: [3.936997890472412, 0.6696285009384155, 3.267369270324707]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 39/43 [D loss 1: 4.646396160125732, D loss 2: 0.6629370748996735, G loss: [3.941500663757324, 0.6688576936721802, 3.2726430892944336]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 40/43 [D loss 1: 4.612391948699951, D loss 2: 0.6805338263511658, G loss: [3.944650411605835, 0.6683120131492615, 3.2763383388519287]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 41/43 [D loss 1: 4.6187756061553955, D loss 2: 0.652235209941864, G loss: [3.9449386596679688, 0.669043242931366, 3.275895357131958]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 42/43 [D loss 1: 4.624312877655029, D loss 2: 0.6892585754394531, G loss: [3.9389195442199707, 0.6682676076889038, 3.2706518173217773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200, Batch 43/43 [D loss 1: 4.645179510116577, D loss 2: 0.6914825439453125, G loss: [3.9379210472106934, 0.6700860857963562, 3.2678349018096924]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 1/43 [D loss 1: 4.597984075546265, D loss 2: 0.701659619808197, G loss: [3.9453446865081787, 0.6686552166938782, 3.2766895294189453]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 2/43 [D loss 1: 4.579174518585205, D loss 2: 0.6677500903606415, G loss: [3.9432485103607178, 0.6689612865447998, 3.274287223815918]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 3/43 [D loss 1: 4.581939220428467, D loss 2: 0.6974158585071564, G loss: [3.937408924102783, 0.66766357421875, 3.269745349884033]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 4/43 [D loss 1: 4.5834736824035645, D loss 2: 0.6845941841602325, G loss: [3.944072723388672, 0.667485237121582, 3.27658748626709]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 5/43 [D loss 1: 4.645793914794922, D loss 2: 0.7234008312225342, G loss: [3.9534480571746826, 0.6659178137779236, 3.2875301837921143]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 6/43 [D loss 1: 4.584415435791016, D loss 2: 0.7069717943668365, G loss: [3.944124460220337, 0.6671634316444397, 3.276961088180542]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 7/43 [D loss 1: 4.58344030380249, D loss 2: 0.7177419364452362, G loss: [3.9404287338256836, 0.6664404273033142, 3.2739882469177246]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 8/43 [D loss 1: 4.572600364685059, D loss 2: 0.6935344338417053, G loss: [3.94962215423584, 0.667811930179596, 3.2818102836608887]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 9/43 [D loss 1: 4.597766399383545, D loss 2: 0.7343233525753021, G loss: [3.9364285469055176, 0.6660501956939697, 3.270378351211548]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 10/43 [D loss 1: 4.608603477478027, D loss 2: 0.7501964271068573, G loss: [3.952569007873535, 0.6695734262466431, 3.2829957008361816]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 11/43 [D loss 1: 4.626301288604736, D loss 2: 0.7561599612236023, G loss: [3.9526877403259277, 0.6680288314819336, 3.284658908843994]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 12/43 [D loss 1: 4.655248165130615, D loss 2: 0.7565250992774963, G loss: [3.957913637161255, 0.6684708595275879, 3.289442777633667]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 13/43 [D loss 1: 4.609448194503784, D loss 2: 0.7700924277305603, G loss: [3.951084613800049, 0.6672946214675903, 3.283790111541748]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 14/43 [D loss 1: 4.61267876625061, D loss 2: 0.7561737596988678, G loss: [3.9475748538970947, 0.6670463681221008, 3.2805285453796387]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 15/43 [D loss 1: 4.597261428833008, D loss 2: 0.7862518429756165, G loss: [3.9510176181793213, 0.6689128279685974, 3.282104730606079]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 16/43 [D loss 1: 4.588409185409546, D loss 2: 0.7674822807312012, G loss: [3.962750196456909, 0.6708889603614807, 3.2918612957000732]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 17/43 [D loss 1: 4.698210954666138, D loss 2: 0.848526656627655, G loss: [3.955287456512451, 0.6687352061271667, 3.2865521907806396]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 18/43 [D loss 1: 4.660617113113403, D loss 2: 0.8248028755187988, G loss: [3.963026285171509, 0.6669012904167175, 3.2961249351501465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 19/43 [D loss 1: 4.606550693511963, D loss 2: 0.8136124908924103, G loss: [3.962890386581421, 0.6724390387535095, 3.2904512882232666]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 20/43 [D loss 1: 4.601467132568359, D loss 2: 0.8063062727451324, G loss: [3.9595818519592285, 0.6725807189941406, 3.287001132965088]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 21/43 [D loss 1: 4.6366119384765625, D loss 2: 0.8023217618465424, G loss: [3.950254440307617, 0.6695722341537476, 3.28068208694458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 22/43 [D loss 1: 4.628800392150879, D loss 2: 0.8323771357536316, G loss: [3.957864284515381, 0.6721450686454773, 3.285719156265259]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 23/43 [D loss 1: 4.642289876937866, D loss 2: 0.838407427072525, G loss: [3.9555206298828125, 0.6718036532402039, 3.283716917037964]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 24/43 [D loss 1: 4.634688854217529, D loss 2: 0.8098551034927368, G loss: [3.964282989501953, 0.67592853307724, 3.2883543968200684]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 25/43 [D loss 1: 4.621780872344971, D loss 2: 0.8352424502372742, G loss: [3.9588799476623535, 0.6727767586708069, 3.2861032485961914]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 26/43 [D loss 1: 4.6294779777526855, D loss 2: 0.8418627083301544, G loss: [3.960911273956299, 0.6703875064849854, 3.2905237674713135]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 27/43 [D loss 1: 4.624643564224243, D loss 2: 0.8348740637302399, G loss: [3.970545768737793, 0.6684086322784424, 3.3021371364593506]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 28/43 [D loss 1: 4.5855560302734375, D loss 2: 0.817063570022583, G loss: [3.965841770172119, 0.6706946492195129, 3.295147180557251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 29/43 [D loss 1: 4.601551532745361, D loss 2: 0.8117220401763916, G loss: [3.968998908996582, 0.6720642447471619, 3.2969346046447754]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 30/43 [D loss 1: 4.633988857269287, D loss 2: 0.8283922672271729, G loss: [3.962944507598877, 0.6735861897468567, 3.289358377456665]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 31/43 [D loss 1: 4.609807729721069, D loss 2: 0.8334704041481018, G loss: [3.970200538635254, 0.678292453289032, 3.291908025741577]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 32/43 [D loss 1: 4.5454089641571045, D loss 2: 0.8019493520259857, G loss: [3.9727156162261963, 0.6728635430335999, 3.299852132797241]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 33/43 [D loss 1: 4.517362594604492, D loss 2: 0.7709218859672546, G loss: [3.9726603031158447, 0.6790807247161865, 3.293579578399658]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 34/43 [D loss 1: 4.529233932495117, D loss 2: 0.7638784348964691, G loss: [3.9883501529693604, 0.6806706190109253, 3.3076794147491455]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 35/43 [D loss 1: 4.540825366973877, D loss 2: 0.7720483541488647, G loss: [3.977139711380005, 0.6801956295967102, 3.2969441413879395]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 36/43 [D loss 1: 4.467360973358154, D loss 2: 0.7484900057315826, G loss: [3.987091064453125, 0.6801263093948364, 3.306964635848999]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 37/43 [D loss 1: 4.485657215118408, D loss 2: 0.7356580197811127, G loss: [3.9873440265655518, 0.6792259812355042, 3.3081181049346924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 38/43 [D loss 1: 4.476463079452515, D loss 2: 0.7365510165691376, G loss: [3.9699597358703613, 0.679696798324585, 3.2902629375457764]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 39/43 [D loss 1: 4.434771537780762, D loss 2: 0.6938517093658447, G loss: [3.988783359527588, 0.6792235374450684, 3.3095595836639404]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 40/43 [D loss 1: 4.4037206172943115, D loss 2: 0.6993520855903625, G loss: [3.984093189239502, 0.6842818856239319, 3.299811363220215]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 41/43 [D loss 1: 4.43590235710144, D loss 2: 0.7036978304386139, G loss: [3.9820632934570312, 0.6792423129081726, 3.302820920944214]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 42/43 [D loss 1: 4.3978376388549805, D loss 2: 0.6854125261306763, G loss: [3.9773850440979004, 0.6827100515365601, 3.294674873352051]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 2/200, Batch 43/43 [D loss 1: 4.358840823173523, D loss 2: 0.6855015158653259, G loss: [3.979038715362549, 0.6781919598579407, 3.300846815109253]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 1/43 [D loss 1: 4.373663663864136, D loss 2: 0.6881555914878845, G loss: [3.982062816619873, 0.6766807436943054, 3.305382013320923]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 2/43 [D loss 1: 4.403443455696106, D loss 2: 0.7025763392448425, G loss: [3.9787449836730957, 0.6738671660423279, 3.304877758026123]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 3/43 [D loss 1: 4.419425964355469, D loss 2: 0.7092121839523315, G loss: [3.9917585849761963, 0.6821235418319702, 3.3096354007720947]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 4/43 [D loss 1: 4.352279424667358, D loss 2: 0.694986879825592, G loss: [3.987779140472412, 0.6702260971069336, 3.3175530433654785]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 5/43 [D loss 1: 4.417563199996948, D loss 2: 0.7165413498878479, G loss: [3.991927146911621, 0.6794511079788208, 3.31247615814209]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 6/43 [D loss 1: 4.38386070728302, D loss 2: 0.7518024444580078, G loss: [3.986557960510254, 0.6731730699539185, 3.313385009765625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 7/43 [D loss 1: 4.355570316314697, D loss 2: 0.7370861172676086, G loss: [3.9882564544677734, 0.679893970489502, 3.3083627223968506]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 8/43 [D loss 1: 4.42660927772522, D loss 2: 0.7927375435829163, G loss: [4.013779163360596, 0.6848878264427185, 3.3288915157318115]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 9/43 [D loss 1: 4.507183313369751, D loss 2: 0.8725006580352783, G loss: [4.006386756896973, 0.683926522731781, 3.322460412979126]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 10/43 [D loss 1: 4.49257755279541, D loss 2: 0.8451419770717621, G loss: [4.003791809082031, 0.68621826171875, 3.3175737857818604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 11/43 [D loss 1: 4.548160076141357, D loss 2: 0.9028001427650452, G loss: [4.001100540161133, 0.6878042221069336, 3.3132965564727783]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 12/43 [D loss 1: 4.503247380256653, D loss 2: 0.8942381739616394, G loss: [4.000831604003906, 0.6845834255218506, 3.3162479400634766]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 13/43 [D loss 1: 4.481887102127075, D loss 2: 0.8568393588066101, G loss: [4.0073089599609375, 0.6913287043571472, 3.3159804344177246]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 14/43 [D loss 1: 4.414548397064209, D loss 2: 0.8293868601322174, G loss: [3.9973106384277344, 0.6870320439338684, 3.31027889251709]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 15/43 [D loss 1: 4.449601411819458, D loss 2: 0.8373588919639587, G loss: [4.011338233947754, 0.6900577545166016, 3.3212802410125732]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 16/43 [D loss 1: 4.457760334014893, D loss 2: 0.8531308770179749, G loss: [4.014747619628906, 0.6886811256408691, 3.326066255569458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 17/43 [D loss 1: 4.458022356033325, D loss 2: 0.8590887188911438, G loss: [4.030500411987305, 0.6926407814025879, 3.337859630584717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 18/43 [D loss 1: 4.38360333442688, D loss 2: 0.8350542783737183, G loss: [4.0072221755981445, 0.6914150714874268, 3.3158071041107178]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 19/43 [D loss 1: 4.380320310592651, D loss 2: 0.8196249604225159, G loss: [4.010223865509033, 0.6889923214912415, 3.3212316036224365]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 20/43 [D loss 1: 4.396154880523682, D loss 2: 0.8562540709972382, G loss: [4.022914886474609, 0.6902017593383789, 3.3327128887176514]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 21/43 [D loss 1: 4.395838141441345, D loss 2: 0.8450175523757935, G loss: [4.006133079528809, 0.6833035349845886, 3.3228297233581543]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 22/43 [D loss 1: 4.3665231466293335, D loss 2: 0.8301506638526917, G loss: [4.012240409851074, 0.6902287006378174, 3.322011947631836]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 23/43 [D loss 1: 4.3612518310546875, D loss 2: 0.8410300612449646, G loss: [4.0261406898498535, 0.6928794980049133, 3.333261013031006]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 24/43 [D loss 1: 4.402194857597351, D loss 2: 0.8331001698970795, G loss: [4.020416736602783, 0.6915480494499207, 3.328868865966797]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 25/43 [D loss 1: 4.336475610733032, D loss 2: 0.8240707814693451, G loss: [4.0082573890686035, 0.6954164505004883, 3.3128409385681152]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 26/43 [D loss 1: 4.330551266670227, D loss 2: 0.8231146335601807, G loss: [4.016005516052246, 0.6950288414955139, 3.320976734161377]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 27/43 [D loss 1: 4.385509967803955, D loss 2: 0.8506339490413666, G loss: [4.037339210510254, 0.6960027813911438, 3.341336488723755]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 28/43 [D loss 1: 4.3781352043151855, D loss 2: 0.8672372400760651, G loss: [4.022403717041016, 0.6939507722854614, 3.3284528255462646]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 29/43 [D loss 1: 4.386341333389282, D loss 2: 0.8731017708778381, G loss: [4.022605895996094, 0.6875107288360596, 3.3350954055786133]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 30/43 [D loss 1: 4.352695941925049, D loss 2: 0.8586775064468384, G loss: [4.018411159515381, 0.6912834644317627, 3.327127695083618]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 31/43 [D loss 1: 4.398108720779419, D loss 2: 0.909452885389328, G loss: [4.005258560180664, 0.6878569722175598, 3.31740140914917]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 32/43 [D loss 1: 4.413248538970947, D loss 2: 0.9157793521881104, G loss: [4.003527641296387, 0.6815061569213867, 3.322021245956421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 33/43 [D loss 1: 4.3311299085617065, D loss 2: 0.8800538778305054, G loss: [4.011931419372559, 0.6869534850120544, 3.3249778747558594]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 34/43 [D loss 1: 4.442421197891235, D loss 2: 0.9128846824169159, G loss: [4.014307498931885, 0.6801998019218445, 3.3341076374053955]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 35/43 [D loss 1: 4.366341829299927, D loss 2: 0.8969854116439819, G loss: [3.999386787414551, 0.6763184070587158, 3.323068380355835]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 36/43 [D loss 1: 4.381929397583008, D loss 2: 0.9119778573513031, G loss: [4.0034942626953125, 0.6789329051971436, 3.324561357498169]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 37/43 [D loss 1: 4.364712595939636, D loss 2: 0.9127121269702911, G loss: [4.003266334533691, 0.6680881977081299, 3.3351778984069824]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 38/43 [D loss 1: 4.365118861198425, D loss 2: 0.9498719274997711, G loss: [3.984997272491455, 0.6691372990608215, 3.315859794616699]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 39/43 [D loss 1: 4.39783239364624, D loss 2: 0.9451631307601929, G loss: [3.9955902099609375, 0.6746546030044556, 3.3209354877471924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 40/43 [D loss 1: 4.351737141609192, D loss 2: 0.914695680141449, G loss: [4.000185966491699, 0.674048125743866, 3.3261380195617676]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 41/43 [D loss 1: 4.33185076713562, D loss 2: 0.9114388525485992, G loss: [4.015169143676758, 0.6763516664505005, 3.3388173580169678]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 42/43 [D loss 1: 4.293276309967041, D loss 2: 0.8884410858154297, G loss: [3.9980099201202393, 0.6795349717140198, 3.318474769592285]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 3/200, Batch 43/43 [D loss 1: 4.231067419052124, D loss 2: 0.8533658981323242, G loss: [4.005630970001221, 0.6788656115531921, 3.326765298843384]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 1/43 [D loss 1: 4.2544344663619995, D loss 2: 0.8423746824264526, G loss: [3.9945125579833984, 0.6736536026000977, 3.320858955383301]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 2/43 [D loss 1: 4.22951340675354, D loss 2: 0.8360477685928345, G loss: [4.008101940155029, 0.681879997253418, 3.3262219429016113]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 3/43 [D loss 1: 4.194126129150391, D loss 2: 0.8338815569877625, G loss: [4.005125045776367, 0.6776587963104248, 3.3274664878845215]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 4/43 [D loss 1: 4.202982664108276, D loss 2: 0.8158175647258759, G loss: [4.001763343811035, 0.6714491844177246, 3.3303139209747314]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 5/43 [D loss 1: 4.123382329940796, D loss 2: 0.7920558750629425, G loss: [3.9997758865356445, 0.6724591255187988, 3.3273165225982666]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 6/43 [D loss 1: 4.135753750801086, D loss 2: 0.8169188797473907, G loss: [3.982945442199707, 0.6693977117538452, 3.3135478496551514]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 7/43 [D loss 1: 4.116321325302124, D loss 2: 0.7963284552097321, G loss: [3.985835552215576, 0.6728723049163818, 3.3129634857177734]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 8/43 [D loss 1: 4.065392374992371, D loss 2: 0.781928539276123, G loss: [3.9861092567443848, 0.6733179092407227, 3.312791109085083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 9/43 [D loss 1: 4.046787977218628, D loss 2: 0.7852844595909119, G loss: [3.976653575897217, 0.6752040982246399, 3.3014495372772217]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 10/43 [D loss 1: 4.047045946121216, D loss 2: 0.8046004772186279, G loss: [3.977041721343994, 0.671019971370697, 3.3060216903686523]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 11/43 [D loss 1: 4.036361575126648, D loss 2: 0.8049847185611725, G loss: [3.961948871612549, 0.6651324033737183, 3.296816349029541]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 12/43 [D loss 1: 3.979491949081421, D loss 2: 0.8283074498176575, G loss: [3.9838504791259766, 0.6675645112991333, 3.316286087036133]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 13/43 [D loss 1: 3.9754912853240967, D loss 2: 0.8024247288703918, G loss: [3.949127674102783, 0.6673444509506226, 3.281783103942871]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 14/43 [D loss 1: 3.914710760116577, D loss 2: 0.7863702178001404, G loss: [3.970275402069092, 0.6744281053543091, 3.2958474159240723]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 15/43 [D loss 1: 3.884441614151001, D loss 2: 0.7769342660903931, G loss: [3.932698965072632, 0.6671295166015625, 3.2655694484710693]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 16/43 [D loss 1: 3.9387084245681763, D loss 2: 0.8057944774627686, G loss: [3.9327564239501953, 0.6572967767715454, 3.2754595279693604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 17/43 [D loss 1: 3.7908931970596313, D loss 2: 0.7412859797477722, G loss: [3.922395706176758, 0.6738914251327515, 3.248504161834717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 18/43 [D loss 1: 3.7801620960235596, D loss 2: 0.7911359369754791, G loss: [3.9238786697387695, 0.6540050506591797, 3.26987361907959]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 19/43 [D loss 1: 3.7879180908203125, D loss 2: 0.8015595078468323, G loss: [3.8688724040985107, 0.6511021256446838, 3.2177703380584717]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 20/43 [D loss 1: 3.70202100276947, D loss 2: 0.7759096026420593, G loss: [3.8678340911865234, 0.6456192135810852, 3.222214937210083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 21/43 [D loss 1: 3.663770079612732, D loss 2: 0.7673473954200745, G loss: [3.8712456226348877, 0.6573376655578613, 3.2139079570770264]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 22/43 [D loss 1: 3.7008044719696045, D loss 2: 0.8393977880477905, G loss: [3.836562156677246, 0.6461362838745117, 3.1904258728027344]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 23/43 [D loss 1: 3.59471595287323, D loss 2: 0.7414770424365997, G loss: [3.785770893096924, 0.6547909379005432, 3.1309800148010254]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 24/43 [D loss 1: 3.5222641229629517, D loss 2: 0.8102940618991852, G loss: [3.785726547241211, 0.6424347758293152, 3.143291711807251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 25/43 [D loss 1: 3.508444309234619, D loss 2: 0.7803544998168945, G loss: [3.7502353191375732, 0.6570368409156799, 3.093198537826538]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 26/43 [D loss 1: 3.478851079940796, D loss 2: 0.8280772268772125, G loss: [3.6937661170959473, 0.6353945732116699, 3.0583715438842773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 27/43 [D loss 1: 3.333854556083679, D loss 2: 0.7399417161941528, G loss: [3.6775259971618652, 0.6452388167381287, 3.032287120819092]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 28/43 [D loss 1: 3.3328038454055786, D loss 2: 0.8293476104736328, G loss: [3.6424508094787598, 0.6392431259155273, 3.0032076835632324]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 29/43 [D loss 1: 3.3354848623275757, D loss 2: 0.717313677072525, G loss: [3.635542392730713, 0.6579939723014832, 2.977548360824585]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 30/43 [D loss 1: 3.271485686302185, D loss 2: 0.7788122594356537, G loss: [3.573418140411377, 0.6395107507705688, 2.9339072704315186]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 31/43 [D loss 1: 3.2074555158615112, D loss 2: 0.7717618048191071, G loss: [3.5301544666290283, 0.6399485468864441, 2.8902058601379395]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 32/43 [D loss 1: 3.218937873840332, D loss 2: 0.7458419501781464, G loss: [3.4901304244995117, 0.6412101984024048, 2.8489201068878174]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 33/43 [D loss 1: 3.1236408948898315, D loss 2: 0.7819456458091736, G loss: [3.4287631511688232, 0.6266705989837646, 2.8020925521850586]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 34/43 [D loss 1: 3.0740723609924316, D loss 2: 0.7643399238586426, G loss: [3.4031777381896973, 0.6418179273605347, 2.761359930038452]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 35/43 [D loss 1: 3.055077910423279, D loss 2: 0.6977293789386749, G loss: [3.3267462253570557, 0.6374773979187012, 2.6892688274383545]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 36/43 [D loss 1: 3.0500491857528687, D loss 2: 0.7147528827190399, G loss: [3.2994890213012695, 0.6342195868492126, 2.665269374847412]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 37/43 [D loss 1: 2.989657998085022, D loss 2: 0.7521062195301056, G loss: [3.2366950511932373, 0.631417453289032, 2.6052775382995605]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 38/43 [D loss 1: 2.974358558654785, D loss 2: 0.6981134116649628, G loss: [3.191150188446045, 0.6245728731155396, 2.566577434539795]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 39/43 [D loss 1: 2.8904061317443848, D loss 2: 0.7682191431522369, G loss: [3.1419436931610107, 0.6206738948822021, 2.5212697982788086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 40/43 [D loss 1: 2.891229033470154, D loss 2: 0.7198057174682617, G loss: [3.094831943511963, 0.6165229082107544, 2.478309154510498]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 41/43 [D loss 1: 2.81144917011261, D loss 2: 0.7267067730426788, G loss: [3.0592000484466553, 0.622072160243988, 2.4371278285980225]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 42/43 [D loss 1: 2.7979131937026978, D loss 2: 0.6869189739227295, G loss: [3.032911539077759, 0.631696879863739, 2.401214599609375]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 4/200, Batch 43/43 [D loss 1: 2.735121726989746, D loss 2: 0.7754694521427155, G loss: [2.992255926132202, 0.6184570789337158, 2.3737988471984863]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 1/43 [D loss 1: 2.755034923553467, D loss 2: 0.7661198079586029, G loss: [2.9822981357574463, 0.6309349536895752, 2.351363182067871]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 2/43 [D loss 1: 2.7023967504501343, D loss 2: 0.7033170163631439, G loss: [2.931335926055908, 0.6256457567214966, 2.305690288543701]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 3/43 [D loss 1: 2.6658767461776733, D loss 2: 0.752815455198288, G loss: [2.8894617557525635, 0.6272035241127014, 2.262258291244507]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 4/43 [D loss 1: 2.6844273805618286, D loss 2: 0.7524745464324951, G loss: [2.8407530784606934, 0.6365647912025452, 2.204188346862793]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 5/43 [D loss 1: 2.663143038749695, D loss 2: 0.8152001202106476, G loss: [2.8338828086853027, 0.628180980682373, 2.2057018280029297]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 6/43 [D loss 1: 2.6321370601654053, D loss 2: 0.7029477059841156, G loss: [2.8074660301208496, 0.6554361581802368, 2.1520297527313232]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 7/43 [D loss 1: 2.6002215147018433, D loss 2: 0.7620640993118286, G loss: [2.790310859680176, 0.638214111328125, 2.152096748352051]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 8/43 [D loss 1: 2.605883479118347, D loss 2: 0.6425932347774506, G loss: [2.7478089332580566, 0.6481643915176392, 2.099644422531128]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 9/43 [D loss 1: 2.6175973415374756, D loss 2: 0.8711031675338745, G loss: [2.7134106159210205, 0.6233005523681641, 2.0901100635528564]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 10/43 [D loss 1: 2.6261366605758667, D loss 2: 0.5791057199239731, G loss: [2.663585901260376, 0.6509396433830261, 2.012646198272705]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 11/43 [D loss 1: 2.6338138580322266, D loss 2: 0.9201488792896271, G loss: [2.658010244369507, 0.6282238364219666, 2.0297863483428955]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 12/43 [D loss 1: 2.4492355585098267, D loss 2: 0.6536107957363129, G loss: [2.622779369354248, 0.6564117670059204, 1.966367483139038]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 13/43 [D loss 1: 2.489737391471863, D loss 2: 0.6646066606044769, G loss: [2.56148624420166, 0.6376429200172424, 1.923843264579773]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 14/43 [D loss 1: 2.4673041105270386, D loss 2: 0.8554222881793976, G loss: [2.5178284645080566, 0.6280264854431152, 1.889802098274231]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 15/43 [D loss 1: 2.5013532638549805, D loss 2: 0.6249277293682098, G loss: [2.4848787784576416, 0.6559663414955139, 1.8289124965667725]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 16/43 [D loss 1: 2.439959764480591, D loss 2: 0.8060529232025146, G loss: [2.4653944969177246, 0.6303697824478149, 1.8350247144699097]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 17/43 [D loss 1: 2.422618865966797, D loss 2: 0.6683896481990814, G loss: [2.412339687347412, 0.6435807943344116, 1.768758773803711]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 18/43 [D loss 1: 2.4273314476013184, D loss 2: 0.816843569278717, G loss: [2.4112377166748047, 0.6252626776695251, 1.7859749794006348]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 19/43 [D loss 1: 2.4145073890686035, D loss 2: 0.6386728286743164, G loss: [2.3908884525299072, 0.653020977973938, 1.7378674745559692]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 20/43 [D loss 1: 2.384902060031891, D loss 2: 0.815075010061264, G loss: [2.3499202728271484, 0.6314993500709534, 1.7184208631515503]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 21/43 [D loss 1: 2.3364514112472534, D loss 2: 0.698426365852356, G loss: [2.3258273601531982, 0.6533212661743164, 1.6725060939788818]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 22/43 [D loss 1: 2.314793348312378, D loss 2: 0.7206433415412903, G loss: [2.2730860710144043, 0.6440277695655823, 1.6290583610534668]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 23/43 [D loss 1: 2.378523051738739, D loss 2: 0.7817190885543823, G loss: [2.2679481506347656, 0.636906623840332, 1.6310416460037231]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 24/43 [D loss 1: 2.2996007204055786, D loss 2: 0.6912529170513153, G loss: [2.242372751235962, 0.6431754231452942, 1.5991973876953125]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 25/43 [D loss 1: 2.2903682589530945, D loss 2: 0.832411527633667, G loss: [2.2138047218322754, 0.6429235339164734, 1.5708811283111572]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 26/43 [D loss 1: 2.286483407020569, D loss 2: 0.6985282003879547, G loss: [2.1754159927368164, 0.6598260402679443, 1.5155900716781616]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 27/43 [D loss 1: 2.291093111038208, D loss 2: 0.7494343221187592, G loss: [2.1464526653289795, 0.6490013003349304, 1.4974514245986938]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 28/43 [D loss 1: 2.269552707672119, D loss 2: 0.7771843373775482, G loss: [2.143981695175171, 0.6393894553184509, 1.5045922994613647]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 29/43 [D loss 1: 2.220482110977173, D loss 2: 0.7255133986473083, G loss: [2.0964949131011963, 0.6529226899147034, 1.4435722827911377]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 30/43 [D loss 1: 2.261390447616577, D loss 2: 0.7923220098018646, G loss: [2.0895581245422363, 0.6498252749443054, 1.4397327899932861]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 31/43 [D loss 1: 2.2039144039154053, D loss 2: 0.6810895800590515, G loss: [2.070728302001953, 0.6596585512161255, 1.4110697507858276]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 32/43 [D loss 1: 2.202182948589325, D loss 2: 0.7894690930843353, G loss: [2.0378518104553223, 0.6482738852500916, 1.389577865600586]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 33/43 [D loss 1: 2.196757435798645, D loss 2: 0.6816384792327881, G loss: [2.008333921432495, 0.6652175188064575, 1.3431164026260376]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 34/43 [D loss 1: 2.198372006416321, D loss 2: 0.8023935556411743, G loss: [2.008026599884033, 0.6559169888496399, 1.3521095514297485]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 35/43 [D loss 1: 2.1988377571105957, D loss 2: 0.6034373939037323, G loss: [1.961071491241455, 0.6614496111869812, 1.2996219396591187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 36/43 [D loss 1: 2.2936493158340454, D loss 2: 0.9359251856803894, G loss: [1.9882441759109497, 0.6355146169662476, 1.3527295589447021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 37/43 [D loss 1: 2.471311569213867, D loss 2: 0.4759421795606613, G loss: [1.9480582475662231, 0.6969118118286133, 1.2511464357376099]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 38/43 [D loss 1: 2.3771783113479614, D loss 2: 0.9619829654693604, G loss: [1.9217839241027832, 0.6059336066246033, 1.3158502578735352]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 39/43 [D loss 1: 2.118688225746155, D loss 2: 0.6642458140850067, G loss: [1.871199607849121, 0.6609563827514648, 1.2102432250976562]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 40/43 [D loss 1: 2.1453664302825928, D loss 2: 0.7169196605682373, G loss: [1.8761515617370605, 0.6666943430900574, 1.209457278251648]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 41/43 [D loss 1: 2.142163634300232, D loss 2: 0.8362297117710114, G loss: [1.8077304363250732, 0.6391295194625854, 1.1686009168624878]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 42/43 [D loss 1: 2.0916590690612793, D loss 2: 0.7555460929870605, G loss: [1.8164211511611938, 0.6513164043426514, 1.1651047468185425]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 5/200, Batch 43/43 [D loss 1: 2.1004804372787476, D loss 2: 0.7250344455242157, G loss: [1.7737102508544922, 0.6537391543388367, 1.1199711561203003]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 1/43 [D loss 1: 2.087941527366638, D loss 2: 0.7722101509571075, G loss: [1.7594428062438965, 0.6448076963424683, 1.1146351099014282]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 2/43 [D loss 1: 2.109066963195801, D loss 2: 0.7346806228160858, G loss: [1.7305200099945068, 0.6673351526260376, 1.0631848573684692]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 3/43 [D loss 1: 2.07627671957016, D loss 2: 0.7909389734268188, G loss: [1.703438401222229, 0.6511873006820679, 1.0522511005401611]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 4/43 [D loss 1: 2.0784939527511597, D loss 2: 0.7516616582870483, G loss: [1.6877415180206299, 0.6683340072631836, 1.0194075107574463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 5/43 [D loss 1: 2.0500649213790894, D loss 2: 0.7677445411682129, G loss: [1.6870489120483398, 0.6667147278785706, 1.020334243774414]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 6/43 [D loss 1: 2.0638004541397095, D loss 2: 0.7694892585277557, G loss: [1.663046956062317, 0.6573113203048706, 1.0057356357574463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 7/43 [D loss 1: 2.0782466530799866, D loss 2: 0.6754185259342194, G loss: [1.6332485675811768, 0.6503310203552246, 0.9829175472259521]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 8/43 [D loss 1: 2.075573980808258, D loss 2: 0.8036783039569855, G loss: [1.606102705001831, 0.6427164077758789, 0.9633862972259521]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 9/43 [D loss 1: 2.0428569316864014, D loss 2: 0.7208528816699982, G loss: [1.617969274520874, 0.6611344814300537, 0.9568347930908203]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 10/43 [D loss 1: 2.011373519897461, D loss 2: 0.7023442685604095, G loss: [1.5642205476760864, 0.6483995914459229, 0.9158209562301636]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 11/43 [D loss 1: 2.074004113674164, D loss 2: 0.8214608728885651, G loss: [1.5454511642456055, 0.6439377069473267, 0.9015134572982788]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 12/43 [D loss 1: 2.001237213611603, D loss 2: 0.7424117922782898, G loss: [1.5381132364273071, 0.6578961610794067, 0.8802170753479004]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 13/43 [D loss 1: 1.9562379121780396, D loss 2: 0.7705677449703217, G loss: [1.534883975982666, 0.6410576105117798, 0.8938263654708862]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 14/43 [D loss 1: 2.015584707260132, D loss 2: 0.669035941362381, G loss: [1.5110775232315063, 0.6452387571334839, 0.8658387660980225]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 15/43 [D loss 1: 2.0010223388671875, D loss 2: 0.7988221049308777, G loss: [1.481903314590454, 0.6333471536636353, 0.8485561013221741]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 16/43 [D loss 1: 2.0103918313980103, D loss 2: 0.7931253015995026, G loss: [1.4678070545196533, 0.6391561031341553, 0.828650951385498]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 17/43 [D loss 1: 2.0332785844802856, D loss 2: 0.6630061864852905, G loss: [1.4517033100128174, 0.6586966514587402, 0.7930067181587219]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 18/43 [D loss 1: 2.07503879070282, D loss 2: 0.8686671257019043, G loss: [1.434328317642212, 0.6251195669174194, 0.8092087507247925]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 19/43 [D loss 1: 1.935235619544983, D loss 2: 0.6791030466556549, G loss: [1.4262704849243164, 0.6522660255432129, 0.7740045189857483]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 20/43 [D loss 1: 1.9700654745101929, D loss 2: 0.840702086687088, G loss: [1.4121043682098389, 0.6336601972579956, 0.7784441709518433]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 21/43 [D loss 1: 1.9416512250900269, D loss 2: 0.7059975862503052, G loss: [1.4203767776489258, 0.6572256088256836, 0.763151228427887]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 22/43 [D loss 1: 2.011933922767639, D loss 2: 0.7477594614028931, G loss: [1.3943777084350586, 0.64838045835495, 0.7459972500801086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 23/43 [D loss 1: 1.9619499444961548, D loss 2: 0.7749089598655701, G loss: [1.3890873193740845, 0.6549704074859619, 0.7341169118881226]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 24/43 [D loss 1: 1.9795133471488953, D loss 2: 0.8039898872375488, G loss: [1.360194444656372, 0.6486713886260986, 0.7115231156349182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 25/43 [D loss 1: 1.9512619972229004, D loss 2: 0.6986000835895538, G loss: [1.3371374607086182, 0.6526902914047241, 0.6844472289085388]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 26/43 [D loss 1: 1.9852479696273804, D loss 2: 0.8284621834754944, G loss: [1.3223621845245361, 0.6227440237998962, 0.6996181607246399]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 27/43 [D loss 1: 1.9358327388763428, D loss 2: 0.6525574922561646, G loss: [1.3196520805358887, 0.6551591157913208, 0.6644929647445679]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 28/43 [D loss 1: 1.9849010705947876, D loss 2: 0.8573966324329376, G loss: [1.2851142883300781, 0.6406645178794861, 0.6444498300552368]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 29/43 [D loss 1: 1.8794505596160889, D loss 2: 0.7131164968013763, G loss: [1.2937943935394287, 0.6560041904449463, 0.6377901434898376]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 30/43 [D loss 1: 1.9513424634933472, D loss 2: 0.7453084588050842, G loss: [1.2566444873809814, 0.6341657042503357, 0.6224787831306458]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 31/43 [D loss 1: 1.9387798309326172, D loss 2: 0.7634325325489044, G loss: [1.26856529712677, 0.640305757522583, 0.628259539604187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 32/43 [D loss 1: 1.9528908729553223, D loss 2: 0.8041618764400482, G loss: [1.2508342266082764, 0.6403567790985107, 0.6104775071144104]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 33/43 [D loss 1: 1.9053893089294434, D loss 2: 0.7001895010471344, G loss: [1.2327682971954346, 0.6512759327888489, 0.5814924240112305]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 34/43 [D loss 1: 1.9853414297103882, D loss 2: 0.8561705350875854, G loss: [1.2288211584091187, 0.6195201277732849, 0.6093010306358337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 35/43 [D loss 1: 1.9188839793205261, D loss 2: 0.60556660592556, G loss: [1.2253494262695312, 0.6409163475036621, 0.5844331383705139]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 36/43 [D loss 1: 1.977709174156189, D loss 2: 0.901260495185852, G loss: [1.2137279510498047, 0.6264976859092712, 0.5872302651405334]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 37/43 [D loss 1: 1.8851234316825867, D loss 2: 0.6546306312084198, G loss: [1.1899240016937256, 0.6512103080749512, 0.5387137532234192]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 38/43 [D loss 1: 1.8881497979164124, D loss 2: 0.7921743392944336, G loss: [1.1615557670593262, 0.6192752122879028, 0.5422805547714233]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 39/43 [D loss 1: 1.8847246170043945, D loss 2: 0.7592438459396362, G loss: [1.172060251235962, 0.64235919713974, 0.5297011137008667]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 40/43 [D loss 1: 1.9524157047271729, D loss 2: 0.780724048614502, G loss: [1.135827660560608, 0.6231492161750793, 0.5126784443855286]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 41/43 [D loss 1: 1.9293287992477417, D loss 2: 0.8068923652172089, G loss: [1.1359665393829346, 0.6113203167915344, 0.5246461629867554]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 42/43 [D loss 1: 1.8975951671600342, D loss 2: 0.7949745953083038, G loss: [1.1493972539901733, 0.6309219002723694, 0.518475353717804]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 6/200, Batch 43/43 [D loss 1: 1.9079289436340332, D loss 2: 0.750560849905014, G loss: [1.1051846742630005, 0.6279304027557373, 0.4772542715072632]]\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 1/43 [D loss 1: 1.8669191598892212, D loss 2: 0.8055874407291412, G loss: [1.1135352849960327, 0.6272737383842468, 0.4862615764141083]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 2/43 [D loss 1: 1.8670032024383545, D loss 2: 0.7396155297756195, G loss: [1.1182620525360107, 0.6419243216514587, 0.4763377010822296]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 3/43 [D loss 1: 1.8823598623275757, D loss 2: 0.8060621321201324, G loss: [1.097130298614502, 0.6216424703598022, 0.4754878878593445]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 4/43 [D loss 1: 1.8400911688804626, D loss 2: 0.7529380023479462, G loss: [1.0820233821868896, 0.6242490410804749, 0.45777428150177]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 5/43 [D loss 1: 1.87808758020401, D loss 2: 0.7690450251102448, G loss: [1.0818312168121338, 0.634959876537323, 0.44687139987945557]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 6/43 [D loss 1: 1.9230499863624573, D loss 2: 0.7870380580425262, G loss: [1.0580816268920898, 0.6280429363250732, 0.4300386607646942]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 7/43 [D loss 1: 1.8533477187156677, D loss 2: 0.7479755282402039, G loss: [1.058266282081604, 0.6194839477539062, 0.43878233432769775]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 8/43 [D loss 1: 1.8351450562477112, D loss 2: 0.7199176847934723, G loss: [1.0757439136505127, 0.6488706469535828, 0.4268733263015747]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 9/43 [D loss 1: 1.8422815203666687, D loss 2: 0.7131587564945221, G loss: [1.034024953842163, 0.6186196804046631, 0.4154052138328552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 10/43 [D loss 1: 1.9484126567840576, D loss 2: 0.8767369091510773, G loss: [1.0236454010009766, 0.617771327495575, 0.40587401390075684]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 11/43 [D loss 1: 1.835508018732071, D loss 2: 0.7612493634223938, G loss: [1.0206931829452515, 0.6198238134384155, 0.40086936950683594]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 12/43 [D loss 1: 1.8466260433197021, D loss 2: 0.7453805208206177, G loss: [1.034809947013855, 0.6323552131652832, 0.40245476365089417]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 13/43 [D loss 1: 1.8328603506088257, D loss 2: 0.7527886629104614, G loss: [1.0344831943511963, 0.6418571472167969, 0.392626017332077]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 14/43 [D loss 1: 1.8733252882957458, D loss 2: 0.8175574541091919, G loss: [1.0085660219192505, 0.6225773096084595, 0.3859887421131134]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 15/43 [D loss 1: 1.8121511936187744, D loss 2: 0.7358048260211945, G loss: [1.0036147832870483, 0.6262210011482239, 0.3773937523365021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 16/43 [D loss 1: 1.8851070404052734, D loss 2: 0.7920580208301544, G loss: [0.9927977323532104, 0.62924724817276, 0.36355048418045044]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 17/43 [D loss 1: 1.9110975861549377, D loss 2: 0.8938256204128265, G loss: [1.00752592086792, 0.6377182602882385, 0.36980772018432617]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 18/43 [D loss 1: 1.858847737312317, D loss 2: 0.6046361476182938, G loss: [0.9918704032897949, 0.6414027810096741, 0.35046759247779846]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 19/43 [D loss 1: 1.9685584902763367, D loss 2: 0.9413855373859406, G loss: [0.9860180616378784, 0.6094540953636169, 0.37656399607658386]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 20/43 [D loss 1: 1.7964401245117188, D loss 2: 0.6249818652868271, G loss: [0.9875273704528809, 0.6610220670700073, 0.3265053331851959]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 21/43 [D loss 1: 1.8725274205207825, D loss 2: 0.8308491408824921, G loss: [0.9710283875465393, 0.6396497488021851, 0.33137863874435425]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 22/43 [D loss 1: 1.8938166499137878, D loss 2: 0.8123546540737152, G loss: [0.9327107667922974, 0.6139203906059265, 0.31879037618637085]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 23/43 [D loss 1: 1.8655627369880676, D loss 2: 0.7979000508785248, G loss: [0.9467611312866211, 0.6278207898139954, 0.31894031167030334]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 24/43 [D loss 1: 1.8431596159934998, D loss 2: 0.8254736065864563, G loss: [0.9345020651817322, 0.619499921798706, 0.3150021433830261]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 25/43 [D loss 1: 1.8932576179504395, D loss 2: 0.887678474187851, G loss: [0.9563455581665039, 0.6417316198348999, 0.3146139681339264]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 26/43 [D loss 1: 1.837461918592453, D loss 2: 0.7630529701709747, G loss: [0.9448081851005554, 0.6427837014198303, 0.3020244836807251]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 27/43 [D loss 1: 1.8428005874156952, D loss 2: 0.7934383451938629, G loss: [0.9237241744995117, 0.6370565891265869, 0.2866675853729248]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 28/43 [D loss 1: 1.8789138793945312, D loss 2: 0.8243609964847565, G loss: [0.9337028861045837, 0.6376146674156189, 0.29608821868896484]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 29/43 [D loss 1: 1.7938556969165802, D loss 2: 0.7588140070438385, G loss: [0.9454946517944336, 0.657065212726593, 0.28842946887016296]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 30/43 [D loss 1: 1.9070464968681335, D loss 2: 0.8739480078220367, G loss: [0.9487965703010559, 0.6509450078010559, 0.2978515625]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 31/43 [D loss 1: 1.7879997491836548, D loss 2: 0.7239483594894409, G loss: [0.9332796335220337, 0.6529651880264282, 0.2803144156932831]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 32/43 [D loss 1: 1.8311635851860046, D loss 2: 0.8248659372329712, G loss: [0.9463552832603455, 0.667258083820343, 0.27909719944000244]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 33/43 [D loss 1: 1.7949832677841187, D loss 2: 0.7236174643039703, G loss: [0.9531057476997375, 0.6735645532608032, 0.2795411944389343]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 34/43 [D loss 1: 1.752824455499649, D loss 2: 0.7265817821025848, G loss: [0.9270728230476379, 0.6517965793609619, 0.275276243686676]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 35/43 [D loss 1: 1.9262471795082092, D loss 2: 0.9001048505306244, G loss: [0.9326790571212769, 0.656324565410614, 0.27635452151298523]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 36/43 [D loss 1: 1.7981088161468506, D loss 2: 0.6728216111660004, G loss: [0.9062274694442749, 0.6412028074264526, 0.26502469182014465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 37/43 [D loss 1: 1.936017394065857, D loss 2: 0.9399389624595642, G loss: [0.9202841520309448, 0.6459618210792542, 0.2743223309516907]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 38/43 [D loss 1: 1.7695072889328003, D loss 2: 0.6852979958057404, G loss: [0.9065102338790894, 0.6551553010940552, 0.25135496258735657]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 39/43 [D loss 1: 1.8798733949661255, D loss 2: 0.8411318063735962, G loss: [0.900032639503479, 0.6432381272315979, 0.2567945122718811]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 40/43 [D loss 1: 1.846584975719452, D loss 2: 0.8309854567050934, G loss: [0.8842452168464661, 0.6456192135810852, 0.23862600326538086]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 41/43 [D loss 1: 1.8260865807533264, D loss 2: 0.802596390247345, G loss: [0.8873146176338196, 0.6505671739578247, 0.23674745857715607]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 42/43 [D loss 1: 1.7809830904006958, D loss 2: 0.7362532317638397, G loss: [0.8957158327102661, 0.6655123233795166, 0.2302035242319107]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 7/200, Batch 43/43 [D loss 1: 1.8091145753860474, D loss 2: 0.8274595737457275, G loss: [0.8917636871337891, 0.6483993530273438, 0.24336430430412292]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 1/43 [D loss 1: 1.8402597308158875, D loss 2: 0.8458036482334137, G loss: [0.8914787769317627, 0.6505990028381348, 0.24087980389595032]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 2/43 [D loss 1: 1.7556453049182892, D loss 2: 0.7712204158306122, G loss: [0.8921387791633606, 0.66293865442276, 0.22920013964176178]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 3/43 [D loss 1: 1.8349778056144714, D loss 2: 0.841178834438324, G loss: [0.8898292779922485, 0.6637805104255676, 0.2260487824678421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 4/43 [D loss 1: 1.7897595763206482, D loss 2: 0.7582050263881683, G loss: [0.8883450031280518, 0.6594656109809875, 0.22887936234474182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 5/43 [D loss 1: 1.9084312915802002, D loss 2: 0.8912112414836884, G loss: [0.8727648854255676, 0.6543617844581604, 0.21840311586856842]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 6/43 [D loss 1: 1.7737368941307068, D loss 2: 0.7752673327922821, G loss: [0.8872854709625244, 0.6753373146057129, 0.21194817125797272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 7/43 [D loss 1: 1.7713741958141327, D loss 2: 0.8175169229507446, G loss: [0.8793627619743347, 0.6665592193603516, 0.21280355751514435]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 8/43 [D loss 1: 1.7673170864582062, D loss 2: 0.7780359983444214, G loss: [0.8703840970993042, 0.6614108085632324, 0.20897327363491058]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 9/43 [D loss 1: 1.8105882108211517, D loss 2: 0.8177607655525208, G loss: [0.8741974234580994, 0.6729714274406433, 0.20122598111629486]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 10/43 [D loss 1: 1.7667303681373596, D loss 2: 0.7933732867240906, G loss: [0.8682281970977783, 0.658452570438385, 0.20977561175823212]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 11/43 [D loss 1: 1.7549498081207275, D loss 2: 0.7638983130455017, G loss: [0.8749902248382568, 0.6756875514984131, 0.19930267333984375]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 12/43 [D loss 1: 1.83994922041893, D loss 2: 0.833269476890564, G loss: [0.8647435903549194, 0.666658878326416, 0.19808469712734222]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 13/43 [D loss 1: 1.7205768823623657, D loss 2: 0.7222530245780945, G loss: [0.8619996309280396, 0.6603760123252869, 0.2016235888004303]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 14/43 [D loss 1: 1.8455491065979004, D loss 2: 0.8923244178295135, G loss: [0.8601408004760742, 0.6645958423614502, 0.19554492831230164]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 15/43 [D loss 1: 1.747382402420044, D loss 2: 0.6255773603916168, G loss: [0.8608014583587646, 0.659740686416626, 0.20106074213981628]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 16/43 [D loss 1: 1.8653151988983154, D loss 2: 0.8759492933750153, G loss: [0.8849194049835205, 0.6997594237327576, 0.18515998125076294]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 17/43 [D loss 1: 1.7927760481834412, D loss 2: 0.8257601857185364, G loss: [0.8609423041343689, 0.6717896461486816, 0.18915265798568726]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 18/43 [D loss 1: 1.7393414080142975, D loss 2: 0.6848233342170715, G loss: [0.8595100045204163, 0.6668857932090759, 0.19262421131134033]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 19/43 [D loss 1: 1.9176040291786194, D loss 2: 0.9153762459754944, G loss: [0.8762953877449036, 0.681711733341217, 0.19458366930484772]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 20/43 [D loss 1: 1.7428885102272034, D loss 2: 0.74803626537323, G loss: [0.8415921926498413, 0.6638088822364807, 0.1777832806110382]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 21/43 [D loss 1: 1.6986794769763947, D loss 2: 0.6771332025527954, G loss: [0.850959837436676, 0.6654634475708008, 0.18549638986587524]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 22/43 [D loss 1: 1.852162778377533, D loss 2: 0.823159784078598, G loss: [0.82225102186203, 0.6543596982955933, 0.16789130866527557]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 23/43 [D loss 1: 1.796493798494339, D loss 2: 0.7524102330207825, G loss: [0.8486813306808472, 0.6724742650985718, 0.17620709538459778]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 24/43 [D loss 1: 1.699842095375061, D loss 2: 0.743010550737381, G loss: [0.8451269865036011, 0.6636618375778198, 0.18146516382694244]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 25/43 [D loss 1: 1.7342790365219116, D loss 2: 0.7936651110649109, G loss: [0.8234867453575134, 0.6572531461715698, 0.1662335991859436]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 26/43 [D loss 1: 1.7496678531169891, D loss 2: 0.7473793923854828, G loss: [0.8327677249908447, 0.6697139739990234, 0.1630537509918213]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 27/43 [D loss 1: 1.6816373467445374, D loss 2: 0.6984413862228394, G loss: [0.8367882966995239, 0.6599736213684082, 0.1768147051334381]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 28/43 [D loss 1: 1.837787389755249, D loss 2: 0.8745092451572418, G loss: [0.8432314991950989, 0.6678587794303894, 0.17537270486354828]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 29/43 [D loss 1: 1.725169837474823, D loss 2: 0.6474595367908478, G loss: [0.836327314376831, 0.6687855124473572, 0.16754181683063507]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 30/43 [D loss 1: 1.8240928053855896, D loss 2: 0.8564231991767883, G loss: [0.8185438513755798, 0.6671102046966553, 0.15143363177776337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 31/43 [D loss 1: 1.7392553687095642, D loss 2: 0.808161735534668, G loss: [0.8324352502822876, 0.6788044571876526, 0.1536308228969574]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 32/43 [D loss 1: 1.6745060682296753, D loss 2: 0.6942162811756134, G loss: [0.8295547962188721, 0.6627293825149536, 0.16682541370391846]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 33/43 [D loss 1: 1.8369551301002502, D loss 2: 0.8816452324390411, G loss: [0.8230448961257935, 0.6631187796592712, 0.15992611646652222]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 34/43 [D loss 1: 1.6291632652282715, D loss 2: 0.6683166325092316, G loss: [0.831507682800293, 0.6866226196289062, 0.14488503336906433]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 35/43 [D loss 1: 1.7469860315322876, D loss 2: 0.7692644894123077, G loss: [0.8388233780860901, 0.6884594559669495, 0.15036393702030182]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 36/43 [D loss 1: 1.7771152257919312, D loss 2: 0.8002570271492004, G loss: [0.8437581062316895, 0.6877270340919495, 0.1560310572385788]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 37/43 [D loss 1: 1.7767375707626343, D loss 2: 0.8218424618244171, G loss: [0.8244526982307434, 0.6726158857345581, 0.1518368273973465]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 38/43 [D loss 1: 1.661433607339859, D loss 2: 0.7367301881313324, G loss: [0.8295122385025024, 0.6862461566925049, 0.14326611161231995]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 39/43 [D loss 1: 1.7571264505386353, D loss 2: 0.8333723247051239, G loss: [0.841681718826294, 0.6936948299407959, 0.14798685908317566]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 40/43 [D loss 1: 1.7115067839622498, D loss 2: 0.7735206186771393, G loss: [0.8599135875701904, 0.7175253629684448, 0.1423882395029068]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 41/43 [D loss 1: 1.7754001319408417, D loss 2: 0.825082927942276, G loss: [0.8409877419471741, 0.7011798620223999, 0.13980787992477417]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 42/43 [D loss 1: 1.6818299889564514, D loss 2: 0.7598327994346619, G loss: [0.8612741231918335, 0.7153556942939758, 0.14591845870018005]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 8/200, Batch 43/43 [D loss 1: 1.7587588429450989, D loss 2: 0.8003195524215698, G loss: [0.8688074350357056, 0.7211030125617981, 0.14770443737506866]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 1/43 [D loss 1: 1.692039042711258, D loss 2: 0.7549664080142975, G loss: [0.8731814026832581, 0.7147785425186157, 0.15840286016464233]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 2/43 [D loss 1: 1.8552005887031555, D loss 2: 0.8861204087734222, G loss: [0.8624323606491089, 0.724534273147583, 0.13789808750152588]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 3/43 [D loss 1: 1.6352953016757965, D loss 2: 0.6530478596687317, G loss: [0.8427923321723938, 0.7013233304023743, 0.14146901667118073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 4/43 [D loss 1: 1.806024968624115, D loss 2: 0.8808345198631287, G loss: [0.8683792352676392, 0.7362629771232605, 0.13211627304553986]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 5/43 [D loss 1: 1.7171461582183838, D loss 2: 0.765421599149704, G loss: [0.8585875034332275, 0.7116694450378418, 0.14691805839538574]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 6/43 [D loss 1: 1.732184499502182, D loss 2: 0.7996846139431, G loss: [0.8624136447906494, 0.7251202464103699, 0.13729341328144073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 7/43 [D loss 1: 1.66819566488266, D loss 2: 0.7765785753726959, G loss: [0.8667157292366028, 0.7332248091697693, 0.1334909349679947]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 8/43 [D loss 1: 1.6182207465171814, D loss 2: 0.6894863545894623, G loss: [0.8600940704345703, 0.7172197103500366, 0.1428743600845337]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 9/43 [D loss 1: 1.8196901082992554, D loss 2: 0.8842717409133911, G loss: [0.8730615377426147, 0.7435332536697388, 0.12952826917171478]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 10/43 [D loss 1: 1.6255329847335815, D loss 2: 0.6592312157154083, G loss: [0.856249988079071, 0.717575192451477, 0.1386748105287552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 11/43 [D loss 1: 1.6834671795368195, D loss 2: 0.7865748107433319, G loss: [0.8381076455116272, 0.7107321619987488, 0.12737548351287842]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 12/43 [D loss 1: 1.6392661333084106, D loss 2: 0.6930260360240936, G loss: [0.8298035860061646, 0.6906940340995789, 0.1391095668077469]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 13/43 [D loss 1: 1.7175404131412506, D loss 2: 0.7513054609298706, G loss: [0.8412503600120544, 0.7147787809371948, 0.1264715939760208]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 14/43 [D loss 1: 1.7461284399032593, D loss 2: 0.8269615769386292, G loss: [0.8604218363761902, 0.7239787578582764, 0.13644307851791382]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 15/43 [D loss 1: 1.6746683418750763, D loss 2: 0.7066468000411987, G loss: [0.8137247562408447, 0.6892964243888855, 0.12442834675312042]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 16/43 [D loss 1: 1.6790272295475006, D loss 2: 0.7353771328926086, G loss: [0.8309539556503296, 0.7037126421928406, 0.127241313457489]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 17/43 [D loss 1: 1.699215829372406, D loss 2: 0.7623102366924286, G loss: [0.8323838114738464, 0.7078232169151306, 0.12456057965755463]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 18/43 [D loss 1: 1.6642837226390839, D loss 2: 0.7382721602916718, G loss: [0.8226494789123535, 0.6990364789962769, 0.12361298501491547]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 19/43 [D loss 1: 1.6718375384807587, D loss 2: 0.7411293685436249, G loss: [0.8422046303749084, 0.7137971520423889, 0.12840749323368073]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 20/43 [D loss 1: 1.6659037470817566, D loss 2: 0.7459340989589691, G loss: [0.8619253635406494, 0.7273573279380798, 0.1345680058002472]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 21/43 [D loss 1: 1.617406189441681, D loss 2: 0.6642006933689117, G loss: [0.8207270503044128, 0.6959471702575684, 0.12477990239858627]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 22/43 [D loss 1: 1.7538845539093018, D loss 2: 0.8761314153671265, G loss: [0.8617294430732727, 0.7452237010002136, 0.11650574207305908]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 23/43 [D loss 1: 1.581718772649765, D loss 2: 0.6396035254001617, G loss: [0.8302496671676636, 0.6824358701705933, 0.1478138118982315]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 24/43 [D loss 1: 1.946024775505066, D loss 2: 1.005948156118393, G loss: [0.8902836441993713, 0.759996235370636, 0.13028742372989655]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 25/43 [D loss 1: 1.5864135026931763, D loss 2: 0.5929377675056458, G loss: [0.8707258701324463, 0.7540464401245117, 0.11667941510677338]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 26/43 [D loss 1: 1.6785675883293152, D loss 2: 0.7040345072746277, G loss: [0.819218099117279, 0.7125279903411865, 0.10669011622667313]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 27/43 [D loss 1: 1.7086012363433838, D loss 2: 0.7869568467140198, G loss: [0.831816554069519, 0.7036072611808777, 0.12820927798748016]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 28/43 [D loss 1: 1.6156622171401978, D loss 2: 0.7141001224517822, G loss: [0.8471068739891052, 0.7268828749656677, 0.12022397667169571]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 29/43 [D loss 1: 1.579797625541687, D loss 2: 0.6759030520915985, G loss: [0.8311259150505066, 0.7109634876251221, 0.12016244977712631]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 30/43 [D loss 1: 1.7538753747940063, D loss 2: 0.8678408861160278, G loss: [0.8655475378036499, 0.744208574295044, 0.12133897095918655]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 31/43 [D loss 1: 1.5760282576084137, D loss 2: 0.6640399992465973, G loss: [0.8581468462944031, 0.7389408349990845, 0.1192060336470604]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 32/43 [D loss 1: 1.6054053902626038, D loss 2: 0.6658749282360077, G loss: [0.8408334255218506, 0.7210169434547424, 0.11981645226478577]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 33/43 [D loss 1: 1.7459412813186646, D loss 2: 0.8576961159706116, G loss: [0.8416340351104736, 0.7175648808479309, 0.12406915426254272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 34/43 [D loss 1: 1.697385162115097, D loss 2: 0.7797947227954865, G loss: [0.8438769578933716, 0.7348222136497498, 0.10905477404594421]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 35/43 [D loss 1: 1.5895203351974487, D loss 2: 0.669230580329895, G loss: [0.8662092089653015, 0.7426763772964478, 0.12353280931711197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 36/43 [D loss 1: 1.6292409002780914, D loss 2: 0.7734272181987762, G loss: [0.8336009383201599, 0.7179516553878784, 0.1156492754817009]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 37/43 [D loss 1: 1.5991036891937256, D loss 2: 0.6747349202632904, G loss: [0.8511424660682678, 0.7375535368919373, 0.11358890682458878]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 38/43 [D loss 1: 1.6444230377674103, D loss 2: 0.7132171988487244, G loss: [0.8409485816955566, 0.7194236516952515, 0.12152490019798279]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 39/43 [D loss 1: 1.5733997523784637, D loss 2: 0.6614870429039001, G loss: [0.8357939124107361, 0.7280052900314331, 0.10778862237930298]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 40/43 [D loss 1: 1.7362358570098877, D loss 2: 0.8391448855400085, G loss: [0.8633577227592468, 0.7523561716079712, 0.11100155860185623]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 41/43 [D loss 1: 1.6543315649032593, D loss 2: 0.7473129630088806, G loss: [0.8626539707183838, 0.7523617148399353, 0.11029224842786789]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 42/43 [D loss 1: 1.5990588665008545, D loss 2: 0.6929618418216705, G loss: [0.8526559472084045, 0.7375963926315308, 0.11505956202745438]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 9/200, Batch 43/43 [D loss 1: 1.7565415501594543, D loss 2: 0.897039920091629, G loss: [0.8577614426612854, 0.7461518049240112, 0.11160963773727417]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 1/43 [D loss 1: 1.6535817384719849, D loss 2: 0.735459178686142, G loss: [0.8872460722923279, 0.7782443165779114, 0.1090017706155777]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 2/43 [D loss 1: 1.5670222640037537, D loss 2: 0.656009703874588, G loss: [0.8845027685165405, 0.7661140561103821, 0.11838869750499725]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 3/43 [D loss 1: 1.7869721055030823, D loss 2: 0.8939661979675293, G loss: [0.8843799233436584, 0.7725469470024109, 0.11183296889066696]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 4/43 [D loss 1: 1.6656945049762726, D loss 2: 0.7479779720306396, G loss: [0.8860855102539062, 0.7707485556602478, 0.11533697694540024]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 5/43 [D loss 1: 1.6965810656547546, D loss 2: 0.8055711090564728, G loss: [0.9083513617515564, 0.8076576590538025, 0.1006937175989151]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 6/43 [D loss 1: 1.7008513808250427, D loss 2: 0.8113875389099121, G loss: [0.9428076148033142, 0.8239535093307495, 0.1188540980219841]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 7/43 [D loss 1: 1.5436462759971619, D loss 2: 0.6274878978729248, G loss: [0.8913621306419373, 0.777927577495575, 0.11343453824520111]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 8/43 [D loss 1: 1.7611858248710632, D loss 2: 0.8728926479816437, G loss: [0.9042130708694458, 0.7940987944602966, 0.11011426150798798]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 9/43 [D loss 1: 1.6422042548656464, D loss 2: 0.7485194504261017, G loss: [0.896252453327179, 0.789033055305481, 0.107219398021698]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 10/43 [D loss 1: 1.6958129107952118, D loss 2: 0.7818354070186615, G loss: [0.9044023752212524, 0.7925863862037659, 0.11181596666574478]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 11/43 [D loss 1: 1.6115809679031372, D loss 2: 0.719307541847229, G loss: [0.9233788251876831, 0.8085364103317261, 0.11484239995479584]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 12/43 [D loss 1: 1.6499479115009308, D loss 2: 0.8039478957653046, G loss: [0.9184897541999817, 0.8199905753135681, 0.09849916398525238]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 13/43 [D loss 1: 1.675185739994049, D loss 2: 0.7703932821750641, G loss: [0.9343026280403137, 0.8114493489265442, 0.12285330146551132]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 14/43 [D loss 1: 1.6987964510917664, D loss 2: 0.8584785163402557, G loss: [0.926916778087616, 0.8169852495193481, 0.10993150621652603]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 15/43 [D loss 1: 1.5544366836547852, D loss 2: 0.6853475570678711, G loss: [0.935566782951355, 0.8085920810699463, 0.1269747018814087]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 16/43 [D loss 1: 1.8801776766777039, D loss 2: 0.9715308248996735, G loss: [0.9540726542472839, 0.8459427356719971, 0.10812992602586746]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 17/43 [D loss 1: 1.6440937221050262, D loss 2: 0.7260192930698395, G loss: [0.933588445186615, 0.8116914629936218, 0.12189696729183197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 18/43 [D loss 1: 1.7093065977096558, D loss 2: 0.8342589437961578, G loss: [0.9690924286842346, 0.8610809445381165, 0.10801150649785995]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 19/43 [D loss 1: 1.6858243644237518, D loss 2: 0.8182710409164429, G loss: [0.9748712778091431, 0.8636594414710999, 0.1112118661403656]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 20/43 [D loss 1: 1.6396313905715942, D loss 2: 0.7466469407081604, G loss: [0.9577327966690063, 0.8416467308998108, 0.11608608812093735]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 21/43 [D loss 1: 1.6863506436347961, D loss 2: 0.8149302899837494, G loss: [0.9461106657981873, 0.8372885584831238, 0.10882212221622467]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 22/43 [D loss 1: 1.6482200026512146, D loss 2: 0.7485990524291992, G loss: [0.9510300159454346, 0.8450471758842468, 0.10598286241292953]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 23/43 [D loss 1: 1.6663945615291595, D loss 2: 0.7587076425552368, G loss: [0.965069055557251, 0.852101743221283, 0.11296732723712921]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 24/43 [D loss 1: 1.7154089212417603, D loss 2: 0.8283106684684753, G loss: [0.9865396618843079, 0.8712478876113892, 0.1152917742729187]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 25/43 [D loss 1: 1.6249175071716309, D loss 2: 0.7414049506187439, G loss: [0.9809084534645081, 0.8414627313613892, 0.1394457370042801]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 26/43 [D loss 1: 1.954918384552002, D loss 2: 1.0540171563625336, G loss: [1.0447957515716553, 0.9398841857910156, 0.10491155833005905]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 27/43 [D loss 1: 1.549596130847931, D loss 2: 0.7200197577476501, G loss: [0.9987517595291138, 0.8865690231323242, 0.11218271404504776]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 28/43 [D loss 1: 1.5367779433727264, D loss 2: 0.6682182550430298, G loss: [1.0175163745880127, 0.8893577456474304, 0.1281585991382599]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 29/43 [D loss 1: 1.8687977194786072, D loss 2: 1.0076063573360443, G loss: [1.054382085800171, 0.9618639945983887, 0.09251805394887924]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 30/43 [D loss 1: 1.4917097687721252, D loss 2: 0.6251477897167206, G loss: [1.0022635459899902, 0.8817328810691833, 0.12053069472312927]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 31/43 [D loss 1: 1.5870462954044342, D loss 2: 0.724706619977951, G loss: [0.988679826259613, 0.8778430819511414, 0.11083674430847168]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 32/43 [D loss 1: 1.7319979667663574, D loss 2: 0.8524730503559113, G loss: [0.9708737134933472, 0.865973174571991, 0.1049005389213562]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 33/43 [D loss 1: 1.6759561896324158, D loss 2: 0.7946515679359436, G loss: [0.9830236434936523, 0.88136225938797, 0.10166138410568237]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 34/43 [D loss 1: 1.596158117055893, D loss 2: 0.716495007276535, G loss: [0.9648115634918213, 0.8497090339660645, 0.11510251462459564]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 35/43 [D loss 1: 1.63313627243042, D loss 2: 0.7809310257434845, G loss: [0.9800891280174255, 0.875290036201477, 0.10479911416769028]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 36/43 [D loss 1: 1.6055120825767517, D loss 2: 0.7268995046615601, G loss: [0.9871436953544617, 0.884722888469696, 0.10242082178592682]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 37/43 [D loss 1: 1.5381822288036346, D loss 2: 0.6631147563457489, G loss: [0.9978348016738892, 0.8761278390884399, 0.12170694768428802]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 38/43 [D loss 1: 1.8070986866950989, D loss 2: 0.9082598090171814, G loss: [1.0004894733428955, 0.8958582878112793, 0.10463118553161621]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 39/43 [D loss 1: 1.622769445180893, D loss 2: 0.7647057175636292, G loss: [0.976351261138916, 0.8685257434844971, 0.10782553255558014]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 40/43 [D loss 1: 1.5947312414646149, D loss 2: 0.7168582677841187, G loss: [1.0128655433654785, 0.9054387211799622, 0.10742687433958054]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 41/43 [D loss 1: 1.5847695171833038, D loss 2: 0.7457383573055267, G loss: [1.0175068378448486, 0.9036036133766174, 0.11390317976474762]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 42/43 [D loss 1: 1.6375168859958649, D loss 2: 0.783266007900238, G loss: [0.9985141158103943, 0.8871641159057617, 0.11134998500347137]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 10/200, Batch 43/43 [D loss 1: 1.5352762937545776, D loss 2: 0.7023265361785889, G loss: [0.9944419860839844, 0.8816897869110107, 0.11275222152471542]]\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 1/43 [D loss 1: 1.5648506581783295, D loss 2: 0.7390713691711426, G loss: [0.9878362417221069, 0.8796943426132202, 0.10814188420772552]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 2/43 [D loss 1: 1.635355681180954, D loss 2: 0.7502670288085938, G loss: [0.9985666275024414, 0.9003386497497559, 0.09822793304920197]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 3/43 [D loss 1: 1.636286437511444, D loss 2: 0.7577040493488312, G loss: [0.9956918954849243, 0.8936219811439514, 0.1020699292421341]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 4/43 [D loss 1: 1.5538159012794495, D loss 2: 0.7075817584991455, G loss: [1.000744104385376, 0.8920411467552185, 0.1087028980255127]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 5/43 [D loss 1: 1.636974424123764, D loss 2: 0.7556804716587067, G loss: [0.9932881593704224, 0.8836796879768372, 0.10960845649242401]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 6/43 [D loss 1: 1.5882719159126282, D loss 2: 0.7651054859161377, G loss: [0.9753621816635132, 0.8758633136749268, 0.09949889779090881]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 7/43 [D loss 1: 1.613550066947937, D loss 2: 0.7496960759162903, G loss: [1.0001128911972046, 0.8927161693572998, 0.1073966696858406]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 8/43 [D loss 1: 1.5861807465553284, D loss 2: 0.759739339351654, G loss: [0.9960049390792847, 0.8940273523330688, 0.10197758674621582]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 9/43 [D loss 1: 1.724562168121338, D loss 2: 0.852980375289917, G loss: [1.0135101079940796, 0.9201017618179321, 0.09340829402208328]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 10/43 [D loss 1: 1.573344498872757, D loss 2: 0.7316723763942719, G loss: [0.9860031604766846, 0.8880048394203186, 0.09799829125404358]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 11/43 [D loss 1: 1.6891454458236694, D loss 2: 0.8271238207817078, G loss: [1.0191820859909058, 0.9242123365402222, 0.0949697494506836]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 12/43 [D loss 1: 1.5734758377075195, D loss 2: 0.7307780683040619, G loss: [1.0116562843322754, 0.899445652961731, 0.11221059411764145]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 13/43 [D loss 1: 1.6687240302562714, D loss 2: 0.8249054551124573, G loss: [1.055112361907959, 0.9660251140594482, 0.08908727020025253]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 14/43 [D loss 1: 1.6542952358722687, D loss 2: 0.8412657082080841, G loss: [1.0608365535736084, 0.9569755792617798, 0.1038610190153122]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 15/43 [D loss 1: 1.617776334285736, D loss 2: 0.7516493201255798, G loss: [1.055446743965149, 0.935697615146637, 0.11974907666444778]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 16/43 [D loss 1: 1.7683783769607544, D loss 2: 0.8882025182247162, G loss: [1.057342290878296, 0.9709916114807129, 0.08635067194700241]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 17/43 [D loss 1: 1.5735016465187073, D loss 2: 0.7202636897563934, G loss: [1.0714340209960938, 0.9596471190452576, 0.1117868423461914]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 18/43 [D loss 1: 1.6247412860393524, D loss 2: 0.7949364185333252, G loss: [1.0604429244995117, 0.9585257768630981, 0.10191714763641357]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 19/43 [D loss 1: 1.6339849829673767, D loss 2: 0.8156591355800629, G loss: [1.0632928609848022, 0.9620469212532043, 0.10124599188566208]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 20/43 [D loss 1: 1.5389695763587952, D loss 2: 0.7419953048229218, G loss: [1.0710898637771606, 0.946495771408081, 0.12459411472082138]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 21/43 [D loss 1: 1.7501757144927979, D loss 2: 0.8687408268451691, G loss: [1.0450283288955688, 0.9654683470726013, 0.07955998182296753]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 22/43 [D loss 1: 1.5493127703666687, D loss 2: 0.7326042652130127, G loss: [1.0553970336914062, 0.9443761706352234, 0.11102081090211868]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 23/43 [D loss 1: 1.642721563577652, D loss 2: 0.7712947130203247, G loss: [1.060319185256958, 0.9671029448509216, 0.09321625530719757]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 24/43 [D loss 1: 1.6102662086486816, D loss 2: 0.7680872082710266, G loss: [1.0517897605895996, 0.94954514503479, 0.10224461555480957]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 25/43 [D loss 1: 1.5917415618896484, D loss 2: 0.752830445766449, G loss: [1.0493583679199219, 0.93231600522995, 0.11704231053590775]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 26/43 [D loss 1: 1.6680595874786377, D loss 2: 0.8472044765949249, G loss: [1.044530987739563, 0.9514914155006409, 0.09303957968950272]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 27/43 [D loss 1: 1.6331333220005035, D loss 2: 0.7613905072212219, G loss: [1.0483859777450562, 0.9486938118934631, 0.09969212114810944]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 28/43 [D loss 1: 1.5650735199451447, D loss 2: 0.7262989580631256, G loss: [1.0399655103683472, 0.933167576789856, 0.1067979708313942]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 29/43 [D loss 1: 1.6625617742538452, D loss 2: 0.8437951505184174, G loss: [1.0577106475830078, 0.9684457182884216, 0.08926496654748917]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 30/43 [D loss 1: 1.5950549840927124, D loss 2: 0.752576619386673, G loss: [1.0405417680740356, 0.9231998324394226, 0.11734197288751602]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 31/43 [D loss 1: 1.743180751800537, D loss 2: 0.8875152468681335, G loss: [1.0272225141525269, 0.9437661170959473, 0.08345640450716019]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 32/43 [D loss 1: 1.596357524394989, D loss 2: 0.7631303668022156, G loss: [1.044336199760437, 0.9369644522666931, 0.1073717251420021]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 33/43 [D loss 1: 1.6277619898319244, D loss 2: 0.7964279651641846, G loss: [1.0303055047988892, 0.9360952973365784, 0.09421022981405258]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 34/43 [D loss 1: 1.5785943865776062, D loss 2: 0.7279449701309204, G loss: [1.0623888969421387, 0.9544757008552551, 0.10791320353746414]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 35/43 [D loss 1: 1.6545082032680511, D loss 2: 0.8234165012836456, G loss: [1.030653476715088, 0.9345077872276306, 0.09614570438861847]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 36/43 [D loss 1: 1.6503645777702332, D loss 2: 0.8027017116546631, G loss: [1.0257236957550049, 0.9320623278617859, 0.09366140514612198]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 37/43 [D loss 1: 1.6289423108100891, D loss 2: 0.8057739436626434, G loss: [1.0611428022384644, 0.9619004726409912, 0.09924229979515076]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 38/43 [D loss 1: 1.745169222354889, D loss 2: 0.8956911563873291, G loss: [1.058675765991211, 0.9597910642623901, 0.09888467192649841]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 39/43 [D loss 1: 1.6563555300235748, D loss 2: 0.8380565941333771, G loss: [1.0465563535690308, 0.9166548848152161, 0.1299014836549759]]\n",
      "65/65 [==============================] - 0s 3ms/step\n",
      "Epoch 11/200, Batch 40/43 [D loss 1: 1.7655034065246582, D loss 2: 0.9221210181713104, G loss: [1.077305793762207, 0.9896154999732971, 0.08769028633832932]]\n",
      "18/65 [=======>......................] - ETA: 0s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate and train the DCGAN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m acgan \u001b[38;5;241m=\u001b[39m ACGAN(img_rows, img_cols, channels)\n\u001b[1;32m----> 6\u001b[0m \u001b[43macgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 128\u001b[0m, in \u001b[0;36mACGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval, gen_steps)\u001b[0m\n\u001b[0;32m    126\u001b[0m noise \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim))\n\u001b[0;32m    127\u001b[0m gen_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, (batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 128\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_labels\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m labels_fake \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    131\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(imgs, [labels_real, y_train[idx]])\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2253\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2253\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2254\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2255\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\p2348935\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "acgan = ACGAN(img_rows, img_cols, channels)\n",
    "acgan.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0370418b23d3e6d627974f5b44612aacb169a42c01386bf7ba5dc9099819d8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
