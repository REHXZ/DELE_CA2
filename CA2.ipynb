{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "from keras.layers import Reshape, Conv2DTranspose, PReLU\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Concatenate\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.image import resize\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, HTML\n",
    "import glob\n",
    "from keras.layers import AveragePooling2D, ZeroPadding2D, BatchNormalization, Activation, MaxPool2D, Add\n",
    "from keras.layers import Normalization, Dense, Conv2D, Dropout, BatchNormalization, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras import Input\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Reshape, UpSampling2D, \\\n",
    "    BatchNormalization, Activation, Input, LeakyReLU, ZeroPadding2D, Dropout, Flatten, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import rotate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, UpSampling2D, Conv2D, BatchNormalization, Activation, Input, LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List physical GPUs and set memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('emnist-letters-train.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0: 'labels'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['labels'] != -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88800, 28, 28, 1)\n",
      "(88800,)\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a NumPy array\n",
    "data = df.to_numpy()\n",
    "\n",
    "# Extract the labels and images from the NumPy array\n",
    "labels = data[:, 0]  # Assuming the first column is the labels\n",
    "images = data[:, 1:]  # The rest are the images\n",
    "\n",
    "# Optionally, you can reshape and normalize the images if needed\n",
    "# For example, if the images are 28x28 pixels\n",
    "images = images.reshape(-1, 28, 28, 1)  # Reshape to (num_samples, 28, 28, 1)\n",
    "# images = images / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define the mapping dictionary\n",
    "mapping = {1: 97, 2: 98, 3: 99, 4: 100, 5: 101, 6: 102, 7: 103, 8: 104, 9: 105, \n",
    "           10: 106, 11: 107, 12: 108, 13: 109, 14: 110, 15: 111, 16: 112, 17: 113, \n",
    "           18: 114, 19: 115, 20: 116, 21: 117, 22: 118, 23: 119, 24: 120, 25: 121, \n",
    "           26: 122, 27: 123}\n",
    "\n",
    "# Re-map the labels using the mapping dictionary\n",
    "mapped_labels = np.vectorize(mapping.get)(labels)\n",
    "\n",
    "# Convert the mapped labels to characters\n",
    "# labels = np.vectorize(chr)(mapped_labels)\n",
    "\n",
    "# Now 'images' and 'labels' are NumPy arrays ready for use\n",
    "print(images.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23,  7, 16, ..., 18, 24, 19], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (56832, 28, 28, 1)\n",
      "Validation set shape: (14208, 28, 28, 1)\n",
      "Test set shape: (17760, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(y_train)\n",
    "def Show_Images(data,unique_labels,labels):\n",
    "    # Create the plot with sufficient subplots\n",
    "    fig, axes = plt.subplots(3, 9, figsize=(20, 8))  # Adjusted to 3 rows and 9 columns for demonstration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for label, ax in zip(unique_labels, axes):\n",
    "        # Select the first image for each unique label\n",
    "        idx = np.where(labels == label)[0][0]\n",
    "        image = data[idx].reshape(28, 28)\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f'Label: {label}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Hide any remaining empty subplots (if any)\n",
    "    for ax in axes[len(unique_labels):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show_Images(X_train,unique_labels,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_per_label_Aug_sheer(images, labels, num_images_per_label=5):\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    # Create the plot with sufficient subplots\n",
    "    fig, axes = plt.subplots(len(unique_labels), num_images_per_label * 2, figsize=(20, len(unique_labels) * 2))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes to easily iterate\n",
    "\n",
    "    # Counter for current axis\n",
    "    ax_index = 0\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Filter images for the current label\n",
    "        indices = np.where(labels == label)[0]\n",
    "        \n",
    "        # Loop through up to num_images_per_label images for the current label\n",
    "        for i in range(min(num_images_per_label, len(indices))):\n",
    "            image = images[indices[i]].reshape(28, 28)\n",
    "            \n",
    "            # Display the original image\n",
    "            axes[ax_index].imshow(image, cmap='gray')\n",
    "            axes[ax_index].set_title(f'Label: {label}')\n",
    "            axes[ax_index].axis('off')\n",
    "            ax_index += 1\n",
    "\n",
    "            if ax_index >= len(axes):\n",
    "                break\n",
    "\n",
    "        for i in range(min(num_images_per_label, len(indices))):\n",
    "            image = images[indices[i]].reshape(28, 28)\n",
    "\n",
    "            # Rotate the image by 15 degrees\n",
    "            rotated_image = rotate(image, -90, reshape=False)\n",
    "            \n",
    "            # Apply shear transformation\n",
    "            # shear_transform = AffineTransform(shear=0.2)\n",
    "            # sheared_image = warp(rotated_image, shear_transform.inverse, mode='wrap')\n",
    "            flipped_image = np.fliplr(rotated_image)\n",
    "\n",
    "            # Display the augmented image\n",
    "            axes[ax_index].imshow(flipped_image, cmap='gray')\n",
    "            axes[ax_index].set_title(f'Labels Augmented: {label}')\n",
    "            axes[ax_index].axis('off')\n",
    "            ax_index += 1\n",
    "\n",
    "            if ax_index >= len(axes):\n",
    "                break\n",
    "\n",
    "    # Hide any remaining empty subplots (if any)\n",
    "    for ax in axes[ax_index:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_images_per_label_Aug_sheer(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = []\n",
    "for image in images:\n",
    "    rotated_image = rotate(image, 90, reshape=False)\n",
    "    flipped_image = np.flipud(rotated_image)  # Flip vertically\n",
    "    augmented_images.append(flipped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = np.array(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Augmented set shape: (84360, 28, 28, 1)\n",
      "Test Augmented set shape: (17760, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(augmented_images, labels, test_size=0.05, random_state=42)\n",
    "print(f\"Training Augmented set shape: {X_train_aug.shape}\")\n",
    "print(f\"Test Augmented set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Shape : (88800, 28, 28, 1)\n",
      "Label Shape : (88800,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Augmented Shape : {augmented_images.shape}')\n",
    "print(f'Label Shape : {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z = 100):\n",
    "        # Input shape\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        optimizer = Adam(0.0001, 0.7)\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "        self.generator = self.build_generator()\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator(img)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "    \n",
    "    def train(self, epochs, batch_size=256, save_interval=50):\n",
    "        # Load the dataset\n",
    "        X_train = X_train_aug\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 255\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "            # Train the discriminator (real classified as ones\n",
    "            # and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            # ---------------------\n",
    "            # Train Generator\n",
    "            # ---------------------\n",
    "            # Train the generator (wants discriminator to mistake\n",
    "            # images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(528, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(1024, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "        model.summary()\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        # gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        os.makedirs('generated_mnist', exist_ok=True)\n",
    "        fig.savefig(\"generated_mnist/dcgan_mnist_improved_{:d}.png\".format(epoch))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_54 (Conv2D)          (None, 14, 14, 32)        320       \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_55 (Conv2D)          (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " zero_padding2d_7 (ZeroPaddi  (None, 8, 8, 64)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_32 (LeakyReLU)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_56 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_33 (LeakyReLU)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_58 (Conv2D)          (None, 4, 4, 528)         1217040   \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 4, 4, 528)        2112      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 4, 4, 528)         0         \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 4, 4, 528)         0         \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 4, 4, 1024)        4867072   \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 4, 4, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_36 (LeakyReLU)  (None, 4, 4, 1024)        0         \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 4, 4, 1024)        0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 16385     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,496,337\n",
      "Trainable params: 6,492,337\n",
      "Non-trainable params: 4,000\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 6272)              633472    \n",
      "                                                                 \n",
      " reshape_7 (Reshape)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_14 (UpSamplin  (None, 14, 14, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_45 (Bat  (None, 14, 14, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_15 (UpSamplin  (None, 28, 28, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 28, 28, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_46 (Bat  (None, 28, 28, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 28, 28, 64)        73792     \n",
      "                                                                 \n",
      " batch_normalization_47 (Bat  (None, 28, 28, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          (None, 28, 28, 1)         577       \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,004,289\n",
      "Trainable params: 1,003,649\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "0 [D loss: 2.847967, acc.: 33.79%] [G loss: 0.719140]\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "1 [D loss: 0.395158, acc.: 79.69%] [G loss: 0.902966]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "2 [D loss: 0.807599, acc.: 59.18%] [G loss: 0.827183]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "3 [D loss: 0.306765, acc.: 88.09%] [G loss: 0.773692]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "4 [D loss: 0.169514, acc.: 95.51%] [G loss: 0.796717]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "5 [D loss: 0.466059, acc.: 78.91%] [G loss: 0.975755]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "6 [D loss: 0.788165, acc.: 62.89%] [G loss: 1.995918]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "7 [D loss: 0.572011, acc.: 75.20%] [G loss: 1.243807]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "8 [D loss: 0.115583, acc.: 96.48%] [G loss: 0.815920]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "9 [D loss: 0.066400, acc.: 98.44%] [G loss: 0.668684]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "10 [D loss: 0.089195, acc.: 97.27%] [G loss: 0.791406]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "11 [D loss: 0.141284, acc.: 93.75%] [G loss: 1.689409]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "12 [D loss: 0.159117, acc.: 93.55%] [G loss: 3.699997]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "13 [D loss: 0.280300, acc.: 89.06%] [G loss: 4.383774]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "14 [D loss: 0.185437, acc.: 93.36%] [G loss: 4.982434]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "15 [D loss: 0.121414, acc.: 95.90%] [G loss: 4.202717]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "16 [D loss: 0.202145, acc.: 92.19%] [G loss: 6.214442]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "17 [D loss: 0.123396, acc.: 96.68%] [G loss: 6.727335]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "18 [D loss: 0.163307, acc.: 93.95%] [G loss: 3.528756]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "19 [D loss: 0.244140, acc.: 88.87%] [G loss: 7.129578]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "20 [D loss: 0.408875, acc.: 82.23%] [G loss: 1.596440]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "21 [D loss: 0.280314, acc.: 88.09%] [G loss: 5.240709]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "22 [D loss: 0.542446, acc.: 78.91%] [G loss: 1.438033]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "23 [D loss: 0.312000, acc.: 85.35%] [G loss: 5.402586]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "24 [D loss: 0.331232, acc.: 86.33%] [G loss: 2.405473]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "25 [D loss: 0.167860, acc.: 93.95%] [G loss: 2.470119]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "26 [D loss: 0.153813, acc.: 94.73%] [G loss: 3.436647]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "27 [D loss: 0.178383, acc.: 92.77%] [G loss: 2.480482]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "28 [D loss: 0.201871, acc.: 92.77%] [G loss: 3.213410]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "29 [D loss: 0.148607, acc.: 94.34%] [G loss: 2.717142]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "30 [D loss: 0.168607, acc.: 94.92%] [G loss: 2.991249]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "31 [D loss: 0.162412, acc.: 94.14%] [G loss: 3.440128]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "32 [D loss: 0.114945, acc.: 96.29%] [G loss: 3.135951]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "33 [D loss: 0.128482, acc.: 94.92%] [G loss: 3.117344]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "34 [D loss: 0.070517, acc.: 97.85%] [G loss: 3.278806]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "35 [D loss: 0.084887, acc.: 97.46%] [G loss: 3.502514]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "36 [D loss: 0.141069, acc.: 94.34%] [G loss: 3.225013]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "37 [D loss: 0.093986, acc.: 96.09%] [G loss: 4.113443]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "38 [D loss: 0.221633, acc.: 91.60%] [G loss: 4.274078]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "39 [D loss: 0.263179, acc.: 89.65%] [G loss: 3.135627]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "40 [D loss: 0.074057, acc.: 98.24%] [G loss: 2.573339]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "41 [D loss: 0.044107, acc.: 98.83%] [G loss: 2.492265]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "42 [D loss: 0.036496, acc.: 98.63%] [G loss: 2.191162]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "43 [D loss: 0.083075, acc.: 97.46%] [G loss: 3.322422]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "44 [D loss: 0.076330, acc.: 97.66%] [G loss: 3.032365]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "45 [D loss: 0.150750, acc.: 94.53%] [G loss: 4.577691]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "46 [D loss: 0.255854, acc.: 87.89%] [G loss: 3.767804]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "47 [D loss: 0.054864, acc.: 98.83%] [G loss: 3.447582]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "48 [D loss: 0.041761, acc.: 98.83%] [G loss: 2.568849]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "49 [D loss: 0.035482, acc.: 98.83%] [G loss: 3.516357]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "50 [D loss: 0.043840, acc.: 98.63%] [G loss: 3.061950]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "51 [D loss: 0.029575, acc.: 99.41%] [G loss: 3.255252]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "52 [D loss: 0.046644, acc.: 98.24%] [G loss: 3.198636]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "53 [D loss: 0.033508, acc.: 99.22%] [G loss: 3.640702]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "54 [D loss: 0.073385, acc.: 97.46%] [G loss: 4.472276]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "55 [D loss: 0.120468, acc.: 95.51%] [G loss: 3.716221]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "56 [D loss: 0.043425, acc.: 99.41%] [G loss: 3.672187]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "57 [D loss: 0.021677, acc.: 99.41%] [G loss: 3.275623]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "58 [D loss: 0.014419, acc.: 100.00%] [G loss: 2.997785]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "59 [D loss: 0.025625, acc.: 99.22%] [G loss: 2.918655]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "60 [D loss: 0.019142, acc.: 99.61%] [G loss: 3.213994]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "61 [D loss: 0.024240, acc.: 99.22%] [G loss: 3.241148]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "62 [D loss: 0.026082, acc.: 99.41%] [G loss: 3.208735]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "63 [D loss: 0.014846, acc.: 99.80%] [G loss: 3.014859]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "64 [D loss: 0.026910, acc.: 99.02%] [G loss: 2.812530]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "65 [D loss: 0.030607, acc.: 98.83%] [G loss: 2.567159]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "66 [D loss: 0.022880, acc.: 99.41%] [G loss: 2.408838]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "67 [D loss: 0.014213, acc.: 100.00%] [G loss: 2.277301]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "68 [D loss: 0.023508, acc.: 99.41%] [G loss: 2.644142]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "69 [D loss: 0.015165, acc.: 99.61%] [G loss: 2.201409]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "70 [D loss: 0.038764, acc.: 98.83%] [G loss: 2.321210]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "71 [D loss: 0.012136, acc.: 99.80%] [G loss: 2.404151]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "72 [D loss: 0.007977, acc.: 99.80%] [G loss: 2.476146]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "73 [D loss: 0.014422, acc.: 99.80%] [G loss: 2.679337]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "74 [D loss: 0.008906, acc.: 100.00%] [G loss: 2.865666]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "75 [D loss: 0.009392, acc.: 100.00%] [G loss: 2.876451]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "76 [D loss: 0.017988, acc.: 99.61%] [G loss: 2.896873]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "77 [D loss: 0.010863, acc.: 100.00%] [G loss: 3.106933]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "78 [D loss: 0.007070, acc.: 100.00%] [G loss: 3.199581]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "79 [D loss: 0.016407, acc.: 99.80%] [G loss: 3.080394]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "80 [D loss: 0.010935, acc.: 99.41%] [G loss: 2.977598]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "81 [D loss: 0.026145, acc.: 99.22%] [G loss: 2.037124]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "82 [D loss: 0.022492, acc.: 99.22%] [G loss: 3.010638]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "83 [D loss: 0.003784, acc.: 100.00%] [G loss: 3.793474]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "84 [D loss: 0.032111, acc.: 98.83%] [G loss: 2.577451]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "85 [D loss: 0.016766, acc.: 99.41%] [G loss: 2.524782]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "86 [D loss: 0.004507, acc.: 100.00%] [G loss: 3.037526]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "87 [D loss: 0.003483, acc.: 100.00%] [G loss: 2.872583]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "88 [D loss: 0.016529, acc.: 99.41%] [G loss: 3.007678]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "89 [D loss: 0.011055, acc.: 99.80%] [G loss: 2.724325]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "90 [D loss: 0.006367, acc.: 100.00%] [G loss: 2.598798]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "91 [D loss: 0.018510, acc.: 99.80%] [G loss: 3.066327]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "92 [D loss: 0.002661, acc.: 100.00%] [G loss: 3.776237]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "93 [D loss: 0.019310, acc.: 99.61%] [G loss: 2.924161]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "94 [D loss: 0.017822, acc.: 99.61%] [G loss: 2.548665]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "95 [D loss: 0.011542, acc.: 99.61%] [G loss: 2.410414]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "96 [D loss: 0.004476, acc.: 100.00%] [G loss: 2.733171]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "97 [D loss: 0.002258, acc.: 100.00%] [G loss: 2.760995]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "98 [D loss: 0.005485, acc.: 99.80%] [G loss: 2.406801]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "99 [D loss: 0.005611, acc.: 100.00%] [G loss: 2.633678]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "100 [D loss: 0.010927, acc.: 99.61%] [G loss: 2.461305]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "101 [D loss: 0.002808, acc.: 100.00%] [G loss: 2.721469]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "102 [D loss: 0.009971, acc.: 99.80%] [G loss: 2.548219]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "103 [D loss: 0.002800, acc.: 100.00%] [G loss: 2.546188]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "104 [D loss: 0.005091, acc.: 99.80%] [G loss: 2.516857]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "105 [D loss: 0.002402, acc.: 100.00%] [G loss: 2.767232]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "106 [D loss: 0.005313, acc.: 99.61%] [G loss: 2.706475]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "107 [D loss: 0.004556, acc.: 100.00%] [G loss: 2.461935]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "108 [D loss: 0.007450, acc.: 99.80%] [G loss: 2.610275]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "109 [D loss: 0.003044, acc.: 100.00%] [G loss: 2.969770]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "110 [D loss: 0.011619, acc.: 99.61%] [G loss: 2.314475]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "111 [D loss: 0.003767, acc.: 100.00%] [G loss: 2.477115]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "112 [D loss: 0.001335, acc.: 100.00%] [G loss: 2.479815]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "113 [D loss: 0.004703, acc.: 100.00%] [G loss: 2.558521]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "114 [D loss: 0.008076, acc.: 99.80%] [G loss: 2.202381]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "115 [D loss: 0.004549, acc.: 100.00%] [G loss: 2.558248]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "116 [D loss: 0.006187, acc.: 99.80%] [G loss: 2.962487]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "117 [D loss: 0.006539, acc.: 100.00%] [G loss: 2.551944]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "118 [D loss: 0.000916, acc.: 100.00%] [G loss: 2.641903]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "119 [D loss: 0.001551, acc.: 100.00%] [G loss: 2.422780]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "120 [D loss: 0.005189, acc.: 99.80%] [G loss: 2.614911]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "121 [D loss: 0.004184, acc.: 100.00%] [G loss: 2.703984]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "122 [D loss: 0.004748, acc.: 99.80%] [G loss: 2.804322]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "123 [D loss: 0.000870, acc.: 100.00%] [G loss: 2.338634]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "124 [D loss: 0.002620, acc.: 100.00%] [G loss: 2.344368]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "125 [D loss: 0.000747, acc.: 100.00%] [G loss: 2.397168]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "126 [D loss: 0.002782, acc.: 100.00%] [G loss: 2.821028]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "127 [D loss: 0.001630, acc.: 100.00%] [G loss: 2.793240]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "128 [D loss: 0.002743, acc.: 100.00%] [G loss: 2.947545]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "129 [D loss: 0.007146, acc.: 99.80%] [G loss: 2.473761]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "130 [D loss: 0.001778, acc.: 100.00%] [G loss: 2.465692]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "131 [D loss: 0.002114, acc.: 100.00%] [G loss: 2.498875]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "132 [D loss: 0.007743, acc.: 100.00%] [G loss: 2.441479]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "133 [D loss: 0.001661, acc.: 100.00%] [G loss: 2.247534]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "134 [D loss: 0.004325, acc.: 99.80%] [G loss: 2.082081]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "135 [D loss: 0.005860, acc.: 100.00%] [G loss: 2.160675]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "136 [D loss: 0.001669, acc.: 100.00%] [G loss: 2.570552]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "137 [D loss: 0.001834, acc.: 100.00%] [G loss: 2.374805]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "138 [D loss: 0.004951, acc.: 100.00%] [G loss: 2.239942]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "139 [D loss: 0.005854, acc.: 99.80%] [G loss: 2.384198]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "140 [D loss: 0.004972, acc.: 99.80%] [G loss: 2.345407]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "141 [D loss: 0.002351, acc.: 100.00%] [G loss: 2.794956]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "142 [D loss: 0.004566, acc.: 100.00%] [G loss: 2.400600]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "143 [D loss: 0.020041, acc.: 99.22%] [G loss: 2.652781]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "144 [D loss: 0.003006, acc.: 100.00%] [G loss: 3.148549]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "145 [D loss: 0.003267, acc.: 100.00%] [G loss: 3.185174]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "146 [D loss: 0.002903, acc.: 100.00%] [G loss: 2.902186]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "147 [D loss: 0.002947, acc.: 100.00%] [G loss: 2.619216]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "148 [D loss: 0.005091, acc.: 99.80%] [G loss: 2.807779]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "149 [D loss: 0.002217, acc.: 100.00%] [G loss: 2.906682]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "150 [D loss: 0.000747, acc.: 100.00%] [G loss: 2.820957]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "151 [D loss: 0.005921, acc.: 99.61%] [G loss: 2.412683]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "152 [D loss: 0.004631, acc.: 100.00%] [G loss: 2.347703]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "153 [D loss: 0.001157, acc.: 100.00%] [G loss: 2.475583]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "154 [D loss: 0.001573, acc.: 100.00%] [G loss: 2.486457]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "155 [D loss: 0.001850, acc.: 100.00%] [G loss: 2.260643]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "156 [D loss: 0.001552, acc.: 100.00%] [G loss: 2.435802]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "157 [D loss: 0.001416, acc.: 100.00%] [G loss: 2.387243]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "158 [D loss: 0.003881, acc.: 99.80%] [G loss: 2.139483]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "159 [D loss: 0.001658, acc.: 100.00%] [G loss: 1.902361]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "160 [D loss: 0.004295, acc.: 99.80%] [G loss: 2.171601]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "161 [D loss: 0.009527, acc.: 99.61%] [G loss: 2.364236]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "162 [D loss: 0.001973, acc.: 100.00%] [G loss: 2.359792]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "163 [D loss: 0.002634, acc.: 99.80%] [G loss: 2.029696]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "164 [D loss: 0.001771, acc.: 100.00%] [G loss: 2.006081]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "165 [D loss: 0.005891, acc.: 99.80%] [G loss: 2.242725]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "166 [D loss: 0.000604, acc.: 100.00%] [G loss: 2.005300]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "167 [D loss: 0.003051, acc.: 100.00%] [G loss: 1.964301]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "168 [D loss: 0.000220, acc.: 100.00%] [G loss: 2.469084]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "169 [D loss: 0.001466, acc.: 100.00%] [G loss: 2.645094]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "170 [D loss: 0.000613, acc.: 100.00%] [G loss: 2.869834]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "171 [D loss: 0.003340, acc.: 99.80%] [G loss: 3.027786]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "172 [D loss: 0.001887, acc.: 100.00%] [G loss: 3.190262]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "173 [D loss: 0.000896, acc.: 100.00%] [G loss: 3.147238]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "174 [D loss: 0.000473, acc.: 100.00%] [G loss: 3.274995]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "175 [D loss: 0.000977, acc.: 100.00%] [G loss: 2.718309]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "176 [D loss: 0.001068, acc.: 100.00%] [G loss: 2.467479]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "177 [D loss: 0.000682, acc.: 100.00%] [G loss: 2.067528]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "178 [D loss: 0.003057, acc.: 100.00%] [G loss: 2.488734]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "179 [D loss: 0.000536, acc.: 100.00%] [G loss: 2.369508]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "180 [D loss: 0.000835, acc.: 100.00%] [G loss: 2.219862]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "181 [D loss: 0.000574, acc.: 100.00%] [G loss: 2.291503]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "182 [D loss: 0.000891, acc.: 100.00%] [G loss: 2.013390]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "183 [D loss: 0.001853, acc.: 100.00%] [G loss: 2.113025]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "184 [D loss: 0.000786, acc.: 100.00%] [G loss: 2.058860]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "185 [D loss: 0.000767, acc.: 100.00%] [G loss: 2.045877]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "186 [D loss: 0.000525, acc.: 100.00%] [G loss: 2.369993]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "187 [D loss: 0.000530, acc.: 100.00%] [G loss: 2.553215]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "188 [D loss: 0.000833, acc.: 100.00%] [G loss: 2.436307]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "189 [D loss: 0.001360, acc.: 100.00%] [G loss: 2.346035]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "190 [D loss: 0.000481, acc.: 100.00%] [G loss: 2.303078]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "191 [D loss: 0.000350, acc.: 100.00%] [G loss: 2.428227]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "192 [D loss: 0.001031, acc.: 100.00%] [G loss: 2.074016]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "193 [D loss: 0.000601, acc.: 100.00%] [G loss: 2.181594]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "194 [D loss: 0.002106, acc.: 100.00%] [G loss: 1.944786]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "195 [D loss: 0.005995, acc.: 99.80%] [G loss: 1.594211]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "196 [D loss: 0.001200, acc.: 100.00%] [G loss: 1.674096]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "197 [D loss: 0.000336, acc.: 100.00%] [G loss: 1.984781]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "198 [D loss: 0.000282, acc.: 100.00%] [G loss: 2.095599]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "199 [D loss: 0.000362, acc.: 100.00%] [G loss: 2.384506]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "200 [D loss: 0.000459, acc.: 100.00%] [G loss: 2.337249]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "201 [D loss: 0.000156, acc.: 100.00%] [G loss: 2.374939]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "202 [D loss: 0.000201, acc.: 100.00%] [G loss: 2.519576]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "203 [D loss: 0.000139, acc.: 100.00%] [G loss: 2.435488]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "204 [D loss: 0.000186, acc.: 100.00%] [G loss: 2.585866]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "205 [D loss: 0.001149, acc.: 100.00%] [G loss: 2.159502]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "206 [D loss: 0.000637, acc.: 100.00%] [G loss: 2.627711]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "207 [D loss: 0.000201, acc.: 100.00%] [G loss: 2.854413]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "208 [D loss: 0.000842, acc.: 100.00%] [G loss: 2.771057]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "209 [D loss: 0.000337, acc.: 100.00%] [G loss: 2.535572]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "210 [D loss: 0.000350, acc.: 100.00%] [G loss: 2.228439]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "211 [D loss: 0.000327, acc.: 100.00%] [G loss: 2.289179]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "212 [D loss: 0.000190, acc.: 100.00%] [G loss: 2.070995]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "213 [D loss: 0.000103, acc.: 100.00%] [G loss: 2.084857]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "214 [D loss: 0.000682, acc.: 100.00%] [G loss: 1.888271]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "215 [D loss: 0.000521, acc.: 100.00%] [G loss: 2.032210]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "216 [D loss: 0.000277, acc.: 100.00%] [G loss: 1.981401]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "217 [D loss: 0.000470, acc.: 100.00%] [G loss: 2.206044]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "218 [D loss: 0.000120, acc.: 100.00%] [G loss: 2.302725]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "219 [D loss: 0.001105, acc.: 100.00%] [G loss: 2.279538]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "220 [D loss: 0.000358, acc.: 100.00%] [G loss: 2.231553]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "221 [D loss: 0.000457, acc.: 100.00%] [G loss: 2.214073]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "222 [D loss: 0.000531, acc.: 100.00%] [G loss: 1.676481]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "223 [D loss: 0.000323, acc.: 100.00%] [G loss: 2.065014]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "224 [D loss: 0.000101, acc.: 100.00%] [G loss: 2.068994]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "225 [D loss: 0.000140, acc.: 100.00%] [G loss: 2.497368]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "226 [D loss: 0.000152, acc.: 100.00%] [G loss: 2.487148]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "227 [D loss: 0.000263, acc.: 100.00%] [G loss: 2.360056]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "228 [D loss: 0.000428, acc.: 100.00%] [G loss: 2.340842]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "229 [D loss: 0.000159, acc.: 100.00%] [G loss: 2.571995]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "230 [D loss: 0.000091, acc.: 100.00%] [G loss: 2.136377]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "231 [D loss: 0.000327, acc.: 100.00%] [G loss: 2.064991]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "232 [D loss: 0.000352, acc.: 100.00%] [G loss: 2.387287]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "233 [D loss: 0.000230, acc.: 100.00%] [G loss: 2.010509]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "234 [D loss: 0.000111, acc.: 100.00%] [G loss: 2.327759]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "235 [D loss: 0.000157, acc.: 100.00%] [G loss: 2.335090]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "236 [D loss: 0.000294, acc.: 100.00%] [G loss: 2.298836]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "237 [D loss: 0.000151, acc.: 100.00%] [G loss: 1.916686]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "238 [D loss: 0.000128, acc.: 100.00%] [G loss: 2.141624]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "239 [D loss: 0.000606, acc.: 100.00%] [G loss: 2.103812]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "240 [D loss: 0.000144, acc.: 100.00%] [G loss: 2.089865]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "241 [D loss: 0.000104, acc.: 100.00%] [G loss: 1.947718]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "242 [D loss: 0.000151, acc.: 100.00%] [G loss: 2.069062]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "243 [D loss: 0.000065, acc.: 100.00%] [G loss: 2.462418]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "244 [D loss: 0.000130, acc.: 100.00%] [G loss: 2.096965]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "245 [D loss: 0.000423, acc.: 100.00%] [G loss: 1.965175]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "246 [D loss: 0.000100, acc.: 100.00%] [G loss: 2.119730]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "247 [D loss: 0.000134, acc.: 100.00%] [G loss: 1.983832]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "248 [D loss: 0.000107, acc.: 100.00%] [G loss: 2.060402]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "249 [D loss: 0.000095, acc.: 100.00%] [G loss: 2.095299]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "250 [D loss: 0.000498, acc.: 100.00%] [G loss: 1.912005]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "8/8 [==============================] - 0s 24ms/step\n",
      "251 [D loss: 0.000084, acc.: 100.00%] [G loss: 1.978021]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "252 [D loss: 0.000132, acc.: 100.00%] [G loss: 2.153656]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "253 [D loss: 0.000218, acc.: 100.00%] [G loss: 2.119426]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "254 [D loss: 0.000157, acc.: 100.00%] [G loss: 2.098802]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "255 [D loss: 0.000147, acc.: 100.00%] [G loss: 2.151108]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "256 [D loss: 0.000327, acc.: 100.00%] [G loss: 2.163814]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "257 [D loss: 0.000165, acc.: 100.00%] [G loss: 2.252737]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "258 [D loss: 0.000111, acc.: 100.00%] [G loss: 2.268168]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "259 [D loss: 0.000381, acc.: 100.00%] [G loss: 2.170495]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "260 [D loss: 0.000139, acc.: 100.00%] [G loss: 1.874082]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "261 [D loss: 0.000095, acc.: 100.00%] [G loss: 2.729072]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "262 [D loss: 0.000152, acc.: 100.00%] [G loss: 2.712633]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "263 [D loss: 0.000049, acc.: 100.00%] [G loss: 2.565436]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "264 [D loss: 0.000131, acc.: 100.00%] [G loss: 2.625064]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "265 [D loss: 0.000101, acc.: 100.00%] [G loss: 2.802017]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "266 [D loss: 0.000089, acc.: 100.00%] [G loss: 2.390343]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "267 [D loss: 0.000059, acc.: 100.00%] [G loss: 1.994155]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "268 [D loss: 0.000224, acc.: 100.00%] [G loss: 1.975505]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "269 [D loss: 0.000977, acc.: 100.00%] [G loss: 1.895711]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "270 [D loss: 0.000230, acc.: 100.00%] [G loss: 1.436306]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "271 [D loss: 0.000085, acc.: 100.00%] [G loss: 1.773818]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "272 [D loss: 0.000138, acc.: 100.00%] [G loss: 2.018457]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "273 [D loss: 0.001462, acc.: 99.80%] [G loss: 2.256064]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "274 [D loss: 0.000244, acc.: 100.00%] [G loss: 2.586712]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "275 [D loss: 0.000188, acc.: 100.00%] [G loss: 2.365719]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "276 [D loss: 0.000218, acc.: 100.00%] [G loss: 2.305777]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "277 [D loss: 0.000082, acc.: 100.00%] [G loss: 2.250335]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "278 [D loss: 0.000252, acc.: 100.00%] [G loss: 2.335008]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "279 [D loss: 0.000161, acc.: 100.00%] [G loss: 2.364633]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "280 [D loss: 0.000090, acc.: 100.00%] [G loss: 2.253842]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "281 [D loss: 0.000052, acc.: 100.00%] [G loss: 2.145240]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "282 [D loss: 0.000091, acc.: 100.00%] [G loss: 2.435208]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "283 [D loss: 0.000061, acc.: 100.00%] [G loss: 2.615781]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "284 [D loss: 0.000064, acc.: 100.00%] [G loss: 2.563309]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "285 [D loss: 0.000037, acc.: 100.00%] [G loss: 2.524245]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "286 [D loss: 0.000055, acc.: 100.00%] [G loss: 2.738724]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "287 [D loss: 0.000119, acc.: 100.00%] [G loss: 2.895516]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "288 [D loss: 0.000056, acc.: 100.00%] [G loss: 2.814859]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "289 [D loss: 0.000103, acc.: 100.00%] [G loss: 2.223764]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "290 [D loss: 0.000152, acc.: 100.00%] [G loss: 2.423868]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "291 [D loss: 0.000523, acc.: 100.00%] [G loss: 2.181123]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "292 [D loss: 0.000130, acc.: 100.00%] [G loss: 2.422520]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "293 [D loss: 0.000202, acc.: 100.00%] [G loss: 2.219757]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "294 [D loss: 0.000140, acc.: 100.00%] [G loss: 2.499293]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "295 [D loss: 0.000048, acc.: 100.00%] [G loss: 2.374059]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "296 [D loss: 0.000046, acc.: 100.00%] [G loss: 2.352122]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "297 [D loss: 0.000106, acc.: 100.00%] [G loss: 2.184261]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "298 [D loss: 0.000074, acc.: 100.00%] [G loss: 2.020619]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "299 [D loss: 0.000036, acc.: 100.00%] [G loss: 2.162562]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "300 [D loss: 0.000115, acc.: 100.00%] [G loss: 2.278250]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "301 [D loss: 0.000098, acc.: 100.00%] [G loss: 1.925177]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "302 [D loss: 0.000266, acc.: 100.00%] [G loss: 1.768640]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "303 [D loss: 0.001015, acc.: 100.00%] [G loss: 1.773640]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "304 [D loss: 0.000118, acc.: 100.00%] [G loss: 2.031541]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "305 [D loss: 0.000064, acc.: 100.00%] [G loss: 1.937837]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "306 [D loss: 0.000117, acc.: 100.00%] [G loss: 1.957736]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "307 [D loss: 0.000070, acc.: 100.00%] [G loss: 1.703680]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "308 [D loss: 0.000060, acc.: 100.00%] [G loss: 1.916229]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "309 [D loss: 0.000080, acc.: 100.00%] [G loss: 2.226699]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "310 [D loss: 0.000081, acc.: 100.00%] [G loss: 2.210240]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "311 [D loss: 0.000269, acc.: 100.00%] [G loss: 2.370787]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "312 [D loss: 0.000105, acc.: 100.00%] [G loss: 1.805069]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "313 [D loss: 0.000234, acc.: 100.00%] [G loss: 1.892898]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "314 [D loss: 0.000075, acc.: 100.00%] [G loss: 1.919915]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "315 [D loss: 0.000087, acc.: 100.00%] [G loss: 2.345022]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "316 [D loss: 0.000034, acc.: 100.00%] [G loss: 2.334657]\n",
      "8/8 [==============================] - 0s 14ms/step\n",
      "317 [D loss: 0.000026, acc.: 100.00%] [G loss: 2.292555]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "318 [D loss: 0.000069, acc.: 100.00%] [G loss: 2.049480]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "319 [D loss: 0.000397, acc.: 100.00%] [G loss: 2.274330]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "320 [D loss: 0.000060, acc.: 100.00%] [G loss: 2.133369]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "321 [D loss: 0.000119, acc.: 100.00%] [G loss: 1.963823]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "322 [D loss: 0.000559, acc.: 100.00%] [G loss: 2.058632]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "323 [D loss: 0.000078, acc.: 100.00%] [G loss: 2.210188]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "324 [D loss: 0.000072, acc.: 100.00%] [G loss: 2.229766]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "325 [D loss: 0.000047, acc.: 100.00%] [G loss: 2.296354]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "326 [D loss: 0.000154, acc.: 100.00%] [G loss: 2.416881]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "327 [D loss: 0.000125, acc.: 100.00%] [G loss: 2.595738]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "328 [D loss: 0.000055, acc.: 100.00%] [G loss: 2.682650]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "329 [D loss: 0.000189, acc.: 100.00%] [G loss: 1.994714]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "330 [D loss: 0.000217, acc.: 100.00%] [G loss: 2.285790]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "331 [D loss: 0.000069, acc.: 100.00%] [G loss: 2.379729]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "332 [D loss: 0.000075, acc.: 100.00%] [G loss: 2.154195]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "333 [D loss: 0.000046, acc.: 100.00%] [G loss: 2.025139]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "334 [D loss: 0.000063, acc.: 100.00%] [G loss: 2.152556]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "335 [D loss: 0.000103, acc.: 100.00%] [G loss: 2.020070]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "336 [D loss: 0.000057, acc.: 100.00%] [G loss: 2.259383]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "337 [D loss: 0.000058, acc.: 100.00%] [G loss: 2.319423]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "338 [D loss: 0.000882, acc.: 100.00%] [G loss: 2.051558]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "339 [D loss: 0.000059, acc.: 100.00%] [G loss: 2.105678]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "340 [D loss: 0.000113, acc.: 100.00%] [G loss: 2.193807]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "341 [D loss: 0.000135, acc.: 100.00%] [G loss: 2.011053]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "342 [D loss: 0.000131, acc.: 100.00%] [G loss: 1.670112]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "343 [D loss: 0.000074, acc.: 100.00%] [G loss: 2.016157]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "344 [D loss: 0.000055, acc.: 100.00%] [G loss: 1.918934]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "345 [D loss: 0.000067, acc.: 100.00%] [G loss: 1.901871]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "346 [D loss: 0.000027, acc.: 100.00%] [G loss: 2.069395]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "347 [D loss: 0.000095, acc.: 100.00%] [G loss: 1.929714]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "348 [D loss: 0.000029, acc.: 100.00%] [G loss: 2.085506]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "349 [D loss: 0.000077, acc.: 100.00%] [G loss: 1.976868]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "350 [D loss: 0.000340, acc.: 100.00%] [G loss: 1.875834]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "351 [D loss: 0.000070, acc.: 100.00%] [G loss: 2.109614]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "352 [D loss: 0.000058, acc.: 100.00%] [G loss: 1.802317]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "353 [D loss: 0.000089, acc.: 100.00%] [G loss: 2.361834]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "354 [D loss: 0.000174, acc.: 100.00%] [G loss: 2.459444]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "355 [D loss: 0.000179, acc.: 100.00%] [G loss: 2.085531]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "356 [D loss: 0.000285, acc.: 100.00%] [G loss: 2.049147]\n",
      "8/8 [==============================] - 0s 56ms/step\n",
      "357 [D loss: 0.000078, acc.: 100.00%] [G loss: 2.140103]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "358 [D loss: 0.000074, acc.: 100.00%] [G loss: 2.532852]\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "359 [D loss: 0.000041, acc.: 100.00%] [G loss: 2.515009]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "360 [D loss: 0.000086, acc.: 100.00%] [G loss: 2.121779]\n",
      "8/8 [==============================] - 0s 49ms/step\n",
      "361 [D loss: 0.000059, acc.: 100.00%] [G loss: 2.408771]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "362 [D loss: 0.000049, acc.: 100.00%] [G loss: 2.341844]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "363 [D loss: 0.000056, acc.: 100.00%] [G loss: 2.368057]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "364 [D loss: 0.000045, acc.: 100.00%] [G loss: 2.610229]\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "365 [D loss: 0.000040, acc.: 100.00%] [G loss: 2.557608]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "366 [D loss: 0.001209, acc.: 100.00%] [G loss: 2.175786]\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "367 [D loss: 0.000099, acc.: 100.00%] [G loss: 2.232283]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "368 [D loss: 0.000045, acc.: 100.00%] [G loss: 2.265420]\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "369 [D loss: 0.000027, acc.: 100.00%] [G loss: 2.002903]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "370 [D loss: 0.000105, acc.: 100.00%] [G loss: 2.025948]\n",
      "8/8 [==============================] - 0s 55ms/step\n",
      "371 [D loss: 0.000081, acc.: 100.00%] [G loss: 2.176762]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "372 [D loss: 0.000045, acc.: 100.00%] [G loss: 2.231792]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "373 [D loss: 0.000044, acc.: 100.00%] [G loss: 2.075760]\n",
      "8/8 [==============================] - 0s 54ms/step\n",
      "374 [D loss: 0.000018, acc.: 100.00%] [G loss: 1.994822]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "375 [D loss: 0.000108, acc.: 100.00%] [G loss: 2.121448]\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "376 [D loss: 0.000063, acc.: 100.00%] [G loss: 2.294527]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "377 [D loss: 0.000017, acc.: 100.00%] [G loss: 1.988279]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "378 [D loss: 0.000285, acc.: 100.00%] [G loss: 2.045279]\n",
      "8/8 [==============================] - 0s 33ms/step\n",
      "379 [D loss: 0.000060, acc.: 100.00%] [G loss: 1.808997]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "380 [D loss: 0.000090, acc.: 100.00%] [G loss: 2.089795]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "381 [D loss: 0.000075, acc.: 100.00%] [G loss: 1.889807]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "382 [D loss: 0.000090, acc.: 100.00%] [G loss: 1.909383]\n",
      "8/8 [==============================] - 0s 51ms/step\n",
      "383 [D loss: 0.000088, acc.: 100.00%] [G loss: 1.796823]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "384 [D loss: 0.000024, acc.: 100.00%] [G loss: 1.739975]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "385 [D loss: 0.000140, acc.: 100.00%] [G loss: 1.753685]\n",
      "8/8 [==============================] - 0s 55ms/step\n",
      "386 [D loss: 0.000037, acc.: 100.00%] [G loss: 1.942765]\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "387 [D loss: 0.000077, acc.: 100.00%] [G loss: 1.958298]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "388 [D loss: 0.000171, acc.: 100.00%] [G loss: 1.791680]\n",
      "8/8 [==============================] - 0s 53ms/step\n",
      "389 [D loss: 0.000051, acc.: 100.00%] [G loss: 2.122641]\n",
      "8/8 [==============================] - 0s 39ms/step\n",
      "390 [D loss: 0.000093, acc.: 100.00%] [G loss: 2.015146]\n",
      "8/8 [==============================] - 0s 41ms/step\n",
      "391 [D loss: 0.000031, acc.: 100.00%] [G loss: 1.942697]\n",
      "8/8 [==============================] - 0s 52ms/step\n",
      "392 [D loss: 0.000074, acc.: 100.00%] [G loss: 1.896922]\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "393 [D loss: 0.000167, acc.: 100.00%] [G loss: 1.815282]\n",
      "8/8 [==============================] - 0s 60ms/step\n",
      "394 [D loss: 0.000100, acc.: 100.00%] [G loss: 1.919407]\n",
      "8/8 [==============================] - 0s 58ms/step\n",
      "395 [D loss: 0.000050, acc.: 100.00%] [G loss: 1.654608]\n",
      "8/8 [==============================] - 0s 57ms/step\n",
      "396 [D loss: 0.000055, acc.: 100.00%] [G loss: 2.145413]\n",
      "8/8 [==============================] - 0s 58ms/step\n",
      "397 [D loss: 0.000041, acc.: 100.00%] [G loss: 2.391992]\n",
      "8/8 [==============================] - 0s 36ms/step\n",
      "398 [D loss: 0.000047, acc.: 100.00%] [G loss: 2.608264]\n",
      "8/8 [==============================] - 0s 38ms/step\n",
      "399 [D loss: 0.000038, acc.: 100.00%] [G loss: 2.204979]\n",
      "8/8 [==============================] - 0s 38ms/step\n",
      "400 [D loss: 0.000044, acc.: 100.00%] [G loss: 2.058103]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "8/8 [==============================] - 0s 67ms/step\n",
      "401 [D loss: 0.000024, acc.: 100.00%] [G loss: 1.911668]\n",
      "8/8 [==============================] - 0s 41ms/step\n",
      "402 [D loss: 0.000019, acc.: 100.00%] [G loss: 1.939976]\n",
      "8/8 [==============================] - 0s 56ms/step\n",
      "403 [D loss: 0.000120, acc.: 100.00%] [G loss: 2.128008]\n",
      "8/8 [==============================] - 0s 64ms/step\n",
      "404 [D loss: 0.000128, acc.: 100.00%] [G loss: 2.276946]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "405 [D loss: 0.000031, acc.: 100.00%] [G loss: 2.230221]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "406 [D loss: 0.000025, acc.: 100.00%] [G loss: 2.062218]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "407 [D loss: 0.000030, acc.: 100.00%] [G loss: 2.077867]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "408 [D loss: 0.000036, acc.: 100.00%] [G loss: 2.161910]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "409 [D loss: 0.000068, acc.: 100.00%] [G loss: 2.460215]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "410 [D loss: 0.000016, acc.: 100.00%] [G loss: 2.137244]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "411 [D loss: 0.000044, acc.: 100.00%] [G loss: 2.152385]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "412 [D loss: 0.000144, acc.: 100.00%] [G loss: 2.008016]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "413 [D loss: 0.000068, acc.: 100.00%] [G loss: 1.997671]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "414 [D loss: 0.000161, acc.: 100.00%] [G loss: 2.002440]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "415 [D loss: 0.000135, acc.: 100.00%] [G loss: 1.903981]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "416 [D loss: 0.000027, acc.: 100.00%] [G loss: 2.059319]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "417 [D loss: 0.000162, acc.: 100.00%] [G loss: 1.933501]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "418 [D loss: 0.000084, acc.: 100.00%] [G loss: 2.350789]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "419 [D loss: 0.000043, acc.: 100.00%] [G loss: 2.299175]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "420 [D loss: 0.000035, acc.: 100.00%] [G loss: 2.450768]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "421 [D loss: 0.000026, acc.: 100.00%] [G loss: 2.226621]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "422 [D loss: 0.000038, acc.: 100.00%] [G loss: 2.483121]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "423 [D loss: 0.000030, acc.: 100.00%] [G loss: 2.196837]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "424 [D loss: 0.000045, acc.: 100.00%] [G loss: 1.938477]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "425 [D loss: 0.000132, acc.: 100.00%] [G loss: 1.925568]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "426 [D loss: 0.000038, acc.: 100.00%] [G loss: 1.903646]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "427 [D loss: 0.000038, acc.: 100.00%] [G loss: 2.116481]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "428 [D loss: 0.000016, acc.: 100.00%] [G loss: 2.029393]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "429 [D loss: 0.000052, acc.: 100.00%] [G loss: 1.946052]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "430 [D loss: 0.000007, acc.: 100.00%] [G loss: 2.206183]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "431 [D loss: 0.000131, acc.: 100.00%] [G loss: 2.506925]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "432 [D loss: 0.000021, acc.: 100.00%] [G loss: 2.199674]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "433 [D loss: 0.000070, acc.: 100.00%] [G loss: 2.086705]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "434 [D loss: 0.000042, acc.: 100.00%] [G loss: 1.867356]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "435 [D loss: 0.000053, acc.: 100.00%] [G loss: 2.102865]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "436 [D loss: 0.000036, acc.: 100.00%] [G loss: 1.632162]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "437 [D loss: 0.000627, acc.: 100.00%] [G loss: 1.754951]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "438 [D loss: 0.000020, acc.: 100.00%] [G loss: 1.573003]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "439 [D loss: 0.000035, acc.: 100.00%] [G loss: 1.691521]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "440 [D loss: 0.000025, acc.: 100.00%] [G loss: 1.560258]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "441 [D loss: 0.000215, acc.: 100.00%] [G loss: 1.840144]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "442 [D loss: 0.000030, acc.: 100.00%] [G loss: 1.691587]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "443 [D loss: 0.000110, acc.: 100.00%] [G loss: 1.414184]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "444 [D loss: 0.000061, acc.: 100.00%] [G loss: 1.516256]\n",
      "8/8 [==============================] - 0s 13ms/step\n",
      "445 [D loss: 0.000019, acc.: 100.00%] [G loss: 1.791091]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "446 [D loss: 0.000025, acc.: 100.00%] [G loss: 1.890863]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "447 [D loss: 0.000399, acc.: 100.00%] [G loss: 1.907116]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "448 [D loss: 0.000025, acc.: 100.00%] [G loss: 2.145824]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "449 [D loss: 0.000026, acc.: 100.00%] [G loss: 1.976189]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "450 [D loss: 0.000046, acc.: 100.00%] [G loss: 1.784855]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "451 [D loss: 0.000102, acc.: 100.00%] [G loss: 1.887120]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "452 [D loss: 0.000037, acc.: 100.00%] [G loss: 1.886652]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "453 [D loss: 0.000026, acc.: 100.00%] [G loss: 2.372736]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "454 [D loss: 0.000027, acc.: 100.00%] [G loss: 2.045469]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "455 [D loss: 0.000073, acc.: 100.00%] [G loss: 2.148444]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "456 [D loss: 0.000070, acc.: 100.00%] [G loss: 1.966191]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "457 [D loss: 0.000090, acc.: 100.00%] [G loss: 1.833404]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "458 [D loss: 0.000022, acc.: 100.00%] [G loss: 1.971560]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "459 [D loss: 0.000043, acc.: 100.00%] [G loss: 2.310534]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "460 [D loss: 0.000013, acc.: 100.00%] [G loss: 2.368667]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "461 [D loss: 0.000083, acc.: 100.00%] [G loss: 2.413286]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "462 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.768138]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "463 [D loss: 0.000034, acc.: 100.00%] [G loss: 2.272793]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "464 [D loss: 0.000041, acc.: 100.00%] [G loss: 2.256700]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "465 [D loss: 0.000049, acc.: 100.00%] [G loss: 2.260401]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "466 [D loss: 0.000015, acc.: 100.00%] [G loss: 2.273662]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "467 [D loss: 0.000011, acc.: 100.00%] [G loss: 2.215437]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "468 [D loss: 0.000019, acc.: 100.00%] [G loss: 2.192564]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "469 [D loss: 0.000270, acc.: 100.00%] [G loss: 2.181834]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "470 [D loss: 0.000043, acc.: 100.00%] [G loss: 2.244966]\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "471 [D loss: 0.000027, acc.: 100.00%] [G loss: 2.206436]\n",
      "8/8 [==============================] - 0s 11ms/step\n",
      "472 [D loss: 0.000098, acc.: 100.00%] [G loss: 2.586903]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "473 [D loss: 0.000012, acc.: 100.00%] [G loss: 2.348155]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "474 [D loss: 0.000028, acc.: 100.00%] [G loss: 2.563012]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "475 [D loss: 0.000019, acc.: 100.00%] [G loss: 2.717134]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "476 [D loss: 0.000010, acc.: 100.00%] [G loss: 2.949352]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "477 [D loss: 0.000010, acc.: 100.00%] [G loss: 2.826553]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "478 [D loss: 0.000020, acc.: 100.00%] [G loss: 2.737001]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "479 [D loss: 0.000015, acc.: 100.00%] [G loss: 2.807401]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "480 [D loss: 0.000024, acc.: 100.00%] [G loss: 2.657491]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "481 [D loss: 0.000038, acc.: 100.00%] [G loss: 2.533090]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "482 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.761128]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "483 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.798382]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "484 [D loss: 0.000010, acc.: 100.00%] [G loss: 2.595652]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "485 [D loss: 0.000027, acc.: 100.00%] [G loss: 2.808017]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "486 [D loss: 0.000018, acc.: 100.00%] [G loss: 2.473029]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "487 [D loss: 0.000006, acc.: 100.00%] [G loss: 2.530684]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "488 [D loss: 0.000007, acc.: 100.00%] [G loss: 2.725523]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "489 [D loss: 0.000012, acc.: 100.00%] [G loss: 2.907309]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "490 [D loss: 0.000007, acc.: 100.00%] [G loss: 2.634938]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "491 [D loss: 0.000045, acc.: 100.00%] [G loss: 3.093535]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "492 [D loss: 0.000011, acc.: 100.00%] [G loss: 2.817969]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "493 [D loss: 0.000020, acc.: 100.00%] [G loss: 2.890182]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "494 [D loss: 0.000018, acc.: 100.00%] [G loss: 2.566940]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "495 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.450252]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "496 [D loss: 0.000025, acc.: 100.00%] [G loss: 2.593394]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "497 [D loss: 0.000007, acc.: 100.00%] [G loss: 2.981790]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "498 [D loss: 0.000015, acc.: 100.00%] [G loss: 2.981441]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "499 [D loss: 0.000078, acc.: 100.00%] [G loss: 2.816312]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "500 [D loss: 0.000014, acc.: 100.00%] [G loss: 2.885189]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "8/8 [==============================] - 0s 7ms/step\n",
      "501 [D loss: 0.000025, acc.: 100.00%] [G loss: 2.531225]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "502 [D loss: 0.000010, acc.: 100.00%] [G loss: 2.328310]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "503 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.561107]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "504 [D loss: 0.000014, acc.: 100.00%] [G loss: 2.688221]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "505 [D loss: 0.000009, acc.: 100.00%] [G loss: 2.567962]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "506 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.559798]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "507 [D loss: 0.000010, acc.: 100.00%] [G loss: 2.826612]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "508 [D loss: 0.000014, acc.: 100.00%] [G loss: 2.810857]\n",
      "8/8 [==============================] - 0s 12ms/step\n",
      "509 [D loss: 0.000009, acc.: 100.00%] [G loss: 2.732985]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "510 [D loss: 0.000005, acc.: 100.00%] [G loss: 2.796009]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "511 [D loss: 0.000017, acc.: 100.00%] [G loss: 2.455249]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "512 [D loss: 0.000083, acc.: 100.00%] [G loss: 2.410691]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "513 [D loss: 0.000012, acc.: 100.00%] [G loss: 2.330791]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "514 [D loss: 0.000006, acc.: 100.00%] [G loss: 2.552381]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "515 [D loss: 0.000015, acc.: 100.00%] [G loss: 2.682499]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "516 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.398271]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "517 [D loss: 0.000031, acc.: 100.00%] [G loss: 2.337513]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "518 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.227345]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "519 [D loss: 0.000016, acc.: 100.00%] [G loss: 2.249710]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "520 [D loss: 0.000020, acc.: 100.00%] [G loss: 2.255381]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "521 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.248787]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "522 [D loss: 0.000034, acc.: 100.00%] [G loss: 2.506990]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "523 [D loss: 0.000007, acc.: 100.00%] [G loss: 2.607820]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "524 [D loss: 0.000022, acc.: 100.00%] [G loss: 2.651552]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "525 [D loss: 0.000009, acc.: 100.00%] [G loss: 2.678905]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "526 [D loss: 0.000012, acc.: 100.00%] [G loss: 2.844199]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "527 [D loss: 0.000011, acc.: 100.00%] [G loss: 2.478903]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "528 [D loss: 0.000011, acc.: 100.00%] [G loss: 2.386969]\n",
      "8/8 [==============================] - 0s 8ms/step\n",
      "529 [D loss: 0.000039, acc.: 100.00%] [G loss: 2.164465]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "530 [D loss: 0.000007, acc.: 100.00%] [G loss: 2.650033]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "531 [D loss: 0.000008, acc.: 100.00%] [G loss: 2.294566]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "532 [D loss: 0.000011, acc.: 100.00%] [G loss: 2.238975]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "533 [D loss: 0.000004, acc.: 100.00%] [G loss: 2.289948]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "534 [D loss: 0.000011, acc.: 100.00%] [G loss: 2.325673]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "535 [D loss: 0.000016, acc.: 100.00%] [G loss: 2.179446]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "536 [D loss: 0.000020, acc.: 100.00%] [G loss: 2.114708]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "537 [D loss: 0.000018, acc.: 100.00%] [G loss: 2.171053]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "538 [D loss: 0.000005, acc.: 100.00%] [G loss: 2.278400]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "539 [D loss: 0.000010, acc.: 100.00%] [G loss: 2.270102]\n",
      "8/8 [==============================] - 0s 9ms/step\n",
      "540 [D loss: 0.000007, acc.: 100.00%] [G loss: 2.112041]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "541 [D loss: 0.000012, acc.: 100.00%] [G loss: 2.240627]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "542 [D loss: 0.000015, acc.: 100.00%] [G loss: 2.043625]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "543 [D loss: 0.000010, acc.: 100.00%] [G loss: 2.017063]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "544 [D loss: 0.000012, acc.: 100.00%] [G loss: 1.932978]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "545 [D loss: 0.000017, acc.: 100.00%] [G loss: 2.182728]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "546 [D loss: 0.000015, acc.: 100.00%] [G loss: 2.083486]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "547 [D loss: 0.000154, acc.: 100.00%] [G loss: 2.165327]\n",
      "8/8 [==============================] - 0s 10ms/step\n",
      "548 [D loss: 0.000014, acc.: 100.00%] [G loss: 1.987510]\n",
      "8/8 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate and train the DCGAN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dcgan \u001b[38;5;241m=\u001b[39m DCGAN(img_rows, img_cols, channels)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 45\u001b[0m, in \u001b[0;36mDCGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     39\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39madd(d_loss_real, d_loss_fake)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Train Generator\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Train the generator (wants discriminator to mistake\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# images as real)\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Plot the progress\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, acc.: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, d_loss[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39md_loss[\u001b[38;5;241m1\u001b[39m], g_loss))\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2383\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[0;32m   2381\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m-> 2383\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logs\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "dcgan = DCGAN(img_rows, img_cols, channels)\n",
    "dcgan.train(epochs=10000, batch_size=256, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z=100):\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.generator = self.build_generator()\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "        self.discriminator.trainable = False\n",
    "        valid = self.discriminator(img)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def train(self, epochs, batch_size=256, save_interval=50):\n",
    "        X_train = X_train_aug\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "                self.save_model()\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 256)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "        model.summary()\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        os.makedirs('generated_mnist', exist_ok=True)\n",
    "        fig.savefig(\"generated_mnist/dcgan_mnist_improved_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "        os.makedirs('saved_model', exist_ok=True)\n",
    "        self.generator.save('saved_model/generator.h5')\n",
    "        self.discriminator.save('saved_model/discriminator.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_64 (Conv2D)          (None, 14, 14, 64)        640       \n",
      "                                                                 \n",
      " leaky_re_lu_37 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 7, 7, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_40 (LeakyReLU)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,555,457\n",
      "Trainable params: 1,553,665\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, 12544)             1266944   \n",
      "                                                                 \n",
      " reshape_8 (Reshape)         (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_16 (UpSamplin  (None, 14, 14, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 14, 14, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_51 (Bat  (None, 14, 14, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_17 (UpSamplin  (None, 28, 28, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_69 (Conv2D)          (None, 28, 28, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_52 (Bat  (None, 28, 28, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_70 (Conv2D)          (None, 28, 28, 128)       295040    \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 28, 28, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv2d_71 (Conv2D)          (None, 28, 28, 1)         1153      \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,745,857\n",
      "Trainable params: 2,744,577\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n",
      "8/8 [==============================] - 1s 23ms/step\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model_26/model_25/sequential_17/batch_normalization_53/FusedBatchNormGradV3' defined at (most recent call last):\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\windows_events.py\", line 316, in run_forever\n      super().run_forever()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4680\\2948311250.py\", line 6, in <module>\n      dcgan.train(epochs=10000, batch_size=256, save_interval=50)\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4680\\1966435465.py\", line 33, in train\n      g_loss = self.combined.train_on_batch(noise, valid)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 2381, in train_on_batch\n      logs = self.train_function(iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model_26/model_25/sequential_17/batch_normalization_53/FusedBatchNormGradV3'\nOOM when allocating tensor with shape[256,128,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_26/model_25/sequential_17/batch_normalization_53/FusedBatchNormGradV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1008970]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate and train the DCGAN\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dcgan \u001b[38;5;241m=\u001b[39m DCGAN(img_rows, img_cols, channels)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 33\u001b[0m, in \u001b[0;36mDCGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     31\u001b[0m d_loss_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch(gen_imgs, fake)\n\u001b[0;32m     32\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39madd(d_loss_real, d_loss_fake)\n\u001b[1;32m---> 33\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, acc.: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, d_loss[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39md_loss[\u001b[38;5;241m1\u001b[39m], g_loss))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m save_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2381\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2379\u001b[0m     )\n\u001b[0;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2381\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2383\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model_26/model_25/sequential_17/batch_normalization_53/FusedBatchNormGradV3' defined at (most recent call last):\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\windows_events.py\", line 316, in run_forever\n      super().run_forever()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4680\\2948311250.py\", line 6, in <module>\n      dcgan.train(epochs=10000, batch_size=256, save_interval=50)\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4680\\1966435465.py\", line 33, in train\n      g_loss = self.combined.train_on_batch(noise, valid)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 2381, in train_on_batch\n      logs = self.train_function(iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"c:\\Users\\user\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model_26/model_25/sequential_17/batch_normalization_53/FusedBatchNormGradV3'\nOOM when allocating tensor with shape[256,128,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_26/model_25/sequential_17/batch_normalization_53/FusedBatchNormGradV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1008970]"
     ]
    }
   ],
   "source": [
    "# Set image dimensions\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "\n",
    "# Instantiate and train the DCGAN\n",
    "dcgan = DCGAN(img_rows, img_cols, channels)\n",
    "dcgan.train(epochs=10000, batch_size=256, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
